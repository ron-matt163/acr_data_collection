[
    {
        "pr_title": "Fix sign-compare warning when compiling with GCC 14",
        "pr_number": 1749,
        "file_name": "src/opentimelineio/stackAlgorithm.cpp",
        "code_diff": "@@ -7,8 +7,6 @@\n#include \"opentimelineio/trackAlgorithm.h\"\n #include \"opentimelineio/transition.h\"\n \n-#include <cstddef>\n-\n namespace opentimelineio { namespace OPENTIMELINEIO_VERSION {\n \n typedef std::map<Track*, std::map<Composable*, TimeRange>> RangeTrackMap;",
        "comments": [
            {
                "comment": "tracks.size() returns a size_t; if you just use size_t, you don't need to include the c header file.",
                "position": null
            }
        ],
        "commit_message": "Use C size_t instead of C++ std::size_t\n\nSigned-off-by: Jean-Christophe Morin <jean_christophe_morin@hotmail.com>",
        "commit_id": "ed4ddb638daefe44523e517ff5a4eff5121f329a",
        "fn_context": ""
    },
    {
        "pr_title": "Added support for pathlib.Path filepath arguments for adapter IO functions",
        "pr_number": 1704,
        "file_name": "src/py-opentimelineio/opentimelineio/adapters/__init__.py",
        "code_diff": "@@ -15,6 +15,7 @@\nimport os\n import itertools\n+import pathlib\n \n from .. import (\n     exceptions,",
        "comments": [
            {
                "comment": "```suggestion\r\n        string_filepath = os.fspath(filepath)\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        string_filepath = os.fspath(filepath)\r\n```",
                "position": null
            }
        ],
        "commit_message": "added support for pathlib.Path filepath arguments for adapter IO functions\n\nSigned-off-by: Tim Lehr <tim.lehr@disneyanimation.com>",
        "commit_id": "e1dd154a8d3713ca2f144b3f2f79472b2a6e12b1",
        "fn_context": ""
    },
    {
        "pr_title": "Added support for pathlib.Path filepath arguments for adapter IO functions",
        "pr_number": 1704,
        "file_name": "src/py-opentimelineio/opentimelineio/adapters/__init__.py",
        "code_diff": "@@ -132,10 +133,15 @@\ndef read_from_file(\n         timeline = read_from_file(\"file_with_no_extension\", \"cmx_3600\")\n     \"\"\"\n \n-    adapter = _from_filepath_or_name(filepath, adapter_name)\n+    # convert pathlib Path objects to simple string\n+    string_filepath = filepath\n+    if isinstance(string_filepath, pathlib.PurePath):\n+        string_filepath = os.fspath(filepath)\n+\n+    adapter = _from_filepath_or_name(string_filepath, adapter_name)\n \n     return adapter.read_from_file(\n-        filepath=filepath,\n+        filepath=string_filepath,\n         media_linker_name=media_linker_name,\n         media_linker_argument_map=media_linker_argument_map,\n         **adapter_argument_map",
        "comments": [
            {
                "comment": "```suggestion\r\n        string_filepath = os.fspath(filepath)\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        string_filepath = os.fspath(filepath)\r\n```",
                "position": null
            }
        ],
        "commit_message": "added support for pathlib.Path filepath arguments for adapter IO functions\n\nSigned-off-by: Tim Lehr <tim.lehr@disneyanimation.com>",
        "commit_id": "e1dd154a8d3713ca2f144b3f2f79472b2a6e12b1",
        "fn_context": "def read_from_file(\n    filepath,\n    adapter_name=None,\n    media_linker_name=media_linker.MediaLinkingPolicy.ForceDefaultLinker,\n    media_linker_argument_map=None,\n    **adapter_argument_map\n):\n    \"\"\"Read filepath using adapter_name.\n\n    If adapter_name is None, try and infer the adapter name from the filepath.\n\n    .. code-block:: python\n       :caption: Example\n\n        timeline = read_from_file(\"example_trailer.otio\")\n        timeline = read_from_file(\"file_with_no_extension\", \"cmx_3600\")\n    \"\"\"\n\n    # convert pathlib Path objects to simple string\n    string_filepath = filepath\n    if isinstance(string_filepath, pathlib.PurePath):\n        string_filepath = os.fspath(filepath)\n\n    adapter = _from_filepath_or_name(string_filepath, adapter_name)\n\n    return adapter.read_from_file(\n        filepath=string_filepath,\n        media_linker_name=media_linker_name,\n        media_linker_argument_map=media_linker_argument_map,\n        **adapter_argument_map\n    )"
    },
    {
        "pr_title": "Added support for pathlib.Path filepath arguments for adapter IO functions",
        "pr_number": 1704,
        "file_name": "tests/test_builtin_adapters.py",
        "code_diff": "@@ -13,6 +13,7 @@\notio_json,\n )\n \n+import pathlib\n import tempfile",
        "comments": [],
        "commit_message": "added support for pathlib.Path filepath arguments for adapter IO functions\n\nSigned-off-by: Tim Lehr <tim.lehr@disneyanimation.com>",
        "commit_id": "e1dd154a8d3713ca2f144b3f2f79472b2a6e12b1"
    },
    {
        "pr_title": "Normalize track lengths with gaps before flatten stacks",
        "pr_number": 1703,
        "file_name": "src/opentimelineio/stackAlgorithm.cpp",
        "code_diff": "@@ -125,7 +125,9 @@\n_flatten_next_item(\n     }\n }\n \n-// make all tracks the same length by adding a gap to the end if they are shorter\n+// add a gap to end of a track if it is shorter then the longest track.\n+// shorter tracks are clones and get added to the tracks_retainer.\n+// a new track will replace the original pointer in the track vector.\n static void\n _normalize_tracks_lengths(std::vector<Track*>& tracks,\n                           TrackRetainerVector& tracks_retainer,",
        "comments": [
            {
                "comment": "In reading this I got confused about the reuse of `track` as both the old one and the new one. Maybe `old_track` and `new_track` could make that more clear?",
                "position": null
            },
            {
                "comment": "Could you add a comment clarifying that this updates the `tracks` vector with a mix of old unchanged tracks and new padded tracks? At first read I was confused by that.",
                "position": null
            },
            {
                "comment": "Also, do you think anyone else might want to call this function for other use cases? I could imagine that some adapters might want to have equal-duration padded tracks. We could leave that for a future PR.",
                "position": null
            },
            {
                "comment": "It could be useful, not sure. I can't think of anywhere atm in the AAF adapter this would be needed. I'm not sure if any other adapters.",
                "position": null
            }
        ],
        "commit_message": "Added more comments about the purpose the track retainer\nRenamed track variables in normalized_tracks_lengths to be more clear.\n\nSigned-off-by: Mark Reid <mindmark@gmail.com>",
        "commit_id": "d5430bd1929c7ae787f8f49e33eea74eb4192182",
        "fn_context": "static void\n_flatten_next_item(\n    RangeTrackMap&             range_track_map,\n    Track*                     flat_track,\n    std::vector<Track*> const& tracks,\n    int                        track_index,\n    optional<TimeRange>        trim_range,\n    ErrorStatus*               error_status)\n{\n    if (track_index < 0)\n    {\n        track_index = int(tracks.size()) - 1;\n    }\n\n    if (track_index < 0)\n    {\n        return;\n    }\n\n    Track* track = tracks[track_index];\n\n    SerializableObject::Retainer<Track> track_retainer;\n    if (trim_range)\n    {\n        track = track_trimmed_to_range(track, *trim_range, error_status);\n        if (track == nullptr || is_error(error_status))\n        {\n            return;\n        }\n        track_retainer = SerializableObject::Retainer<Track>(track);\n    }\n\n    std::map<Composable*, TimeRange>* track_map;\n    auto                              it = range_track_map.find(track);\n    if (it != range_track_map.end())\n    {\n        track_map = &it->second;\n    }\n    else\n    {\n        auto result = range_track_map.emplace(\n            track,\n            track->range_of_all_children(error_status));\n        if (is_error(error_status))\n        {\n            return;\n        }\n        track_map = &result.first->second;\n    }\n    for (auto child: track->children())\n    {\n        auto item = dynamic_retainer_cast<Item>(child);\n        if (!item)\n        {\n            if (!dynamic_retainer_cast<Transition>(child))\n            {\n                if (error_status)\n                {\n                    *error_status = ErrorStatus(\n                        ErrorStatus::TYPE_MISMATCH,\n                        \"expected item of type Item* || Transition*\",\n                        child);\n                }\n                return;\n            }\n        }\n\n        if (!item || item->visible() || track_index == 0)\n        {\n            flat_track->insert_child(\n                static_cast<int>(flat_track->children().size()),\n                static_cast<Composable*>(child->clone(error_status)),\n                error_status);\n            if (is_error(error_status))\n            {\n                return;\n            }\n        }\n        else\n        {\n            TimeRange trim = (*track_map)[item];\n            if (trim_range)\n            {\n                trim = TimeRange(\n                    trim.start_time() + trim_range->start_time(),\n                    trim.duration());\n                (*track_map)[item] = trim;\n            }\n\n            _flatten_next_item(\n                range_track_map,\n                flat_track,\n                tracks,\n                track_index - 1,\n                trim,\n                error_status);\n            if (track == nullptr || is_error(error_status))\n            {\n                return;\n            }\n        }\n    }\n\n    // range_track_map persists over the entire duration of flatten_stack\n    // track_retainer.value is about to be deleted; it's entirely possible\n    // that a new item will be created at the same pointer location, so we\n    // have to clean this value out of the map now.\n    if (track_retainer)\n    {\n        range_track_map.erase(track_retainer);\n    }\n}\n\nstatic void\n_normalize_tracks_lengths(std::vector<Track*>& tracks,\n                          TrackRetainerVector& tracks_retainer,\n                          ErrorStatus*         error_status)\n{\n    RationalTime duration;\n    for (auto track: tracks)\n    {\n        duration = std::max(duration, track->duration(error_status));\n        if (is_error(error_status))\n        {\n            return;\n        }\n    }\n\n    for(int i = 0; i < tracks.size(); i++)\n    {\n        Track *old_track = tracks[i];\n        RationalTime track_duration = old_track->duration(error_status);\n        if (track_duration < duration)\n        {\n            Track *new_track = static_cast<Track*>(old_track->clone(error_status));\n            if (is_error(error_status))\n            {\n                return;\n            }\n            // add track to retainer so it can be freed later\n            tracks_retainer.push_back(SerializableObject::Retainer<Track>(new_track));\n            new_track->append_child(new Gap(duration - track_duration), error_status);\n            if (is_error(error_status))\n            {\n                return;\n            }\n            tracks[i] = new_track;\n        }\n    }\n}"
    },
    {
        "pr_title": "Normalize track lengths with gaps before flatten stacks",
        "pr_number": 1703,
        "file_name": "src/opentimelineio/stackAlgorithm.cpp",
        "code_diff": "@@ -143,22 +145,23 @@\n_normalize_tracks_lengths(std::vector<Track*>& tracks,\n \n     for(int i = 0; i < tracks.size(); i++)\n     {\n-        Track *track = tracks[i];\n-        RationalTime track_duration = track->duration(error_status);\n+        Track *old_track = tracks[i];\n+        RationalTime track_duration = old_track->duration(error_status);\n         if (track_duration < duration)\n         {\n-            track = static_cast<Track*>(track->clone(error_status));\n+            Track *new_track = static_cast<Track*>(old_track->clone(error_status));\n             if (is_error(error_status))\n             {\n                 return;\n             }\n-            tracks_retainer.push_back(SerializableObject::Retainer<Track>(track));\n-            track->append_child(new Gap(duration - track_duration), error_status);\n+            // add track to retainer so it can be freed later\n+            tracks_retainer.push_back(SerializableObject::Retainer<Track>(new_track));\n+            new_track->append_child(new Gap(duration - track_duration), error_status);\n             if (is_error(error_status))\n             {\n                 return;\n             }\n-            tracks[i] = track;\n+            tracks[i] = new_track;\n         }\n     }\n }",
        "comments": [
            {
                "comment": "In reading this I got confused about the reuse of `track` as both the old one and the new one. Maybe `old_track` and `new_track` could make that more clear?",
                "position": null
            },
            {
                "comment": "Could you add a comment clarifying that this updates the `tracks` vector with a mix of old unchanged tracks and new padded tracks? At first read I was confused by that.",
                "position": null
            },
            {
                "comment": "Also, do you think anyone else might want to call this function for other use cases? I could imagine that some adapters might want to have equal-duration padded tracks. We could leave that for a future PR.",
                "position": null
            },
            {
                "comment": "It could be useful, not sure. I can't think of anywhere atm in the AAF adapter this would be needed. I'm not sure if any other adapters.",
                "position": null
            }
        ],
        "commit_message": "Added more comments about the purpose the track retainer\nRenamed track variables in normalized_tracks_lengths to be more clear.\n\nSigned-off-by: Mark Reid <mindmark@gmail.com>",
        "commit_id": "d5430bd1929c7ae787f8f49e33eea74eb4192182",
        "fn_context": "static void\n_normalize_tracks_lengths(std::vector<Track*>& tracks,\n                          TrackRetainerVector& tracks_retainer,\n                          ErrorStatus*         error_status)\n{\n    RationalTime duration;\n    for (auto track: tracks)\n    {\n        duration = std::max(duration, track->duration(error_status));\n        if (is_error(error_status))\n        {\n            return;\n        }\n    }\n\n    for(int i = 0; i < tracks.size(); i++)\n    {\n        Track *old_track = tracks[i];\n        RationalTime track_duration = old_track->duration(error_status);\n        if (track_duration < duration)\n        {\n            Track *new_track = static_cast<Track*>(old_track->clone(error_status));\n            if (is_error(error_status))\n            {\n                return;\n            }\n            // add track to retainer so it can be freed later\n            tracks_retainer.push_back(SerializableObject::Retainer<Track>(new_track));\n            new_track->append_child(new Gap(duration - track_duration), error_status);\n            if (is_error(error_status))\n            {\n                return;\n            }\n            tracks[i] = new_track;\n        }\n    }\n}\n\nTrack*\nflatten_stack(Stack* in_stack, ErrorStatus* error_status)\n{\n    std::vector<Track*> tracks;\n    // tracks are cloned if they need to be normalized\n    // they get added to this retainer so they can be\n    // freed when the algorithm is complete\n    TrackRetainerVector tracks_retainer;\n    tracks.reserve(in_stack->children().size());\n\n    for (auto c: in_stack->children())\n    {\n        if (auto track = dynamic_retainer_cast<Track>(c))\n        {\n            if (track->enabled())\n            {\n                tracks.push_back(track);\n            }\n        }\n        else\n        {\n            if (error_status)\n            {\n                *error_status = ErrorStatus(\n                    ErrorStatus::TYPE_MISMATCH,\n                    \"expected item of type Track*\",\n                    c);\n            }\n            return nullptr;\n        }\n    }\n\n    _normalize_tracks_lengths(tracks, tracks_retainer, error_status);\n    if (is_error(error_status))\n    {\n        return nullptr;\n    }\n\n    Track* flat_track = new Track;\n    flat_track->set_name(\"Flattened\");\n\n    RangeTrackMap range_track_map;\n    _flatten_next_item(\n        range_track_map,\n        flat_track,\n        tracks,\n        -1,\n        nullopt,\n        error_status);\n    return flat_track;\n}"
    },
    {
        "pr_title": "Normalize track lengths with gaps before flatten stacks",
        "pr_number": 1703,
        "file_name": "src/opentimelineio/stackAlgorithm.cpp",
        "code_diff": "@@ -167,6 +170,9 @@\nTrack*\n flatten_stack(Stack* in_stack, ErrorStatus* error_status)\n {\n     std::vector<Track*> tracks;\n+    // tracks are cloned if they need to be normalized\n+    // they get added to this retainer so they can be\n+    // freed when the algorithm is complete\n     TrackRetainerVector tracks_retainer;\n     tracks.reserve(in_stack->children().size());",
        "comments": [
            {
                "comment": "In reading this I got confused about the reuse of `track` as both the old one and the new one. Maybe `old_track` and `new_track` could make that more clear?",
                "position": null
            },
            {
                "comment": "Could you add a comment clarifying that this updates the `tracks` vector with a mix of old unchanged tracks and new padded tracks? At first read I was confused by that.",
                "position": null
            },
            {
                "comment": "Also, do you think anyone else might want to call this function for other use cases? I could imagine that some adapters might want to have equal-duration padded tracks. We could leave that for a future PR.",
                "position": null
            },
            {
                "comment": "It could be useful, not sure. I can't think of anywhere atm in the AAF adapter this would be needed. I'm not sure if any other adapters.",
                "position": null
            }
        ],
        "commit_message": "Added more comments about the purpose the track retainer\nRenamed track variables in normalized_tracks_lengths to be more clear.\n\nSigned-off-by: Mark Reid <mindmark@gmail.com>",
        "commit_id": "d5430bd1929c7ae787f8f49e33eea74eb4192182",
        "fn_context": "Track*\nflatten_stack(Stack* in_stack, ErrorStatus* error_status)\n{\n    std::vector<Track*> tracks;\n    // tracks are cloned if they need to be normalized\n    // they get added to this retainer so they can be\n    // freed when the algorithm is complete\n    TrackRetainerVector tracks_retainer;\n    tracks.reserve(in_stack->children().size());\n\n    for (auto c: in_stack->children())\n    {\n        if (auto track = dynamic_retainer_cast<Track>(c))\n        {\n            if (track->enabled())\n            {\n                tracks.push_back(track);\n            }\n        }\n        else\n        {\n            if (error_status)\n            {\n                *error_status = ErrorStatus(\n                    ErrorStatus::TYPE_MISMATCH,\n                    \"expected item of type Track*\",\n                    c);\n            }\n            return nullptr;\n        }\n    }\n\n    _normalize_tracks_lengths(tracks, tracks_retainer, error_status);\n    if (is_error(error_status))\n    {\n        return nullptr;\n    }\n\n    Track* flat_track = new Track;\n    flat_track->set_name(\"Flattened\");\n\n    RangeTrackMap range_track_map;\n    _flatten_next_item(\n        range_track_map,\n        flat_track,\n        tracks,\n        -1,\n        nullopt,\n        error_status);\n    return flat_track;\n}"
    },
    {
        "pr_title": "Adds support to `filepath_from_url` to support non-encoded URLs",
        "pr_number": 1664,
        "file_name": "src/py-opentimelineio/opentimelineio/url_utils.py",
        "code_diff": "@@ -9,8 +9,10 @@\nparse as urlparse,\n     request\n )\n-import pathlib\n-from pathlib import Path, PureWindowsPath\n+from pathlib import (\n+    Path,\n+    PureWindowsPath\n+)\n \n \n def url_from_filepath(fpath):",
        "comments": [
            {
                "comment": "@apetrynet, it occurs to me that since this is an OS dependent check it will cause the unit tests I made to fail. Any suggestions on how to test Windows paths properly in OTIO's CI/CD system? I could make a regex that looks for a something like `/S:/` before omitting the leading slash but I'm not sure if a colon is ever used in a Unix path since I do not typically work in those operating systems.",
                "position": null
            },
            {
                "comment": "Nvm, I looked into it further and colons are not used in Unix paths so a regex seemed like the best approach for testing locally can and in CI/CD",
                "position": null
            },
            {
                "comment": "Here is a suggestion of an alternative way to do this.\r\nIn this approach I use pathlib to detect, handle and convert Windows paths to posix if needed.\r\nWe default to the original behavior of returning the parsed result.\r\n\r\nPlease note this requires `from pathlib import Path, PureWindowsPath`\r\nPlease also note that this is tested on Linux only.\r\n\r\n```suggestion\r\n    filepath = Path(request.url2pathname(parsed_result.path))\r\n    \r\n    # Check for window drive\r\n    if PureWindowsPath(parsed_result.netloc).drive:\r\n        filepath = Path(parsed_result.netloc + parsed_result.path)\r\n\r\n    elif PureWindowsPath(filepath.parts[1]).drive:\r\n        # Remove leading \"/\" if/when `request.url2pathname` yields \"/S:/path/file.ext\"\r\n        filepath = filepath.relative_to(filepath.root)\r\n\r\n    # This should convert \"\\\" to \"/\" if neeeded \r\n    return filepath.as_posix()\r\n```\r\n",
                "position": null
            },
            {
                "comment": "Oh wow, I really like this suggestion! Thank you for the suggestion!\r\n\r\nI don't know the `pathlib` library well but I had to look a few things up to understand what is happening. I added a few comments to your suggestion to explain to future devs.",
                "position": null
            },
            {
                "comment": "```suggestion\r\n```\r\n\r\nThe unused \"re\" import stopped the CI.",
                "position": null
            },
            {
                "comment": "Super nitpick request for consistency.\r\nCould you please remove the `import pathlib` statement and use \"Path\" directly in the `url_from_filepath()` function?",
                "position": null
            },
            {
                "comment": "Sure can do! Had no idea the CI was so thorough!",
                "position": null
            },
            {
                "comment": "Sure, can do!",
                "position": null
            }
        ],
        "commit_message": "Updated imports per PR feedback\n\nSigned-off-by: Doug Halley <douglascomet@gmail.com>",
        "commit_id": "e5947131caed2876b9cd9a1270951d9e76bca7aa",
        "fn_context": "def url_from_filepath(fpath):\n    \"\"\"Convert a filesystem path to an url in a portable way using / path sep\"\"\"\n\n    try:\n        # appears to handle absolute windows paths better, which are absolute\n        # and start with a drive letter.\n        return urlparse.unquote(Path(fpath).as_uri())\n    except ValueError:\n        # scheme is \"file\" for absolute paths, else \"\"\n        scheme = \"file\" if os.path.isabs(fpath) else \"\"\n\n        # handles relative paths\n        return urlparse.urlunparse(\n            urlparse.ParseResult(\n                scheme=scheme,\n                path=fpath,\n                netloc=\"\",\n                params=\"\",\n                query=\"\",\n                fragment=\"\"\n            )\n        )"
    },
    {
        "pr_title": "Make serialize_json_to_string output compact when indent is less than 1",
        "pr_number": 1578,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -1232,10 +1232,7 @@\nserialize_json_to_string_pretty(\n         OTIO_rapidjson::kWriteNanAndInfFlag>\n         json_writer(output_string_buffer);\n \n-    if (indent >= 0)\n-    {\n-        json_writer.SetIndent(' ', indent);\n-    }\n+    json_writer.SetIndent(' ', indent);\n \n     JSONEncoder<decltype(json_writer)> json_encoder(json_writer);",
        "comments": [
            {
                "comment": "in the compact form, is `indent` the number of spaces between things?  or is that unused?",
                "position": null
            },
            {
                "comment": "It's unused. I forgot to remove it.",
                "position": null
            }
        ],
        "commit_message": "Remove unused indent param in serialize_json_to_string_pretty and remove useless condition in serialize_json_to_string_pretty\n\nSigned-off-by: Jean-Christophe Morin <jean_christophe_morin@hotmail.com>",
        "commit_id": "5520f1cfb12771e1fcc92c514c8308f66ed429e2",
        "fn_context": "std::string\nserialize_json_to_string_pretty(\n    const any&                value,\n    const schema_version_map* schema_version_targets,\n    ErrorStatus*              error_status,\n    int                       indent)\n{\n    OTIO_rapidjson::StringBuffer output_string_buffer;\n\n    OTIO_rapidjson::PrettyWriter<\n        decltype(output_string_buffer),\n        OTIO_rapidjson::UTF8<>,\n        OTIO_rapidjson::UTF8<>,\n        OTIO_rapidjson::CrtAllocator,\n        OTIO_rapidjson::kWriteNanAndInfFlag>\n        json_writer(output_string_buffer);\n\n    json_writer.SetIndent(' ', indent);\n\n    JSONEncoder<decltype(json_writer)> json_encoder(json_writer);\n\n    if (!SerializableObject::Writer::write_root(\n            value,\n            json_encoder,\n            schema_version_targets,\n            error_status))\n    {\n        return std::string();\n    }\n\n    return std::string(output_string_buffer.GetString());\n}"
    },
    {
        "pr_title": "Make serialize_json_to_string output compact when indent is less than 1",
        "pr_number": 1578,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -1256,8 +1253,7 @@\nstd::string\n serialize_json_to_string_compact(\n     const any&                value,\n     const schema_version_map* schema_version_targets,\n-    ErrorStatus*              error_status,\n-    int                       indent)\n+    ErrorStatus*              error_status)\n {\n     OTIO_rapidjson::StringBuffer output_string_buffer;",
        "comments": [
            {
                "comment": "in the compact form, is `indent` the number of spaces between things?  or is that unused?",
                "position": null
            },
            {
                "comment": "It's unused. I forgot to remove it.",
                "position": null
            }
        ],
        "commit_message": "Remove unused indent param in serialize_json_to_string_pretty and remove useless condition in serialize_json_to_string_pretty\n\nSigned-off-by: Jean-Christophe Morin <jean_christophe_morin@hotmail.com>",
        "commit_id": "5520f1cfb12771e1fcc92c514c8308f66ed429e2",
        "fn_context": "std::string\nserialize_json_to_string_compact(\n    const any&                value,\n    const schema_version_map* schema_version_targets,\n    ErrorStatus*              error_status)\n{\n    OTIO_rapidjson::StringBuffer output_string_buffer;\n\n    OTIO_rapidjson::Writer<\n        decltype(output_string_buffer),\n        OTIO_rapidjson::UTF8<>,\n        OTIO_rapidjson::UTF8<>,\n        OTIO_rapidjson::CrtAllocator,\n        OTIO_rapidjson::kWriteNanAndInfFlag>\n        json_writer(output_string_buffer);\n\n    JSONEncoder<decltype(json_writer)> json_encoder(json_writer);\n\n    if (!SerializableObject::Writer::write_root(\n            value,\n            json_encoder,\n            schema_version_targets,\n            error_status))\n    {\n        return std::string();\n    }\n\n    return std::string(output_string_buffer.GetString());\n}"
    },
    {
        "pr_title": "Fix exception when __doc__ is None with portable Python",
        "pr_number": 1550,
        "file_name": "src/py-opentimelineio/opentimelineio/core/_core_utils.py",
        "code_diff": "@@ -177,7 +177,7 @@\ndef __deepcopy__(self, memo):\n \n                 # Hide the method frm Sphinx doc.\n                 # See https://www.sphinx-doc.org/en/master/usage/restructuredtext/domains.html#info-field-lists  # noqa\n-                # __doc__ will be None on an embedded python interpreter where the built-in modules are provided  # noqa\n+                # __doc__ will be None on a Windows embeddable package where the built-in modules are provided  # noqa\n                 # as pyc files which do not include the docstrings.\n                 if getattr(mapClass, name).__doc__ is not None:\n                     getattr(mapClass, name).__doc__ += \"\\n\\n:meta private:\"",
        "comments": [
            {
                "comment": "```suggestion\r\n                # Add comment here to explain why we need to check for None\r\n                if getattr(mapClass, name).__doc__ is not None:\r\n                    getattr(mapClass, name).__doc__ += \"\\n\\n:meta private:\"\r\n```\r\nI think it would be clearer if we checked for `None` specifically. Also a little comment would be great so that we don't forget why sometimes it's None.",
                "position": null
            },
            {
                "comment": "You're right! Sphinx won't ever be run to generate documentation on an embedded python, so there is no need to add the tag if \\_\\_doc\\_\\_ is None.",
                "position": null
            },
            {
                "comment": "Last nitpick I promise: Could you change \"embedded python interpreter\" with \"Windows embeddable package\" please? Embedded in this case has multiple meaning (for example it could be a Pybind11 embedded interpreter, etc). \"Windows embeddable package\" will make it clear that it's really the Windows \"portable\" install.",
                "position": null
            },
            {
                "comment": "All good! :) I'll make the change",
                "position": null
            }
        ],
        "commit_message": "Reworded comment\n\nSigned-off-by: Rob Szabo <robbiesz79@gmail.com>",
        "commit_id": "dfa33aa8ba784bea3db5ae1b29826cd740281b24",
        "fn_context": "def _add_mutable_mapping_methods(mapClass):\n    def __setitem__(self, key, item):\n        self.__internal_setitem__(key, _value_to_any(item))\n\n    def __str__(self):\n        return str(dict(self))\n\n    def __repr__(self):\n        return repr(dict(self))\n\n    def setdefault(self, key, default_value):\n        if key in self:\n            return self[key]\n        else:\n            self[key] = default_value\n            return self[key]\n\n    def pop(self, key, default=_marker_):\n        try:\n            value = self[key]\n        except KeyError:\n            if default is _marker_:\n                raise\n            return default\n        else:\n            del self[key]\n            return value\n\n    def __copy__(self):\n        m = mapClass()\n        m.update({k: v for (k, v) in self.items()})\n        return m\n\n    def __deepcopy__(self, memo):\n        m = mapClass()\n        m.update({k: copy.deepcopy(v, memo) for (k, v) in self.items()})\n        return m\n\n    collections.abc.MutableMapping.register(mapClass)\n    mapClass.__setitem__ = __setitem__\n    mapClass.__str__ = __str__\n    mapClass.__repr__ = __repr__\n\n    seen = set()\n    for klass in (collections.abc.MutableMapping, collections.abc.Mapping):\n        for name in klass.__dict__.keys():\n            if name in seen:\n                continue\n\n            seen.add(name)\n            func = getattr(klass, name)\n            if (\n                    isinstance(func, types.FunctionType)\n                    and name not in klass.__abstractmethods__\n            ):\n                setattr(mapClass, name, func)\n                if name.startswith('__') or name.endswith('__'):  # noqa\n                    continue\n\n                # Hide the method frm Sphinx doc.\n                # See https://www.sphinx-doc.org/en/master/usage/restructuredtext/domains.html#info-field-lists  # noqa\n                # __doc__ will be None on a Windows embeddable package where the built-in modules are provided  # noqa\n                # as pyc files which do not include the docstrings.\n                if getattr(mapClass, name).__doc__ is not None:\n                    getattr(mapClass, name).__doc__ += \"\\n\\n:meta private:\"\n\n    mapClass.setdefault = setdefault\n    mapClass.pop = pop\n    mapClass.__copy__ = __copy__\n    mapClass.__deepcopy__ = __deepcopy__"
    },
    {
        "pr_title": "Add support for editable installs",
        "pr_number": 1460,
        "file_name": "setup.py",
        "code_diff": "@@ -55,12 +55,17 @@\ndef initialize_options(self):\n \n     def run(self):\n         self.announce('running OTIO build_ext', level=2)\n+        # Let the original build_ext class do its job.\n+        # This is rather important because build_ext.run takes care of a\n+        # couple of things, one of which is to copy the built files into\n+        # the source tree (in src/py-opentimelineio/opentimelineio)\n+        # when building in editable mode.\n         super().run()\n \n     def build_extension(self, _ext: Extension):\n         # This works around the fact that we build _opentime and _otio\n-        # extensions as a one-shot cmake invocation. Usually we'd build each\n-        # separately using build_extension.\n+        # extensions as a one-shot cmake invocation. Setuptools calls\n+        # build_extension for each Extension registered in the setup function.\n         if not self.built:\n             self.build()\n             self.built = True",
        "comments": [
            {
                "comment": "`self.editable_mode` will be set to True when `-e` is given to the `pip install` command. See https://setuptools.pypa.io/en/latest/userguide/extension.html#setuptools.command.build.SubCommand.editable_mode.",
                "position": 57
            }
        ],
        "commit_message": "Add comments\n\nSigned-off-by: Jean-Christophe Morin <jean_christophe_morin@hotmail.com>",
        "commit_id": "ad62152a9c0b6f3c9c5efafd7d134b88fe72d202",
        "fn_context": "    def run(self):\n        self.announce('running OTIO build_ext', level=2)\n        # Let the original build_ext class do its job.\n        # This is rather important because build_ext.run takes care of a\n        # couple of things, one of which is to copy the built files into\n        # the source tree (in src/py-opentimelineio/opentimelineio)\n        # when building in editable mode.\n        super().run()\n\n    def build_extension(self, _ext: Extension):\n        # This works around the fact that we build _opentime and _otio\n        # extensions as a one-shot cmake invocation. Setuptools calls\n        # build_extension for each Extension registered in the setup function.\n        if not self.built:\n            self.build()\n            self.built = True\n\n    def build(self):\n        self.build_temp_dir = (\n            os.environ.get(\"OTIO_CXX_BUILD_TMP_DIR\")\n            or os.path.abspath(self.build_temp)\n        )\n\n        if not os.path.exists(self.build_temp_dir):\n            os.makedirs(self.build_temp_dir)\n\n        debug = (self.debug or bool(os.environ.get(\"OTIO_CXX_DEBUG_BUILD\")))\n        self.build_config = ('Debug' if debug else 'Release')\n\n        self.cmake_preflight_check()\n        self.cmake_generate()\n        self.cmake_install()"
    },
    {
        "pr_title": "Add support for editable installs",
        "pr_number": 1460,
        "file_name": "setup.py",
        "code_diff": "@@ -263,6 +268,8 @@\ndef run(self):\n             # Only run when not in dry-mode (a dry run should not have any side effect)\n             # and in non-editable mode. We don't want to edit files when in editable\n             # mode because that could lead to modifications to the source files.\n+            # Note that setuptools will set self.editable_mode to True\n+            # when \"pip install -e .\" is run.\n             _append_version_info_to_init_scripts(self.build_lib)",
        "comments": [],
        "commit_message": "Add comments\n\nSigned-off-by: Jean-Christophe Morin <jean_christophe_morin@hotmail.com>",
        "commit_id": "ad62152a9c0b6f3c9c5efafd7d134b88fe72d202"
    },
    {
        "pr_title": "Remove deprecated each_child and each_clip functions",
        "pr_number": 1437,
        "file_name": "contrib/opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -242,7 +242,7 @@\ndef test_aaf_read(self):\n \n         self.assertEqual(len(timeline.audio_tracks()), 2)\n \n-        clips = list(video_track.find_clips())\n+        clips = video_track.find_clips()\n \n         self.assertEqual(\n             [",
        "comments": [
            {
                "comment": "I suspect the `list(...)` wrapper here is redundant now that `find_clips()` returns a list.",
                "position": null
            },
            {
                "comment": "Good catch, I also removed a couple of other redundant ```list()``` calls.",
                "position": null
            }
        ],
        "commit_message": "Remove redundant list() calls\n\nSigned-off-by: Darby Johnston <darbyjohnston@yahoo.com>",
        "commit_id": "2771a6da2949c95b9b9ccb2acf6e8fa09c410a5b",
        "fn_context": "    def test_aaf_read(self):\n        aaf_path = SIMPLE_EXAMPLE_PATH\n        timeline = otio.adapters.read_from_file(aaf_path)\n        self.assertEqual(timeline.name, \"OTIO TEST 1.Exported.01\")\n        fps = timeline.duration().rate\n        self.assertEqual(fps, 24.0)\n        self.assertEqual(\n            timeline.duration(),\n            otio.opentime.from_timecode(\"00:02:16:18\", fps)\n        )\n\n        self.assertEqual(len(timeline.tracks), 3)\n\n        self.assertEqual(len(timeline.video_tracks()), 1)\n        video_track = timeline.video_tracks()[0]\n        self.assertEqual(len(video_track), 5)\n\n        self.assertEqual(len(timeline.audio_tracks()), 2)\n\n        clips = video_track.find_clips()\n\n        self.assertEqual(\n            [\n                \"tech.fux (loop)-HD.mp4\",\n                \"t-hawk (loop)-HD.mp4\",\n                \"out-b (loop)-HD.mp4\",\n                \"KOLL-HD.mp4\",\n                \"brokchrd (loop)-HD.mp4\"\n            ],\n            [clip.name for clip in clips]\n        )\n        self.maxDiff = None\n        self.assertEqual(\n            [clip.source_range for clip in clips],\n            [\n                otio.opentime.TimeRange(\n                    otio.opentime.from_timecode(\"01:00:00:00\", fps),\n                    otio.opentime.from_timecode(\"00:00:30:00\", fps)\n                ),\n                otio.opentime.TimeRange(\n                    otio.opentime.from_timecode(\"01:00:00:00\", fps),\n                    otio.opentime.from_timecode(\"00:00:20:00\", fps)\n                ),\n                otio.opentime.TimeRange(\n                    otio.opentime.from_timecode(\"01:00:00:00\", fps),\n                    otio.opentime.from_timecode(\"00:00:30:02\", fps)\n                ),\n                otio.opentime.TimeRange(\n                    otio.opentime.from_timecode(\"01:00:00:00\", fps),\n                    otio.opentime.from_timecode(\"00:00:26:16\", fps)\n                ),\n                otio.opentime.TimeRange(\n                    otio.opentime.from_timecode(\"01:00:00:00\", fps),\n                    otio.opentime.from_timecode(\"00:00:30:00\", fps)\n                )\n            ]\n        )"
    },
    {
        "pr_title": "Remove deprecated each_child and each_clip functions",
        "pr_number": 1437,
        "file_name": "contrib/opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -322,7 +322,7 @@\ndef test_aaf_read_trims(self):\n             ]\n         )\n \n-        clips = list(video_track.find_clips())\n+        clips = video_track.find_clips()\n \n         self.assertEqual(\n             [item.name for item in video_track],",
        "comments": [
            {
                "comment": "I suspect the `list(...)` wrapper here is redundant now that `find_clips()` returns a list.",
                "position": null
            },
            {
                "comment": "Good catch, I also removed a couple of other redundant ```list()``` calls.",
                "position": null
            }
        ],
        "commit_message": "Remove redundant list() calls\n\nSigned-off-by: Darby Johnston <darbyjohnston@yahoo.com>",
        "commit_id": "2771a6da2949c95b9b9ccb2acf6e8fa09c410a5b",
        "fn_context": "    def test_aaf_read_trims(self):\n        aaf_path = TRIMS_EXAMPLE_PATH\n        timeline = otio.adapters.read_from_file(aaf_path)\n        self.assertEqual(\n            timeline.name,\n            \"OTIO TEST 1.Exported.01 - trims.Exported.02\"\n        )\n        fps = timeline.duration().rate\n        self.assertEqual(fps, 24.0)\n\n        video_tracks = timeline.video_tracks()\n        self.assertEqual(len(video_tracks), 1)\n        video_track = video_tracks[0]\n        self.assertEqual(len(video_track), 6)\n\n        self.assertEqual(\n            [type(item) for item in video_track],\n            [\n                otio.schema.Clip,\n                otio.schema.Clip,\n                otio.schema.Clip,\n                otio.schema.Clip,\n                otio.schema.Gap,\n                otio.schema.Clip,\n            ]\n        )\n\n        clips = video_track.find_clips()\n\n        self.assertEqual(\n            [item.name for item in video_track],\n            [\n                \"tech.fux (loop)-HD.mp4\",\n                \"t-hawk (loop)-HD.mp4\",\n                \"out-b (loop)-HD.mp4\",\n                \"KOLL-HD.mp4\",\n                \"Filler\",   # Gap\n                \"brokchrd (loop)-HD.mp4\"\n            ]\n        )\n\n        self.maxDiff = None\n        desired_ranges = [\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(86400, fps),\n                otio.opentime.from_frames(720 - 0, fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(86400 + 121, fps),\n                otio.opentime.from_frames(480 - 121, fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(86400 + 123, fps),\n                otio.opentime.from_frames(523 - 123, fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(0, fps),\n                otio.opentime.from_frames(559 - 0, fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(86400 + 69, fps),\n                otio.opentime.from_frames(720 - 69, fps)\n            )\n        ]\n        for clip, desired in zip(clips, desired_ranges):\n            actual = clip.source_range\n            self.assertEqual(\n                actual,\n                desired,\n                \"clip '{}' source_range should be {} not {}\".format(\n                    clip.name,\n                    desired,\n                    actual\n                )\n            )\n\n        desired_ranges = [\n            otio.opentime.TimeRange(\n                otio.opentime.from_timecode(\"00:00:00:00\", fps),\n                otio.opentime.from_timecode(\"00:00:30:00\", fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_timecode(\"00:00:30:00\", fps),\n                otio.opentime.from_timecode(\"00:00:14:23\", fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_timecode(\"00:00:44:23\", fps),\n                otio.opentime.from_timecode(\"00:00:16:16\", fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_timecode(\"00:01:01:15\", fps),\n                otio.opentime.from_timecode(\"00:00:23:07\", fps)\n            ),\n            otio.opentime.TimeRange(    # Gap\n                otio.opentime.from_timecode(\"00:01:24:22\", fps),\n                otio.opentime.from_timecode(\"00:00:04:12\", fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_timecode(\"00:01:29:10\", fps),\n                otio.opentime.from_timecode(\"00:00:27:03\", fps)\n            )\n        ]\n        for item, desired in zip(video_track, desired_ranges):\n            actual = item.trimmed_range_in_parent()\n            self.assertEqual(\n                actual,\n                desired,\n                \"item '{}' trimmed_range_in_parent should be {} not {}\".format(\n                    clip.name,\n                    desired,\n                    actual\n                )\n            )\n\n        self.assertEqual(\n            timeline.duration(),\n            otio.opentime.from_timecode(\"00:01:56:13\", fps)\n        )"
    },
    {
        "pr_title": "Remove deprecated each_child and each_clip functions",
        "pr_number": 1437,
        "file_name": "contrib/opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -426,7 +426,7 @@\ndef test_aaf_read_transitions(self):\n         video_track = video_tracks[0]\n         self.assertEqual(len(video_track), 12)\n \n-        clips = list(video_track.find_clips())\n+        clips = video_track.find_clips()\n         self.assertEqual(len(clips), 4)\n \n         self.assertEqual(",
        "comments": [
            {
                "comment": "I suspect the `list(...)` wrapper here is redundant now that `find_clips()` returns a list.",
                "position": null
            },
            {
                "comment": "Good catch, I also removed a couple of other redundant ```list()``` calls.",
                "position": null
            }
        ],
        "commit_message": "Remove redundant list() calls\n\nSigned-off-by: Darby Johnston <darbyjohnston@yahoo.com>",
        "commit_id": "2771a6da2949c95b9b9ccb2acf6e8fa09c410a5b",
        "fn_context": "    def test_aaf_read_transitions(self):\n        aaf_path = TRANSITIONS_EXAMPLE_PATH\n        timeline = otio.adapters.read_from_file(aaf_path)\n        self.assertEqual(timeline.name, \"OTIO TEST - transitions.Exported.01\")\n        fps = timeline.duration().rate\n        self.assertEqual(fps, 24.0)\n\n        video_tracks = timeline.video_tracks()\n        self.assertEqual(len(video_tracks), 1)\n        video_track = video_tracks[0]\n        self.assertEqual(len(video_track), 12)\n\n        clips = video_track.find_clips()\n        self.assertEqual(len(clips), 4)\n\n        self.assertEqual(\n            [type(item) for item in video_track],\n            [\n                otio.schema.Gap,\n                otio.schema.Transition,\n                otio.schema.Clip,\n                otio.schema.Transition,\n                otio.schema.Clip,\n                otio.schema.Transition,\n                otio.schema.Gap,\n                otio.schema.Transition,\n                otio.schema.Clip,\n                otio.schema.Clip,\n                otio.schema.Transition,\n                otio.schema.Gap,\n            ]\n        )\n\n        self.assertEqual(\n            [item.name for item in video_track],\n            [\n                \"Filler\",\n                \"Transition\",\n                \"tech.fux (loop)-HD.mp4\",\n                \"Transition\",\n                \"t-hawk (loop)-HD.mp4\",\n                \"Transition\",\n                \"Filler\",\n                \"Transition\",\n                \"KOLL-HD.mp4\",\n                \"brokchrd (loop)-HD.mp4\",\n                \"Transition\",\n                \"Filler\"\n            ]\n        )\n\n        self.maxDiff = None\n        desired_ranges = [\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(86400 + 0, fps),\n                otio.opentime.from_frames(117, fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(86400 + 123, fps),\n                otio.opentime.from_frames(200 - 123, fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(55, fps),\n                otio.opentime.from_frames(199 - 55, fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(86400 + 0, fps),\n                otio.opentime.from_frames(130, fps)\n            )\n        ]\n        for clip, desired in zip(clips, desired_ranges):\n            actual = clip.source_range\n            self.assertEqual(\n                actual,\n                desired,\n                \"clip '{}' source_range should be {} not {}\".format(\n                    clip.name,\n                    desired,\n                    actual\n                )\n            )\n\n        desired_ranges = [\n            otio.opentime.TimeRange(    # Gap\n                otio.opentime.from_timecode(\"00:00:00:00\", fps),\n                otio.opentime.from_timecode(\"00:00:00:00\", fps)\n            ),\n            otio.opentime.TimeRange(    # Transition\n                otio.opentime.from_timecode(\"00:00:00:00\", fps),\n                otio.opentime.from_timecode(\"00:00:00:12\", fps)\n            ),\n            otio.opentime.TimeRange(    # tech.fux\n                otio.opentime.from_timecode(\"00:00:00:00\", fps),\n                otio.opentime.from_timecode(\"00:00:04:21\", fps)\n            ),\n            otio.opentime.TimeRange(    # Transition\n                otio.opentime.from_timecode(\"00:00:02:21\", fps),\n                otio.opentime.from_timecode(\"00:00:02:00\", fps)\n            ),\n            otio.opentime.TimeRange(    # t-hawk\n                otio.opentime.from_timecode(\"00:00:04:21\", fps),\n                otio.opentime.from_timecode(\"00:00:03:05\", fps)\n            ),\n            otio.opentime.TimeRange(    # Transition\n                otio.opentime.from_timecode(\"00:00:07:14\", fps),\n                otio.opentime.from_timecode(\"00:00:01:00\", fps)\n            ),\n            otio.opentime.TimeRange(    # Gap\n                otio.opentime.from_timecode(\"00:00:08:02\", fps),\n                otio.opentime.from_timecode(\"00:00:02:05\", fps)\n            ),\n            otio.opentime.TimeRange(    # Transition\n                otio.opentime.from_timecode(\"00:00:09:07\", fps),\n                otio.opentime.from_timecode(\"00:00:02:00\", fps)\n            ),\n            otio.opentime.TimeRange(    # KOLL-HD\n                otio.opentime.from_timecode(\"00:00:10:07\", fps),\n                otio.opentime.from_timecode(\"00:00:06:00\", fps)\n            ),\n            otio.opentime.TimeRange(    # brokchrd\n                otio.opentime.from_timecode(\"00:00:16:07\", fps),\n                otio.opentime.from_timecode(\"00:00:05:10\", fps)\n            ),\n            otio.opentime.TimeRange(    # Transition\n                otio.opentime.from_timecode(\"00:00:19:17\", fps),\n                otio.opentime.from_timecode(\"00:00:02:00\", fps)\n            ),\n            otio.opentime.TimeRange(    # Gap\n                otio.opentime.from_timecode(\"00:00:21:17\", fps),\n                otio.opentime.from_timecode(\"00:00:00:00\", fps)\n            )\n        ]\n        for item, desired in zip(video_track, desired_ranges):\n            actual = item.trimmed_range_in_parent()\n            self.assertEqual(\n                desired,\n                actual,\n                \"item '{}' trimmed_range_in_parent should be {} not {}\".format(\n                    clip.name,\n                    desired,\n                    actual\n                )\n            )\n\n        self.assertEqual(\n            timeline.duration(),\n            otio.opentime.from_timecode(\"00:00:21:17\", fps)\n        )"
    },
    {
        "pr_title": "Remove deprecated each_child and each_clip functions",
        "pr_number": 1437,
        "file_name": "contrib/opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -1621,8 +1621,8 @@\ndef _target_url_fixup(timeline):\n     def _verify_first_clip(self, original_timeline, aaf_path):\n         timeline_from_aaf = otio.adapters.read_from_file(aaf_path)\n \n-        original_clips = list(original_timeline.find_clips())\n-        aaf_clips = list(timeline_from_aaf.find_clips())\n+        original_clips = original_timeline.find_clips()\n+        aaf_clips = timeline_from_aaf.find_clips()\n \n         self.assertTrue(len(original_clips) > 0)\n         self.assertEqual(len(aaf_clips), len(original_clips))",
        "comments": [
            {
                "comment": "I suspect the `list(...)` wrapper here is redundant now that `find_clips()` returns a list.",
                "position": null
            },
            {
                "comment": "Good catch, I also removed a couple of other redundant ```list()``` calls.",
                "position": null
            }
        ],
        "commit_message": "Remove redundant list() calls\n\nSigned-off-by: Darby Johnston <darbyjohnston@yahoo.com>",
        "commit_id": "2771a6da2949c95b9b9ccb2acf6e8fa09c410a5b",
        "fn_context": "    def _verify_first_clip(self, original_timeline, aaf_path):\n        timeline_from_aaf = otio.adapters.read_from_file(aaf_path)\n\n        original_clips = original_timeline.find_clips()\n        aaf_clips = timeline_from_aaf.find_clips()\n\n        self.assertTrue(len(original_clips) > 0)\n        self.assertEqual(len(aaf_clips), len(original_clips))\n\n        first_clip_in_original_timeline = original_clips[0]\n        first_clip_in_aaf_timeline = aaf_clips[0]\n\n        # Comparing stuff\n        for prop in ['source_range']:\n            self.assertEqual(getattr(first_clip_in_original_timeline, prop),\n                             getattr(first_clip_in_aaf_timeline, prop),\n                             f\"`{prop}` did not match\")\n\n        for method in ['visible_range', 'trimmed_range']:\n            self.assertEqual(getattr(first_clip_in_original_timeline, method)(),\n                             getattr(first_clip_in_aaf_timeline, method)(),\n                             f\"`{method}` did not match\")"
    },
    {
        "pr_title": "Improve test coverage",
        "pr_number": 1436,
        "file_name": "setup.py",
        "code_diff": "@@ -42,7 +42,7 @@\ndef _debugInstance(x):\n         print(\"{}:     {}\".format(a, getattr(x, a)))\n \n \n-def joinArgs(args):\n+def join_args(args):\n     return ' '.join(map(shlex.quote, args))",
        "comments": [
            {
                "comment": "We're using snake case in python for functions:\r\n```suggestion\r\ndef join_args(args):\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        self.announce(join_args(cmake_args), level=2)\r\n```",
                "position": null
            }
        ],
        "commit_message": "Rename function\n\nSigned-off-by: Jean-Christophe Morin <jean_christophe_morin@hotmail.com>",
        "commit_id": "17036ffaf5542830e0616e4573bfc41510ae4252",
        "fn_context": "def _debugInstance(x):\n    for a in sorted(dir(x)):\n        print(\"{}:     {}\".format(a, getattr(x, a)))\n\ndef join_args(args):\n    return ' '.join(map(shlex.quote, args))"
    },
    {
        "pr_title": "Improve test coverage",
        "pr_number": 1436,
        "file_name": "setup.py",
        "code_diff": "@@ -165,7 +165,7 @@\ndef cmake_generate(self):\n         self.announce('running cmake generation', level=2)\n \n         cmake_args = ['cmake', SOURCE_DIR] + self.generate_cmake_arguments()\n-        self.announce(joinArgs(cmake_args), level=2)\n+        self.announce(join_args(cmake_args), level=2)\n \n         subprocess.check_call(\n             cmake_args,",
        "comments": [
            {
                "comment": "We're using snake case in python for functions:\r\n```suggestion\r\ndef join_args(args):\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        self.announce(join_args(cmake_args), level=2)\r\n```",
                "position": null
            }
        ],
        "commit_message": "Rename function\n\nSigned-off-by: Jean-Christophe Morin <jean_christophe_morin@hotmail.com>",
        "commit_id": "17036ffaf5542830e0616e4573bfc41510ae4252",
        "fn_context": "    def cmake_generate(self):\n        self.announce('running cmake generation', level=2)\n\n        cmake_args = ['cmake', SOURCE_DIR] + self.generate_cmake_arguments()\n        self.announce(join_args(cmake_args), level=2)\n\n        subprocess.check_call(\n            cmake_args,\n            cwd=self.build_temp_dir,\n            env=os.environ.copy()\n        )"
    },
    {
        "pr_title": "Fix for SerializableCollection::children_if",
        "pr_number": 1404,
        "file_name": "tests/test_serializable_collection.py",
        "code_diff": "@@ -81,7 +81,7 @@\ndef test_children_if(self):\n         sc.append(tl)\n         result = sc.children_if(otio.schema.Clip)\n         self.assertEqual(len(result), 1)\n-        self.assertTrue(result[0], cl)\n+        self.assertEqual(result[0], cl)\n \n     def test_children_if_search_range(self):\n         range = otio.opentime.TimeRange(",
        "comments": [
            {
                "comment": "Is this also meant to be `assertEqual`?\r\n```suggestion\r\n        self.assertEqual(result[0], cl0)\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        self.assertEqual(result[0], cl)\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        self.assertEqual(result[0], cl)\r\n```",
                "position": null
            },
            {
                "comment": "Oops, sorry, I forgot the Python tests. Should be fixed now.",
                "position": null
            }
        ],
        "commit_message": "Assert fixes\n\nSigned-off-by: Darby Johnston <darbyjohnston@yahoo.com>",
        "commit_id": "737dc5f33751570c0dd487b266c52473963a1a45",
        "fn_context": "    def test_children_if(self):\n        cl = otio.schema.Clip()\n        tr = otio.schema.Track()\n        tr.append(cl)\n        tl = otio.schema.Timeline()\n        tl.tracks.append(tr)\n        sc = otio.schema.SerializableCollection()\n        sc.append(tl)\n        result = sc.children_if(otio.schema.Clip)\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0], cl)\n\n    def test_children_if_search_range(self):\n        range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(0.0, 24.0),\n            otio.opentime.RationalTime(24.0, 24.0))\n        cl0 = otio.schema.Clip()\n        cl0.source_range = range\n        cl1 = otio.schema.Clip()\n        cl1.source_range = range\n        cl2 = otio.schema.Clip()\n        cl2.source_range = range\n        tr = otio.schema.Track()\n        tr.append(cl0)\n        tr.append(cl1)\n        tr.append(cl2)\n        tl = otio.schema.Timeline()\n        tl.tracks.append(tr)\n        sc = otio.schema.SerializableCollection()\n        sc.append(tl)\n        result = sc.children_if(otio.schema.Clip, range)\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0], cl0)"
    },
    {
        "pr_title": "Fix for SerializableCollection::children_if",
        "pr_number": 1404,
        "file_name": "tests/test_serializable_collection.py",
        "code_diff": "@@ -103,7 +103,7 @@\ndef test_children_if_search_range(self):\n         sc.append(tl)\n         result = sc.children_if(otio.schema.Clip, range)\n         self.assertEqual(len(result), 1)\n-        self.assertTrue(result[0], cl0)\n+        self.assertEqual(result[0], cl0)\n \n     def test_children_if_shallow_search(self):\n         cl = otio.schema.Clip()",
        "comments": [
            {
                "comment": "Is this also meant to be `assertEqual`?\r\n```suggestion\r\n        self.assertEqual(result[0], cl0)\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        self.assertEqual(result[0], cl)\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        self.assertEqual(result[0], cl)\r\n```",
                "position": null
            },
            {
                "comment": "Oops, sorry, I forgot the Python tests. Should be fixed now.",
                "position": null
            }
        ],
        "commit_message": "Assert fixes\n\nSigned-off-by: Darby Johnston <darbyjohnston@yahoo.com>",
        "commit_id": "737dc5f33751570c0dd487b266c52473963a1a45",
        "fn_context": "    def test_children_if_search_range(self):\n        range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(0.0, 24.0),\n            otio.opentime.RationalTime(24.0, 24.0))\n        cl0 = otio.schema.Clip()\n        cl0.source_range = range\n        cl1 = otio.schema.Clip()\n        cl1.source_range = range\n        cl2 = otio.schema.Clip()\n        cl2.source_range = range\n        tr = otio.schema.Track()\n        tr.append(cl0)\n        tr.append(cl1)\n        tr.append(cl2)\n        tl = otio.schema.Timeline()\n        tl.tracks.append(tr)\n        sc = otio.schema.SerializableCollection()\n        sc.append(tl)\n        result = sc.children_if(otio.schema.Clip, range)\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0], cl0)\n\n    def test_children_if_shallow_search(self):\n        cl = otio.schema.Clip()\n        tr = otio.schema.Track()\n        tr.append(cl)\n        tl = otio.schema.Timeline()\n        tl.tracks.append(tr)\n        sc = otio.schema.SerializableCollection()\n        sc.append(tl)\n        result = sc.children_if(otio.schema.Clip, shallow_search=True)\n        self.assertEqual(len(result), 0)\n        result = sc.children_if(otio.schema.Clip, shallow_search=False)\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0], cl)"
    },
    {
        "pr_title": "Fix for SerializableCollection::children_if",
        "pr_number": 1404,
        "file_name": "tests/test_timeline.py",
        "code_diff": "@@ -565,7 +565,7 @@\ndef test_children_if(self):\n         tl.tracks.append(tr)\n         result = tl.children_if(otio.schema.Clip)\n         self.assertEqual(len(result), 1)\n-        self.assertTrue(result[0], cl)\n+        self.assertEqual(result[0], cl)\n \n     def test_children_if_search_range(self):\n         range = otio.opentime.TimeRange(",
        "comments": [
            {
                "comment": "```suggestion\r\n        self.assertEqual(result[0], cl)\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        self.assertEqual(result[0], cl0)\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        self.assertEqual(result[0], cl)\r\n```",
                "position": null
            }
        ],
        "commit_message": "Assert fixes\n\nSigned-off-by: Darby Johnston <darbyjohnston@yahoo.com>",
        "commit_id": "737dc5f33751570c0dd487b266c52473963a1a45",
        "fn_context": "    def test_children_if(self):\n        cl = otio.schema.Clip()\n        tr = otio.schema.Track()\n        tr.append(cl)\n        tl = otio.schema.Timeline()\n        tl.tracks.append(tr)\n        result = tl.children_if(otio.schema.Clip)\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0], cl)\n\n    def test_children_if_search_range(self):\n        range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(0.0, 24.0),\n            otio.opentime.RationalTime(24.0, 24.0))\n        cl0 = otio.schema.Clip()\n        cl0.source_range = range\n        cl1 = otio.schema.Clip()\n        cl1.source_range = range\n        cl2 = otio.schema.Clip()\n        cl2.source_range = range\n        tr = otio.schema.Track()\n        tr.append(cl0)\n        tr.append(cl1)\n        tr.append(cl2)\n        tl = otio.schema.Timeline()\n        tl.tracks.append(tr)\n        result = tl.children_if(otio.schema.Clip, range)\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0], cl0)"
    },
    {
        "pr_title": "Fix for SerializableCollection::children_if",
        "pr_number": 1404,
        "file_name": "tests/test_timeline.py",
        "code_diff": "@@ -585,7 +585,7 @@\ndef test_children_if_search_range(self):\n         tl.tracks.append(tr)\n         result = tl.children_if(otio.schema.Clip, range)\n         self.assertEqual(len(result), 1)\n-        self.assertTrue(result[0], cl0)\n+        self.assertEqual(result[0], cl0)\n \n     def test_children_if_shallow_search(self):\n         cl = otio.schema.Clip()",
        "comments": [
            {
                "comment": "```suggestion\r\n        self.assertEqual(result[0], cl)\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        self.assertEqual(result[0], cl0)\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        self.assertEqual(result[0], cl)\r\n```",
                "position": null
            }
        ],
        "commit_message": "Assert fixes\n\nSigned-off-by: Darby Johnston <darbyjohnston@yahoo.com>",
        "commit_id": "737dc5f33751570c0dd487b266c52473963a1a45",
        "fn_context": "    def test_children_if_search_range(self):\n        range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(0.0, 24.0),\n            otio.opentime.RationalTime(24.0, 24.0))\n        cl0 = otio.schema.Clip()\n        cl0.source_range = range\n        cl1 = otio.schema.Clip()\n        cl1.source_range = range\n        cl2 = otio.schema.Clip()\n        cl2.source_range = range\n        tr = otio.schema.Track()\n        tr.append(cl0)\n        tr.append(cl1)\n        tr.append(cl2)\n        tl = otio.schema.Timeline()\n        tl.tracks.append(tr)\n        result = tl.children_if(otio.schema.Clip, range)\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0], cl0)\n\n    def test_children_if_shallow_search(self):\n        cl = otio.schema.Clip()\n        tr = otio.schema.Track()\n        tr.append(cl)\n        tl = otio.schema.Timeline()\n        tl.tracks.append(tr)\n        result = tl.children_if(otio.schema.Clip, shallow_search=True)\n        self.assertEqual(len(result), 0)\n        result = tl.children_if(otio.schema.Clip, shallow_search=False)\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0], cl)"
    },
    {
        "pr_title": "Fix for SerializableCollection::children_if",
        "pr_number": 1404,
        "file_name": "tests/test_track.py",
        "code_diff": "@@ -17,7 +17,7 @@\ndef test_children_if(self):\n         tr.append(cl)\n         result = tr.children_if(otio.schema.Clip)\n         self.assertEqual(len(result), 1)\n-        self.assertTrue(result[0], cl)\n+        self.assertEqual(result[0], cl)\n \n     def test_children_if_search_range(self):\n         range = otio.opentime.TimeRange(",
        "comments": [
            {
                "comment": "```suggestion\r\n        self.assertEqual(result[0], cl0)\r\n        self.assertEqual(result[1], cl1)\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        self.assertEqual(result[0], cl)\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        self.assertEqual(result[0], cl0)\r\n```",
                "position": null
            }
        ],
        "commit_message": "Assert fixes\n\nSigned-off-by: Darby Johnston <darbyjohnston@yahoo.com>",
        "commit_id": "737dc5f33751570c0dd487b266c52473963a1a45",
        "fn_context": "    def test_children_if(self):\n        cl = otio.schema.Clip()\n        tr = otio.schema.Track()\n        tr.append(cl)\n        result = tr.children_if(otio.schema.Clip)\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0], cl)\n\n    def test_children_if_search_range(self):\n        range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(0.0, 24.0),\n            otio.opentime.RationalTime(24.0, 24.0))\n        cl0 = otio.schema.Clip()\n        cl0.source_range = range\n        cl1 = otio.schema.Clip()\n        cl1.source_range = range\n        cl2 = otio.schema.Clip()\n        cl2.source_range = range\n        tr = otio.schema.Track()\n        tr.append(cl0)\n        tr.append(cl1)\n        tr.append(cl2)\n        result = tr.children_if(otio.schema.Clip, range)\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0], cl0)"
    },
    {
        "pr_title": "Fix for SerializableCollection::children_if",
        "pr_number": 1404,
        "file_name": "tests/test_track.py",
        "code_diff": "@@ -35,7 +35,7 @@\ndef test_children_if_search_range(self):\n         tr.append(cl2)\n         result = tr.children_if(otio.schema.Clip, range)\n         self.assertEqual(len(result), 1)\n-        self.assertTrue(result[0], cl0)\n+        self.assertEqual(result[0], cl0)\n \n     def test_children_if_shallow_search(self):\n         cl0 = otio.schema.Clip()",
        "comments": [
            {
                "comment": "```suggestion\r\n        self.assertEqual(result[0], cl0)\r\n        self.assertEqual(result[1], cl1)\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        self.assertEqual(result[0], cl)\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        self.assertEqual(result[0], cl0)\r\n```",
                "position": null
            }
        ],
        "commit_message": "Assert fixes\n\nSigned-off-by: Darby Johnston <darbyjohnston@yahoo.com>",
        "commit_id": "737dc5f33751570c0dd487b266c52473963a1a45",
        "fn_context": "    def test_children_if_search_range(self):\n        range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(0.0, 24.0),\n            otio.opentime.RationalTime(24.0, 24.0))\n        cl0 = otio.schema.Clip()\n        cl0.source_range = range\n        cl1 = otio.schema.Clip()\n        cl1.source_range = range\n        cl2 = otio.schema.Clip()\n        cl2.source_range = range\n        tr = otio.schema.Track()\n        tr.append(cl0)\n        tr.append(cl1)\n        tr.append(cl2)\n        result = tr.children_if(otio.schema.Clip, range)\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0], cl0)\n\n    def test_children_if_shallow_search(self):\n        cl0 = otio.schema.Clip()\n        cl1 = otio.schema.Clip()\n        st = otio.schema.Stack()\n        st.append(cl1)\n        tr = otio.schema.Track()\n        tr.append(cl0)\n        tr.append(st)\n        result = tr.children_if(otio.schema.Clip, shallow_search=True)\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0], cl0)\n        result = tr.children_if(otio.schema.Clip, shallow_search=False)\n        self.assertEqual(len(result), 2)\n        self.assertEqual(result[0], cl0)\n        self.assertEqual(result[1], cl1)"
    },
    {
        "pr_title": "Fix parsing of time_strings lacking leading zeroes",
        "pr_number": 1297,
        "file_name": "src/opentime/rationalTime.cpp",
        "code_diff": "@@ -148,8 +148,8 @@\nparseFloat(char const* pCurr, char const* pEnd, bool allow_negative, double* res\n         ++pCurr;\n     }\n \n-    ret = (double) uintPart;\n-    if (uintPart != (uint64_t) ret)\n+    ret = static_cast<double>(uintPart);\n+    if (uintPart != static_cast<uint64_t>(ret))\n     {\n         // if the double cannot be casted precisely back to uint64_t, fail\n         // A double has 15 digits of precision, but a uint64_t can encode more.",
        "comments": [
            {
                "comment": "If `*pCurr != '.'` then there's extra junk at the end, in which case I would think that it should return false, maybe?",
                "position": null
            },
            {
                "comment": "That's a good point.",
                "position": null
            },
            {
                "comment": "fixed",
                "position": null
            },
            {
                "comment": "Some places in this code use `0.f` and others `0.0` is there a meaningful difference?",
                "position": null
            },
            {
                "comment": "On first read, I was stuck on whether this is a \"scalar\" or a scale factor. I see that it is the order of magnitude, or place-value scale. Maybe just a comment would help others avoid the confusion?",
                "position": null
            },
            {
                "comment": "The success/fail return value of parseFloat should be checked and turned into a set_error call, right? Same thing for the 2nd use of parseFloat below.",
                "position": null
            },
            {
                "comment": "0.f is a floating point constant, 0.0 is a double. It should be 0.0 here, although it won't affect codegen.",
                "position": null
            },
            {
                "comment": "I've renamed it \"position_scale\" for unambiguity.",
                "position": null
            },
            {
                "comment": "done",
                "position": null
            },
            {
                "comment": "Should either this message be more generic, or this specific message passed in?  Or maybe the function name made more specific?  It seems very focused on invalid time string but has the name `set_error`",
                "position": null
            },
            {
                "comment": "You might want to consider a static_cast here instead of a C cast -- static_casts are verified at compile team and C casts at runtime (and can thus fail).",
                "position": null
            },
            {
                "comment": "ok, made set_error generic",
                "position": null
            },
            {
                "comment": "sounds good, done",
                "position": null
            }
        ],
        "commit_message": "replace C casts with static_cast in parseFloat",
        "commit_id": "043f8112341f36d36b4b00d94e3e990bb892acf2",
        "fn_context": "static bool\nparseFloat(char const* pCurr, char const* pEnd, bool allow_negative, double* result)\n{\n    if (pCurr >= pEnd || !pCurr)\n    {\n        *result = 0.0;\n        return false;\n    }\n\n    double ret = 0.0;\n    double sign = 1.0;\n\n    if (*pCurr == '+')\n    {\n        ++pCurr;\n    }\n    else if (*pCurr == '-') \n    {\n        if (!allow_negative) \n        {\n            *result = 0.0;\n            return false;\n        }\n        sign = -1.0;\n        ++pCurr;\n    }\n\n    // get integer part\n    //\n    // Note that uint64_t is used because overflow is well defined for\n    // unsigned integers, but it is undefined behavior for signed integers,\n    // and floating point values are couched in the specification with\n    // the caveat that an implementation may be IEEE-754 compliant, or only\n    // partially compliant.\n    //\n    uint64_t uintPart = 0;\n    while (pCurr < pEnd) \n    {\n        char c = *pCurr;\n        if (c < '0' || c > '9')\n        {\n            break;\n        }\n        uint64_t accumulated = uintPart * 10 + c - '0';\n        if (accumulated < uintPart)\n        {\n            // if there are too many digits, resulting in an overflow, fail\n            *result = 0.0;\n            return false;\n        }\n        uintPart = accumulated;\n        ++pCurr;\n    }\n\n    ret = static_cast<double>(uintPart);\n    if (uintPart != static_cast<uint64_t>(ret))\n    {\n        // if the double cannot be casted precisely back to uint64_t, fail\n        // A double has 15 digits of precision, but a uint64_t can encode more.\n        *result = 0.0;\n        return false;\n    }\n\n    // check for end of string or delimiter\n    if (pCurr == pEnd || *pCurr == '\\0') \n    {\n        *result = sign * ret;\n        return true;\n    }\n\n    // if the next character is not a decimal point, the string is malformed.\n    if (*pCurr != '.') \n    {\n        *result = 0.0; // zero consistent with earlier error condition\n        return false;\n    }\n\n    ++pCurr; // skip decimal\n\n    double position_scale = 0.1;\n    while (pCurr < pEnd) \n    {\n        char c = *pCurr;\n        if (c < '0' || c > '9')\n        {\n            break;\n        }\n        ret = ret + static_cast<double>(c - '0') * position_scale;\n        ++pCurr;\n        position_scale *= 0.1;\n    }\n\n    *result = sign * ret;\n    return true;\n}"
    },
    {
        "pr_title": "OpenTimelineIO License & CLA Change",
        "pr_number": 1285,
        "file_name": "maintainers/verify_license.py",
        "code_diff": "@@ -33,11 +33,12 @@\ntotal += 1\n                 try:\n                     content = open(fullpath, 'r').read()\n-                except:\n-                    print(\"ERROR: Unable to read file:\", fullpath)\n+                except Exception as ex:\n+                    print(\"ERROR: Unable to read file: {}\\n{}\".format(\n+                        fullpath, ex))\n                     # raise\n                     continue\n-                if len(content)>10 and license not in content:\n+                if len(content) > 10 and license not in content:\n                     print(\"MISSING: {}\".format(fullpath))\n                     no += 1\n                 else:",
        "comments": [],
        "commit_message": "Lint verify_license.py",
        "commit_id": "986c0eb1b0a811e94c20416fd9383fd844d3ba4a"
    },
    {
        "pr_title": "Add a test case and fix for transcribing markers over a single audio AAF file",
        "pr_number": 1259,
        "file_name": "contrib/opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -1155,7 +1155,7 @@\ndef test_aaf_marker_over_audio_file(self):\n         self.assertTrue(1 == len(timeline.tracks))\n \n         track = timeline.tracks[0]\n-        self.assertTrue(3 == len(track.markers))\n+        self.assertEqual(3, len(track.markers))\n \n         fps = 24.0\n         expected_markers = [",
        "comments": [],
        "commit_message": "Use assertEqual instead",
        "commit_id": "c0426184b926d397a468d646fb6289bcb72a96f7"
    },
    {
        "pr_title": "AAF Adapter: Mob transcription heuristics",
        "pr_number": 1249,
        "file_name": "contrib/opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -1454,7 +1454,7 @@\ndef _contains_something_valuable(thing):\n     return True\n \n \n-def _get_mobs_for_transcribe(storage):\n+def _get_mobs_for_transcription(storage):\n     \"\"\"\n     When we describe our AAF into OTIO space, we apply the following heuristic:",
        "comments": [
            {
                "comment": "```suggestion\r\ndef _get_mobs_for_transcription(storage):\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        mobs_to_transcribe = _get_mobs_for_transcription(storage)\r\n```",
                "position": null
            }
        ],
        "commit_message": "address review notes",
        "commit_id": "279526439fc8d552245d048f875c424307c76ed2",
        "fn_context": "def _contains_something_valuable(thing):\n    if isinstance(thing, otio.core.Item):\n        if len(thing.effects) > 0 or len(thing.markers) > 0:\n            return True\n\n    if isinstance(thing, otio.core.Composition):\n\n        if len(thing) == 0:\n            # NOT valuable because it is empty\n            return False\n\n        for child in thing:\n            if _contains_something_valuable(child):\n                # valuable because this child is valuable\n                return True\n\n        # none of the children were valuable, so thing is NOT valuable\n        return False\n\n    if isinstance(thing, otio.schema.Gap):\n        # TODO: Are there other valuable things we should look for on a Gap?\n        return False\n\n    # anything else is presumed to be valuable\n    return True\n\ndef _get_mobs_for_transcription(storage):\n    \"\"\"\n    When we describe our AAF into OTIO space, we apply the following heuristic:\n\n    1) First look for top level mobs and if found use that to transcribe.\n\n    2) If we don't have top level mobs, look for composition mobs and use them to\n    transcribe.\n\n    3) Lastly if we don't have either, try to use master mobs to transcribe.\n\n    If we don't find any Mobs, just tell the user and do transcrption on an empty\n    list (to generate some 'empty-level' OTIO structure)\n\n    This heuristic is based on 'real-world' examples. There may still be some\n    corner cases / open questions (like could there be metadata on both\n    a composition mob and master mob? And if so, who would 'win'?)\n\n    In any way, this heuristic satisfies the current set of AAFs we are using\n    in our test-environment.\n\n    \"\"\"\n\n    top_level_mobs = list(storage.toplevel())\n\n    if len(top_level_mobs) > 0:\n        _transcribe_log(\"---\\nTranscribing top level mobs\\n---\")\n        return top_level_mobs\n\n    composition_mobs = list(storage.compositionmobs())\n    if len(composition_mobs) > 0:\n        _transcribe_log(\"---\\nTranscribing composition mobs\\n---\")\n        return composition_mobs\n\n    master_mobs = list(storage.mastermobs())\n    if len(master_mobs) > 0:\n        _transcribe_log(\"---\\nTranscribing master mobs\\n---\")\n        return master_mobs\n\n    _transcribe_log(\"---\\nNo mobs found to transcribe\\n---\")\n\n    return []"
    },
    {
        "pr_title": "Media Multi-Reference Feature",
        "pr_number": 1241,
        "file_name": "src/opentimelineio/clip.cpp",
        "code_diff": "@@ -44,18 +44,34 @@\nClip::media_references() const noexcept\n template <typename MediaRefMap>\n bool\n Clip::check_for_valid_media_reference_key(\n+    std::string const& caller,\n     std::string const& key,\n     MediaRefMap const& media_references,\n     ErrorStatus*       error_status)\n {\n+    auto empty_key = media_references.find(\"\");\n+    if (empty_key != media_references.end())\n+    {\n+        if (error_status)\n+        {\n+            *error_status = ErrorStatus(\n+                ErrorStatus::MEDIA_REFERENCES_CONTAIN_EMPTY_KEY,\n+                caller +\n+                    \" failed because the media references contain an empty string key\",\n+                this);\n+        }\n+        return false;\n+    }\n+\n     auto found = media_references.find(key);\n     if (found == media_references.end())\n     {\n         if (error_status)\n         {\n             *error_status = ErrorStatus(\n-                ErrorStatus::MEDIA_REFERENCES_MISSING_ACTIVE_KEY,\n-                \"The media references do not contain the active key\",\n+                ErrorStatus::MEDIA_REFERENCES_DO_NOT_CONTAIN_ACTIVE_KEY,\n+                caller +\n+                    \" failed because the media references do not contain the active key\",\n                 this);\n         }\n         return false;",
        "comments": [
            {
                "comment": "There exists potential here to set a map where none of the keys match the `active_reference`.  Currently, if this is the case when they call `media_reference`, it will return a MissingReference. Is that sufficient for error handling? Or do we want to take an ErrorStatus parameter here like some other methods do? Or throw an exception? ",
                "position": null
            },
            {
                "comment": "The corollary to my comment above is that someone can also set a key here that does not match any in the map, so whatever error handling we decide on we should do the same here.",
                "position": null
            },
            {
                "comment": "Checking the current behavior of `Clip` in python, it appears that setting `media_reference` to `None` results in it being set to a blank `MissingReference` - so I think returning `MissingReference` feels pretty consistent with our current behavior.",
                "position": null
            },
            {
                "comment": "Yes, sorry, I remember now that I look back on the code.  You're right in that setting None will result in writing a MissingReference into the map, but I think if it doesn't exist at all in that map we end up with nullptr (which will be None in python).  Is that what we want or do we want a MissingReference there too? If so, is it acceptable to have a static MissingReference that we return whenever this occurs (so we don't leak it). ",
                "position": null
            },
            {
                "comment": "Should we do this instead?\r\n```\r\n_media_references[m.first] = m.second ? m.second : new MissingReference\r\n```\r\n\r\nSo that this:\r\n```\r\nclip.set_media_reference(nullptr);\r\n```\r\nbehaves the same as this:\r\n```\r\nclip.set_media_references({\"default\", nullptr});\r\n```\r\n",
                "position": null
            },
            {
                "comment": "What's the precedent elsewhere, within OTIO? \r\n\r\nAs a C++ programmer, I intuitively expect set(nullptr) means whatever is set, is going to be cleared, in the general case.\r\nIf a new map is going to be created, I'd want that to consistently match other interfaces. @davidbaraff Have you got thoughts on this?",
                "position": null
            },
            {
                "comment": "I think set_media_reference has to clear the active_reference. The reason is that we are REPLACING active media_references, not merging them. So if before/after maps both have a \"fizzbin\" tag, and \"fizzbin\" was the active reference, the correspondence is superficial. \r\n\r\nIn general, setting the media references, and thereby introducing a potential consistency problem (before has \"fizzbin\", after does not), seems like an undesirable behavior.",
                "position": null
            },
            {
                "comment": "If you set a nullptr (or None via the Python bindings) with `set_media_reference` on a Clip, it will result in the Clip's media reference being set to a MissingReference.  Personally, I find this a bit misleading and prefer explicit behaviour (ie if I wanted a MissingReference I would have done `clip.set_media_reference( new MissingReference() );`). \r\n\r\nSince set_media_reference is also called in the ctor, this effectively means a Clip can never have a null MediaReference, perhaps there is some historical reason for this?  Currently, all the code I can find is properly checking for nullptr even though it will never be the case.  For example in Clip::available_range:\r\n\r\n```\r\n    auto active_media = media_reference();\r\n    if (!active_media)\r\n    {\r\n        if (error_status)\r\n        {\r\n            *error_status = ErrorStatus(\r\n                ErrorStatus::CANNOT_COMPUTE_AVAILABLE_RANGE,\r\n                \"No media reference set on clip\",\r\n                this);\r\n        }\r\n        return TimeRange();\r\n    }\r\n\r\n    if (!active_media->available_range())\r\n    {\r\n        if (error_status)\r\n        {\r\n            *error_status = ErrorStatus(\r\n                ErrorStatus::CANNOT_COMPUTE_AVAILABLE_RANGE,\r\n                \"No available_range set on media reference on clip\",\r\n                this);\r\n        }\r\n        return TimeRange();\r\n    }\r\n``` ",
                "position": null
            },
            {
                "comment": "I think you mean `set_media_references` (plural, not singular, correct?). \r\n\r\nAssuming that's the case, we could potentially modify the signature to `set_media_references( MediaReferences, std::string);` forcing the caller to tell us which reference in the new map is the active one.\r\n",
                "position": null
            },
            {
                "comment": "I don't know the history, but it's consistent, thanks for looking that up.",
                "position": null
            },
            {
                "comment": "Should there be a second error status of `Active key exists but has no media references`?",
                "position": 76
            },
            {
                "comment": "is MissingReference to be considered valid or invalid? Either way, there should be a line  of doc, methinks",
                "position": 75
            },
            {
                "comment": "this is the test for  which I thought we should have simply early-exited",
                "position": null
            },
            {
                "comment": "should test key for empty, and report that as a special failure",
                "position": 63
            },
            {
                "comment": " It's valid (as it was in the original code as well).  While we do convert nullptr into MissingReference, a MissingReference could be explicitly added by a user, and I don't think we want to make presumptions about that being invalid.",
                "position": 75
            },
            {
                "comment": "Hmm. The find on line ensure its has some kind of value (since a key must have a value), and a nullptr will be converted into a MissingReference later on in the setter.  Is there another type of invalid value we should be checking for that I am not thinking of?",
                "position": 76
            },
            {
                "comment": "ok",
                "position": 75
            },
            {
                "comment": "MissingReference will be returned",
                "position": 76
            }
        ],
        "commit_message": "Avoid empty keys, clearer error handling",
        "commit_id": "ef89a387d4cca0664ded3700b5250e0db30e4da4",
        "fn_context": "bool\nClip::check_for_valid_media_reference_key(\n    std::string const& caller,\n    std::string const& key,\n    MediaRefMap const& media_references,\n    ErrorStatus*       error_status)\n{\n    auto empty_key = media_references.find(\"\");\n    if (empty_key != media_references.end())\n    {\n        if (error_status)\n        {\n            *error_status = ErrorStatus(\n                ErrorStatus::MEDIA_REFERENCES_CONTAIN_EMPTY_KEY,\n                caller +\n                    \" failed because the media references contain an empty string key\",\n                this);\n        }\n        return false;\n    }\n\n    auto found = media_references.find(key);\n    if (found == media_references.end())\n    {\n        if (error_status)\n        {\n            *error_status = ErrorStatus(\n                ErrorStatus::MEDIA_REFERENCES_DO_NOT_CONTAIN_ACTIVE_KEY,\n                caller +\n                    \" failed because the media references do not contain the active key\",\n                this);\n        }\n        return false;\n    }\n    return true;\n}"
    },
    {
        "pr_title": "Media Multi-Reference Feature",
        "pr_number": 1241,
        "file_name": "src/opentimelineio/clip.cpp",
        "code_diff": "@@ -70,7 +86,10 @@\nClip::set_media_references(\n     ErrorStatus*           error_status) noexcept\n {\n     if (!check_for_valid_media_reference_key(\n-            new_active_key, media_references, error_status))\n+            \"set_media_references\",\n+            new_active_key,\n+            media_references,\n+            error_status))\n     {\n         return;\n     }",
        "comments": [
            {
                "comment": "There exists potential here to set a map where none of the keys match the `active_reference`.  Currently, if this is the case when they call `media_reference`, it will return a MissingReference. Is that sufficient for error handling? Or do we want to take an ErrorStatus parameter here like some other methods do? Or throw an exception? ",
                "position": null
            },
            {
                "comment": "The corollary to my comment above is that someone can also set a key here that does not match any in the map, so whatever error handling we decide on we should do the same here.",
                "position": null
            },
            {
                "comment": "Checking the current behavior of `Clip` in python, it appears that setting `media_reference` to `None` results in it being set to a blank `MissingReference` - so I think returning `MissingReference` feels pretty consistent with our current behavior.",
                "position": null
            },
            {
                "comment": "Yes, sorry, I remember now that I look back on the code.  You're right in that setting None will result in writing a MissingReference into the map, but I think if it doesn't exist at all in that map we end up with nullptr (which will be None in python).  Is that what we want or do we want a MissingReference there too? If so, is it acceptable to have a static MissingReference that we return whenever this occurs (so we don't leak it). ",
                "position": null
            },
            {
                "comment": "Should we do this instead?\r\n```\r\n_media_references[m.first] = m.second ? m.second : new MissingReference\r\n```\r\n\r\nSo that this:\r\n```\r\nclip.set_media_reference(nullptr);\r\n```\r\nbehaves the same as this:\r\n```\r\nclip.set_media_references({\"default\", nullptr});\r\n```\r\n",
                "position": null
            },
            {
                "comment": "What's the precedent elsewhere, within OTIO? \r\n\r\nAs a C++ programmer, I intuitively expect set(nullptr) means whatever is set, is going to be cleared, in the general case.\r\nIf a new map is going to be created, I'd want that to consistently match other interfaces. @davidbaraff Have you got thoughts on this?",
                "position": null
            },
            {
                "comment": "I think set_media_reference has to clear the active_reference. The reason is that we are REPLACING active media_references, not merging them. So if before/after maps both have a \"fizzbin\" tag, and \"fizzbin\" was the active reference, the correspondence is superficial. \r\n\r\nIn general, setting the media references, and thereby introducing a potential consistency problem (before has \"fizzbin\", after does not), seems like an undesirable behavior.",
                "position": null
            },
            {
                "comment": "If you set a nullptr (or None via the Python bindings) with `set_media_reference` on a Clip, it will result in the Clip's media reference being set to a MissingReference.  Personally, I find this a bit misleading and prefer explicit behaviour (ie if I wanted a MissingReference I would have done `clip.set_media_reference( new MissingReference() );`). \r\n\r\nSince set_media_reference is also called in the ctor, this effectively means a Clip can never have a null MediaReference, perhaps there is some historical reason for this?  Currently, all the code I can find is properly checking for nullptr even though it will never be the case.  For example in Clip::available_range:\r\n\r\n```\r\n    auto active_media = media_reference();\r\n    if (!active_media)\r\n    {\r\n        if (error_status)\r\n        {\r\n            *error_status = ErrorStatus(\r\n                ErrorStatus::CANNOT_COMPUTE_AVAILABLE_RANGE,\r\n                \"No media reference set on clip\",\r\n                this);\r\n        }\r\n        return TimeRange();\r\n    }\r\n\r\n    if (!active_media->available_range())\r\n    {\r\n        if (error_status)\r\n        {\r\n            *error_status = ErrorStatus(\r\n                ErrorStatus::CANNOT_COMPUTE_AVAILABLE_RANGE,\r\n                \"No available_range set on media reference on clip\",\r\n                this);\r\n        }\r\n        return TimeRange();\r\n    }\r\n``` ",
                "position": null
            },
            {
                "comment": "I think you mean `set_media_references` (plural, not singular, correct?). \r\n\r\nAssuming that's the case, we could potentially modify the signature to `set_media_references( MediaReferences, std::string);` forcing the caller to tell us which reference in the new map is the active one.\r\n",
                "position": null
            },
            {
                "comment": "I don't know the history, but it's consistent, thanks for looking that up.",
                "position": null
            },
            {
                "comment": "should the cases I noted above also be tested against MissingReference?",
                "position": 97
            },
            {
                "comment": "the ErrorStatus will be `ErrorStatus::MEDIA_REFERENCES_MISSING_ACTIVE_KEY, \"The media references do not contain the active key\"`. Should it instead be `ErrorStatus::MEDIA_REFERENCES_SET_FAILED_DUE_TO_MISSING_ACTIVE_KEY, \"set_media_reference failed because the media references do not contain the active key\"` ~~~~ or somesuch",
                "position": 91
            },
            {
                "comment": "this is the test for  which I thought we should have simply early-exited",
                "position": null
            },
            {
                "comment": "Sorry, I am a bit lost with the resolved conversations now... are those cases MissingReference with `available_range` and `available_image_bounds`?",
                "position": 97
            },
            {
                "comment": "You mentioned that we'll never return nullptrs, but MissingReferences, and the MissingReferences have well defined semantics through the optional. So this one is resolved as well.  ",
                "position": 97
            },
            {
                "comment": "I left the MEDIA_REFERENCES_SET part out of the ErrorStatus name since the validation method is also called by `set_active_media_reference`, and I passed the caller name as a parameter so we can add that to the string error message.  Otherwise, I followed your suggestion.",
                "position": 91
            }
        ],
        "commit_message": "Avoid empty keys, clearer error handling",
        "commit_id": "ef89a387d4cca0664ded3700b5250e0db30e4da4",
        "fn_context": "void\nClip::set_media_references(\n    MediaReferences const& media_references,\n    std::string const&     new_active_key,\n    ErrorStatus*           error_status) noexcept\n{\n    if (!check_for_valid_media_reference_key(\n            \"set_media_references\",\n            new_active_key,\n            media_references,\n            error_status))\n    {\n        return;\n    }\n\n    _media_references.clear();\n    for (auto const& m: media_references)\n    {\n        _media_references[m.first] = m.second ? m.second : new MissingReference;\n    }\n\n    _active_media_reference_key = new_active_key;\n}"
    },
    {
        "pr_title": "Media Multi-Reference Feature",
        "pr_number": 1241,
        "file_name": "src/opentimelineio/clip.cpp",
        "code_diff": "@@ -81,10 +100,7 @@\nClip::set_media_references(\n         _media_references[m.first] = m.second ? m.second : new MissingReference;\n     }\n \n-    if (!new_active_key.empty())\n-    {\n-        _active_media_reference_key = new_active_key;\n-    }\n+    _active_media_reference_key = new_active_key;\n }\n \n std::string",
        "comments": [
            {
                "comment": "There exists potential here to set a map where none of the keys match the `active_reference`.  Currently, if this is the case when they call `media_reference`, it will return a MissingReference. Is that sufficient for error handling? Or do we want to take an ErrorStatus parameter here like some other methods do? Or throw an exception? ",
                "position": null
            },
            {
                "comment": "The corollary to my comment above is that someone can also set a key here that does not match any in the map, so whatever error handling we decide on we should do the same here.",
                "position": null
            },
            {
                "comment": "Checking the current behavior of `Clip` in python, it appears that setting `media_reference` to `None` results in it being set to a blank `MissingReference` - so I think returning `MissingReference` feels pretty consistent with our current behavior.",
                "position": null
            },
            {
                "comment": "Yes, sorry, I remember now that I look back on the code.  You're right in that setting None will result in writing a MissingReference into the map, but I think if it doesn't exist at all in that map we end up with nullptr (which will be None in python).  Is that what we want or do we want a MissingReference there too? If so, is it acceptable to have a static MissingReference that we return whenever this occurs (so we don't leak it). ",
                "position": null
            },
            {
                "comment": "Should we do this instead?\r\n```\r\n_media_references[m.first] = m.second ? m.second : new MissingReference\r\n```\r\n\r\nSo that this:\r\n```\r\nclip.set_media_reference(nullptr);\r\n```\r\nbehaves the same as this:\r\n```\r\nclip.set_media_references({\"default\", nullptr});\r\n```\r\n",
                "position": null
            },
            {
                "comment": "What's the precedent elsewhere, within OTIO? \r\n\r\nAs a C++ programmer, I intuitively expect set(nullptr) means whatever is set, is going to be cleared, in the general case.\r\nIf a new map is going to be created, I'd want that to consistently match other interfaces. @davidbaraff Have you got thoughts on this?",
                "position": null
            },
            {
                "comment": "I think set_media_reference has to clear the active_reference. The reason is that we are REPLACING active media_references, not merging them. So if before/after maps both have a \"fizzbin\" tag, and \"fizzbin\" was the active reference, the correspondence is superficial. \r\n\r\nIn general, setting the media references, and thereby introducing a potential consistency problem (before has \"fizzbin\", after does not), seems like an undesirable behavior.",
                "position": null
            },
            {
                "comment": "If you set a nullptr (or None via the Python bindings) with `set_media_reference` on a Clip, it will result in the Clip's media reference being set to a MissingReference.  Personally, I find this a bit misleading and prefer explicit behaviour (ie if I wanted a MissingReference I would have done `clip.set_media_reference( new MissingReference() );`). \r\n\r\nSince set_media_reference is also called in the ctor, this effectively means a Clip can never have a null MediaReference, perhaps there is some historical reason for this?  Currently, all the code I can find is properly checking for nullptr even though it will never be the case.  For example in Clip::available_range:\r\n\r\n```\r\n    auto active_media = media_reference();\r\n    if (!active_media)\r\n    {\r\n        if (error_status)\r\n        {\r\n            *error_status = ErrorStatus(\r\n                ErrorStatus::CANNOT_COMPUTE_AVAILABLE_RANGE,\r\n                \"No media reference set on clip\",\r\n                this);\r\n        }\r\n        return TimeRange();\r\n    }\r\n\r\n    if (!active_media->available_range())\r\n    {\r\n        if (error_status)\r\n        {\r\n            *error_status = ErrorStatus(\r\n                ErrorStatus::CANNOT_COMPUTE_AVAILABLE_RANGE,\r\n                \"No available_range set on media reference on clip\",\r\n                this);\r\n        }\r\n        return TimeRange();\r\n    }\r\n``` ",
                "position": null
            },
            {
                "comment": "I think you mean `set_media_references` (plural, not singular, correct?). \r\n\r\nAssuming that's the case, we could potentially modify the signature to `set_media_references( MediaReferences, std::string);` forcing the caller to tell us which reference in the new map is the active one.\r\n",
                "position": null
            },
            {
                "comment": "I don't know the history, but it's consistent, thanks for looking that up.",
                "position": null
            },
            {
                "comment": "this is the test for  which I thought we should have simply early-exited",
                "position": null
            }
        ],
        "commit_message": "Avoid empty keys, clearer error handling",
        "commit_id": "ef89a387d4cca0664ded3700b5250e0db30e4da4",
        "fn_context": "void\nClip::set_media_references(\n    MediaReferences const& media_references,\n    std::string const&     new_active_key,\n    ErrorStatus*           error_status) noexcept\n{\n    if (!check_for_valid_media_reference_key(\n            \"set_media_references\",\n            new_active_key,\n            media_references,\n            error_status))\n    {\n        return;\n    }\n\n    _media_references.clear();\n    for (auto const& m: media_references)\n    {\n        _media_references[m.first] = m.second ? m.second : new MissingReference;\n    }\n\n    _active_media_reference_key = new_active_key;\n}\n\nstd::string\nClip::active_media_reference_key() const noexcept\n{\n    return _active_media_reference_key;\n}"
    },
    {
        "pr_title": "Media Multi-Reference Feature",
        "pr_number": 1241,
        "file_name": "tests/test_clip.cpp",
        "code_diff": "@@ -187,7 +187,7 @@\nmain(int argc, char** argv)\n         assertTrue(otio::is_error(error));\n         assertEqual(\n             error.outcome,\n-            otio::ErrorStatus::MEDIA_REFERENCES_MISSING_ACTIVE_KEY);\n+            otio::ErrorStatus::MEDIA_REFERENCES_DO_NOT_CONTAIN_ACTIVE_KEY);\n         assertEqual(clip->media_reference(), ref1.value);\n \n         // setting the references that doesn't have the active key should",
        "comments": [],
        "commit_message": "Avoid empty keys, clearer error handling",
        "commit_id": "ef89a387d4cca0664ded3700b5250e0db30e4da4"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/otio_reader.py",
        "code_diff": "@@ -33,6 +33,19 @@\nfrom rv import extra_commands\n \n import opentimelineio as otio\n+from contextlib import contextmanager\n+\n+\n+@contextmanager\n+def set_context(context, **kwargs):\n+    old_context = context.copy()\n+    context.update(**kwargs)\n+\n+    try:\n+        yield\n+    finally:\n+        context.clear()\n+        context.update(old_context)\n \n \n class NoMappingForOtioTypeError(otio.exceptions.OTIOError):",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/otio_reader.py",
        "code_diff": "@@ -52,7 +65,7 @@\ndef read_otio_file(otio_file):\n     return create_rv_node_from_otio(input_otio)\n \n \n-def create_rv_node_from_otio(otio_obj, track_kind=None):\n+def create_rv_node_from_otio(otio_obj, context=None):\n     WRITE_TYPE_MAP = {\n         otio.schema.Timeline: _create_timeline,\n         otio.schema.Stack: _create_stack,",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/otio_reader.py",
        "code_diff": "@@ -64,14 +77,14 @@\ndef create_rv_node_from_otio(otio_obj, track_kind=None):\n     }\n \n     if type(otio_obj) in WRITE_TYPE_MAP:\n-        return WRITE_TYPE_MAP[type(otio_obj)](otio_obj, track_kind)\n+        return WRITE_TYPE_MAP[type(otio_obj)](otio_obj, context)\n \n     raise NoMappingForOtioTypeError(\n         str(type(otio_obj)) + \" on object: {}\".format(otio_obj)\n     )\n \n \n-def _create_dissolve(pre_item, in_dissolve, post_item, track_kind=None):\n+def _create_dissolve(pre_item, in_dissolve, post_item, context=None):\n     rv_trx = commands.newNode(\"CrossDissolve\", in_dissolve.name or \"dissolve\")\n     extra_commands.setUIName(rv_trx, str(in_dissolve.name or \"dissolve\"))",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/otio_reader.py",
        "code_diff": "@@ -89,9 +102,9 @@\ndef _create_dissolve(pre_item, in_dissolve, post_item, track_kind=None):\n                               [float(pre_item.trimmed_range().duration.rate)],\n                               True)\n \n-    pre_item_rv = create_rv_node_from_otio(pre_item, track_kind)\n+    pre_item_rv = create_rv_node_from_otio(pre_item, context)\n \n-    post_item_rv = create_rv_node_from_otio(post_item, track_kind)\n+    post_item_rv = create_rv_node_from_otio(post_item, context)\n \n     node_to_insert = post_item_rv",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/otio_reader.py",
        "code_diff": "@@ -113,7 +126,7 @@\ndef _create_dissolve(pre_item, in_dissolve, post_item, track_kind=None):\n             pre_item.media_reference.available_range.start_time.rate\n         )\n \n-        post_item_rv = create_rv_node_from_otio(post_item, track_kind)\n+        post_item_rv = create_rv_node_from_otio(post_item, context)\n \n         rt_node.addInput(post_item_rv)\n         node_to_insert = rt_node",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/otio_reader.py",
        "code_diff": "@@ -123,7 +136,7 @@\ndef _create_dissolve(pre_item, in_dissolve, post_item, track_kind=None):\n     return rv_trx\n \n \n-def _create_transition(pre_item, in_trx, post_item, track_kind=None):\n+def _create_transition(pre_item, in_trx, post_item, context=None):\n     trx_map = {\n         otio.schema.TransitionTypes.SMPTE_Dissolve: _create_dissolve,\n     }",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/otio_reader.py",
        "code_diff": "@@ -135,17 +148,17 @@\ndef _create_transition(pre_item, in_trx, post_item, track_kind=None):\n         pre_item,\n         in_trx,\n         post_item,\n-        track_kind\n+        context\n     )\n \n \n-def _create_stack(in_stack, track_kind=None):\n+def _create_stack(in_stack, context=None):\n     new_stack = commands.newNode(\"RVStackGroup\", in_stack.name or \"tracks\")\n     extra_commands.setUIName(new_stack, str(in_stack.name or \"tracks\"))\n \n     new_inputs = []\n     for seq in in_stack:\n-        result = create_rv_node_from_otio(seq, track_kind)\n+        result = create_rv_node_from_otio(seq, context)\n         if result:\n             new_inputs.append(result)",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/otio_reader.py",
        "code_diff": "@@ -154,53 +167,93 @@\ndef _create_stack(in_stack, track_kind=None):\n     return new_stack\n \n \n-def _create_track(in_seq, _=None):\n+def _create_track(in_seq, context=None):\n+    context = context or {}\n+\n     new_seq = commands.newNode(\"RVSequenceGroup\", str(in_seq.name or \"track\"))\n     extra_commands.setUIName(new_seq, str(in_seq.name or \"track\"))\n \n     items_to_serialize = otio.algorithms.track_with_expanded_transitions(\n         in_seq\n     )\n \n-    track_kind = in_seq.kind\n+    with set_context(context, track_kind=in_seq.kind):\n+        new_inputs = []\n+        for thing in items_to_serialize:\n+            if isinstance(thing, tuple):\n+                result = _create_transition(*thing, context=context)\n+            elif thing.duration().value == 0:\n+                continue\n+            else:\n+                result = create_rv_node_from_otio(thing, context)\n \n-    new_inputs = []\n-    for thing in items_to_serialize:\n-        if isinstance(thing, tuple):\n-            result = _create_transition(*thing, track_kind=track_kind)\n-        elif thing.duration().value == 0:\n-            continue\n-        else:\n-            result = create_rv_node_from_otio(thing, track_kind)\n+            if result:\n+                new_inputs.append(result)\n \n-        if result:\n-            new_inputs.append(result)\n+        commands.setNodeInputs(new_seq, new_inputs)\n+        _add_metadata_to_node(in_seq, new_seq)\n \n-    commands.setNodeInputs(new_seq, new_inputs)\n-    _add_metadata_to_node(in_seq, new_seq)\n     return new_seq\n \n \n-def _create_timeline(tl, _=None):\n-    return create_rv_node_from_otio(tl.tracks)\n+def _get_global_transform(tl):\n+    # since there's no global scale in otio, use the first source with\n+    # bounds as the global bounds\n+    def find_display_bounds(tl):\n+        for clip in tl.clip_if():\n+            try:\n+                bounds = clip.media_reference.available_image_bounds\n+                if bounds:\n+                    return bounds\n+            except AttributeError:\n+                continue\n+        return None\n+\n+    bounds = find_display_bounds(tl)\n+    if bounds is None:\n+        return {}\n+\n+    translate = bounds.center()\n+    scale = bounds.max - bounds.min\n+\n+    # RV's global coordinate system has a width and height of 1 where the\n+    # width will be scaled to the image aspect ratio.  So scale globally by\n+    # height. The source width will later be scaled to aspect ratio.\n+    global_scale = otio.schema.V2d(1.0 / scale.y, 1.0 / scale.y)\n+\n+    return {\n+        'global_scale': global_scale,\n+        'global_translate': translate * global_scale,\n+    }\n+\n \n+def _create_timeline(tl, context=None):\n+    context = context or {}\n+\n+    with set_context(\n+        context,\n+        **_get_global_transform(tl)\n+    ):\n+        return create_rv_node_from_otio(tl.tracks, context)\n \n-def _create_collection(collection, track_kind=None):\n+\n+def _create_collection(collection, context=None):\n     results = []\n     for item in collection:\n-        result = create_rv_node_from_otio(item, track_kind)\n+        result = create_rv_node_from_otio(item, context)\n         if result:\n             results.append(result)\n \n     if results:\n         return results[0]\n \n \n-def _create_media_reference(item, track_kind=None):\n+def _create_media_reference(item, context=None):\n+    context = context or {}\n     if hasattr(item, \"media_reference\") and item.media_reference:\n         if isinstance(item.media_reference, otio.schema.ExternalReference):\n             media = [str(item.media_reference.target_url)]\n-            if track_kind == otio.schema.TrackKind.Audio:\n+            if context.get('track_kind') == otio.schema.TrackKind.Audio:\n                 # Create blank video media to accompany audio for valid source\n                 blank = _create_movieproc(item.available_range())\n                 # Appending blank to media promotes name of audio file in RV",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/otio_reader.py",
        "code_diff": "@@ -226,7 +279,8 @@\ndef _create_media_reference(item, track_kind=None):\n     return None\n \n \n-def _create_item(it, track_kind=None):\n+def _create_item(it, context=None):\n+    context = context or {}\n     range_to_read = it.trimmed_range()\n \n     if not range_to_read:",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/otio_reader.py",
        "code_diff": "@@ -236,7 +290,7 @@\ndef _create_item(it, track_kind=None):\n             )\n         )\n \n-    new_media = _create_media_reference(it, track_kind)\n+    new_media = _create_media_reference(it, context)\n     if not new_media:\n         kind = \"smptebars\"\n         if isinstance(it, otio.schema.Gap):",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/otio_reader.py",
        "code_diff": "@@ -269,6 +323,8 @@\ndef _create_item(it, track_kind=None):\n                     range_to_read\n                 )\n \n+        _add_source_bounds(it.media_reference, src, context)\n+\n     if not in_frame and not out_frame:\n         # because OTIO has no global concept of FPS, the rate of the duration\n         # is used as the rate for the range of the source.",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/tests/test_rv_reader.py",
        "code_diff": "@@ -74,7 +74,22 @@\nglobal_start_time=otio.opentime.RationalTime(1, 24)\n )\n track = otio.schema.Track('v1')\n-for clipnum in range(1, 4):\n+bounds = [\n+    otio.schema.Box2d(\n+        otio.schema.V2d(0.0, 0.0),\n+        otio.schema.V2d(16.0, 9.0)\n+    ),  # sets viewing area\n+    otio.schema.Box2d(\n+        otio.schema.V2d(8.0, 0),\n+        otio.schema.V2d(24.0, 9.0)\n+    ),  # shifted right by half the viewing area\n+    otio.schema.Box2d(\n+        otio.schema.V2d(0.0, 0.0),\n+        otio.schema.V2d(8.0, 4.5)\n+    )  # scale to 1/4 of viewing area (lower left)\n+]\n+\n+for clipnum, box in zip(range(1, 4), bounds):\n     clip_name = 'clip{n}'.format(n=clipnum)\n     track.append(\n         otio.schema.Clip(",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/tests/test_rv_reader.py",
        "code_diff": "@@ -84,7 +99,8 @@\navailable_range=otio.opentime.TimeRange(\n                     otio.opentime.RationalTime(1, 24),\n                     otio.opentime.RationalTime(50, 24)\n-                )\n+                ),\n+                available_image_bounds=box\n             ),\n             source_range=otio.opentime.TimeRange(\n                 otio.opentime.RationalTime(11, 24),",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/tests/test_rv_reader.py",
        "code_diff": "@@ -215,12 +231,31 @@\ndef test_read_otio_file(self):\n             clip1 = rv_media_name_at_frame(rvc, 1)\n             self.assertEqual(clip1, 'clip1.mov')\n \n+            # note RV has a default res of 1280,720 when the media doesn't exist\n+            aspect_ratio = 1280.0 / 720.0\n+\n+            clip1_scale, clip1_translate = rv_transform_at_frame(rvc, 1)\n+            self.assertEqual(clip1_scale, [1.0, 1.0])\n+            self.assertEqual(clip1_translate, [0.0, 0.0])\n+\n             clip2 = rv_media_name_at_frame(rvc, 4)\n             self.assertEqual(clip2, 'clip2.mov')\n \n+            clip2_scale, clip2_translate = rv_transform_at_frame(rvc, 4)\n+            self.assertEqual(clip2_scale, [1.0, 1.0])\n+\n+            self.assertAlmostEqual(clip2_translate[0], 0.5 * aspect_ratio)\n+            self.assertEqual(clip2_translate[1], 0)\n+\n             clip3 = rv_media_name_at_frame(rvc, 7)\n             self.assertEqual(clip3, 'clip3.mov')\n \n+            clip3_scale, clip3_translate = rv_transform_at_frame(rvc, 7)\n+            self.assertEqual(clip3_scale, [0.5, 0.5])\n+\n+            self.assertAlmostEqual(clip3_translate[0], -0.25 * aspect_ratio)\n+            self.assertEqual(clip3_translate[1], -0.25)\n+\n             rvc.disconnect()\n \n         finally:",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/opentimelineio/composition.cpp",
        "code_diff": "@@ -1,4 +1,5 @@\n#include \"opentimelineio/composition.h\"\n+#include \"opentimelineio/clip.h\"\n #include \"opentimelineio/vectorIndexing.h\"\n \n #include <assert.h>",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/opentimelineio/deserialization.cpp",
        "code_diff": "@@ -591,6 +591,19 @@\nSerializableObject::Reader::_decode(_Resolver& resolver)\n \n         return any(SerializableObject::ReferenceId{ ref_id });\n     }\n+    else if (schema_name_and_version == \"V2d.1\")\n+    {\n+        double x, y;\n+        return _fetch(\"x\", &x) && _fetch(\"y\", &y) ? any(Imath::V2d(x, y))\n+                                                  : any();\n+    }\n+    else if (schema_name_and_version == \"Box2d.1\")\n+    {\n+        Imath::V2d min, max;\n+        return _fetch(\"min\", &min) && _fetch(\"max\", &max)\n+                   ? any(Imath::Box2d(std::move(min), std::move(max)))\n+                   : any();\n+    }\n     else\n     {\n         std::string ref_id;",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/opentimelineio/deserialization.cpp",
        "code_diff": "@@ -711,6 +724,18 @@\nSerializableObject::Reader::read(std::string const& key, AnyVector* value)\n     return _fetch(key, value);\n }\n \n+bool\n+SerializableObject::Reader::read(std::string const& key, Imath::V2d* value)\n+{\n+    return _fetch(key, value);\n+}\n+\n+bool\n+SerializableObject::Reader::read(std::string const& key, Imath::Box2d* value)\n+{\n+    return _fetch(key, value);\n+}\n+\n template <typename T>\n bool\n SerializableObject::Reader::_read_optional(",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/opentimelineio/deserialization.cpp",
        "code_diff": "@@ -766,6 +791,13 @@\nSerializableObject::Reader::read(\n     return _read_optional(key, value);\n }\n \n+bool\n+SerializableObject::Reader::read(\n+    std::string const& key, optional<Imath::Box2d>* value)\n+{\n+    return _read_optional(key, value);\n+}\n+\n bool\n SerializableObject::Reader::read(std::string const& key, any* value)\n {",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/opentimelineio/mediaReference.cpp",
        "code_diff": "@@ -3,11 +3,13 @@\nnamespace opentimelineio { namespace OPENTIMELINEIO_VERSION {\n \n MediaReference::MediaReference(\n-    std::string const&         name,\n-    optional<TimeRange> const& available_range,\n-    AnyDictionary const&       metadata)\n+    std::string const&            name,\n+    optional<TimeRange> const&    available_range,\n+    AnyDictionary const&          metadata,\n+    optional<Imath::Box2d> const& available_image_bounds)\n     : Parent(name, metadata)\n     , _available_range(available_range)\n+    , _available_image_bounds(available_image_bounds)\n {}\n \n MediaReference::~MediaReference()",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/opentimelineio/mediaReference.cpp",
        "code_diff": "@@ -23,6 +25,8 @@\nbool\n MediaReference::read_from(Reader& reader)\n {\n     return reader.read_if_present(\"available_range\", &_available_range) &&\n+           reader.read_if_present(\n+               \"available_image_bounds\", &_available_image_bounds) &&\n            Parent::read_from(reader);\n }",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/opentimelineio/safely_typed_any.cpp",
        "code_diff": "@@ -56,6 +56,18 @@\ncreate_safely_typed_any(TimeTransform&& value)\n     return any(value);\n }\n \n+any\n+create_safely_typed_any(Imath::V2d&& value)\n+{\n+    return any(value);\n+}\n+\n+any\n+create_safely_typed_any(Imath::Box2d&& value)\n+{\n+    return any(value);\n+}\n+\n any\n create_safely_typed_any(AnyVector&& value)\n {",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -66,6 +66,7 @@\nclass Encoder\n     virtual void write_value(class TimeRange const& value)           = 0;\n     virtual void write_value(class TimeTransform const& value)       = 0;\n     virtual void write_value(struct SerializableObject::ReferenceId) = 0;\n+    virtual void write_value(Imath::Box2d const&)                    = 0;\n \n protected:\n     void _error(ErrorStatus const& error_status)",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -162,6 +163,10 @@\nclass CloningEncoder : public Encoder\n         _store(any(value));\n     }\n \n+    void write_value(Imath::V2d const& value) { _store(any(value)); }\n+\n+    void write_value(Imath::Box2d const& value) { _store(any(value)); }\n+\n     void start_array(size_t /* n */)\n     {\n         if (has_errored())",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -372,6 +377,38 @@\nclass JSONEncoder : public Encoder\n         _writer.EndObject();\n     }\n \n+    void write_value(Imath::V2d const& value)\n+    {\n+        _writer.StartObject();\n+\n+        _writer.Key(\"OTIO_SCHEMA\");\n+        _writer.String(\"V2d.1\");\n+\n+        _writer.Key(\"x\");\n+        _writer.Double(value.x);\n+\n+        _writer.Key(\"y\");\n+        _writer.Double(value.y);\n+\n+        _writer.EndObject();\n+    }\n+\n+    void write_value(Imath::Box2d const& value)\n+    {\n+        _writer.StartObject();\n+\n+        _writer.Key(\"OTIO_SCHEMA\");\n+        _writer.String(\"Box2d.1\");\n+\n+        _writer.Key(\"min\");\n+        write_value(value.min);\n+\n+        _writer.Key(\"max\");\n+        write_value(value.max);\n+\n+        _writer.EndObject();\n+    }\n+\n     void start_array(size_t) { _writer.StartArray(); }\n \n     void start_object() { _writer.StartObject(); }",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -440,6 +477,12 @@\nSerializableObject::Writer::_build_dispatch_tables()\n     wt[&typeid(TimeTransform)] = [this](any const& value) {\n         _encoder.write_value(any_cast<TimeTransform const&>(value));\n     };\n+    wt[&typeid(Imath::V2d)] = [this](any const& value) {\n+        _encoder.write_value(any_cast<Imath::V2d const&>(value));\n+    };\n+    wt[&typeid(Imath::Box2d)] = [this](any const& value) {\n+        _encoder.write_value(any_cast<Imath::Box2d const&>(value));\n+    };\n \n     /*\n      * These next recurse back through the Writer itself:",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -477,6 +520,8 @@\nSerializableObject::Writer::_build_dispatch_tables()\n     et[&typeid(TimeTransform)] = &_simple_any_comparison<TimeTransform>;\n     et[&typeid(SerializableObject::ReferenceId)] =\n         &_simple_any_comparison<SerializableObject::ReferenceId>;\n+    et[&typeid(Imath::V2d)]   = &_simple_any_comparison<Imath::V2d>;\n+    et[&typeid(Imath::Box2d)] = &_simple_any_comparison<Imath::Box2d>;\n \n     /*\n      * These next recurse back through the Writer itself:",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -631,6 +676,14 @@\nSerializableObject::Writer::write(\n     value ? _encoder.write_value(*value) : _encoder.write_null_value();\n }\n \n+void\n+SerializableObject::Writer::write(\n+    std::string const& key, optional<Imath::Box2d> value)\n+{\n+    _encoder_write_key(key);\n+    value ? _encoder.write_value(*value) : _encoder.write_null_value();\n+}\n+\n void\n SerializableObject::Writer::write(std::string const& key, TimeTransform value)\n {",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -716,6 +769,20 @@\nSerializableObject::Writer::write(\n #endif\n }\n \n+void\n+SerializableObject::Writer::write(std::string const& key, Imath::V2d value)\n+{\n+    _encoder_write_key(key);\n+    _encoder.write_value(value);\n+}\n+\n+void\n+SerializableObject::Writer::write(std::string const& key, Imath::Box2d value)\n+{\n+    _encoder_write_key(key);\n+    _encoder.write_value(value);\n+}\n+\n void\n SerializableObject::Writer::write(\n     std::string const& key, AnyDictionary const& value)",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/opentimelineview/console.py",
        "code_diff": "@@ -28,7 +28,12 @@\nimport os\n import sys\n import argparse\n-from PySide2 import QtWidgets, QtGui\n+try:\n+    from PySide6 import QtWidgets, QtGui\n+    from PySide6.QtGui import QAction\n+except ImportError:\n+    from PySide2 import QtWidgets, QtGui\n+    from PySide2.QtWidgets import QAction\n \n import opentimelineio as otio\n import opentimelineio.console as otio_console",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/opentimelineview/console.py",
        "code_diff": "@@ -148,11 +153,11 @@\ndef __init__(\n         # menu\n         menubar = self.menuBar()\n \n-        file_load = QtWidgets.QAction('Open...', menubar)\n+        file_load = QAction('Open...', menubar)\n         file_load.setShortcut(QtGui.QKeySequence.Open)\n         file_load.triggered.connect(self._file_load)\n \n-        exit_action = QtWidgets.QAction('Exit', menubar)\n+        exit_action = QAction('Exit', menubar)\n         exit_action.setShortcut(QtGui.QKeySequence.Quit)\n         exit_action.triggered.connect(self.close)",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/opentimelineview/console.py",
        "code_diff": "@@ -234,7 +239,7 @@\ndef _create_navigation_menu(self, navigation_menu):\n \n         def __callback():\n             self._navigation_filter_callback(actions)\n-        navigation_menu.triggered[[QtWidgets.QAction]].connect(__callback)\n+        navigation_menu.triggered[[QAction]].connect(__callback)\n \n     def _navigation_filter_callback(self, filters):\n         nav_filter = 0",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/opentimelineview/details_widget.py",
        "code_diff": "@@ -22,7 +22,10 @@\n# language governing permissions and limitations under the Apache License.\n #\n \n-from PySide2 import QtWidgets, QtGui, QtCore\n+try:\n+    from PySide6 import QtWidgets, QtGui, QtCore\n+except ImportError:\n+    from PySide2 import QtWidgets, QtGui, QtCore\n \n import opentimelineio as otio",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/opentimelineview/timeline_widget.py",
        "code_diff": "@@ -22,7 +22,10 @@\n# language governing permissions and limitations under the Apache License.\n #\n \n-from PySide2 import QtGui, QtCore, QtWidgets\n+try:\n+    from PySide6 import QtGui, QtCore, QtWidgets\n+except ImportError:\n+    from PySide2 import QtGui, QtCore, QtWidgets\n from collections import OrderedDict, namedtuple\n \n import opentimelineio as otio",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/opentimelineview/timeline_widget.py",
        "code_diff": "@@ -461,7 +464,7 @@\ndef mouseReleaseEvent(self, mouse_event):\n     def wheelEvent(self, event):\n         scale_by = 1.0 + float(event.delta()) / 1000\n         self.scale(scale_by, 1)\n-        zoom_level = 1.0 / self.matrix().m11()\n+        zoom_level = 1.0 / self.transform().m11()\n         track_widgets.CURRENT_ZOOM_LEVEL = zoom_level\n \n         # some items we do want to keep the same visual size. So we need to",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/opentimelineview/track_widgets.py",
        "code_diff": "@@ -22,9 +22,14 @@\n# language governing permissions and limitations under the Apache License.\n #\n \n-from PySide2 import QtGui, QtCore, QtWidgets\n+try:\n+    from PySide6 import QtGui, QtCore, QtWidgets\n+    from PySide6.QtGui import QFontMetrics\n+except ImportError:\n+    from PySide2 import QtGui, QtCore, QtWidgets\n+    from PySide2.QtGui import QFontMetrics\n+\n import opentimelineio as otio\n-from PySide2.QtGui import QFontMetrics\n \n TIME_SLIDER_HEIGHT = 20\n MEDIA_TYPE_SEPARATOR_HEIGHT = 5",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/py-opentimelineio/opentimelineio-bindings/otio_serializableObjects.cpp",
        "code_diff": "@@ -29,6 +29,8 @@\n#include \"otio_utils.h\"\n #include \"otio_anyDictionary.h\"\n \n+#include \"ImathBox.h\"\n+\n namespace py = pybind11;\n using namespace pybind11::literals;",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/py-opentimelineio/opentimelineio-bindings/otio_serializableObjects.cpp",
        "code_diff": "@@ -290,6 +292,7 @@\nstatic void define_bases2(py::module m) {\n         .def(\"children_if\", [](SerializableCollection* t, py::object descended_from_type, optional<TimeRange> const& search_range) {\n                 return children_if(t, descended_from_type, search_range);\n             }, \"descended_from_type\"_a = py::none(), \"search_range\"_a = nullopt);\n+\n }\n \n static void define_items_and_compositions(py::module m) {",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/py-opentimelineio/opentimelineio-bindings/otio_serializableObjects.cpp",
        "code_diff": "@@ -338,7 +341,10 @@\nstatic void define_items_and_compositions(py::module m) {\n             }, \"time\"_a, \"to_item\"_a)\n         .def(\"transformed_time_range\", [](Item* item, TimeRange time_range, Item* to_item) {\n             return item->transformed_time_range(time_range, to_item, ErrorStatusHandler());\n-            }, \"time_range\"_a, \"to_item\"_a);\n+            }, \"time_range\"_a, \"to_item\"_a)\n+        .def_property_readonly(\"available_image_bounds\", [](Item* item) {\n+            return item->available_image_bounds(ErrorStatusHandler());\n+            });\n \n     auto transition_class =\n         py::class_<Transition, Composable, managing_ptr<Transition>>(m, \"Transition\", py::dynamic_attr())",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/py-opentimelineio/opentimelineio-bindings/otio_serializableObjects.cpp",
        "code_diff": "@@ -468,6 +474,7 @@\nstatic void define_items_and_compositions(py::module m) {\n                 auto result = c->handles_of_child(child, ErrorStatusHandler());\n                 return py::make_tuple(py::cast(result.first), py::cast(result.second));\n             }, \"child_a\")\n+        .def(\"has_clips\", &Composition::has_clips)\n         .def(\"__internal_getitem__\", [](Composition* c, int index) {\n                 index = adjusted_vector_index(index, c->children());\n                 if (index < 0 || index >= int(c->children().size())) {",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/py-opentimelineio/opentimelineio-bindings/otio_serializableObjects.cpp",
        "code_diff": "@@ -643,28 +650,35 @@\nstatic void define_media_references(py::module m) {\n                managing_ptr<MediaReference>>(m, \"MediaReference\", py::dynamic_attr())\n         .def(py::init([](std::string name,\n                          optional<TimeRange> available_range,\n-                         py::object metadata) {\n-                          return new MediaReference(name, available_range, py_to_any_dictionary(metadata)); }),\n+                         py::object metadata,\n+                         optional<Imath::Box2d> const& available_image_bounds) {\n+                          return new MediaReference(name, available_range, py_to_any_dictionary(metadata), available_image_bounds); }),\n              py::arg_v(\"name\"_a = std::string()),\n              \"available_range\"_a = nullopt,\n-             py::arg_v(\"metadata\"_a = py::none()))\n+             py::arg_v(\"metadata\"_a = py::none()),\n+             \"available_image_bounds\"_a = nullopt)\n+\n         .def_property(\"available_range\", &MediaReference::available_range, &MediaReference::set_available_range)\n+        .def_property(\"available_image_bounds\", &MediaReference::available_image_bounds, &MediaReference::set_available_image_bounds) \n         .def_property_readonly(\"is_missing_reference\", &MediaReference::is_missing_reference);\n \n     py::class_<GeneratorReference, MediaReference,\n                managing_ptr<GeneratorReference>>(m, \"GeneratorReference\", py::dynamic_attr())\n         .def(py::init([](std::string name, std::string generator_kind,\n                          optional<TimeRange> const& available_range,\n-                         py::object parameters, py::object metadata) {\n+                         py::object parameters, py::object metadata,\n+                         optional<Imath::Box2d> const& available_image_bounds) {\n                           return new GeneratorReference(name, generator_kind,\n                                                         available_range,\n                                                         py_to_any_dictionary(parameters),\n-                                                        py_to_any_dictionary(metadata)); }),\n+                                                        py_to_any_dictionary(metadata),\n+                                                        available_image_bounds); }),\n              py::arg_v(\"name\"_a = std::string()),\n              \"generator_kind\"_a = std::string(),\n              \"available_range\"_a = nullopt,\n              \"parameters\"_a = py::none(),\n-             py::arg_v(\"metadata\"_a = py::none()))\n+             py::arg_v(\"metadata\"_a = py::none()),\n+             \"available_image_bounds\"_a = nullopt)\n         .def_property(\"generator_kind\", &GeneratorReference::generator_kind, &GeneratorReference::set_generator_kind)\n         .def_property_readonly(\"parameters\", [](GeneratorReference* g) {\n                 auto ptr = g->parameters().get_or_create_mutation_stamp();",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/py-opentimelineio/opentimelineio-bindings/otio_serializableObjects.cpp",
        "code_diff": "@@ -676,28 +690,34 @@\nstatic void define_media_references(py::module m) {\n         .def(py::init([](\n                         py::object name,\n                         optional<TimeRange> available_range,\n-                        py::object metadata) {\n+                        py::object metadata,\n+                        optional<Imath::Box2d> const& available_image_bounds) {\n                     return new MissingReference(\n                                   string_or_none_converter(name),\n                                   available_range,\n-                                  py_to_any_dictionary(metadata)); \n+                                  py_to_any_dictionary(metadata),\n+                                  available_image_bounds); \n                     }),\n              py::arg_v(\"name\"_a = std::string()),\n              \"available_range\"_a = nullopt,\n-             py::arg_v(\"metadata\"_a = py::none()));\n+             py::arg_v(\"metadata\"_a = py::none()),\n+             \"available_image_bounds\"_a = nullopt);\n \n \n     py::class_<ExternalReference, MediaReference,\n                managing_ptr<ExternalReference>>(m, \"ExternalReference\", py::dynamic_attr())\n         .def(py::init([](std::string target_url,\n                          optional<TimeRange> const& available_range,\n-                         py::object metadata) {\n+                         py::object metadata,\n+                         optional<Imath::Box2d> const& available_image_bounds) {\n                           return new ExternalReference(target_url,\n                                                         available_range,\n-                                                        py_to_any_dictionary(metadata)); }),\n+                                                        py_to_any_dictionary(metadata),\n+                                                        available_image_bounds); }),\n              \"target_url\"_a = std::string(),\n              \"available_range\"_a = nullopt,\n-             py::arg_v(\"metadata\"_a = py::none()))\n+             py::arg_v(\"metadata\"_a = py::none()),\n+             \"available_image_bounds\"_a = nullopt)\n         .def_property(\"target_url\", &ExternalReference::target_url, &ExternalReference::set_target_url);\n \n     auto imagesequencereference_class = py:: class_<ImageSequenceReference, MediaReference,",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/py-opentimelineio/opentimelineio-bindings/otio_serializableObjects.cpp",
        "code_diff": "@@ -778,7 +798,8 @@\nNegative ``start_frame`` is also handled. The above example with a ``start_frame\n                          int frame_zero_padding,\n                          ImageSequenceReference::MissingFramePolicy const missing_frame_policy,\n                          optional<TimeRange> const& available_range,\n-                         py::object metadata) {\n+                         py::object metadata,\n+                         optional<Imath::Box2d> const& available_image_bounds) {\n                           return new ImageSequenceReference(target_url_base,\n                                                             name_prefix,\n                                                             name_suffix,",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/py-opentimelineio/opentimelineio-bindings/otio_serializableObjects.cpp",
        "code_diff": "@@ -788,7 +809,8 @@\nNegative ``start_frame`` is also handled. The above example with a ``start_frame\n                                                             frame_zero_padding,\n                                                             missing_frame_policy,\n                                                             available_range,\n-                                                            py_to_any_dictionary(metadata)); }),\n+                                                            py_to_any_dictionary(metadata),\n+                                                            available_image_bounds); }),\n                         \"target_url_base\"_a = std::string(),\n                         \"name_prefix\"_a = std::string(),\n                         \"name_suffix\"_a = std::string(),",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/py-opentimelineio/opentimelineio/core/mediaReference.py",
        "code_diff": "@@ -4,10 +4,11 @@\n@add_method(_otio.MediaReference)\n def __str__(self):\n-    return \"{}({}, {}, {})\".format(\n+    return \"{}({}, {}, {}, {})\".format(\n         self.__class__.__name__,\n         repr(self.name),\n         repr(self.available_range),\n+        repr(self.available_image_bounds),\n         repr(self.metadata)\n     )",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/py-opentimelineio/opentimelineio/schema/__init__.py",
        "code_diff": "@@ -27,6 +27,7 @@\n\"\"\"User facing classes.\"\"\"\n \n from .. _otio import (\n+    Box2d,\n     Clip,\n     Effect,\n     TimeEffect,",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/py-opentimelineio/opentimelineio/schema/__init__.py",
        "code_diff": "@@ -42,7 +43,8 @@\nStack,\n     Timeline,\n     Track,\n-    Transition\n+    Transition,\n+    V2d,\n )\n \n MarkerColor = Marker.Color",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/py-opentimelineio/opentimelineio/schema/__init__.py",
        "code_diff": "@@ -55,6 +57,7 @@\n)\n \n from . import (\n+    box2d,\n     clip,\n     effect,\n     external_reference,",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/py-opentimelineio/opentimelineio/schema/generator_reference.py",
        "code_diff": "@@ -4,10 +4,11 @@\n@add_method(_otio.GeneratorReference)\n def __str__(self):\n-    return 'GeneratorReference(\"{}\", \"{}\", {}, {})'.format(\n+    return 'GeneratorReference(\"{}\", \"{}\", {}, {}, {})'.format(\n         self.name,\n         self.generator_kind,\n         self.parameters,\n+        self.available_image_bounds,\n         self.metadata\n     )",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/py-opentimelineio/opentimelineio/schema/image_sequence_reference.py",
        "code_diff": "@@ -6,7 +6,7 @@\ndef __str__(self):\n     return (\n         'ImageSequenceReference('\n-        '\"{}\", \"{}\", \"{}\", {}, {}, {}, {}, {}, {}, {})' .format(\n+        '\"{}\", \"{}\", \"{}\", {}, {}, {}, {}, {}, {}, {}, {})' .format(\n             self.target_url_base,\n             self.name_prefix,\n             self.name_suffix,",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/py-opentimelineio/opentimelineio/schema/image_sequence_reference.py",
        "code_diff": "@@ -16,6 +16,7 @@\ndef __str__(self):\n             self.frame_zero_padding,\n             self.missing_frame_policy,\n             self.available_range,\n+            self.available_image_bounds,\n             self.metadata,\n         )\n     )",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "src/py-opentimelineio/opentimelineio/schema/image_sequence_reference.py",
        "code_diff": "@@ -34,6 +35,7 @@\ndef __repr__(self):\n         'frame_zero_padding={}, '\n         'missing_frame_policy={}, '\n         'available_range={}, '\n+        'available_image_bounds={}, '\n         'metadata={}'\n         ')' .format(\n             repr(self.target_url_base),",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "tests/test_clip.py",
        "code_diff": "@@ -62,7 +62,7 @@\ndef test_str(self):\n \n         self.assertMultiLineEqual(\n             str(cl),\n-            'Clip(\"test_clip\", MissingReference(\\'\\', None, {}), None, {})'\n+            'Clip(\"test_clip\", MissingReference(\\'\\', None, None, {}), None, {})'\n         )\n         self.assertMultiLineEqual(\n             repr(cl),",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "tests/test_composition.py",
        "code_diff": "@@ -377,6 +377,42 @@\ndef test_insert_slice(self):\n         self.assertNotIn(cl, st)\n         self.assertIn(cl2, st)\n \n+    def test_has_clip(self):\n+        st = otio.schema.Stack(name=\"ST\")\n+\n+        tr1 = otio.schema.Track(name=\"tr1\")\n+        st.append(tr1)\n+\n+        self.assertFalse(st.has_clips())\n+        self.assertFalse(tr1.has_clips())\n+\n+        c1 = otio.schema.Clip(name=\"c1\")\n+        tr1.append(c1)\n+\n+        self.assertTrue(st.has_clips())\n+        self.assertTrue(tr1.has_clips())\n+\n+        tr2 = otio.schema.Track(name=\"tr2\")\n+        st.append(tr2)\n+\n+        self.assertTrue(st.has_clips())\n+        self.assertTrue(tr1.has_clips())\n+        self.assertFalse(tr2.has_clips())\n+\n+        g1 = otio.schema.Gap(name=\"g1\")\n+        tr2.append(g1)\n+\n+        self.assertTrue(st.has_clips())\n+        self.assertTrue(tr1.has_clips())\n+        self.assertFalse(tr2.has_clips())\n+\n+        c2 = otio.schema.Clip(name=\"c2\")\n+        tr2.append(c2)\n+\n+        self.assertTrue(st.has_clips())\n+        self.assertTrue(tr1.has_clips())\n+        self.assertTrue(tr2.has_clips())\n+\n \n class StackTest(unittest.TestCase, otio_test_utils.OTIOAssertions):",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "tests/test_generator_reference.py",
        "code_diff": "@@ -24,7 +24,11 @@\ndef setUp(self):\n             },\n             metadata={\n                 \"foo\": \"bar\"\n-            }\n+            },\n+            available_image_bounds=otio.schema.Box2d(\n+                otio.schema.V2d(0.0, 0.0),\n+                otio.schema.V2d(16.0, 9.0)\n+            )\n         )\n \n     def test_constructor(self):",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "tests/test_generator_reference.py",
        "code_diff": "@@ -39,6 +43,13 @@\ndef test_constructor(self):\n                 otio.opentime.RationalTime(100, 24),\n             )\n         )\n+        self.assertEqual(\n+            self.gen.available_image_bounds,\n+            otio.schema.Box2d(\n+                otio.schema.V2d(0.0, 0.0),\n+                otio.schema.V2d(16.0, 9.0)\n+            )\n+        )\n \n     def test_serialize(self):\n         encoded = otio.adapters.otio_json.write_to_string(self.gen)",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "tests/test_generator_reference.py",
        "code_diff": "@@ -59,11 +70,13 @@\ndef test_stringify(self):\n             '\"{}\", '\n             '\"{}\", '\n             '{}, '\n+            '{}, '\n             \"{}\"\n             \")\".format(\n                 str(self.gen.name),\n                 str(self.gen.generator_kind),\n                 str(self.gen.parameters),\n+                str(self.gen.available_image_bounds),\n                 str(self.gen.metadata),\n             )\n         )",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "tests/test_image_sequence_reference.py",
        "code_diff": "@@ -64,6 +64,10 @@\ndef test_str(self):\n                 otio.opentime.RationalTime(60, 30),\n             ),\n             metadata={\"custom\": {\"foo\": \"bar\"}},\n+            available_image_bounds=otio.schema.Box2d(\n+                otio.schema.V2d(0.0, 0.0),\n+                otio.schema.V2d(16.0, 9.0)\n+            ),\n         )\n         self.assertEqual(\n             str(ref),",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "tests/test_image_sequence_reference.py",
        "code_diff": "@@ -77,6 +81,7 @@\ndef test_str(self):\n             '5, '\n             'MissingFramePolicy.error, '\n             'TimeRange(RationalTime(0, 30), RationalTime(60, 30)), '\n+            'Box2d(V2d(0.0, 0.0), V2d(16.0, 9.0)), '\n             \"{'custom': {'foo': 'bar'}}\"\n             ')'\n         )",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "tests/test_image_sequence_reference.py",
        "code_diff": "@@ -95,6 +100,10 @@\ndef test_repr(self):\n                 otio.opentime.RationalTime(0, 30),\n                 otio.opentime.RationalTime(60, 30),\n             ),\n+            available_image_bounds=otio.schema.Box2d(\n+                otio.schema.V2d(0.0, 0.0),\n+                otio.schema.V2d(16.0, 9.0)\n+            ),\n             metadata={\"custom\": {\"foo\": \"bar\"}},\n         )\n         ref_value = (",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "Install full OTIO Python env with CMake",
        "pr_number": 1238,
        "file_name": "tests/test_image_sequence_reference.py",
        "code_diff": "@@ -108,6 +117,9 @@\ndef test_repr(self):\n             'frame_zero_padding=5, '\n             'missing_frame_policy=<MissingFramePolicy.error: 0>, '\n             'available_range={}, '\n+            'available_image_bounds=otio.schema.Box2d('\n+            'min=otio.schema.V2d(x=0.0, y=0.0), '\n+            'max=otio.schema.V2d(x=16.0, y=9.0)), '\n             \"metadata={{'custom': {{'foo': 'bar'}}}}\"\n             ')'.format(repr(ref.available_range))\n         )",
        "comments": [],
        "commit_message": "Merge branch 'main' into pure_python_install",
        "commit_id": "d5a368aee2b70862a4366cf353bf1234bc0801e2"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/otio_reader.py",
        "code_diff": "@@ -33,6 +33,19 @@\nfrom rv import extra_commands\n \n import opentimelineio as otio\n+from contextlib import contextmanager\n+\n+\n+@contextmanager\n+def set_context(context, **kwargs):\n+    old_context = context.copy()\n+    context.update(**kwargs)\n+\n+    try:\n+        yield\n+    finally:\n+        context.clear()\n+        context.update(old_context)\n \n \n class NoMappingForOtioTypeError(otio.exceptions.OTIOError):",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/otio_reader.py",
        "code_diff": "@@ -52,7 +65,7 @@\ndef read_otio_file(otio_file):\n     return create_rv_node_from_otio(input_otio)\n \n \n-def create_rv_node_from_otio(otio_obj, track_kind=None):\n+def create_rv_node_from_otio(otio_obj, context=None):\n     WRITE_TYPE_MAP = {\n         otio.schema.Timeline: _create_timeline,\n         otio.schema.Stack: _create_stack,",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/otio_reader.py",
        "code_diff": "@@ -64,14 +77,14 @@\ndef create_rv_node_from_otio(otio_obj, track_kind=None):\n     }\n \n     if type(otio_obj) in WRITE_TYPE_MAP:\n-        return WRITE_TYPE_MAP[type(otio_obj)](otio_obj, track_kind)\n+        return WRITE_TYPE_MAP[type(otio_obj)](otio_obj, context)\n \n     raise NoMappingForOtioTypeError(\n         str(type(otio_obj)) + \" on object: {}\".format(otio_obj)\n     )\n \n \n-def _create_dissolve(pre_item, in_dissolve, post_item, track_kind=None):\n+def _create_dissolve(pre_item, in_dissolve, post_item, context=None):\n     rv_trx = commands.newNode(\"CrossDissolve\", in_dissolve.name or \"dissolve\")\n     extra_commands.setUIName(rv_trx, str(in_dissolve.name or \"dissolve\"))",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/otio_reader.py",
        "code_diff": "@@ -89,9 +102,9 @@\ndef _create_dissolve(pre_item, in_dissolve, post_item, track_kind=None):\n                               [float(pre_item.trimmed_range().duration.rate)],\n                               True)\n \n-    pre_item_rv = create_rv_node_from_otio(pre_item, track_kind)\n+    pre_item_rv = create_rv_node_from_otio(pre_item, context)\n \n-    post_item_rv = create_rv_node_from_otio(post_item, track_kind)\n+    post_item_rv = create_rv_node_from_otio(post_item, context)\n \n     node_to_insert = post_item_rv",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/otio_reader.py",
        "code_diff": "@@ -113,7 +126,7 @@\ndef _create_dissolve(pre_item, in_dissolve, post_item, track_kind=None):\n             pre_item.media_reference.available_range.start_time.rate\n         )\n \n-        post_item_rv = create_rv_node_from_otio(post_item, track_kind)\n+        post_item_rv = create_rv_node_from_otio(post_item, context)\n \n         rt_node.addInput(post_item_rv)\n         node_to_insert = rt_node",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/otio_reader.py",
        "code_diff": "@@ -123,7 +136,7 @@\ndef _create_dissolve(pre_item, in_dissolve, post_item, track_kind=None):\n     return rv_trx\n \n \n-def _create_transition(pre_item, in_trx, post_item, track_kind=None):\n+def _create_transition(pre_item, in_trx, post_item, context=None):\n     trx_map = {\n         otio.schema.TransitionTypes.SMPTE_Dissolve: _create_dissolve,\n     }",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/otio_reader.py",
        "code_diff": "@@ -135,17 +148,17 @@\ndef _create_transition(pre_item, in_trx, post_item, track_kind=None):\n         pre_item,\n         in_trx,\n         post_item,\n-        track_kind\n+        context\n     )\n \n \n-def _create_stack(in_stack, track_kind=None):\n+def _create_stack(in_stack, context=None):\n     new_stack = commands.newNode(\"RVStackGroup\", in_stack.name or \"tracks\")\n     extra_commands.setUIName(new_stack, str(in_stack.name or \"tracks\"))\n \n     new_inputs = []\n     for seq in in_stack:\n-        result = create_rv_node_from_otio(seq, track_kind)\n+        result = create_rv_node_from_otio(seq, context)\n         if result:\n             new_inputs.append(result)",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/otio_reader.py",
        "code_diff": "@@ -154,53 +167,93 @@\ndef _create_stack(in_stack, track_kind=None):\n     return new_stack\n \n \n-def _create_track(in_seq, _=None):\n+def _create_track(in_seq, context=None):\n+    context = context or {}\n+\n     new_seq = commands.newNode(\"RVSequenceGroup\", str(in_seq.name or \"track\"))\n     extra_commands.setUIName(new_seq, str(in_seq.name or \"track\"))\n \n     items_to_serialize = otio.algorithms.track_with_expanded_transitions(\n         in_seq\n     )\n \n-    track_kind = in_seq.kind\n+    with set_context(context, track_kind=in_seq.kind):\n+        new_inputs = []\n+        for thing in items_to_serialize:\n+            if isinstance(thing, tuple):\n+                result = _create_transition(*thing, context=context)\n+            elif thing.duration().value == 0:\n+                continue\n+            else:\n+                result = create_rv_node_from_otio(thing, context)\n \n-    new_inputs = []\n-    for thing in items_to_serialize:\n-        if isinstance(thing, tuple):\n-            result = _create_transition(*thing, track_kind=track_kind)\n-        elif thing.duration().value == 0:\n-            continue\n-        else:\n-            result = create_rv_node_from_otio(thing, track_kind)\n+            if result:\n+                new_inputs.append(result)\n \n-        if result:\n-            new_inputs.append(result)\n+        commands.setNodeInputs(new_seq, new_inputs)\n+        _add_metadata_to_node(in_seq, new_seq)\n \n-    commands.setNodeInputs(new_seq, new_inputs)\n-    _add_metadata_to_node(in_seq, new_seq)\n     return new_seq\n \n \n-def _create_timeline(tl, _=None):\n-    return create_rv_node_from_otio(tl.tracks)\n+def _get_global_transform(tl):\n+    # since there's no global scale in otio, use the first source with\n+    # bounds as the global bounds\n+    def find_display_bounds(tl):\n+        for clip in tl.clip_if():\n+            try:\n+                bounds = clip.media_reference.available_image_bounds\n+                if bounds:\n+                    return bounds\n+            except AttributeError:\n+                continue\n+        return None\n+\n+    bounds = find_display_bounds(tl)\n+    if bounds is None:\n+        return {}\n+\n+    translate = bounds.center()\n+    scale = bounds.max - bounds.min\n+\n+    # RV's global coordinate system has a width and height of 1 where the\n+    # width will be scaled to the image aspect ratio.  So scale globally by\n+    # height. The source width will later be scaled to aspect ratio.\n+    global_scale = otio.schema.V2d(1.0 / scale.y, 1.0 / scale.y)\n+\n+    return {\n+        'global_scale': global_scale,\n+        'global_translate': translate * global_scale,\n+    }\n+\n \n+def _create_timeline(tl, context=None):\n+    context = context or {}\n+\n+    with set_context(\n+        context,\n+        **_get_global_transform(tl)\n+    ):\n+        return create_rv_node_from_otio(tl.tracks, context)\n \n-def _create_collection(collection, track_kind=None):\n+\n+def _create_collection(collection, context=None):\n     results = []\n     for item in collection:\n-        result = create_rv_node_from_otio(item, track_kind)\n+        result = create_rv_node_from_otio(item, context)\n         if result:\n             results.append(result)\n \n     if results:\n         return results[0]\n \n \n-def _create_media_reference(item, track_kind=None):\n+def _create_media_reference(item, context=None):\n+    context = context or {}\n     if hasattr(item, \"media_reference\") and item.media_reference:\n         if isinstance(item.media_reference, otio.schema.ExternalReference):\n             media = [str(item.media_reference.target_url)]\n-            if track_kind == otio.schema.TrackKind.Audio:\n+            if context.get('track_kind') == otio.schema.TrackKind.Audio:\n                 # Create blank video media to accompany audio for valid source\n                 blank = _create_movieproc(item.available_range())\n                 # Appending blank to media promotes name of audio file in RV",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/otio_reader.py",
        "code_diff": "@@ -226,7 +279,8 @@\ndef _create_media_reference(item, track_kind=None):\n     return None\n \n \n-def _create_item(it, track_kind=None):\n+def _create_item(it, context=None):\n+    context = context or {}\n     range_to_read = it.trimmed_range()\n \n     if not range_to_read:",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/otio_reader.py",
        "code_diff": "@@ -236,7 +290,7 @@\ndef _create_item(it, track_kind=None):\n             )\n         )\n \n-    new_media = _create_media_reference(it, track_kind)\n+    new_media = _create_media_reference(it, context)\n     if not new_media:\n         kind = \"smptebars\"\n         if isinstance(it, otio.schema.Gap):",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/otio_reader.py",
        "code_diff": "@@ -269,6 +323,8 @@\ndef _create_item(it, track_kind=None):\n                     range_to_read\n                 )\n \n+        _add_source_bounds(it.media_reference, src, context)\n+\n     if not in_frame and not out_frame:\n         # because OTIO has no global concept of FPS, the rate of the duration\n         # is used as the rate for the range of the source.",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/tests/test_rv_reader.py",
        "code_diff": "@@ -74,7 +74,22 @@\nglobal_start_time=otio.opentime.RationalTime(1, 24)\n )\n track = otio.schema.Track('v1')\n-for clipnum in range(1, 4):\n+bounds = [\n+    otio.schema.Box2d(\n+        otio.schema.V2d(0.0, 0.0),\n+        otio.schema.V2d(16.0, 9.0)\n+    ),  # sets viewing area\n+    otio.schema.Box2d(\n+        otio.schema.V2d(8.0, 0),\n+        otio.schema.V2d(24.0, 9.0)\n+    ),  # shifted right by half the viewing area\n+    otio.schema.Box2d(\n+        otio.schema.V2d(0.0, 0.0),\n+        otio.schema.V2d(8.0, 4.5)\n+    )  # scale to 1/4 of viewing area (lower left)\n+]\n+\n+for clipnum, box in zip(range(1, 4), bounds):\n     clip_name = 'clip{n}'.format(n=clipnum)\n     track.append(\n         otio.schema.Clip(",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/tests/test_rv_reader.py",
        "code_diff": "@@ -84,7 +99,8 @@\navailable_range=otio.opentime.TimeRange(\n                     otio.opentime.RationalTime(1, 24),\n                     otio.opentime.RationalTime(50, 24)\n-                )\n+                ),\n+                available_image_bounds=box\n             ),\n             source_range=otio.opentime.TimeRange(\n                 otio.opentime.RationalTime(11, 24),",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/tests/test_rv_reader.py",
        "code_diff": "@@ -215,12 +231,31 @@\ndef test_read_otio_file(self):\n             clip1 = rv_media_name_at_frame(rvc, 1)\n             self.assertEqual(clip1, 'clip1.mov')\n \n+            # note RV has a default res of 1280,720 when the media doesn't exist\n+            aspect_ratio = 1280.0 / 720.0\n+\n+            clip1_scale, clip1_translate = rv_transform_at_frame(rvc, 1)\n+            self.assertEqual(clip1_scale, [1.0, 1.0])\n+            self.assertEqual(clip1_translate, [0.0, 0.0])\n+\n             clip2 = rv_media_name_at_frame(rvc, 4)\n             self.assertEqual(clip2, 'clip2.mov')\n \n+            clip2_scale, clip2_translate = rv_transform_at_frame(rvc, 4)\n+            self.assertEqual(clip2_scale, [1.0, 1.0])\n+\n+            self.assertAlmostEqual(clip2_translate[0], 0.5 * aspect_ratio)\n+            self.assertEqual(clip2_translate[1], 0)\n+\n             clip3 = rv_media_name_at_frame(rvc, 7)\n             self.assertEqual(clip3, 'clip3.mov')\n \n+            clip3_scale, clip3_translate = rv_transform_at_frame(rvc, 7)\n+            self.assertEqual(clip3_scale, [0.5, 0.5])\n+\n+            self.assertAlmostEqual(clip3_translate[0], -0.25 * aspect_ratio)\n+            self.assertEqual(clip3_translate[1], -0.25)\n+\n             rvc.disconnect()\n \n         finally:",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentime/rationalTime.cpp",
        "code_diff": "@@ -21,21 +21,19 @@\nstatic constexpr std::array<double, 4> dropframe_timecode_rates{ {\n     60000.0 / 1001.0,\n } };\n \n-// currently unused:\n-/*\n-static constexpr std::array<double, 10> non_dropframe_timecode_rates\n-{{  1,\n-    12,\n-    23.976,\n-    23.98,\n-    24,\n-    25,\n-    30,\n-    48,\n-    50,\n-    60\n-}};\n-*/\n+static constexpr std::array<double, 11> smpte_timecode_rates{\n+    { 1.0,\n+      12.0,\n+      24000.0 / 1001.0,\n+      24.0,\n+      25.0,\n+      30000.0 / 1001.0,\n+      30.0,\n+      48.0,\n+      50.0,\n+      60000.0 / 1001.0,\n+      60.0 }\n+};\n \n static constexpr std::array<double, 16> valid_timecode_rates{\n     { 1.0,",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineio/composition.cpp",
        "code_diff": "@@ -1,4 +1,5 @@\n#include \"opentimelineio/composition.h\"\n+#include \"opentimelineio/clip.h\"\n #include \"opentimelineio/vectorIndexing.h\"\n \n #include <assert.h>",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineio/deserialization.cpp",
        "code_diff": "@@ -591,6 +591,19 @@\nSerializableObject::Reader::_decode(_Resolver& resolver)\n \n         return any(SerializableObject::ReferenceId{ ref_id });\n     }\n+    else if (schema_name_and_version == \"V2d.1\")\n+    {\n+        double x, y;\n+        return _fetch(\"x\", &x) && _fetch(\"y\", &y) ? any(Imath::V2d(x, y))\n+                                                  : any();\n+    }\n+    else if (schema_name_and_version == \"Box2d.1\")\n+    {\n+        Imath::V2d min, max;\n+        return _fetch(\"min\", &min) && _fetch(\"max\", &max)\n+                   ? any(Imath::Box2d(std::move(min), std::move(max)))\n+                   : any();\n+    }\n     else\n     {\n         std::string ref_id;",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineio/deserialization.cpp",
        "code_diff": "@@ -711,6 +724,18 @@\nSerializableObject::Reader::read(std::string const& key, AnyVector* value)\n     return _fetch(key, value);\n }\n \n+bool\n+SerializableObject::Reader::read(std::string const& key, Imath::V2d* value)\n+{\n+    return _fetch(key, value);\n+}\n+\n+bool\n+SerializableObject::Reader::read(std::string const& key, Imath::Box2d* value)\n+{\n+    return _fetch(key, value);\n+}\n+\n template <typename T>\n bool\n SerializableObject::Reader::_read_optional(",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineio/deserialization.cpp",
        "code_diff": "@@ -766,6 +791,13 @@\nSerializableObject::Reader::read(\n     return _read_optional(key, value);\n }\n \n+bool\n+SerializableObject::Reader::read(\n+    std::string const& key, optional<Imath::Box2d>* value)\n+{\n+    return _read_optional(key, value);\n+}\n+\n bool\n SerializableObject::Reader::read(std::string const& key, any* value)\n {",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineio/item.cpp",
        "code_diff": "@@ -12,11 +12,13 @@\nItem::Item(\n     optional<TimeRange> const&  source_range,\n     AnyDictionary const&        metadata,\n     std::vector<Effect*> const& effects,\n-    std::vector<Marker*> const& markers)\n+    std::vector<Marker*> const& markers,\n+    bool                        enabled)\n     : Parent(name, metadata)\n     , _source_range(source_range)\n     , _effects(effects.begin(), effects.end())\n     , _markers(markers.begin(), markers.end())\n+    , _enabled(enabled)\n {}\n \n Item::~Item()",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineio/item.cpp",
        "code_diff": "@@ -25,7 +27,7 @@\nItem::~Item()\n bool\n Item::visible() const\n {\n-    return true;\n+    return _enabled;\n }\n \n bool",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineio/item.cpp",
        "code_diff": "@@ -165,6 +167,7 @@\nItem::read_from(Reader& reader)\n     return reader.read_if_present(\"source_range\", &_source_range) &&\n            reader.read_if_present(\"effects\", &_effects) &&\n            reader.read_if_present(\"markers\", &_markers) &&\n+           reader.read_if_present(\"enabled\", &_enabled) &&\n            Parent::read_from(reader);\n }",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineio/mediaReference.cpp",
        "code_diff": "@@ -3,11 +3,13 @@\nnamespace opentimelineio { namespace OPENTIMELINEIO_VERSION {\n \n MediaReference::MediaReference(\n-    std::string const&         name,\n-    optional<TimeRange> const& available_range,\n-    AnyDictionary const&       metadata)\n+    std::string const&            name,\n+    optional<TimeRange> const&    available_range,\n+    AnyDictionary const&          metadata,\n+    optional<Imath::Box2d> const& available_image_bounds)\n     : Parent(name, metadata)\n     , _available_range(available_range)\n+    , _available_image_bounds(available_image_bounds)\n {}\n \n MediaReference::~MediaReference()",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineio/mediaReference.cpp",
        "code_diff": "@@ -23,6 +25,8 @@\nbool\n MediaReference::read_from(Reader& reader)\n {\n     return reader.read_if_present(\"available_range\", &_available_range) &&\n+           reader.read_if_present(\n+               \"available_image_bounds\", &_available_image_bounds) &&\n            Parent::read_from(reader);\n }",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineio/safely_typed_any.cpp",
        "code_diff": "@@ -56,6 +56,18 @@\ncreate_safely_typed_any(TimeTransform&& value)\n     return any(value);\n }\n \n+any\n+create_safely_typed_any(Imath::V2d&& value)\n+{\n+    return any(value);\n+}\n+\n+any\n+create_safely_typed_any(Imath::Box2d&& value)\n+{\n+    return any(value);\n+}\n+\n any\n create_safely_typed_any(AnyVector&& value)\n {",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -66,6 +66,7 @@\nclass Encoder\n     virtual void write_value(class TimeRange const& value)           = 0;\n     virtual void write_value(class TimeTransform const& value)       = 0;\n     virtual void write_value(struct SerializableObject::ReferenceId) = 0;\n+    virtual void write_value(Imath::Box2d const&)                    = 0;\n \n protected:\n     void _error(ErrorStatus const& error_status)",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -162,6 +163,10 @@\nclass CloningEncoder : public Encoder\n         _store(any(value));\n     }\n \n+    void write_value(Imath::V2d const& value) { _store(any(value)); }\n+\n+    void write_value(Imath::Box2d const& value) { _store(any(value)); }\n+\n     void start_array(size_t /* n */)\n     {\n         if (has_errored())",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -372,6 +377,38 @@\nclass JSONEncoder : public Encoder\n         _writer.EndObject();\n     }\n \n+    void write_value(Imath::V2d const& value)\n+    {\n+        _writer.StartObject();\n+\n+        _writer.Key(\"OTIO_SCHEMA\");\n+        _writer.String(\"V2d.1\");\n+\n+        _writer.Key(\"x\");\n+        _writer.Double(value.x);\n+\n+        _writer.Key(\"y\");\n+        _writer.Double(value.y);\n+\n+        _writer.EndObject();\n+    }\n+\n+    void write_value(Imath::Box2d const& value)\n+    {\n+        _writer.StartObject();\n+\n+        _writer.Key(\"OTIO_SCHEMA\");\n+        _writer.String(\"Box2d.1\");\n+\n+        _writer.Key(\"min\");\n+        write_value(value.min);\n+\n+        _writer.Key(\"max\");\n+        write_value(value.max);\n+\n+        _writer.EndObject();\n+    }\n+\n     void start_array(size_t) { _writer.StartArray(); }\n \n     void start_object() { _writer.StartObject(); }",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -440,6 +477,12 @@\nSerializableObject::Writer::_build_dispatch_tables()\n     wt[&typeid(TimeTransform)] = [this](any const& value) {\n         _encoder.write_value(any_cast<TimeTransform const&>(value));\n     };\n+    wt[&typeid(Imath::V2d)] = [this](any const& value) {\n+        _encoder.write_value(any_cast<Imath::V2d const&>(value));\n+    };\n+    wt[&typeid(Imath::Box2d)] = [this](any const& value) {\n+        _encoder.write_value(any_cast<Imath::Box2d const&>(value));\n+    };\n \n     /*\n      * These next recurse back through the Writer itself:",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -477,6 +520,8 @@\nSerializableObject::Writer::_build_dispatch_tables()\n     et[&typeid(TimeTransform)] = &_simple_any_comparison<TimeTransform>;\n     et[&typeid(SerializableObject::ReferenceId)] =\n         &_simple_any_comparison<SerializableObject::ReferenceId>;\n+    et[&typeid(Imath::V2d)]   = &_simple_any_comparison<Imath::V2d>;\n+    et[&typeid(Imath::Box2d)] = &_simple_any_comparison<Imath::Box2d>;\n \n     /*\n      * These next recurse back through the Writer itself:",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -631,6 +676,14 @@\nSerializableObject::Writer::write(\n     value ? _encoder.write_value(*value) : _encoder.write_null_value();\n }\n \n+void\n+SerializableObject::Writer::write(\n+    std::string const& key, optional<Imath::Box2d> value)\n+{\n+    _encoder_write_key(key);\n+    value ? _encoder.write_value(*value) : _encoder.write_null_value();\n+}\n+\n void\n SerializableObject::Writer::write(std::string const& key, TimeTransform value)\n {",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -716,6 +769,20 @@\nSerializableObject::Writer::write(\n #endif\n }\n \n+void\n+SerializableObject::Writer::write(std::string const& key, Imath::V2d value)\n+{\n+    _encoder_write_key(key);\n+    _encoder.write_value(value);\n+}\n+\n+void\n+SerializableObject::Writer::write(std::string const& key, Imath::Box2d value)\n+{\n+    _encoder_write_key(key);\n+    _encoder.write_value(value);\n+}\n+\n void\n SerializableObject::Writer::write(\n     std::string const& key, AnyDictionary const& value)",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineview/console.py",
        "code_diff": "@@ -28,7 +28,12 @@\nimport os\n import sys\n import argparse\n-from PySide2 import QtWidgets, QtGui\n+try:\n+    from PySide6 import QtWidgets, QtGui\n+    from PySide6.QtGui import QAction\n+except ImportError:\n+    from PySide2 import QtWidgets, QtGui\n+    from PySide2.QtWidgets import QAction\n \n import opentimelineio as otio\n import opentimelineio.console as otio_console",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineview/console.py",
        "code_diff": "@@ -148,11 +153,11 @@\ndef __init__(\n         # menu\n         menubar = self.menuBar()\n \n-        file_load = QtWidgets.QAction('Open...', menubar)\n+        file_load = QAction('Open...', menubar)\n         file_load.setShortcut(QtGui.QKeySequence.Open)\n         file_load.triggered.connect(self._file_load)\n \n-        exit_action = QtWidgets.QAction('Exit', menubar)\n+        exit_action = QAction('Exit', menubar)\n         exit_action.setShortcut(QtGui.QKeySequence.Quit)\n         exit_action.triggered.connect(self.close)",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineview/console.py",
        "code_diff": "@@ -234,7 +239,7 @@\ndef _create_navigation_menu(self, navigation_menu):\n \n         def __callback():\n             self._navigation_filter_callback(actions)\n-        navigation_menu.triggered[[QtWidgets.QAction]].connect(__callback)\n+        navigation_menu.triggered[[QAction]].connect(__callback)\n \n     def _navigation_filter_callback(self, filters):\n         nav_filter = 0",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineview/details_widget.py",
        "code_diff": "@@ -22,7 +22,10 @@\n# language governing permissions and limitations under the Apache License.\n #\n \n-from PySide2 import QtWidgets, QtGui, QtCore\n+try:\n+    from PySide6 import QtWidgets, QtGui, QtCore\n+except ImportError:\n+    from PySide2 import QtWidgets, QtGui, QtCore\n \n import opentimelineio as otio",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineview/timeline_widget.py",
        "code_diff": "@@ -22,7 +22,10 @@\n# language governing permissions and limitations under the Apache License.\n #\n \n-from PySide2 import QtGui, QtCore, QtWidgets\n+try:\n+    from PySide6 import QtGui, QtCore, QtWidgets\n+except ImportError:\n+    from PySide2 import QtGui, QtCore, QtWidgets\n from collections import OrderedDict, namedtuple\n \n import opentimelineio as otio",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineview/timeline_widget.py",
        "code_diff": "@@ -461,7 +464,7 @@\ndef mouseReleaseEvent(self, mouse_event):\n     def wheelEvent(self, event):\n         scale_by = 1.0 + float(event.delta()) / 1000\n         self.scale(scale_by, 1)\n-        zoom_level = 1.0 / self.matrix().m11()\n+        zoom_level = 1.0 / self.transform().m11()\n         track_widgets.CURRENT_ZOOM_LEVEL = zoom_level\n \n         # some items we do want to keep the same visual size. So we need to",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineview/track_widgets.py",
        "code_diff": "@@ -22,9 +22,14 @@\n# language governing permissions and limitations under the Apache License.\n #\n \n-from PySide2 import QtGui, QtCore, QtWidgets\n+try:\n+    from PySide6 import QtGui, QtCore, QtWidgets\n+    from PySide6.QtGui import QFontMetrics\n+except ImportError:\n+    from PySide2 import QtGui, QtCore, QtWidgets\n+    from PySide2.QtGui import QFontMetrics\n+\n import opentimelineio as otio\n-from PySide2.QtGui import QFontMetrics\n \n TIME_SLIDER_HEIGHT = 20\n MEDIA_TYPE_SEPARATOR_HEIGHT = 5",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineview/track_widgets.py",
        "code_diff": "@@ -258,7 +263,12 @@\ndef __init__(self, track, rect, *args, **kwargs):\n         self.font = self.source_name_label.font()\n         self.short_width = TRACK_NAME_WIDGET_WIDTH\n         font_metrics = QFontMetrics(self.font)\n-        self.full_width = font_metrics.width(self.full_track_name) + 40\n+        self.full_width = font_metrics.horizontalAdvance(self.full_track_name) + 40\n+\n+        if not self.track.enabled:\n+            self.setBrush(\n+                QtGui.QBrush(QtGui.QColor(100, 100, 100, 255))\n+            )\n \n     def mouseDoubleClickEvent(self, event):\n         super(TrackNameItem, self).mouseDoubleClickEvent(event)",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/opentimelineview/track_widgets.py",
        "code_diff": "@@ -412,7 +422,8 @@\nclass ClipItem(BaseItem):\n \n     def __init__(self, *args, **kwargs):\n         super(ClipItem, self).__init__(*args, **kwargs)\n-        self.setBrush(QtGui.QBrush(QtGui.QColor(168, 197, 255, 255)))\n+        self.setBrush(QtGui.QBrush(QtGui.QColor(168, 197, 255, 255) if self.item.enabled\n+                                   else QtGui.QColor(100, 100, 100, 255)))\n         self.source_name_label.setText(self.item.name)",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentime-bindings/opentime_rationalTime.cpp",
        "code_diff": "@@ -50,7 +50,7 @@\nRationalTime _type_checked(py::object const& rhs, char const* op) {\n         return py::cast<RationalTime>(rhs);\n     }\n     catch (...) {\n-        std::string rhs_type = py::cast<std::string>(rhs.get_type().attr(\"__name__\"));\n+        std::string rhs_type = py::cast<std::string>(py::type::of(rhs).attr(\"__name__\"));\n         throw py::type_error(string_printf(\"unsupported operand type(s) for %s: \"\n                                            \"RationalTime and %s\", op, rhs_type.c_str()));\n     }",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentime-bindings/opentime_rationalTime.cpp",
        "code_diff": "@@ -71,9 +71,9 @@\nvoid opentime_rationalTime_bindings(py::module m) {\n         .def(\"value_rescaled_to\", (double (RationalTime::*)(RationalTime) const) &RationalTime::value_rescaled_to,\n              \"other\"_a, R\"docstring(Returns the time value for self converted to new_rate.)docstring\")\n         .def(\"almost_equal\", &RationalTime::almost_equal, \"other\"_a, \"delta\"_a = 0)\n-        .def(\"__copy__\", [](RationalTime rt, py::object) {\n+        .def(\"__copy__\", [](RationalTime rt) {\n                 return rt;\n-            }, \"copier\"_a = py::none())\n+            })\n         .def(\"__deepcopy__\", [](RationalTime rt, py::object) {\n                 return rt;\n             }, \"copier\"_a = py::none())",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentimelineio-bindings/otio_serializableObjects.cpp",
        "code_diff": "@@ -29,6 +29,8 @@\n#include \"otio_utils.h\"\n #include \"otio_anyDictionary.h\"\n \n+#include \"ImathBox.h\"\n+\n namespace py = pybind11;\n using namespace pybind11::literals;",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentimelineio-bindings/otio_serializableObjects.cpp",
        "code_diff": "@@ -290,21 +292,25 @@\nstatic void define_bases2(py::module m) {\n         .def(\"children_if\", [](SerializableCollection* t, py::object descended_from_type, optional<TimeRange> const& search_range) {\n                 return children_if(t, descended_from_type, search_range);\n             }, \"descended_from_type\"_a = py::none(), \"search_range\"_a = nullopt);\n+\n }\n \n static void define_items_and_compositions(py::module m) {\n     py::class_<Item, Composable, managing_ptr<Item>>(m, \"Item\", py::dynamic_attr())\n         .def(py::init([](std::string name, optional<TimeRange> source_range,\n-                         py::object effects, py::object markers, py::object metadata) {\n+                         py::object effects, py::object markers, py::bool_ enabled, py::object metadata) {\n                           return new Item(name, source_range,\n                                           py_to_any_dictionary(metadata),\n                                           py_to_vector<Effect*>(effects),\n-                                          py_to_vector<Marker*>(markers)); }),\n+                                          py_to_vector<Marker*>(markers),\n+                                          enabled); }),\n              py::arg_v(\"name\"_a = std::string()),\n              \"source_range\"_a = nullopt,\n              \"effects\"_a = py::none(),\n              \"markers\"_a = py::none(),\n+             \"enabled\"_a = true,\n              py::arg_v(\"metadata\"_a = py::none()))\n+        .def_property(\"enabled\", &Item::enabled, &Item::set_enabled, \"If true, an Item contributes to compositions. Analogous to Mute in various NLEs.\")\n         .def_property(\"source_range\", &Item::source_range, &Item::set_source_range)\n         .def(\"available_range\", [](Item* item) {\n             return item->available_range(ErrorStatusHandler());",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentimelineio-bindings/otio_serializableObjects.cpp",
        "code_diff": "@@ -335,7 +341,10 @@\nstatic void define_items_and_compositions(py::module m) {\n             }, \"time\"_a, \"to_item\"_a)\n         .def(\"transformed_time_range\", [](Item* item, TimeRange time_range, Item* to_item) {\n             return item->transformed_time_range(time_range, to_item, ErrorStatusHandler());\n-            }, \"time_range\"_a, \"to_item\"_a);\n+            }, \"time_range\"_a, \"to_item\"_a)\n+        .def_property_readonly(\"available_image_bounds\", [](Item* item) {\n+            return item->available_image_bounds(ErrorStatusHandler());\n+            });\n \n     auto transition_class =\n         py::class_<Transition, Composable, managing_ptr<Transition>>(m, \"Transition\", py::dynamic_attr())",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentimelineio-bindings/otio_serializableObjects.cpp",
        "code_diff": "@@ -465,6 +474,7 @@\nstatic void define_items_and_compositions(py::module m) {\n                 auto result = c->handles_of_child(child, ErrorStatusHandler());\n                 return py::make_tuple(py::cast(result.first), py::cast(result.second));\n             }, \"child_a\")\n+        .def(\"has_clips\", &Composition::has_clips)\n         .def(\"__internal_getitem__\", [](Composition* c, int index) {\n                 index = adjusted_vector_index(index, c->children());\n                 if (index < 0 || index >= int(c->children().size())) {",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentimelineio-bindings/otio_serializableObjects.cpp",
        "code_diff": "@@ -640,28 +650,35 @@\nstatic void define_media_references(py::module m) {\n                managing_ptr<MediaReference>>(m, \"MediaReference\", py::dynamic_attr())\n         .def(py::init([](std::string name,\n                          optional<TimeRange> available_range,\n-                         py::object metadata) {\n-                          return new MediaReference(name, available_range, py_to_any_dictionary(metadata)); }),\n+                         py::object metadata,\n+                         optional<Imath::Box2d> const& available_image_bounds) {\n+                          return new MediaReference(name, available_range, py_to_any_dictionary(metadata), available_image_bounds); }),\n              py::arg_v(\"name\"_a = std::string()),\n              \"available_range\"_a = nullopt,\n-             py::arg_v(\"metadata\"_a = py::none()))\n+             py::arg_v(\"metadata\"_a = py::none()),\n+             \"available_image_bounds\"_a = nullopt)\n+\n         .def_property(\"available_range\", &MediaReference::available_range, &MediaReference::set_available_range)\n+        .def_property(\"available_image_bounds\", &MediaReference::available_image_bounds, &MediaReference::set_available_image_bounds) \n         .def_property_readonly(\"is_missing_reference\", &MediaReference::is_missing_reference);\n \n     py::class_<GeneratorReference, MediaReference,\n                managing_ptr<GeneratorReference>>(m, \"GeneratorReference\", py::dynamic_attr())\n         .def(py::init([](std::string name, std::string generator_kind,\n                          optional<TimeRange> const& available_range,\n-                         py::object parameters, py::object metadata) {\n+                         py::object parameters, py::object metadata,\n+                         optional<Imath::Box2d> const& available_image_bounds) {\n                           return new GeneratorReference(name, generator_kind,\n                                                         available_range,\n                                                         py_to_any_dictionary(parameters),\n-                                                        py_to_any_dictionary(metadata)); }),\n+                                                        py_to_any_dictionary(metadata),\n+                                                        available_image_bounds); }),\n              py::arg_v(\"name\"_a = std::string()),\n              \"generator_kind\"_a = std::string(),\n              \"available_range\"_a = nullopt,\n              \"parameters\"_a = py::none(),\n-             py::arg_v(\"metadata\"_a = py::none()))\n+             py::arg_v(\"metadata\"_a = py::none()),\n+             \"available_image_bounds\"_a = nullopt)\n         .def_property(\"generator_kind\", &GeneratorReference::generator_kind, &GeneratorReference::set_generator_kind)\n         .def_property_readonly(\"parameters\", [](GeneratorReference* g) {\n                 auto ptr = g->parameters().get_or_create_mutation_stamp();",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentimelineio-bindings/otio_serializableObjects.cpp",
        "code_diff": "@@ -673,28 +690,34 @@\nstatic void define_media_references(py::module m) {\n         .def(py::init([](\n                         py::object name,\n                         optional<TimeRange> available_range,\n-                        py::object metadata) {\n+                        py::object metadata,\n+                        optional<Imath::Box2d> const& available_image_bounds) {\n                     return new MissingReference(\n                                   string_or_none_converter(name),\n                                   available_range,\n-                                  py_to_any_dictionary(metadata)); \n+                                  py_to_any_dictionary(metadata),\n+                                  available_image_bounds); \n                     }),\n              py::arg_v(\"name\"_a = std::string()),\n              \"available_range\"_a = nullopt,\n-             py::arg_v(\"metadata\"_a = py::none()));\n+             py::arg_v(\"metadata\"_a = py::none()),\n+             \"available_image_bounds\"_a = nullopt);\n \n \n     py::class_<ExternalReference, MediaReference,\n                managing_ptr<ExternalReference>>(m, \"ExternalReference\", py::dynamic_attr())\n         .def(py::init([](std::string target_url,\n                          optional<TimeRange> const& available_range,\n-                         py::object metadata) {\n+                         py::object metadata,\n+                         optional<Imath::Box2d> const& available_image_bounds) {\n                           return new ExternalReference(target_url,\n                                                         available_range,\n-                                                        py_to_any_dictionary(metadata)); }),\n+                                                        py_to_any_dictionary(metadata),\n+                                                        available_image_bounds); }),\n              \"target_url\"_a = std::string(),\n              \"available_range\"_a = nullopt,\n-             py::arg_v(\"metadata\"_a = py::none()))\n+             py::arg_v(\"metadata\"_a = py::none()),\n+             \"available_image_bounds\"_a = nullopt)\n         .def_property(\"target_url\", &ExternalReference::target_url, &ExternalReference::set_target_url);\n \n     auto imagesequencereference_class = py:: class_<ImageSequenceReference, MediaReference,",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentimelineio-bindings/otio_serializableObjects.cpp",
        "code_diff": "@@ -775,7 +798,8 @@\nNegative ``start_frame`` is also handled. The above example with a ``start_frame\n                          int frame_zero_padding,\n                          ImageSequenceReference::MissingFramePolicy const missing_frame_policy,\n                          optional<TimeRange> const& available_range,\n-                         py::object metadata) {\n+                         py::object metadata,\n+                         optional<Imath::Box2d> const& available_image_bounds) {\n                           return new ImageSequenceReference(target_url_base,\n                                                             name_prefix,\n                                                             name_suffix,",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentimelineio-bindings/otio_serializableObjects.cpp",
        "code_diff": "@@ -785,7 +809,8 @@\nNegative ``start_frame`` is also handled. The above example with a ``start_frame\n                                                             frame_zero_padding,\n                                                             missing_frame_policy,\n                                                             available_range,\n-                                                            py_to_any_dictionary(metadata)); }),\n+                                                            py_to_any_dictionary(metadata),\n+                                                            available_image_bounds); }),\n                         \"target_url_base\"_a = std::string(),\n                         \"name_prefix\"_a = std::string(),\n                         \"name_suffix\"_a = std::string(),",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -924,9 +924,9 @@\ndef write_to_string(input_otio, rate=None, style='avid', reelname_len=8):\n     # also only works for a single video track at the moment\n \n     video_tracks = [t for t in input_otio.tracks\n-                    if t.kind == schema.TrackKind.Video]\n+                    if t.kind == schema.TrackKind.Video and t.enabled]\n     audio_tracks = [t for t in input_otio.tracks\n-                    if t.kind == schema.TrackKind.Audio]\n+                    if t.kind == schema.TrackKind.Audio and t.enabled]\n \n     if len(video_tracks) != 1:\n         raise exceptions.NotSupportedError(",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentimelineio/adapters/svg.py",
        "code_diff": "@@ -519,7 +519,7 @@\nclass ClipData(object):\n \n     def __init__(self, src_start=0.0, src_end=0.0, avlbl_start=0.0,\n                  avlbl_end=0.0, avlbl_duration=0.0,\n-                 trim_start=0.0, trim_duration=0.0, target_url='', clip_id=0,\n+                 trim_start=0.0, trim_duration=0.0, clip_id=0,\n                  transition_begin=None, transition_end=None):\n         self.src_start = src_start\n         self.src_end = src_end",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentimelineio/adapters/svg.py",
        "code_diff": "@@ -528,7 +528,6 @@\ndef __init__(self, src_start=0.0, src_end=0.0, avlbl_start=0.0,\n         self.avlbl_duration = avlbl_duration\n         self.trim_start = trim_start\n         self.trim_duration = trim_duration\n-        self.target_url = target_url\n         self.clip_id = clip_id\n         self.transition_begin = transition_begin\n         self.transition_end = transition_end",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentimelineio/adapters/svg.py",
        "code_diff": "@@ -587,7 +586,7 @@\ndef _draw_timeline(timeline, svg_writer, extra_data=()):\n                 clip_data = ClipData(src_start, src_end, avlbl_start,\n                                      avlbl_end, avlbl_duration, trim_start,\n                                      trim_duration,\n-                                     item.media_reference.target_url, clip_count - 1)\n+                                     clip_count - 1)\n                 if current_transition is not None:\n                     clip_data.transition_begin = current_transition\n                     current_transition = None",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentimelineio/adapters/svg.py",
        "code_diff": "@@ -808,7 +807,7 @@\ndef _draw_track(track, svg_writer, extra_data=()):\n             draw_item(item, svg_writer, (clips_data[item_count],))\n             item_count += 1\n         elif isinstance(item, otio.schema.Transition):\n-            cut_x = svg_writer.x_origin + (clips_data[item_count].src_start *\n+            cut_x = svg_writer.x_origin + (clips_data[clip_count].src_start *\n                                            svg_writer.scale_x)\n             draw_item(item, svg_writer, (cut_x,))\n             transition_count += 1",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentimelineio/core/item.py",
        "code_diff": "@@ -4,12 +4,13 @@\n@add_method(_otio.Item)\n def __str__(self):\n-    return \"{}({}, {}, {}, {}, {})\".format(\n+    return \"{}({}, {}, {}, {}, {}, {})\".format(\n         self.__class__.__name__,\n         self.name,\n         str(self.source_range),\n         str(self.effects),\n         str(self.markers),\n+        str(self.enabled),\n         str(self.metadata)\n     )",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentimelineio/core/item.py",
        "code_diff": "@@ -22,6 +23,7 @@\ndef __repr__(self):\n         \"source_range={}, \"\n         \"effects={}, \"\n         \"markers={}, \"\n+        \"enabled={}, \"\n         \"metadata={}\"\n         \")\".format(\n             \"core\" if self.__class__ is _otio.Item else \"schema\",",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentimelineio/core/mediaReference.py",
        "code_diff": "@@ -4,10 +4,11 @@\n@add_method(_otio.MediaReference)\n def __str__(self):\n-    return \"{}({}, {}, {})\".format(\n+    return \"{}({}, {}, {}, {})\".format(\n         self.__class__.__name__,\n         repr(self.name),\n         repr(self.available_range),\n+        repr(self.available_image_bounds),\n         repr(self.metadata)\n     )",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentimelineio/schema/__init__.py",
        "code_diff": "@@ -27,6 +27,7 @@\n\"\"\"User facing classes.\"\"\"\n \n from .. _otio import (\n+    Box2d,\n     Clip,\n     Effect,\n     TimeEffect,",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentimelineio/schema/__init__.py",
        "code_diff": "@@ -42,7 +43,8 @@\nStack,\n     Timeline,\n     Track,\n-    Transition\n+    Transition,\n+    V2d,\n )\n \n MarkerColor = Marker.Color",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentimelineio/schema/__init__.py",
        "code_diff": "@@ -55,6 +57,7 @@\n)\n \n from . import (\n+    box2d,\n     clip,\n     effect,\n     external_reference,",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentimelineio/schema/generator_reference.py",
        "code_diff": "@@ -4,10 +4,11 @@\n@add_method(_otio.GeneratorReference)\n def __str__(self):\n-    return 'GeneratorReference(\"{}\", \"{}\", {}, {})'.format(\n+    return 'GeneratorReference(\"{}\", \"{}\", {}, {}, {})'.format(\n         self.name,\n         self.generator_kind,\n         self.parameters,\n+        self.available_image_bounds,\n         self.metadata\n     )",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentimelineio/schema/image_sequence_reference.py",
        "code_diff": "@@ -6,7 +6,7 @@\ndef __str__(self):\n     return (\n         'ImageSequenceReference('\n-        '\"{}\", \"{}\", \"{}\", {}, {}, {}, {}, {}, {}, {})' .format(\n+        '\"{}\", \"{}\", \"{}\", {}, {}, {}, {}, {}, {}, {}, {})' .format(\n             self.target_url_base,\n             self.name_prefix,\n             self.name_suffix,",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentimelineio/schema/image_sequence_reference.py",
        "code_diff": "@@ -16,6 +16,7 @@\ndef __str__(self):\n             self.frame_zero_padding,\n             self.missing_frame_policy,\n             self.available_range,\n+            self.available_image_bounds,\n             self.metadata,\n         )\n     )",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "src/py-opentimelineio/opentimelineio/schema/image_sequence_reference.py",
        "code_diff": "@@ -34,6 +35,7 @@\ndef __repr__(self):\n         'frame_zero_padding={}, '\n         'missing_frame_policy={}, '\n         'available_range={}, '\n+        'available_image_bounds={}, '\n         'metadata={}'\n         ')' .format(\n             repr(self.target_url_base),",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "tests/test_clip.py",
        "code_diff": "@@ -62,7 +62,7 @@\ndef test_str(self):\n \n         self.assertMultiLineEqual(\n             str(cl),\n-            'Clip(\"test_clip\", MissingReference(\\'\\', None, {}), None, {})'\n+            'Clip(\"test_clip\", MissingReference(\\'\\', None, None, {}), None, {})'\n         )\n         self.assertMultiLineEqual(\n             repr(cl),",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "tests/test_cmx_3600_adapter.py",
        "code_diff": "@@ -61,6 +61,7 @@\n)\n MULTIPLE_TARGET_AUDIO_PATH = os.path.join(SAMPLE_DATA_DIR, \"multi_audio.edl\")\n TRANSITION_DURATION_TEST = os.path.join(SAMPLE_DATA_DIR, \"transition_duration.edl\")\n+ENABLED_TEST = os.path.join(SAMPLE_DATA_DIR, \"enabled.otio\")\n \n \n class EDLAdapterTest(unittest.TestCase, otio_test_utils.OTIOAssertions):",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "tests/test_composition.py",
        "code_diff": "@@ -377,6 +377,42 @@\ndef test_insert_slice(self):\n         self.assertNotIn(cl, st)\n         self.assertIn(cl2, st)\n \n+    def test_has_clip(self):\n+        st = otio.schema.Stack(name=\"ST\")\n+\n+        tr1 = otio.schema.Track(name=\"tr1\")\n+        st.append(tr1)\n+\n+        self.assertFalse(st.has_clips())\n+        self.assertFalse(tr1.has_clips())\n+\n+        c1 = otio.schema.Clip(name=\"c1\")\n+        tr1.append(c1)\n+\n+        self.assertTrue(st.has_clips())\n+        self.assertTrue(tr1.has_clips())\n+\n+        tr2 = otio.schema.Track(name=\"tr2\")\n+        st.append(tr2)\n+\n+        self.assertTrue(st.has_clips())\n+        self.assertTrue(tr1.has_clips())\n+        self.assertFalse(tr2.has_clips())\n+\n+        g1 = otio.schema.Gap(name=\"g1\")\n+        tr2.append(g1)\n+\n+        self.assertTrue(st.has_clips())\n+        self.assertTrue(tr1.has_clips())\n+        self.assertFalse(tr2.has_clips())\n+\n+        c2 = otio.schema.Clip(name=\"c2\")\n+        tr2.append(c2)\n+\n+        self.assertTrue(st.has_clips())\n+        self.assertTrue(tr1.has_clips())\n+        self.assertTrue(tr2.has_clips())\n+\n \n class StackTest(unittest.TestCase, otio_test_utils.OTIOAssertions):",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "tests/test_generator_reference.py",
        "code_diff": "@@ -24,7 +24,11 @@\ndef setUp(self):\n             },\n             metadata={\n                 \"foo\": \"bar\"\n-            }\n+            },\n+            available_image_bounds=otio.schema.Box2d(\n+                otio.schema.V2d(0.0, 0.0),\n+                otio.schema.V2d(16.0, 9.0)\n+            )\n         )\n \n     def test_constructor(self):",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "tests/test_generator_reference.py",
        "code_diff": "@@ -39,6 +43,13 @@\ndef test_constructor(self):\n                 otio.opentime.RationalTime(100, 24),\n             )\n         )\n+        self.assertEqual(\n+            self.gen.available_image_bounds,\n+            otio.schema.Box2d(\n+                otio.schema.V2d(0.0, 0.0),\n+                otio.schema.V2d(16.0, 9.0)\n+            )\n+        )\n \n     def test_serialize(self):\n         encoded = otio.adapters.otio_json.write_to_string(self.gen)",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "tests/test_generator_reference.py",
        "code_diff": "@@ -59,11 +70,13 @@\ndef test_stringify(self):\n             '\"{}\", '\n             '\"{}\", '\n             '{}, '\n+            '{}, '\n             \"{}\"\n             \")\".format(\n                 str(self.gen.name),\n                 str(self.gen.generator_kind),\n                 str(self.gen.parameters),\n+                str(self.gen.available_image_bounds),\n                 str(self.gen.metadata),\n             )\n         )",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "tests/test_image_sequence_reference.py",
        "code_diff": "@@ -64,6 +64,10 @@\ndef test_str(self):\n                 otio.opentime.RationalTime(60, 30),\n             ),\n             metadata={\"custom\": {\"foo\": \"bar\"}},\n+            available_image_bounds=otio.schema.Box2d(\n+                otio.schema.V2d(0.0, 0.0),\n+                otio.schema.V2d(16.0, 9.0)\n+            ),\n         )\n         self.assertEqual(\n             str(ref),",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "tests/test_image_sequence_reference.py",
        "code_diff": "@@ -77,6 +81,7 @@\ndef test_str(self):\n             '5, '\n             'MissingFramePolicy.error, '\n             'TimeRange(RationalTime(0, 30), RationalTime(60, 30)), '\n+            'Box2d(V2d(0.0, 0.0), V2d(16.0, 9.0)), '\n             \"{'custom': {'foo': 'bar'}}\"\n             ')'\n         )",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "tests/test_image_sequence_reference.py",
        "code_diff": "@@ -95,6 +100,10 @@\ndef test_repr(self):\n                 otio.opentime.RationalTime(0, 30),\n                 otio.opentime.RationalTime(60, 30),\n             ),\n+            available_image_bounds=otio.schema.Box2d(\n+                otio.schema.V2d(0.0, 0.0),\n+                otio.schema.V2d(16.0, 9.0)\n+            ),\n             metadata={\"custom\": {\"foo\": \"bar\"}},\n         )\n         ref_value = (",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "tests/test_image_sequence_reference.py",
        "code_diff": "@@ -108,6 +117,9 @@\ndef test_repr(self):\n             'frame_zero_padding=5, '\n             'missing_frame_policy=<MissingFramePolicy.error: 0>, '\n             'available_range={}, '\n+            'available_image_bounds=otio.schema.Box2d('\n+            'min=otio.schema.V2d(x=0.0, y=0.0), '\n+            'max=otio.schema.V2d(x=16.0, y=9.0)), '\n             \"metadata={{'custom': {{'foo': 'bar'}}}}\"\n             ')'.format(repr(ref.available_range))\n         )",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "tests/test_item.py",
        "code_diff": "@@ -44,6 +44,7 @@\ndef test_str_gap(self):\n             str(gp.source_range) + \", \" +\n             str(gp.effects) + \", \" +\n             str(gp.markers) + \", \" +\n+            str(gp.enabled) + \", \" +\n             str(gp.metadata) +\n             \")\"\n         )",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "tests/test_item.py",
        "code_diff": "@@ -54,12 +55,14 @@\ndef test_str_gap(self):\n             \"source_range={}, \"\n             \"effects={}, \"\n             \"markers={}, \"\n+            \"enabled={}, \"\n             \"metadata={}\"\n             \")\".format(\n                 repr(gp.name),\n                 repr(gp.source_range),\n                 repr(gp.effects),\n                 repr(gp.markers),\n+                repr(gp.enabled),\n                 repr(gp.metadata),\n             )\n         )",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "tests/test_item.py",
        "code_diff": "@@ -211,12 +214,14 @@\ndef test_stringify(self):\n             \"{}, \"\n             \"{}, \"\n             \"{}, \"\n+            \"{}, \"\n             \"{}\"\n             \")\".format(\n                 str(it.name),\n                 str(it.source_range),\n                 str(it.effects),\n                 str(it.markers),\n+                str(it.enabled),\n                 str(it.metadata),\n             )\n         )",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "tests/test_item.py",
        "code_diff": "@@ -228,12 +233,14 @@\ndef test_stringify(self):\n             \"source_range={}, \"\n             \"effects={}, \"\n             \"markers={}, \"\n+            \"enabled={}, \"\n             \"metadata={}\"\n             \")\".format(\n                 repr(it.name),\n                 repr(it.source_range),\n                 repr(it.effects),\n                 repr(it.markers),\n+                repr(it.enabled),\n                 repr(it.metadata),\n             )\n         )",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "tests/test_opentime.py",
        "code_diff": "@@ -685,6 +685,29 @@\ndef test_passing_ndf_tc_at_df_rate(self):\n         t2 = otio.opentime.from_timecode(NDF_TC, 29.97)\n         self.assertEqual(t2.value, frames)\n \n+    def test_nearest_valid_timecode_rate(self):\n+        invalid_valid_rates = (\n+            (23.97602397602397, 24000.0 / 1001.0),\n+            (23.97, 24000.0 / 1001.0),\n+            (23.976, 24000.0 / 1001.0),\n+            (23.98, 24000.0 / 1001.0),\n+            (29.97, 30000.0 / 1001.0),\n+            (59.94, 60000.0 / 1001.0),\n+        )\n+\n+        for invalid_rate, nearest_valid_rate in invalid_valid_rates:\n+            self.assertTrue(\n+                otio.opentime.RationalTime.is_valid_timecode_rate(\n+                    nearest_valid_rate\n+                )\n+            )\n+            self.assertEqual(\n+                otio.opentime.RationalTime.nearest_valid_timecode_rate(\n+                    invalid_rate\n+                ),\n+                nearest_valid_rate,\n+            )\n+\n \n class TestTimeTransform(unittest.TestCase):",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "C++ tests",
        "pr_number": 1161,
        "file_name": "tests/test_stack_algo.py",
        "code_diff": "@@ -75,6 +75,79 @@\ndef setUp(self):\n         }\n         \"\"\", \"otio_json\")\n \n+        self.trackZd = otio.adapters.read_from_string(\"\"\"\n+        {\n+            \"OTIO_SCHEMA\": \"Track.1\",\n+            \"children\": [\n+                {\n+                    \"OTIO_SCHEMA\": \"Clip.1\",\n+                    \"effects\": [],\n+                    \"markers\": [],\n+                    \"media_reference\": null,\n+                    \"enabled\": false,\n+                    \"metadata\": {},\n+                    \"name\": \"Z\",\n+                    \"source_range\": {\n+                        \"OTIO_SCHEMA\": \"TimeRange.1\",\n+                        \"duration\": {\n+                            \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                            \"rate\": 24,\n+                            \"value\": 150\n+                        },\n+                        \"start_time\": {\n+                            \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                            \"rate\": 24,\n+                            \"value\": 0.0\n+                        }\n+                    }\n+                }\n+            ],\n+            \"effects\": [],\n+            \"kind\": \"Video\",\n+            \"markers\": [],\n+            \"metadata\": {},\n+            \"name\": \"Sequence1\",\n+            \"source_range\": null\n+        }\n+        \"\"\", \"otio_json\")\n+\n+        self.track_d = otio.adapters.read_from_string(\"\"\"\n+        {\n+            \"OTIO_SCHEMA\": \"Track.1\",\n+            \"children\": [\n+                {\n+                    \"OTIO_SCHEMA\": \"Clip.1\",\n+                    \"effects\": [],\n+                    \"markers\": [],\n+                    \"media_reference\": null,\n+                    \"enabled\": true,\n+                    \"metadata\": {},\n+                    \"name\": \"Z\",\n+                    \"source_range\": {\n+                        \"OTIO_SCHEMA\": \"TimeRange.1\",\n+                        \"duration\": {\n+                            \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                            \"rate\": 24,\n+                            \"value\": 150\n+                        },\n+                        \"start_time\": {\n+                            \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                            \"rate\": 24,\n+                            \"value\": 0.0\n+                        }\n+                    }\n+                }\n+            ],\n+            \"effects\": [],\n+            \"kind\": \"Video\",\n+            \"markers\": [],\n+            \"enabled\": false,\n+            \"metadata\": {},\n+            \"name\": \"Sequence1\",\n+            \"source_range\": null\n+        }\n+        \"\"\", \"otio_json\")\n+\n         self.trackABC = otio.adapters.read_from_string(\"\"\"\n         {\n             \"OTIO_SCHEMA\": \"Track.1\",",
        "comments": [],
        "commit_message": "Merge branch 'main' into cxx_tests",
        "commit_id": "cf32715c7d20b446f70741d3f0ee32ab919bfa82"
    },
    {
        "pr_title": "Fix RV plugin test for Mac",
        "pr_number": 1149,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/tests/test_rv_reader.py",
        "code_diff": "@@ -38,6 +38,7 @@\nimport shlex\n import time\n import imp\n+import platform\n from subprocess import call, Popen, PIPE\n \n import opentimelineio as otio",
        "comments": [],
        "commit_message": "Fix RV plugin test on Mac",
        "commit_id": "142bd539ae845cd5536b5e116125ede271078c09"
    },
    {
        "pr_title": "Fix RV plugin test for Mac",
        "pr_number": 1149,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/tests/test_rv_reader.py",
        "code_diff": "@@ -46,7 +47,10 @@\nRV_OTIO_READER_VERSION = '1.0'\n \n RV_ROOT_DIR = os.getenv('OTIO_RV_ROOT_DIR', '')\n-RV_BIN_DIR = os.path.join(RV_ROOT_DIR, 'bin')\n+RV_BIN_DIR = os.path.join(\n+    RV_ROOT_DIR,\n+    'MacOS' if platform.system() == 'Darwin' else 'bin'\n+)\n \n RV_OTIO_READER_DIR = os.path.join(\n     '..',",
        "comments": [],
        "commit_message": "Fix RV plugin test on Mac",
        "commit_id": "142bd539ae845cd5536b5e116125ede271078c09"
    },
    {
        "pr_title": "Fix RV plugin test for Mac",
        "pr_number": 1149,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/tests/test_rv_reader.py",
        "code_diff": "@@ -146,7 +150,7 @@\ndef test_install_plugin(self):\n             'I L - {version} \"{package_name}\" {pkg_path}'.format(\n                 version=RV_OTIO_READER_VERSION,\n                 package_name=RV_OTIO_READER_NAME,\n-                pkg_path=installed_package_path\n+                pkg_path=os.path.realpath(installed_package_path)\n             )\n \n         self.assertIn(desired_result, stdout.split('\\n'))",
        "comments": [],
        "commit_message": "Fix RV plugin test on Mac",
        "commit_id": "142bd539ae845cd5536b5e116125ede271078c09"
    },
    {
        "pr_title": "Fix RV plugin test for Mac",
        "pr_number": 1149,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/tests/test_rv_reader.py",
        "code_diff": "@@ -171,13 +175,14 @@\ndef test_read_otio_file(self):\n             delete=False\n         )\n         otio.adapters.write_to_file(sample_timeline, sample_file.name)\n-        run_cmd = '{root}/rv ' \\\n+        run_cmd = '{root}/{exe} ' \\\n                   '-nc ' \\\n                   '-network ' \\\n                   '-networkHost localhost ' \\\n                   '-networkPort {port} ' \\\n                   '{sample_file}' \\\n                   .format(\n+                      exe='RV' if platform.system() == 'Darwin' else 'rv',\n                       root=RV_BIN_DIR,\n                       port=9876,\n                       sample_file=sample_file.name",
        "comments": [],
        "commit_message": "Fix RV plugin test on Mac",
        "commit_id": "142bd539ae845cd5536b5e116125ede271078c09"
    },
    {
        "pr_title": "Docs: Line wrap long lines of text for a display",
        "pr_number": 1140,
        "file_name": "src/py-opentimelineio/opentimelineio/console/autogen_serialized_datamodel.py",
        "code_diff": "@@ -294,7 +294,7 @@\ndef _write_documentation(model):\n             modname = this_mod\n             label = model[cl][\"OTIO_SCHEMA\"]\n \n-            if(cl.__doc__ is not None):\n+            if (cl.__doc__ is not None):\n                 docstring = cl.__doc__.split(\"\\n\")\n                 new_docstring = []\n                 for line in docstring:",
        "comments": [
            {
                "comment": "```suggestion\r\n            if(cl.__doc__ is not None):\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n                    line = textwrap.wrap(\r\n                                    line, width=100, \r\n                                     expand_tabs=False, \r\n                                     replace_whitespace=False, \r\n                                     drop_whitespace=False, \r\n                                     break_long_words=False)\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n                    line = textwrap.wrap(line, width=100,\r\n                                         expand_tabs=False,\r\n                                         replace_whitespace=False,\r\n                                         drop_whitespace=False,\r\n                                         break_long_words=False)\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n                    for wrapped_line in line:\r\n                        new_docstring.append(wrapped_line)\r\n```",
                "position": null
            }
        ],
        "commit_message": "Fix lint errors\n\nSigned-off-by: Thomas Wilshaw <thomaswilshaw@gmail.com>",
        "commit_id": "f2cacb9bf509e2be50dcda9cdc771a6681f39a75",
        "fn_context": "def _write_documentation(model):\n    md_with_helpstrings = io.StringIO()\n    md_only_fields = io.StringIO()\n\n    md_with_helpstrings.write(DOCUMENT_HEADER)\n    md_only_fields.write(FIELDS_ONLY_HEADER)\n\n    modules = {}\n    for cl in model:\n        modobj = cl.__module__\n\n        if modobj in ['opentimelineio._opentime', 'opentimelineio._otio']:\n            modobj = _remap_to_python_modules(cl)\n\n        modules.setdefault(modobj, []).append(cl)\n\n    CURRENT_MODULE = None\n    for module_list in sorted(modules):\n        this_mod = \".\".join(module_list.split('.')[:2])\n        if this_mod != CURRENT_MODULE:\n            CURRENT_MODULE = this_mod\n            md_with_helpstrings.write(MODULE_HEADER.format(modname=this_mod))\n            md_only_fields.write(MODULE_HEADER.format(modname=this_mod))\n\n        # because these are classes, they need to sort on their stringified\n        # names\n        for cl in sorted(modules[module_list], key=lambda cl: str(cl)):\n            modname = this_mod\n            label = model[cl][\"OTIO_SCHEMA\"]\n\n            if (cl.__doc__ is not None):\n                docstring = cl.__doc__.split(\"\\n\")\n                new_docstring = []\n                for line in docstring:\n                    line = textwrap.wrap(line, width=100,\n                                         expand_tabs=False,\n                                         replace_whitespace=False,\n                                         drop_whitespace=False,\n                                         break_long_words=False)\n                    if (line == []):\n                        line = [\"\"]\n                    for wrapped_line in line:\n                        new_docstring.append(wrapped_line)\n                new_docstring = \"\\n\".join(new_docstring)\n            else:\n                new_docstring = cl.__doc__\n\n            md_with_helpstrings.write(\n                CLASS_HEADER_WITH_DOCS.format(\n                    classname=label,\n                    modpath=modname + \".\" + cl.__name__,\n                    docstring=new_docstring\n                )\n            )\n            md_only_fields.write(\n                CLASS_HEADER_ONLY_FIELDS.format(\n                    classname=label,\n                )\n            )\n\n            for key, helpstr in sorted(model[cl].items()):\n                if key in SKIP_KEYS:\n                    continue\n                md_with_helpstrings.write(\n                    PROP_HEADER.format(propkey=key, prophelp=helpstr)\n                )\n                md_only_fields.write(\n                    PROP_HEADER_NO_HELP.format(propkey=key)\n                )\n\n    return md_with_helpstrings.getvalue(), md_only_fields.getvalue()"
    },
    {
        "pr_title": "Unicode file names on Windows",
        "pr_number": 1081,
        "file_name": "src/opentimelineio/deserialization.cpp",
        "code_diff": "@@ -10,8 +10,15 @@\n#include <rapidjson/reader.h>\n #include <rapidjson/error/en.h>\n \n-#include <codecvt>\n-#include <locale>\n+#if defined(_WINDOWS)\n+#ifndef WIN32_LEAN_AND_MEAN\n+#define WIN32_LEAN_AND_MEAN\n+#endif // WIN32_LEAN_AND_MEAN\n+#ifndef NOMINMAX\n+#define NOMINMAX\n+#endif // NOMINMAX\n+#include <windows.h>\n+#endif\n \n namespace opentimelineio { namespace OPENTIMELINEIO_VERSION  {",
        "comments": [],
        "commit_message": "Use MultiByteToWideChar()",
        "commit_id": "0158c6f6076b1ebe3b34d55cdfb7a1e35293599d"
    },
    {
        "pr_title": "Unicode file names on Windows",
        "pr_number": 1081,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -8,9 +8,17 @@\n#include <rapidjson/prettywriter.h>\n #include <rapidjson/ostreamwrapper.h>\n \n-#include <codecvt>\n #include <fstream>\n-#include <locale>\n+\n+#if defined(_WINDOWS)\n+#ifndef WIN32_LEAN_AND_MEAN\n+#define WIN32_LEAN_AND_MEAN\n+#endif // WIN32_LEAN_AND_MEAN\n+#ifndef NOMINMAX\n+#define NOMINMAX\n+#endif // NOMINMAX\n+#include <windows.h>\n+#endif\n \n namespace opentimelineio { namespace OPENTIMELINEIO_VERSION  {",
        "comments": [],
        "commit_message": "Use MultiByteToWideChar()",
        "commit_id": "0158c6f6076b1ebe3b34d55cdfb7a1e35293599d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "examples/conform.cpp",
        "code_diff": "@@ -99,7 +99,7 @@\nint conform_timeline(\n     \n     otio::ErrorStatus error_status;\n     const auto clips = timeline->clip_if(&error_status);\n-    if (error_status)\n+    if (otio::is_error(error_status))\n     {\n         examples::print_error(error_status);\n         exit(1);",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "examples/conform.cpp",
        "code_diff": "@@ -139,7 +139,7 @@\nint main(int argc, char** argv)\n     otio::ErrorStatus error_status;\n     otio::SerializableObject::Retainer<otio::Timeline> timeline(\n         dynamic_cast<otio::Timeline*>(otio::Timeline::from_json_file(input, &error_status)));\n-    if (!timeline || error_status)\n+    if (!timeline || otio::is_error(error_status))\n     {\n         examples::print_error(error_status);\n         exit(1);",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "examples/flatten_video_tracks.cpp",
        "code_diff": "@@ -35,7 +35,7 @@\nint main(int argc, char** argv)\n     // into a single track.\n     std::cout << \"Flattening \" << video_tracks.size() << \" video tracks into one...\" << std::endl;\n     auto onetrack = otio::flatten_stack(video_tracks, &error_status);\n-    if (!onetrack)\n+    if (!onetrack or is_error(error_status))\n     {\n         examples::print_error(error_status);\n         return 1;",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "setup.py",
        "code_diff": "@@ -107,6 +107,12 @@\ndef generate_cmake_arguments(self):\n         if cxx_coverage:\n             cmake_args += ['-DOTIO_CXX_COVERAGE=1']\n \n+        # allow external arguments to cmake via the CMAKE_ARGS env var\n+        cmake_args += [\n+            arg for arg in os.environ.get(\"CMAKE_ARGS\", \"\").split(\" \")\n+            if arg\n+        ]\n+\n         return cmake_args\n \n     def cmake_preflight_check(self):",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "setup.py",
        "code_diff": "@@ -180,7 +186,7 @@\ndef cmake_install(self):\n \n # Metadata that gets stamped into the __init__ files during the build phase.\n PROJECT_METADATA = {\n-    \"version\": \"0.14.0.dev1\",\n+    \"version\": \"0.15.0.dev1\",\n     \"author\": 'Contributors to the OpenTimelineIO project',\n     \"author_email\": 'opentimelineio@pixar.com',\n     \"license\": 'Modified Apache 2.0 License',",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentime/rationalTime.cpp",
        "code_diff": "@@ -1,26 +1,25 @@\n#include \"opentime/rationalTime.h\"\n #include \"opentime/stringPrintf.h\"\n-#include <array>\n #include <algorithm>\n+#include <array>\n #include <ciso646>\n #include <cmath>\n #include <vector>\n \n-namespace opentime { namespace OPENTIME_VERSION  {\n-    \n-RationalTime RationalTime::_invalid_time {0, RationalTime::_invalid_rate};\n+namespace opentime { namespace OPENTIME_VERSION {\n \n-static constexpr std::array<double, 4> dropframe_timecode_rates\n-{{\n+RationalTime RationalTime::_invalid_time{ 0, RationalTime::_invalid_rate };\n+\n+static constexpr std::array<double, 4> dropframe_timecode_rates{ {\n     // 23.976,\n     // 23.98,\n     // 23.97,\n     // 24000.0/1001.0,\n     29.97,\n-    30000.0/1001.0,\n+    30000.0 / 1001.0,\n     59.94,\n-    60000.0/1001.0,\n-}};\n+    60000.0 / 1001.0,\n+} };\n \n // currently unused:\n /*",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentime/rationalTime.cpp",
        "code_diff": "@@ -38,98 +37,135 @@\nstatic constexpr std::array<double, 10> non_dropframe_timecode_rates\n }};\n */\n \n-static constexpr std::array<double, 16> valid_timecode_rates\n-{{  \n-    1.0,\n-    12.0,\n-    23.97,\n-    23.976,\n-    23.98,\n-    24000.0/1001.0,\n-    24.0,\n-    25.0,\n-    29.97,\n-    30000.0/1001.0,\n-    30.0,\n-    48.0,\n-    50.0,\n-    59.94,\n-    60000.0/1001.0,\n-    60.0\n-}};\n-\n-bool RationalTime::is_valid_timecode_rate(double fps) {\n-    auto b = valid_timecode_rates.begin(),\n-         e = valid_timecode_rates.end();\n+static constexpr std::array<double, 16> valid_timecode_rates{\n+    { 1.0,\n+      12.0,\n+      23.97,\n+      23.976,\n+      23.98,\n+      24000.0 / 1001.0,\n+      24.0,\n+      25.0,\n+      29.97,\n+      30000.0 / 1001.0,\n+      30.0,\n+      48.0,\n+      50.0,\n+      59.94,\n+      60000.0 / 1001.0,\n+      60.0 }\n+};\n+\n+bool\n+RationalTime::is_valid_timecode_rate(double fps)\n+{\n+    auto b = valid_timecode_rates.begin(), e = valid_timecode_rates.end();\n     return std::find(b, e, fps) != e;\n }\n \n-static bool is_dropframe_rate(double rate) {\n+static bool\n+is_dropframe_rate(double rate)\n+{\n     auto b = dropframe_timecode_rates.begin(),\n          e = dropframe_timecode_rates.end();\n     return std::find(b, e, rate) != e;\n }\n \n RationalTime\n-RationalTime::from_timecode(std::string const& timecode, double rate, ErrorStatus* error_status) {\n-    if (!RationalTime::is_valid_timecode_rate(rate)) {\n-        *error_status = ErrorStatus {ErrorStatus::INVALID_TIMECODE_RATE};\n+RationalTime::from_timecode(\n+    std::string const& timecode, double rate, ErrorStatus* error_status)\n+{\n+    if (!RationalTime::is_valid_timecode_rate(rate))\n+    {\n+        if (error_status)\n+        {\n+            *error_status = ErrorStatus{ ErrorStatus::INVALID_TIMECODE_RATE };\n+        }\n         return RationalTime::_invalid_time;\n     }\n \n     bool rate_is_dropframe = is_dropframe_rate(rate);\n \n-    if (timecode.find(';') != std::string::npos) {\n-        if (!rate_is_dropframe) {\n-            *error_status = ErrorStatus(ErrorStatus::NON_DROPFRAME_RATE,\n-                                        string_printf(\"Timecode '%s' indicates drop frame rate due \"\n-                                                      \"to the ';' frame divider. \"\n-                                                      \"Passed in rate %g is of non-drop-frame-rate.\",\n-                                                      timecode.c_str(), rate));\n+    if (timecode.find(';') != std::string::npos)\n+    {\n+        if (!rate_is_dropframe)\n+        {\n+            if (error_status)\n+            {\n+                *error_status = ErrorStatus(\n+                    ErrorStatus::INVALID_RATE_FOR_DROP_FRAME_TIMECODE,\n+                    string_printf(\n+                        \"Timecode '%s' indicates drop frame rate due \"\n+                        \"to the ';' frame divider. \"\n+                        \"Passed in rate %g is not a valid drop frame rate.\",\n+                        timecode.c_str(),\n+                        rate));\n+            }\n             return RationalTime::_invalid_time;\n         }\n-    } else {\n+    }\n+    else\n+    {\n         rate_is_dropframe = false;\n     }\n \n-    std::vector<std::string> fields {\"\",\"\",\"\",\"\"};\n-    int hours, minutes, seconds, frames;\n+    std::vector<std::string> fields{ \"\", \"\", \"\", \"\" };\n+    int                      hours, minutes, seconds, frames;\n \n-    try {\n+    try\n+    {\n         // split the fields\n         unsigned int last_pos = 0;\n-        for (unsigned int i = 0; i < 4; i++) {\n+        for (unsigned int i = 0; i < 4; i++)\n+        {\n             fields[i] = timecode.substr(last_pos, 2);\n-            last_pos = last_pos+3;\n+            last_pos  = last_pos + 3;\n         }\n \n         hours   = std::stoi(fields[0]);\n         minutes = std::stoi(fields[1]);\n         seconds = std::stoi(fields[2]);\n         frames  = std::stoi(fields[3]);\n-    } catch(std::exception const& e) {\n-        *error_status = ErrorStatus(ErrorStatus::INVALID_TIMECODE_STRING,\n-                                    string_printf(\"Input timecode '%s' is an invalid timecode\",\n-                                                  timecode.c_str()));\n+    }\n+    catch (std::exception const& e)\n+    {\n+        if (error_status)\n+        {\n+            *error_status = ErrorStatus(\n+                ErrorStatus::INVALID_TIMECODE_STRING,\n+                string_printf(\n+                    \"Input timecode '%s' is an invalid timecode\",\n+                    timecode.c_str()));\n+        }\n         return RationalTime::_invalid_time;\n     }\n \n     const int nominal_fps = static_cast<int>(std::ceil(rate));\n \n-    if (frames >= nominal_fps) {\n-        *error_status = ErrorStatus(ErrorStatus::TIMECODE_RATE_MISMATCH,\n-                                    string_printf(\"Frame rate mismatch.  Timecode '%s' has \"\n-                                                  \"frames beyond %d\", timecode.c_str(),\n-                                                  nominal_fps - 1));\n+    if (frames >= nominal_fps)\n+    {\n+        if (error_status)\n+        {\n+            *error_status = ErrorStatus(\n+                ErrorStatus::TIMECODE_RATE_MISMATCH,\n+                string_printf(\n+                    \"Frame rate mismatch.  Timecode '%s' has \"\n+                    \"frames beyond %d\",\n+                    timecode.c_str(),\n+                    nominal_fps - 1));\n+        }\n         return RationalTime::_invalid_time;\n     }\n \n     int dropframes = 0;\n-    if (rate_is_dropframe) {\n-        if ((rate == 29.97) or (rate == 30000/1001.0)) {\n+    if (rate_is_dropframe)\n+    {\n+        if ((rate == 29.97) or (rate == 30000 / 1001.0))\n+        {\n             dropframes = 2;\n         }\n-        else if ((rate == 59.94) or (rate == 60000/1001.0)) {\n+        else if ((rate == 59.94) or (rate == 60000 / 1001.0))\n+        {\n             dropframes = 4;\n         }\n     }",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentime/rationalTime.cpp",
        "code_diff": "@@ -138,22 +174,24 @@\nRationalTime::from_timecode(std::string const& timecode, double rate, ErrorStatu\n     int total_minutes = hours * 60 + minutes;\n \n     // convert to frames\n-    const int value = (\n-        ((total_minutes * 60) + seconds) * nominal_fps \n-        + frames\n-        - (\n-            dropframes \n-            * (total_minutes - static_cast<int>(std::floor(total_minutes/10)))\n-          )\n-    );\n-\n-    return RationalTime {double(value), rate};\n+    const int value =\n+        (((total_minutes * 60) + seconds) * nominal_fps + frames -\n+         (dropframes *\n+          (total_minutes - static_cast<int>(std::floor(total_minutes / 10)))));\n+\n+    return RationalTime{ double(value), rate };\n }\n \n RationalTime\n-RationalTime::from_time_string(std::string const& time_string, double rate, ErrorStatus* error_status) {\n-    if (!RationalTime::is_valid_timecode_rate(rate)) {\n-        *error_status = ErrorStatus(ErrorStatus::INVALID_TIMECODE_RATE);\n+RationalTime::from_time_string(\n+    std::string const& time_string, double rate, ErrorStatus* error_status)\n+{\n+    if (!RationalTime::is_valid_timecode_rate(rate))\n+    {\n+        if (error_status)\n+        {\n+            *error_status = ErrorStatus(ErrorStatus::INVALID_TIMECODE_RATE);\n+        }\n         return RationalTime::_invalid_time;\n     }",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentime/rationalTime.cpp",
        "code_diff": "@@ -162,81 +200,109 @@\nRationalTime::from_time_string(std::string const& time_string, double rate, Erro\n     // split the fields\n     int last_pos = 0;\n \n-    for (int i = 0; i < 2; i++) {\n+    for (int i = 0; i < 2; i++)\n+    {\n         fields[i] = time_string.substr(last_pos, 2);\n-        last_pos = last_pos+3;\n+        last_pos  = last_pos + 3;\n     }\n \n     fields[2] = time_string.substr(last_pos, time_string.length());\n \n     double hours, minutes, seconds;\n-    \n-    try {\n+\n+    try\n+    {\n         hours   = std::stod(fields[0]);\n         minutes = std::stod(fields[1]);\n         seconds = std::stod(fields[2]);\n-    } catch(std::exception const& e) {\n-        *error_status = ErrorStatus(ErrorStatus::INVALID_TIME_STRING,\n-                                    string_printf(\"Input time string '%s' is an invalid time string\",\n-                                                  time_string.c_str()));\n+    }\n+    catch (std::exception const& e)\n+    {\n+        if (error_status)\n+        {\n+            *error_status = ErrorStatus(\n+                ErrorStatus::INVALID_TIME_STRING,\n+                string_printf(\n+                    \"Input time string '%s' is an invalid time string\",\n+                    time_string.c_str()));\n+        }\n         return RationalTime::_invalid_time;\n     }\n \n-    return from_seconds(seconds + minutes * 60 + hours * 60 * 60).rescaled_to(rate);\n+    return from_seconds(seconds + minutes * 60 + hours * 60 * 60)\n+        .rescaled_to(rate);\n }\n \n std::string\n RationalTime::to_timecode(\n-        double rate,\n-        IsDropFrameRate drop_frame,\n-        ErrorStatus* error_status\n-) const {\n-\n-    *error_status = ErrorStatus();\n+    double rate, IsDropFrameRate drop_frame, ErrorStatus* error_status) const\n+{\n+    if (error_status)\n+    {\n+        *error_status = ErrorStatus();\n+    }\n \n     double frames_in_target_rate = this->value_rescaled_to(rate);\n-    \n-    if (frames_in_target_rate < 0) {\n-        *error_status = ErrorStatus(ErrorStatus::NEGATIVE_VALUE);\n+\n+    if (frames_in_target_rate < 0)\n+    {\n+        if (error_status)\n+        {\n+            *error_status = ErrorStatus(ErrorStatus::NEGATIVE_VALUE);\n+        }\n         return std::string();\n     }\n-        \n-    if (!is_valid_timecode_rate(rate)) {\n-        *error_status = ErrorStatus(ErrorStatus::INVALID_TIMECODE_RATE);\n+\n+    if (!is_valid_timecode_rate(rate))\n+    {\n+        if (error_status)\n+        {\n+            *error_status = ErrorStatus(ErrorStatus::INVALID_TIMECODE_RATE);\n+        }\n         return std::string();\n     }\n \n     bool rate_is_dropframe = is_dropframe_rate(rate);\n-    if (drop_frame == IsDropFrameRate::ForceYes and not rate_is_dropframe) {\n-        *error_status = ErrorStatus(\n-                ErrorStatus::INVALID_RATE_FOR_DROP_FRAME_TIMECODE\n-        );\n+    if (drop_frame == IsDropFrameRate::ForceYes and not rate_is_dropframe)\n+    {\n+        if (error_status)\n+        {\n+            *error_status =\n+                ErrorStatus(ErrorStatus::INVALID_RATE_FOR_DROP_FRAME_TIMECODE);\n+        }\n         return std::string();\n     }\n \n-    if (drop_frame != IsDropFrameRate::InferFromRate) {\n-        if (drop_frame == IsDropFrameRate::ForceYes) {\n+    if (drop_frame != IsDropFrameRate::InferFromRate)\n+    {\n+        if (drop_frame == IsDropFrameRate::ForceYes)\n+        {\n             rate_is_dropframe = true;\n         }\n-        else {\n+        else\n+        {\n             rate_is_dropframe = false;\n         }\n     }\n \n     // extra math for dropframes stuff\n-    int dropframes = 0;\n-    char div = ':';\n+    int  dropframes = 0;\n+    char div        = ':';\n     if (!rate_is_dropframe)\n     {\n-        if (std::round(rate) == 24) {\n+        if (std::round(rate) == 24)\n+        {\n             rate = 24.0;\n         }\n     }\n-    else {\n-        if ((rate == 29.97) or (rate == 30000/1001.0)) {\n+    else\n+    {\n+        if ((rate == 29.97) or (rate == 30000 / 1001.0))\n+        {\n             dropframes = 2;\n         }\n-        else if(rate == 59.94) {\n+        else if (rate == 59.94)\n+        {\n             dropframes = 4;\n         }\n         div = ';';",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentime/rationalTime.cpp",
        "code_diff": "@@ -250,64 +316,77 @@\nRationalTime::to_timecode(\n     int frames_per_10_minutes = static_cast<int>(std::round(rate * 60 * 10));\n     // Number of frames per minute is the round of the framerate * 60 minus\n     // the number of dropped frames\n-    int frames_per_minute = static_cast<int>(\n-            (std::round(rate) * 60) - dropframes);\n+    int frames_per_minute =\n+        static_cast<int>((std::round(rate) * 60) - dropframes);\n \n     // If the number of frames is more than 24 hours, roll over clock\n     double value = std::fmod(frames_in_target_rate, frames_per_24_hours);\n \n-    if (rate_is_dropframe) {\n-        int ten_minute_chunks = static_cast<int>(std::floor(value/frames_per_10_minutes));\n-        int frames_over_ten_minutes = static_cast<int>(std::fmod(value, frames_per_10_minutes));\n+    if (rate_is_dropframe)\n+    {\n+        int ten_minute_chunks =\n+            static_cast<int>(std::floor(value / frames_per_10_minutes));\n+        int frames_over_ten_minutes =\n+            static_cast<int>(std::fmod(value, frames_per_10_minutes));\n \n-        if (frames_over_ten_minutes > dropframes) {\n+        if (frames_over_ten_minutes > dropframes)\n+        {\n             value += (dropframes * 9 * ten_minute_chunks) +\n-                dropframes * std::floor((frames_over_ten_minutes - dropframes) / frames_per_minute);\n+                     dropframes * std::floor(\n+                                      (frames_over_ten_minutes - dropframes) /\n+                                      frames_per_minute);\n         }\n-        else {\n+        else\n+        {\n             value += dropframes * 9 * ten_minute_chunks;\n         }\n     }\n \n     int nominal_fps = static_cast<int>(std::ceil(rate));\n \n     // compute the fields\n-    int frames = static_cast<int>(std::fmod(value, nominal_fps));\n+    int frames        = static_cast<int>(std::fmod(value, nominal_fps));\n     int seconds_total = static_cast<int>(std::floor(value / nominal_fps));\n-    int seconds = static_cast<int>(std::fmod(seconds_total, 60));\n-    int minutes = static_cast<int>(std::fmod(std::floor(seconds_total / 60), 60));\n-    int hours = static_cast<int>(std::floor(std::floor(seconds_total / 60) / 60));\n+    int seconds       = static_cast<int>(std::fmod(seconds_total, 60));\n+    int minutes =\n+        static_cast<int>(std::fmod(std::floor(seconds_total / 60), 60));\n+    int hours =\n+        static_cast<int>(std::floor(std::floor(seconds_total / 60) / 60));\n \n-    return string_printf(\"%02d:%02d:%02d%c%02d\", hours, minutes, seconds, div, frames);\n+    return string_printf(\n+        \"%02d:%02d:%02d%c%02d\", hours, minutes, seconds, div, frames);\n }\n \n std::string\n-RationalTime::to_time_string() const {\n+RationalTime::to_time_string() const\n+{\n     double total_seconds = to_seconds();\n-    bool is_negative = false;\n+    bool   is_negative   = false;\n \n     // We always want to compute with positive numbers to get the right string\n     // result and return the string at the end with a '-'. This provides\n     // compatibility with ffmpeg, which allows negative time strings.\n-    if (std::signbit(total_seconds)) {\n+    if (std::signbit(total_seconds))\n+    {\n         total_seconds = fabs(total_seconds);\n-        is_negative = true;\n+        is_negative   = true;\n     }\n \n     // @TODO: fun fact, this will print the wrong values for numbers at a\n     // certain number of decimal places, if you just std::cerr << total_seconds\n \n     // reformat in time string\n     constexpr double time_units_per_minute = 60.0;\n-    constexpr double time_units_per_hour = time_units_per_minute * 60.0;\n-    constexpr double time_units_per_day = time_units_per_hour * 24.0;\n+    constexpr double time_units_per_hour   = time_units_per_minute * 60.0;\n+    constexpr double time_units_per_day    = time_units_per_hour * 24.0;\n \n-    double hour_units = std::fmod((double)total_seconds, time_units_per_day);\n+    double hour_units = std::fmod((double) total_seconds, time_units_per_day);\n \n     int hours = static_cast<int>(std::floor(hour_units / time_units_per_hour));\n     double minute_units = std::fmod(hour_units, time_units_per_hour);\n \n-    int minutes = static_cast<int>(std::floor(minute_units / time_units_per_minute));\n+    int minutes =\n+        static_cast<int>(std::floor(minute_units / time_units_per_minute));\n     double seconds = std::fmod(minute_units, time_units_per_minute);\n \n     // split the seconds string apart",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentime/rationalTime.cpp",
        "code_diff": "@@ -316,7 +395,7 @@\nRationalTime::to_time_string() const {\n     fractpart = modf(seconds, &intpart);\n \n     // clamp to 2 digits and zero-pad\n-    std::string seconds_str = string_printf(\"%02d\", (int)intpart);\n+    std::string seconds_str = string_printf(\"%02d\", (int) intpart);\n \n     // get the fractional component (with enough digits of resolution)\n     std::string microseconds_str = string_printf(\"%.7g\", fractpart);",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentime/rationalTime.cpp",
        "code_diff": "@@ -325,10 +404,12 @@\nRationalTime::to_time_string() const {\n     microseconds_str = microseconds_str.substr(1);\n \n     // enforce the minimum string of '.0'\n-    if (microseconds_str.length() == 0) {\n+    if (microseconds_str.length() == 0)\n+    {\n         microseconds_str = std::string(\".0\");\n     }\n-    else {\n+    else\n+    {\n         // ...and the string size\n         microseconds_str.resize(7, '\\0');\n     }",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/composition.cpp",
        "code_diff": "@@ -4,89 +4,120 @@\n#include <assert.h>\n #include <set>\n \n-namespace opentimelineio { namespace OPENTIMELINEIO_VERSION  {\n-    \n-Composition::Composition(std::string const& name,\n-                         optional<TimeRange> const& source_range,\n-                         AnyDictionary const& metadata,\n-         std::vector<Effect*> const& effects,\n-         std::vector<Marker*> const& markers)\n+namespace opentimelineio { namespace OPENTIMELINEIO_VERSION {\n+\n+Composition::Composition(\n+    std::string const&          name,\n+    optional<TimeRange> const&  source_range,\n+    AnyDictionary const&        metadata,\n+    std::vector<Effect*> const& effects,\n+    std::vector<Marker*> const& markers)\n     : Parent(name, source_range, metadata, effects, markers)\n-{\n-}\n+{}\n \n-Composition::~Composition() {\n+Composition::~Composition()\n+{\n     clear_children();\n }\n \n-std::string Composition::composition_kind() const {\n+std::string\n+Composition::composition_kind() const\n+{\n     static std::string kind = \"Composition\";\n     return kind;\n }\n \n-void \n-Composition::clear_children() {\n-    for (Composable* child: _children) {\n+void\n+Composition::clear_children()\n+{\n+    for (Composable* child: _children)\n+    {\n         child->_set_parent(nullptr);\n     }\n \n     _children.clear();\n     _child_set.clear();\n }\n \n-bool \n-Composition::set_children(std::vector<Composable*> const& children, ErrorStatus* error_status) {\n-    for (auto child : children) {\n-        if (child->parent()) {\n-            *error_status = ErrorStatus::CHILD_ALREADY_PARENTED;\n+bool\n+Composition::set_children(\n+    std::vector<Composable*> const& children, ErrorStatus* error_status)\n+{\n+    for (auto child: children)\n+    {\n+        if (child->parent())\n+        {\n+            if (error_status)\n+            {\n+                *error_status = ErrorStatus::CHILD_ALREADY_PARENTED;\n+            }\n             return false;\n         }\n     }\n \n-    for (auto child : children) {\n+    for (auto child: children)\n+    {\n         child->_set_parent(this);\n     }\n \n-    _children = decltype(_children)(children.begin(), children.end());\n+    _children  = decltype(_children)(children.begin(), children.end());\n     _child_set = std::set<Composable*>(children.begin(), children.end());\n     return true;\n }\n \n-bool \n-Composition::insert_child(int index, Composable* child, ErrorStatus* error_status) {\n-    if (child->parent()) {\n-        *error_status = ErrorStatus::CHILD_ALREADY_PARENTED;\n+bool\n+Composition::insert_child(\n+    int index, Composable* child, ErrorStatus* error_status)\n+{\n+    if (child->parent())\n+    {\n+        if (error_status)\n+        {\n+            *error_status = ErrorStatus::CHILD_ALREADY_PARENTED;\n+        }\n         return false;\n     }\n \n     child->_set_parent(this);\n-        \n+\n     index = adjusted_vector_index(index, _children);\n-    if (index >= int(_children.size())) {\n+    if (index >= int(_children.size()))\n+    {\n         _children.emplace_back(child);\n     }\n-    else {\n+    else\n+    {\n         _children.insert(_children.begin() + std::max(index, 0), child);\n     }\n \n     _child_set.insert(child);\n     return true;\n }\n \n-bool \n-Composition::set_child(int index, Composable* child, ErrorStatus* error_status) {\n+bool\n+Composition::set_child(int index, Composable* child, ErrorStatus* error_status)\n+{\n     index = adjusted_vector_index(index, _children);\n-    if (index < 0 || index >= int(_children.size())) {\n-        *error_status = ErrorStatus::ILLEGAL_INDEX;\n+    if (index < 0 || index >= int(_children.size()))\n+    {\n+        if (error_status)\n+        {\n+            *error_status = ErrorStatus::ILLEGAL_INDEX;\n+        }\n         return false;\n     }\n \n-    if (_children[index] != child) {\n-        if (child->parent()) {\n-            *error_status = ErrorStatus::CHILD_ALREADY_PARENTED;\n+    if (_children[index] != child)\n+    {\n+        if (child->parent())\n+        {\n+            if (error_status)\n+            {\n+                *error_status = ErrorStatus::CHILD_ALREADY_PARENTED;\n+            }\n             return false;\n         }\n-        \n+\n         _children[index]->_set_parent(nullptr);\n         _child_set.erase(_children[index]);\n         child->_set_parent(this);",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/composition.cpp",
        "code_diff": "@@ -96,22 +127,29 @@\nComposition::set_child(int index, Composable* child, ErrorStatus* error_status)\n     return true;\n }\n \n-bool \n-Composition::remove_child(int index, ErrorStatus* error_status) {\n-    if (_children.empty()) {\n-        *error_status = ErrorStatus::ILLEGAL_INDEX;\n+bool\n+Composition::remove_child(int index, ErrorStatus* error_status)\n+{\n+    if (_children.empty())\n+    {\n+        if (error_status)\n+        {\n+            *error_status = ErrorStatus::ILLEGAL_INDEX;\n+        }\n         return false;\n     }\n \n     index = adjusted_vector_index(index, _children);\n \n     _child_set.erase(_children[index]);\n-    \n-    if (size_t(index) >= _children.size()) {\n+\n+    if (size_t(index) >= _children.size())\n+    {\n         _children.back()->_set_parent(nullptr);\n         _children.pop_back();\n     }\n-    else {\n+    else\n+    {\n         index = std::max(index, 0);\n         _children[index]->_set_parent(nullptr);\n         _children.erase(_children.begin() + index);",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/composition.cpp",
        "code_diff": "@@ -120,11 +158,15 @@\nComposition::remove_child(int index, ErrorStatus* error_status) {\n     return true;\n }\n \n-bool Composition::read_from(Reader& reader) {\n-    if (reader.read(\"children\", &_children) &&\n-        Parent::read_from(reader)) {\n-        for (Composable* child : _children) {\n-            if (!child->_set_parent(this)) {\n+bool\n+Composition::read_from(Reader& reader)\n+{\n+    if (reader.read(\"children\", &_children) && Parent::read_from(reader))\n+    {\n+        for (Composable* child: _children)\n+        {\n+            if (!child->_set_parent(this))\n+            {\n                 reader.error(ErrorStatus::CHILD_ALREADY_PARENTED);\n                 return false;\n             }",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/composition.cpp",
        "code_diff": "@@ -133,18 +175,23 @@\nbool Composition::read_from(Reader& reader) {\n     return true;\n }\n \n-void Composition::write_to(Writer& writer) const {\n+void\n+Composition::write_to(Writer& writer) const\n+{\n     Parent::write_to(writer);\n     writer.write(\"children\", _children);\n }\n \n-bool Composition::is_parent_of(Composable const* other) const {\n+bool\n+Composition::is_parent_of(Composable const* other) const\n+{\n     Composition const* cur_parent = other->_parent;\n     if (cur_parent == this)\n         return true;\n-    \n+\n     std::set<Composition const*> visited;\n-    while (cur_parent && visited.count(cur_parent) == 0) {\n+    while (cur_parent && visited.count(cur_parent) == 0)\n+    {\n         if (cur_parent == this)\n             return true;",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/composition.cpp",
        "code_diff": "@@ -155,32 +202,49 @@\nbool Composition::is_parent_of(Composable const* other) const {\n }\n \n std::pair<optional<RationalTime>, optional<RationalTime>>\n-Composition::handles_of_child(Composable const* /* child */, ErrorStatus* /* error_status */) const {\n+Composition::handles_of_child(\n+    Composable const* /* child */, ErrorStatus* /* error_status */) const\n+{\n     return std::make_pair(optional<RationalTime>(), optional<RationalTime>());\n }\n \n-int Composition::_index_of_child(Composable const* child, ErrorStatus* error_status) const {\n-    for (size_t i = 0; i < _children.size(); i++) {\n-        if (_children[i] == child) {\n+int\n+Composition::_index_of_child(\n+    Composable const* child, ErrorStatus* error_status) const\n+{\n+    for (size_t i = 0; i < _children.size(); i++)\n+    {\n+        if (_children[i] == child)\n+        {\n             return int(i);\n         }\n     }\n-    \n-    *error_status = ErrorStatus::NOT_A_CHILD_OF;\n-    error_status->object_details = this;\n+\n+    if (error_status)\n+    {\n+        *error_status                = ErrorStatus::NOT_A_CHILD_OF;\n+        error_status->object_details = this;\n+    }\n     return -1;\n }\n \n-std::vector<Composition*> Composition::_path_from_child(Composable const* child,\n-                                                        ErrorStatus* error_status) const {\n-    auto current = child->parent();\n-    std::vector<Composition*> parents { current };\n-    \n-    while (current != this) {\n+std::vector<Composition*>\n+Composition::_path_from_child(\n+    Composable const* child, ErrorStatus* error_status) const\n+{\n+    auto                      current = child->parent();\n+    std::vector<Composition*> parents{ current };\n+\n+    while (current != this)\n+    {\n         current = current->parent();\n-        if (!current) {\n-            *error_status = ErrorStatus::NOT_DESCENDED_FROM;\n-            error_status->object_details = this;\n+        if (!current)\n+        {\n+            if (error_status)\n+            {\n+                *error_status                = ErrorStatus::NOT_DESCENDED_FROM;\n+                error_status->object_details = this;\n+            }\n             return parents;\n         }\n         parents.push_back(current);",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/composition.cpp",
        "code_diff": "@@ -189,188 +253,259 @@\nstd::vector<Composition*> Composition::_path_from_child(Composable const* child,\n     return parents;\n }\n \n-TimeRange Composition::range_of_child_at_index(int /* index */, ErrorStatus* error_status) const {\n-    *error_status = ErrorStatus::NOT_IMPLEMENTED;\n+TimeRange\n+Composition::range_of_child_at_index(\n+    int /* index */, ErrorStatus* error_status) const\n+{\n+    if (error_status)\n+    {\n+        *error_status = ErrorStatus::NOT_IMPLEMENTED;\n+    }\n     return TimeRange();\n }\n \n-TimeRange Composition::trimmed_range_of_child_at_index(int /* index */, ErrorStatus* error_status) const {\n-    *error_status = ErrorStatus::NOT_IMPLEMENTED;\n+TimeRange\n+Composition::trimmed_range_of_child_at_index(\n+    int /* index */, ErrorStatus* error_status) const\n+{\n+    if (error_status)\n+    {\n+        *error_status = ErrorStatus::NOT_IMPLEMENTED;\n+    }\n     return TimeRange();\n }\n \n std::map<Composable*, TimeRange>\n-Composition::range_of_all_children(ErrorStatus* error_status) const {\n-    *error_status = ErrorStatus::NOT_IMPLEMENTED;\n+Composition::range_of_all_children(ErrorStatus* error_status) const\n+{\n+    if (error_status)\n+    {\n+        *error_status = ErrorStatus::NOT_IMPLEMENTED;\n+    }\n     return std::map<Composable*, TimeRange>();\n }\n \n // XXX should have reference_space argument or something\n-TimeRange Composition::range_of_child(Composable const* child, ErrorStatus* error_status) const {\n+TimeRange\n+Composition::range_of_child(\n+    Composable const* child, ErrorStatus* error_status) const\n+{\n     auto parents = _path_from_child(child, error_status);\n-    if (*error_status) {\n+    if (is_error(error_status))\n+    {\n         return TimeRange();\n     }\n-    \n-    Composition const* reference_space = this;    // XXX\n+\n+    Composition const*  reference_space = this; // XXX\n     optional<TimeRange> result_range;\n-    auto current = child;\n-    \n+    auto                current = child;\n+\n     assert(!parents.empty());\n-    for (auto parent: parents) {\n+    for (auto parent: parents)\n+    {\n         auto index = parent->_index_of_child(current, error_status);\n-        if (*error_status) {\n+        if (is_error(error_status))\n+        {\n             return TimeRange();\n         }\n \n-        auto parent_range = parent->range_of_child_at_index(index, error_status);\n-        if (*error_status) {\n+        auto parent_range =\n+            parent->range_of_child_at_index(index, error_status);\n+        if (is_error(error_status))\n+        {\n             return TimeRange();\n         }\n-        \n-        if (!result_range) {\n+\n+        if (!result_range)\n+        {\n             result_range = parent_range;\n-            current = parent;\n+            current      = parent;\n             continue;\n         }\n \n-         result_range = TimeRange(result_range->start_time() + parent_range.start_time(),\n-                                   result_range->duration());\n+        result_range = TimeRange(\n+            result_range->start_time() + parent_range.start_time(),\n+            result_range->duration());\n         current = parent;\n     }\n-    \n-    return (reference_space != this) ? transformed_time_range(*result_range, reference_space, error_status) : *result_range;\n+\n+    return (reference_space != this)\n+               ? transformed_time_range(\n+                     *result_range, reference_space, error_status)\n+               : *result_range;\n }\n \n // XXX should have reference_space argument or something\n-optional<TimeRange> Composition::trimmed_range_of_child(Composable const* child, ErrorStatus* error_status) const {\n+optional<TimeRange>\n+Composition::trimmed_range_of_child(\n+    Composable const* child, ErrorStatus* error_status) const\n+{\n     auto parents = _path_from_child(child, error_status);\n-    if (*error_status) {\n+    if (is_error(error_status))\n+    {\n         return TimeRange();\n     }\n-    \n+\n     optional<TimeRange> result_range;\n-    auto current = child;\n-    \n+    auto                current = child;\n+\n     assert(!parents.empty());\n-    for (auto parent: parents) {\n+    for (auto parent: parents)\n+    {\n         auto index = parent->_index_of_child(current, error_status);\n-        if (*error_status) {\n+        if (is_error(error_status))\n+        {\n             return TimeRange();\n         }\n \n-        auto parent_range = parent->trimmed_range_of_child_at_index(index, error_status);\n-        if (*error_status) {\n+        auto parent_range =\n+            parent->trimmed_range_of_child_at_index(index, error_status);\n+        if (is_error(error_status))\n+        {\n             return TimeRange();\n         }\n-        \n-        if (!result_range) {\n+\n+        if (!result_range)\n+        {\n             result_range = parent_range;\n-            current = parent;\n+            current      = parent;\n             continue;\n         }\n-        \n-        result_range = TimeRange(result_range->start_time() + parent_range.start_time(),\n-                                  result_range->duration());\n+\n+        result_range = TimeRange(\n+            result_range->start_time() + parent_range.start_time(),\n+            result_range->duration());\n     }\n-    \n-    if (!source_range()) {\n+\n+    if (!source_range())\n+    {\n         return result_range;\n     }\n-    \n-    auto new_start_time = std::max(source_range()->start_time(), result_range->start_time());\n-    if (new_start_time > result_range->end_time_exclusive()) {\n+\n+    auto new_start_time =\n+        std::max(source_range()->start_time(), result_range->start_time());\n+    if (new_start_time > result_range->end_time_exclusive())\n+    {\n         return nullopt;\n     }\n-    \n-    auto new_duration = std::min(result_range->end_time_exclusive(),\n-                                 source_range()->end_time_exclusive()) - new_start_time;\n-    if (new_duration.value() < 0) {\n+\n+    auto new_duration = std::min(\n+                            result_range->end_time_exclusive(),\n+                            source_range()->end_time_exclusive()) -\n+                        new_start_time;\n+    if (new_duration.value() < 0)\n+    {\n         return nullopt;\n     }\n-    \n+\n     return TimeRange(new_start_time, new_duration);\n }\n \n-std::vector<Composable*> Composition::_children_at_time(RationalTime t, ErrorStatus* error_status) const {\n+std::vector<Composable*>\n+Composition::_children_at_time(RationalTime t, ErrorStatus* error_status) const\n+{\n     std::vector<Composable*> result;\n-    \n+\n     // range_of_child_at_index is O(i), so this loop is quadratic:\n-    for (size_t i = 0; i < _children.size() && !(*error_status); i++) {\n-        if (range_of_child_at_index(int(i), error_status).contains(t)) {\n+    for (size_t i = 0; i < _children.size() && !is_error(error_status); i++)\n+    {\n+        if (range_of_child_at_index(int(i), error_status).contains(t))\n+        {\n             result.push_back(_children[i]);\n         }\n     }\n-    \n+\n     return result;\n }\n \n-optional<TimeRange> Composition::trim_child_range(TimeRange child_range) const {\n-    if (!source_range()) {\n+optional<TimeRange>\n+Composition::trim_child_range(TimeRange child_range) const\n+{\n+    if (!source_range())\n+    {\n         return child_range;\n     }\n-    \n+\n     const TimeRange sr = *source_range();\n     bool past_end_time = sr.start_time() >= child_range.end_time_exclusive();\n-    bool before_start_time = sr.end_time_exclusive() <= child_range.start_time();\n-    \n-    if (past_end_time|| before_start_time) {\n+    bool before_start_time =\n+        sr.end_time_exclusive() <= child_range.start_time();\n+\n+    if (past_end_time || before_start_time)\n+    {\n         return nullopt;\n     }\n-            \n-    if (child_range.start_time() < sr.start_time()) {\n-        child_range = TimeRange::range_from_start_end_time(sr.start_time(),\n-                                                           child_range.end_time_exclusive());\n+\n+    if (child_range.start_time() < sr.start_time())\n+    {\n+        child_range = TimeRange::range_from_start_end_time(\n+            sr.start_time(), child_range.end_time_exclusive());\n     }\n-    \n-    if (child_range.end_time_exclusive() > sr.end_time_exclusive()) {\n-        child_range = TimeRange::range_from_start_end_time(child_range.start_time(),\n-                                                           sr.end_time_exclusive());\n+\n+    if (child_range.end_time_exclusive() > sr.end_time_exclusive())\n+    {\n+        child_range = TimeRange::range_from_start_end_time(\n+            child_range.start_time(), sr.end_time_exclusive());\n     }\n \n     return child_range;\n }\n \n-bool Composition::has_child(Composable* child) const {\n+bool\n+Composition::has_child(Composable* child) const\n+{\n     return _child_set.find(child) != _child_set.end();\n }\n \n-SerializableObject::Retainer<Composable> Composition::child_at_time(\n+SerializableObject::Retainer<Composable>\n+Composition::child_at_time(\n     RationalTime const& search_time,\n-    ErrorStatus* error_status,\n-    bool shallow_search) const\n+    ErrorStatus*        error_status,\n+    bool                shallow_search) const\n {\n+    Retainer<Composable> result;\n+\n     auto range_map = range_of_all_children(error_status);\n-    if (!error_status) {\n-        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n+    if (is_error(error_status))\n+    {\n+        return result;\n     }\n \n     // find the first item whose end_time_exclusive is after the\n     const auto first_inside_range = _bisect_left(\n         search_time,\n-        [&range_map](Composable* child) { return range_map[child].end_time_exclusive(); },\n+        [&range_map](Composable* child) {\n+            return range_map[child].end_time_exclusive();\n+        },\n         error_status);\n-    if (!error_status) {\n-        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n+    if (is_error(error_status))\n+    {\n+        return result;\n     }\n \n     // find the last item whose start_time is before the\n     const auto last_in_range = _bisect_right(\n         search_time,\n-        [&range_map](Composable* child) { return range_map[child].start_time(); },\n+        [&range_map](Composable* child) {\n+            return range_map[child].start_time();\n+        },\n         error_status,\n         first_inside_range);\n-    if (!error_status) {\n-        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n+    if (is_error(error_status))\n+    {\n+        return result;\n     }\n \n     // limit the search to children who are in the search_range\n     std::vector<Retainer<Composable>> possible_matches;\n-    for (auto child = _children.begin() + first_inside_range; child < _children.begin() + last_in_range; ++child) {\n+    for (auto child = _children.begin() + first_inside_range;\n+         child < _children.begin() + last_in_range;\n+         ++child)\n+    {\n         possible_matches.push_back(child->value);\n     }\n-    Retainer<Composable> result;\n-    for (const auto& thing : possible_matches) {\n+    for (const auto& thing: possible_matches)\n+    {\n         if (range_map[thing].overlaps(search_time))\n         {\n             result = thing;",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/deserialization.cpp",
        "code_diff": "@@ -1,310 +1,398 @@\n-#include \"opentimelineio/serializableObject.h\"\n-#include \"opentimelineio/serializableObjectWithMetadata.h\"\n #include \"opentime/rationalTime.h\"\n #include \"opentime/timeRange.h\"\n #include \"opentime/timeTransform.h\"\n+#include \"opentimelineio/serializableObject.h\"\n+#include \"opentimelineio/serializableObjectWithMetadata.h\"\n+#include \"stringUtils.h\"\n \n #define RAPIDJSON_NAMESPACE OTIO_rapidjson\n-#include <rapidjson/filereadstream.h>\n #include <rapidjson/cursorstreamwrapper.h>\n-#include <rapidjson/reader.h>\n #include <rapidjson/error/en.h>\n+#include <rapidjson/filereadstream.h>\n+#include <rapidjson/reader.h>\n \n #if defined(_WINDOWS)\n-#ifndef WIN32_LEAN_AND_MEAN\n-#define WIN32_LEAN_AND_MEAN\n-#endif // WIN32_LEAN_AND_MEAN\n-#ifndef NOMINMAX\n-#define NOMINMAX\n-#endif // NOMINMAX\n-#include <windows.h>\n+#    ifndef WIN32_LEAN_AND_MEAN\n+#        define WIN32_LEAN_AND_MEAN\n+#    endif // WIN32_LEAN_AND_MEAN\n+#    ifndef NOMINMAX\n+#        define NOMINMAX\n+#    endif // NOMINMAX\n+#    include <windows.h>\n #endif\n \n-namespace opentimelineio { namespace OPENTIMELINEIO_VERSION  {\n-    \n-class JSONDecoder : public OTIO_rapidjson::BaseReaderHandler<OTIO_rapidjson::UTF8<>, JSONDecoder> {\n+namespace opentimelineio { namespace OPENTIMELINEIO_VERSION {\n+\n+class JSONDecoder : public OTIO_rapidjson::\n+                        BaseReaderHandler<OTIO_rapidjson::UTF8<>, JSONDecoder>\n+{\n public:\n-    JSONDecoder(std::function<size_t ()> line_number_function)\n-        : _line_number_function {line_number_function} {\n+    JSONDecoder(std::function<size_t()> line_number_function)\n+        : _line_number_function{ line_number_function }\n+    {\n         using namespace std::placeholders;\n         _error_function = std::bind(&JSONDecoder::_error, this, _1);\n     }\n-    \n-    bool has_errored(ErrorStatus* error_status) {\n-        *error_status = _error_status;\n-        return bool(_error_status);\n-    }\n \n-    bool has_errored() {\n-        return bool(_error_status);\n+    bool has_errored(ErrorStatus* error_status)\n+    {\n+        if (error_status)\n+        {\n+            *error_status = _error_status;\n+        }\n+        return is_error(_error_status);\n     }\n \n-    void finalize() {\n-        if (!has_errored()) {\n+    bool has_errored() { return is_error(_error_status); }\n+\n+    void finalize()\n+    {\n+        if (!has_errored())\n+        {\n             _resolver.finalize(_error_function);\n         }\n     }\n-    \n+\n     bool Null() { return store(any()); }\n     bool Bool(bool b) { return store(any(b)); }\n- \n+\n     // coerce all integer types to int64_t...\n     bool Int(int i) { return store(any(static_cast<int64_t>(i))); }\n     bool Int64(int64_t i) { return store(any(static_cast<int64_t>(i))); }\n     bool Uint(unsigned u) { return store(any(static_cast<int64_t>(u))); }\n-    bool Uint64(uint64_t u) {\n+    bool Uint64(uint64_t u)\n+    {\n         /// prevent an overflow\n         return store(any(static_cast<int64_t>(u & 0x7FFFFFFFFFFFFFFF)));\n     }\n \n     // ...and all floating point types to double\n     bool Double(double d) { return store(any(d)); }\n \n-    bool String(const char* str, OTIO_rapidjson::SizeType length, bool /* copy */) {\n+    bool\n+    String(const char* str, OTIO_rapidjson::SizeType length, bool /* copy */)\n+    {\n         return store(any(std::string(str, length)));\n     }\n \n-    bool Key(const char* str, OTIO_rapidjson::SizeType length, bool /* copy */) {\n-        if (has_errored()) {\n+    bool Key(const char* str, OTIO_rapidjson::SizeType length, bool /* copy */)\n+    {\n+        if (has_errored())\n+        {\n             return false;\n         }\n \n-        if (_stack.empty() || !_stack.back().is_dict) {\n-            _internal_error(\"RapidJSONDecoder:: _handle_key called while not decoding an object\");\n+        if (_stack.empty() || !_stack.back().is_dict)\n+        {\n+            _internal_error(\n+                \"RapidJSONDecoder:: _handle_key called while not decoding an object\");\n             return false;\n         }\n \n         _stack.back().cur_key = std::string(str, length);\n         return true;\n     }\n \n-    bool StartArray() {\n-        if (has_errored()) {\n+    bool StartArray()\n+    {\n+        if (has_errored())\n+        {\n             return false;\n         }\n \n-        _stack.emplace_back(_DictOrArray { false /* is_dict*/ });\n+        _stack.emplace_back(_DictOrArray{ false /* is_dict*/ });\n         return true;\n     }\n \n-    bool StartObject() {\n-        if (has_errored()) {\n+    bool StartObject()\n+    {\n+        if (has_errored())\n+        {\n             return false;\n         }\n \n-        _stack.emplace_back(_DictOrArray { true /* is_dict*/ });\n+        _stack.emplace_back(_DictOrArray{ true /* is_dict*/ });\n         return true;\n     }\n \n-    bool EndArray(OTIO_rapidjson::SizeType) {\n-        if (has_errored()) {\n+    bool EndArray(OTIO_rapidjson::SizeType)\n+    {\n+        if (has_errored())\n+        {\n             return false;\n         }\n \n-        if (_stack.empty()) {\n-            _internal_error(\"RapidJSONDecoder::_handle_end_array() called without matching _handle_start_array()\");\n+        if (_stack.empty())\n+        {\n+            _internal_error(\n+                \"RapidJSONDecoder::_handle_end_array() called without matching _handle_start_array()\");\n         }\n-        else {\n+        else\n+        {\n             auto& top = _stack.back();\n-            if (top.is_dict) {\n-                _internal_error(\"RapidJSONDecoder::_handle_end_array() called without matching _handle_start_array()\");\n+            if (top.is_dict)\n+            {\n+                _internal_error(\n+                    \"RapidJSONDecoder::_handle_end_array() called without matching _handle_start_array()\");\n                 _stack.pop_back();\n             }\n-            else {\n+            else\n+            {\n                 AnyVector va;\n                 va.swap(top.array);\n                 _stack.pop_back();\n                 store(any(std::move(va)));\n             }\n-\n         }\n         return true;\n     }\n \n-    bool EndObject(OTIO_rapidjson::SizeType) {\n-        if (has_errored()) {\n+    bool EndObject(OTIO_rapidjson::SizeType)\n+    {\n+        if (has_errored())\n+        {\n             return false;\n         }\n \n-        if (_stack.empty()) {\n-            _internal_error(\"JSONDecoder::_handle_end_object() called without matching _handle_start_object()\");\n+        if (_stack.empty())\n+        {\n+            _internal_error(\n+                \"JSONDecoder::_handle_end_object() called without matching _handle_start_object()\");\n         }\n-        else {\n+        else\n+        {\n             auto& top = _stack.back();\n-            if (!top.is_dict) {\n-                _internal_error(\"JSONDecoder::_handle_end_object() called without matching _handle_start_object\");\n+            if (!top.is_dict)\n+            {\n+                _internal_error(\n+                    \"JSONDecoder::_handle_end_object() called without matching _handle_start_object\");\n                 _stack.pop_back();\n             }\n-            else {\n+            else\n+            {\n                 // when we end a dictionary, we immediately convert it\n                 // to the type it really represents, if it is a schema object.\n-                SerializableObject::Reader reader(top.dict, _error_function, nullptr, static_cast<int>(_line_number_function()));\n-                _stack.pop_back();                \n+                SerializableObject::Reader reader(\n+                    top.dict,\n+                    _error_function,\n+                    nullptr,\n+                    static_cast<int>(_line_number_function()));\n+                _stack.pop_back();\n                 store(reader._decode(_resolver));\n             }\n         }\n         return true;\n     }\n \n-    bool store(any&& a) {\n-        if (has_errored()) {\n+    bool store(any&& a)\n+    {\n+        if (has_errored())\n+        {\n             return false;\n         }\n-        \n-        if (_stack.empty()) {\n+\n+        if (_stack.empty())\n+        {\n             _root.swap(a);\n         }\n-        else {\n+        else\n+        {\n             auto& top = _stack.back();\n-            if (top.is_dict) {\n+            if (top.is_dict)\n+            {\n                 top.dict.emplace(_stack.back().cur_key, a);\n             }\n-            else {\n+            else\n+            {\n                 top.array.emplace_back(a);\n             }\n         }\n         return true;\n     }\n-    \n+\n     template <typename T>\n-    static T const* _lookup(AnyDictionary const& d, std::string const& key) {\n+    static T const* _lookup(AnyDictionary const& d, std::string const& key)\n+    {\n         auto e = d.find(key);\n-        if (e != d.end() && typeid(T) == e->second.type()) {\n+        if (e != d.end() && typeid(T) == e->second.type())\n+        {\n             return &any_cast<const T&>(e->second);\n         }\n         return nullptr;\n     }\n \n     any _root;\n \n-    void _internal_error(std::string const& err_msg) {\n-        _error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR,\n-                                    string_printf(\"%s (near line %d)\", err_msg.c_str(),\n-                                                  _line_number_function()));\n+    void _internal_error(std::string const& err_msg)\n+    {\n+        _error_status = ErrorStatus(\n+            ErrorStatus::INTERNAL_ERROR,\n+            string_printf(\n+                \"%s (near line %d)\", err_msg.c_str(), _line_number_function()));\n     }\n \n-    void _error(ErrorStatus const& error_status) {\n+    void _error(ErrorStatus const& error_status)\n+    {\n         _error_status = error_status;\n     }\n \n     ErrorStatus _error_status;\n \n-    struct _DictOrArray {\n-        _DictOrArray(bool is_dict) {\n-            this->is_dict = is_dict;\n-        }\n-        \n-        bool is_dict;\n+    struct _DictOrArray\n+    {\n+        _DictOrArray(bool is_dict) { this->is_dict = is_dict; }\n+\n+        bool          is_dict;\n         AnyDictionary dict;\n-        AnyVector array;\n-        std::string cur_key;\n+        AnyVector     array;\n+        std::string   cur_key;\n     };\n \n-    std::vector<_DictOrArray> _stack;\n-    std::function<void (ErrorStatus const&)> _error_function;\n-    std::function<size_t ()> _line_number_function;\n+    std::vector<_DictOrArray>               _stack;\n+    std::function<void(ErrorStatus const&)> _error_function;\n+    std::function<size_t()>                 _line_number_function;\n \n     SerializableObject::Reader::_Resolver _resolver;\n };\n \n-SerializableObject::Reader::Reader(AnyDictionary& source, error_function_t const& error_function,\n-                                   SerializableObject* so, int line_number)\n-    : _error_function(error_function),\n-      _source(so),\n-      _line_number(line_number)\n+SerializableObject::Reader::Reader(\n+    AnyDictionary&          source,\n+    error_function_t const& error_function,\n+    SerializableObject*     so,\n+    int                     line_number)\n+    : _error_function(error_function)\n+    , _source(so)\n+    , _line_number(line_number)\n {\n     // destructively read from source.  Decoding it will either return it back\n     // anyway, or convert it to another type, in which case we want to destroy\n     // the original so as to not keep extra data around.\n     _dict.swap(source);\n }\n \n-void SerializableObject::Reader::_error(ErrorStatus const& error_status) {\n-    if (!_source) {\n-        if (_line_number > 0) {\n-            _error_function(ErrorStatus(error_status.outcome,\n-                                        string_printf(\"near line %d\", _line_number)));\n+void\n+SerializableObject::Reader::_error(ErrorStatus const& error_status)\n+{\n+    if (!_source)\n+    {\n+        if (_line_number > 0)\n+        {\n+            _error_function(ErrorStatus(\n+                error_status.outcome,\n+                string_printf(\"near line %d\", _line_number)));\n         }\n-        else {\n+        else\n+        {\n             _error_function(error_status);\n         }\n         return;\n     }\n \n     std::string line_description;\n-    if (_line_number > 0) {\n+    if (_line_number > 0)\n+    {\n         line_description = string_printf(\" (near line %d)\", _line_number);\n     }\n \n-\n     std::string name = \"<unknown>\";\n-    auto e = _dict.find(\"name\");\n-    if (e != _dict.end() && e->second.type() == typeid(std::string)) {\n+    auto        e    = _dict.find(\"name\");\n+    if (e != _dict.end() && e->second.type() == typeid(std::string))\n+    {\n         name = any_cast<std::string>(e->second);\n     }\n-    \n \n-    _error_function(ErrorStatus(error_status.outcome,\n-                                string_printf(\"While reading object named '%s' (of type '%s'): %s%s\",\n-                                              name.c_str(), demangled_type_name(_source).c_str(),\n-                                              error_status.details.c_str(),\n-                                              line_description.c_str())));\n+    _error_function(ErrorStatus(\n+        error_status.outcome,\n+        string_printf(\n+            \"While reading object named '%s' (of type '%s'): %s%s\",\n+            name.c_str(),\n+            type_name_for_error_message(_source).c_str(),\n+            error_status.details.c_str(),\n+            line_description.c_str())));\n }\n \n-void SerializableObject::Reader::_fix_reference_ids(AnyDictionary& m,\n-                                                    error_function_t const& error_function,\n-                                                    _Resolver& resolver, int line_number) {\n-    for (auto& e: m) {\n+void\n+SerializableObject::Reader::_fix_reference_ids(\n+    AnyDictionary&          m,\n+    error_function_t const& error_function,\n+    _Resolver&              resolver,\n+    int                     line_number)\n+{\n+    for (auto& e: m)\n+    {\n         _fix_reference_ids(e.second, error_function, resolver, line_number);\n     }\n }\n \n-void SerializableObject::Reader::_fix_reference_ids(any& a,\n-                                                    error_function_t const& error_function,\n-                                                    _Resolver& resolver, int line_number) {\n-    if (a.type() == typeid(AnyDictionary)) {\n-        _fix_reference_ids(any_cast<AnyDictionary&>(a), error_function, resolver, line_number);\n+void\n+SerializableObject::Reader::_fix_reference_ids(\n+    any&                    a,\n+    error_function_t const& error_function,\n+    _Resolver&              resolver,\n+    int                     line_number)\n+{\n+    if (a.type() == typeid(AnyDictionary))\n+    {\n+        _fix_reference_ids(\n+            any_cast<AnyDictionary&>(a), error_function, resolver, line_number);\n     }\n-    else if (a.type() == typeid(AnyVector)) {\n+    else if (a.type() == typeid(AnyVector))\n+    {\n         AnyVector& child_array = any_cast<AnyVector&>(a);\n-        for (size_t i = 0; i < child_array.size(); i++) {\n-            _fix_reference_ids(child_array[i], error_function, resolver, line_number);\n+        for (size_t i = 0; i < child_array.size(); i++)\n+        {\n+            _fix_reference_ids(\n+                child_array[i], error_function, resolver, line_number);\n         }\n     }\n-    else if (a.type() == typeid(SerializableObject::ReferenceId)) {\n+    else if (a.type() == typeid(SerializableObject::ReferenceId))\n+    {\n         std::string id = any_cast<SerializableObject::ReferenceId>(a).id;\n-        auto e = resolver.object_for_id.find(id);\n-        if (e == resolver.object_for_id.end()) {\n-            error_function(ErrorStatus(ErrorStatus::UNRESOLVED_OBJECT_REFERENCE,\n-                                       string_printf(\"%s (near line %d)\", id.c_str(), line_number)));\n+        auto        e  = resolver.object_for_id.find(id);\n+        if (e == resolver.object_for_id.end())\n+        {\n+            error_function(ErrorStatus(\n+                ErrorStatus::UNRESOLVED_OBJECT_REFERENCE,\n+                string_printf(\"%s (near line %d)\", id.c_str(), line_number)));\n         }\n-        else {\n+        else\n+        {\n             a = any(Retainer<>(e->second));\n         }\n     }\n }\n \n template <typename T>\n-bool SerializableObject::Reader::_fetch(std::string const& key, T* dest, bool* had_null) {\n+bool\n+SerializableObject::Reader::_fetch(\n+    std::string const& key, T* dest, bool* had_null)\n+{\n     auto e = _dict.find(key);\n-    if (e == _dict.end()) {\n+    if (e == _dict.end())\n+    {\n         _error(ErrorStatus(ErrorStatus::KEY_NOT_FOUND, key));\n         return false;\n     }\n-    else if (e->second.type() == typeid(void) && had_null) {\n+    else if (e->second.type() == typeid(void) && had_null)\n+    {\n         _dict.erase(e);\n         *had_null = true;\n         return true;\n     }\n-    else if (e->second.type() != typeid(T)) {\n-        _error(ErrorStatus(ErrorStatus::TYPE_MISMATCH,\n-                           string_printf(\"expected type %s under key '%s': found type %s instead\",\n-                                         demangled_type_name(typeid(T)).c_str(), key.c_str(),\n-                                         demangled_type_name(e->second.type()).c_str())));\n+    else if (e->second.type() != typeid(T))\n+    {\n+        _error(ErrorStatus(\n+            ErrorStatus::TYPE_MISMATCH,\n+            string_printf(\n+                \"expected type %s under key '%s': found type %s instead\",\n+                type_name_for_error_message(typeid(T)).c_str(),\n+                key.c_str(),\n+                type_name_for_error_message(e->second.type()).c_str())));\n         return false;\n     }\n \n-    if (had_null) {\n+    if (had_null)\n+    {\n         *had_null = false;\n     }",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/deserialization.cpp",
        "code_diff": "@@ -313,78 +401,103 @@\nbool SerializableObject::Reader::_fetch(std::string const& key, T* dest, bool* h\n     return true;\n }\n \n-bool SerializableObject::Reader::_fetch(std::string const& key, double* dest) {\n+bool\n+SerializableObject::Reader::_fetch(std::string const& key, double* dest)\n+{\n     auto e = _dict.find(key);\n-    if (e == _dict.end()) {\n+    if (e == _dict.end())\n+    {\n         _error(ErrorStatus(ErrorStatus::KEY_NOT_FOUND, key));\n         return false;\n     }\n \n-    if (e->second.type() == typeid(double)) {\n+    if (e->second.type() == typeid(double))\n+    {\n         *dest = any_cast<double>(e->second);\n         _dict.erase(e);\n         return true;\n     }\n-    else if (e->second.type() == typeid(int)) {\n+    else if (e->second.type() == typeid(int))\n+    {\n         *dest = static_cast<double>(any_cast<int>(e->second));\n         _dict.erase(e);\n         return true;\n     }\n-    else if (e->second.type() == typeid(int64_t)) {\n+    else if (e->second.type() == typeid(int64_t))\n+    {\n         *dest = static_cast<double>(any_cast<int64_t>(e->second));\n         _dict.erase(e);\n         return true;\n     }\n-            \n-    _error(ErrorStatus(ErrorStatus::TYPE_MISMATCH,\n-                       string_printf(\"expected type %s under key '%s': found type %s instead\",\n-                                     demangled_type_name(typeid(double)).c_str(), key.c_str(),\n-                                     demangled_type_name(e->second.type()).c_str())));\n+\n+    _error(ErrorStatus(\n+        ErrorStatus::TYPE_MISMATCH,\n+        string_printf(\n+            \"expected type %s under key '%s': found type %s instead\",\n+            type_name_for_error_message(typeid(double)).c_str(),\n+            key.c_str(),\n+            type_name_for_error_message(e->second.type()).c_str())));\n     return false;\n }\n \n-bool SerializableObject::Reader::_fetch(std::string const& key, int64_t* dest) {\n+bool\n+SerializableObject::Reader::_fetch(std::string const& key, int64_t* dest)\n+{\n     auto e = _dict.find(key);\n-    if (e == _dict.end()) {\n+    if (e == _dict.end())\n+    {\n         _error(ErrorStatus(ErrorStatus::KEY_NOT_FOUND, key));\n         return false;\n     }\n \n-    if (e->second.type() == typeid(int64_t)) {\n+    if (e->second.type() == typeid(int64_t))\n+    {\n         *dest = any_cast<int64_t>(e->second);\n         _dict.erase(e);\n         return true;\n     }\n-    else if (e->second.type() == typeid(int)) {\n+    else if (e->second.type() == typeid(int))\n+    {\n         *dest = any_cast<int>(e->second);\n         _dict.erase(e);\n         return true;\n     }\n-            \n-    _error(ErrorStatus(ErrorStatus::TYPE_MISMATCH,\n-                       string_printf(\"expected type %s under key '%s': found type %s instead\",\n-                                     demangled_type_name(typeid(int64_t)).c_str(), key.c_str(),\n-                                     demangled_type_name(e->second.type()).c_str())));\n+\n+    _error(ErrorStatus(\n+        ErrorStatus::TYPE_MISMATCH,\n+        string_printf(\n+            \"expected type %s under key '%s': found type %s instead\",\n+            type_name_for_error_message(typeid(int64_t)).c_str(),\n+            key.c_str(),\n+            type_name_for_error_message(e->second.type()).c_str())));\n     return false;\n }\n \n-\n-bool SerializableObject::Reader::_fetch(std::string const& key, SerializableObject** dest) {\n+bool\n+SerializableObject::Reader::_fetch(\n+    std::string const& key, SerializableObject** dest)\n+{\n     auto e = _dict.find(key);\n-    if (e == _dict.end()) {\n+    if (e == _dict.end())\n+    {\n         _error(ErrorStatus(ErrorStatus::KEY_NOT_FOUND, key));\n         return false;\n     }\n \n-    if (e->second.type() == typeid(void)) {\n+    if (e->second.type() == typeid(void))\n+    {\n         *dest = nullptr;\n         _dict.erase(e);\n         return true;\n     }\n-    else if (e->second.type() != typeid(SerializableObject::Retainer<>)) {\n-        _error(ErrorStatus(ErrorStatus::TYPE_MISMATCH,\n-                           string_printf(\"expected SerializableObject* under key '%s': found type %s instead\", key.c_str(),\n-                                         demangled_type_name(e->second.type()).c_str())));\n+    else if (e->second.type() != typeid(SerializableObject::Retainer<>))\n+    {\n+        _error(ErrorStatus(\n+            ErrorStatus::TYPE_MISMATCH,\n+            string_printf(\n+                \"expected SerializableObject* under key '%s': found type %s instead\",\n+                key.c_str(),\n+                type_name_for_error_message(e->second.type()).c_str())));\n         return false;\n     }",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/deserialization.cpp",
        "code_diff": "@@ -393,90 +506,135 @@\nbool SerializableObject::Reader::_fetch(std::string const& key, SerializableObje\n     return true;\n }\n \n-bool SerializableObject::Reader::_type_check(std::type_info const& wanted, std::type_info const& found) {\n-    if (wanted != found) {\n-        _error(ErrorStatus(ErrorStatus::TYPE_MISMATCH,\n-                           string_printf(\"while decoding complex STL type, expected type '%s', found type '%s' instead\",\n-                                         demangled_type_name(wanted).c_str(), demangled_type_name(found).c_str())));\n+bool\n+SerializableObject::Reader::_type_check(\n+    std::type_info const& wanted, std::type_info const& found)\n+{\n+    if (wanted != found)\n+    {\n+        _error(ErrorStatus(\n+            ErrorStatus::TYPE_MISMATCH,\n+            string_printf(\n+                \"while decoding complex STL type, expected type '%s', found type '%s' instead\",\n+                type_name_for_error_message(wanted).c_str(),\n+                type_name_for_error_message(found).c_str())));\n         return false;\n     }\n     return true;\n }\n \n-bool SerializableObject::Reader::_type_check_so(std::type_info const& wanted, std::type_info const& found,\n-                                                std::type_info const& so_type) {\n-    if (wanted != found) {\n-        _error(ErrorStatus(ErrorStatus::TYPE_MISMATCH,\n-                           string_printf(\"expected to read a %s, found a %s instead\",\n-                                         demangled_type_name(so_type).c_str(),\n-                                         demangled_type_name(found).c_str())));\n+bool\n+SerializableObject::Reader::_type_check_so(\n+    std::type_info const& wanted,\n+    std::type_info const& found,\n+    std::type_info const& so_type)\n+{\n+    if (wanted != found)\n+    {\n+        _error(ErrorStatus(\n+            ErrorStatus::TYPE_MISMATCH,\n+            string_printf(\n+                \"expected to read a %s, found a %s instead\",\n+                type_name_for_error_message(so_type).c_str(),\n+                type_name_for_error_message(found).c_str())));\n         return false;\n     }\n     return true;\n }\n \n-any SerializableObject::Reader::_decode(_Resolver& resolver) {\n-    if (_dict.find(\"OTIO_SCHEMA\") == _dict.end()) {\n+any\n+SerializableObject::Reader::_decode(_Resolver& resolver)\n+{\n+    if (_dict.find(\"OTIO_SCHEMA\") == _dict.end())\n+    {\n         return any(std::move(_dict));\n     }\n-    \n+\n     std::string schema_name_and_version;\n \n-    if (!_fetch(\"OTIO_SCHEMA\", &schema_name_and_version)) {\n+    if (!_fetch(\"OTIO_SCHEMA\", &schema_name_and_version))\n+    {\n         return any();\n     }\n \n-    if (schema_name_and_version == \"RationalTime.1\") {\n+    if (schema_name_and_version == \"RationalTime.1\")\n+    {\n         double rate, value;\n-        return _fetch(\"rate\", &rate) && _fetch(\"value\", &value) ? any(RationalTime(value, rate)) : any();\n+        return _fetch(\"rate\", &rate) && _fetch(\"value\", &value)\n+                   ? any(RationalTime(value, rate))\n+                   : any();\n     }\n-    else if (schema_name_and_version == \"TimeRange.1\") {\n+    else if (schema_name_and_version == \"TimeRange.1\")\n+    {\n         RationalTime start_time, duration;\n-        return _fetch(\"start_time\", &start_time) && _fetch(\"duration\", &duration) ?\n-                any(TimeRange(start_time, duration)) : any();\n+        return _fetch(\"start_time\", &start_time) &&\n+                       _fetch(\"duration\", &duration)\n+                   ? any(TimeRange(start_time, duration))\n+                   : any();\n     }\n-    else if (schema_name_and_version == \"TimeTransform.1\") {\n+    else if (schema_name_and_version == \"TimeTransform.1\")\n+    {\n         RationalTime offset;\n-        double rate, scale;\n-        return _fetch(\"offset\", &offset) && _fetch(\"rate\", &rate) && _fetch(\"scale\", &scale) ?\n-            any(TimeTransform(offset, scale, rate)) : any();\n+        double       rate, scale;\n+        return _fetch(\"offset\", &offset) && _fetch(\"rate\", &rate) &&\n+                       _fetch(\"scale\", &scale)\n+                   ? any(TimeTransform(offset, scale, rate))\n+                   : any();\n     }\n-    else if (schema_name_and_version == \"SerializableObjectRef.1\") {\n+    else if (schema_name_and_version == \"SerializableObjectRef.1\")\n+    {\n         std::string ref_id;\n-        if (!_fetch(\"id\", &ref_id)) {\n+        if (!_fetch(\"id\", &ref_id))\n+        {\n             return any();\n         }\n \n-        return any(SerializableObject::ReferenceId { ref_id });\n+        return any(SerializableObject::ReferenceId{ ref_id });\n     }\n-    else {\n+    else\n+    {\n         std::string ref_id;\n-        if (_dict.find(\"OTIO_REF_ID\") != _dict.end()) {\n-            if (!_fetch(\"OTIO_REF_ID\", &ref_id)) {\n+        if (_dict.find(\"OTIO_REF_ID\") != _dict.end())\n+        {\n+            if (!_fetch(\"OTIO_REF_ID\", &ref_id))\n+            {\n                 return any();\n             }\n-        \n+\n             auto e = resolver.object_for_id.find(ref_id);\n-            if (e != resolver.object_for_id.end()) {\n-                _error(ErrorStatus(ErrorStatus::DUPLICATE_OBJECT_REFERENCE, ref_id));\n+            if (e != resolver.object_for_id.end())\n+            {\n+                _error(ErrorStatus(\n+                    ErrorStatus::DUPLICATE_OBJECT_REFERENCE, ref_id));\n                 return any();\n             }\n         }\n \n         TypeRegistry& r = TypeRegistry::instance();\n-        std::string schema_name;\n-        int schema_version;\n-        \n-        if (!split_schema_string(schema_name_and_version, &schema_name, &schema_version)) {\n-            _error(ErrorStatus(ErrorStatus::MALFORMED_SCHEMA,\n-                               string_printf(\"badly formed schema version string '%s'\", schema_name_and_version.c_str())));\n+        std::string   schema_name;\n+        int           schema_version;\n+\n+        if (!split_schema_string(\n+                schema_name_and_version, &schema_name, &schema_version))\n+        {\n+            _error(ErrorStatus(\n+                ErrorStatus::MALFORMED_SCHEMA,\n+                string_printf(\n+                    \"badly formed schema version string '%s'\",\n+                    schema_name_and_version.c_str())));\n             return any();\n         }\n-            \n+\n         ErrorStatus error_status;\n-        if (SerializableObject* so = r._instance_from_schema(schema_name, schema_version, _dict,\n-                                                             true /* internal_read */, &error_status)) {\n-            if (!ref_id.empty()) {\n+        if (SerializableObject* so = r._instance_from_schema(\n+                schema_name,\n+                schema_version,\n+                _dict,\n+                true /* internal_read */,\n+                &error_status))\n+        {\n+            if (!ref_id.empty())\n+            {\n                 resolver.object_for_id[ref_id] = so;\n             }\n             resolver.data_for_object.emplace(so, std::move(_dict));",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/imageSequenceReference.cpp",
        "code_diff": "@@ -1,186 +1,253 @@\n#include \"opentimelineio/imageSequenceReference.h\"\n \n-namespace opentimelineio { namespace OPENTIMELINEIO_VERSION  {\n-\n-ImageSequenceReference::ImageSequenceReference(std::string const& target_url_base,\n-                      std::string const& name_prefix,\n-                      std::string const& name_suffix,\n-                      int start_frame,\n-                      int frame_step,\n-                      double rate,\n-                      int frame_zero_padding,\n-                      MissingFramePolicy const missing_frame_policy,\n-                      optional<TimeRange> const& available_range,\n-                      AnyDictionary const& metadata)\n-    : Parent(std::string(), available_range, metadata),\n-    _target_url_base(target_url_base),\n-    _name_prefix(name_prefix),\n-    _name_suffix(name_suffix),\n-    _start_frame {start_frame},\n-    _frame_step {frame_step},\n-    _rate {rate},\n-    _frame_zero_padding {frame_zero_padding},\n-    _missing_frame_policy {missing_frame_policy} {\n-    }\n-\n-    ImageSequenceReference::~ImageSequenceReference() {\n-    }\n-\n-    RationalTime\n-    ImageSequenceReference::frame_duration() const noexcept {\n-        return RationalTime((double)_frame_step, _rate);\n-    }\n-\n-    int ImageSequenceReference::end_frame() const {\n-        if (!this->available_range().has_value()) {\n-            return _start_frame;\n-        }\n+namespace opentimelineio { namespace OPENTIMELINEIO_VERSION {\n+\n+ImageSequenceReference::ImageSequenceReference(\n+    std::string const&         target_url_base,\n+    std::string const&         name_prefix,\n+    std::string const&         name_suffix,\n+    int                        start_frame,\n+    int                        frame_step,\n+    double                     rate,\n+    int                        frame_zero_padding,\n+    MissingFramePolicy const   missing_frame_policy,\n+    optional<TimeRange> const& available_range,\n+    AnyDictionary const&       metadata)\n+    : Parent(std::string(), available_range, metadata)\n+    , _target_url_base(target_url_base)\n+    , _name_prefix(name_prefix)\n+    , _name_suffix(name_suffix)\n+    , _start_frame{ start_frame }\n+    , _frame_step{ frame_step }\n+    , _rate{ rate }\n+    , _frame_zero_padding{ frame_zero_padding }\n+    , _missing_frame_policy{ missing_frame_policy }\n+{}\n+\n+ImageSequenceReference::~ImageSequenceReference()\n+{}\n \n-        int num_frames = this->available_range().value().duration().to_frames(_rate);\n+RationalTime\n+ImageSequenceReference::frame_duration() const noexcept\n+{\n+    return RationalTime((double) _frame_step, _rate);\n+}\n \n-        // Subtract 1 for inclusive frame ranges\n-        return (_start_frame + num_frames - 1);\n+int\n+ImageSequenceReference::end_frame() const\n+{\n+    if (!this->available_range().has_value())\n+    {\n+        return _start_frame;\n     }\n \n-    int ImageSequenceReference::number_of_images_in_sequence() const {\n-        if (!this->available_range().has_value()) {\n-            return 0;\n-        }\n+    int num_frames =\n+        this->available_range().value().duration().to_frames(_rate);\n+\n+    // Subtract 1 for inclusive frame ranges\n+    return (_start_frame + num_frames - 1);\n+}\n \n-        double playback_rate = (_rate / (double)_frame_step);\n-        int num_frames = this->available_range().value().duration().to_frames(playback_rate);\n-        return num_frames;\n+int\n+ImageSequenceReference::number_of_images_in_sequence() const\n+{\n+    if (!this->available_range().has_value())\n+    {\n+        return 0;\n     }\n \n-    int ImageSequenceReference::frame_for_time(RationalTime const& time, ErrorStatus* error_status) const {\n-        if (!this->available_range().has_value() || !this->available_range().value().contains(time)) {\n+    double playback_rate = (_rate / (double) _frame_step);\n+    int    num_frames =\n+        this->available_range().value().duration().to_frames(playback_rate);\n+    return num_frames;\n+}\n+\n+int\n+ImageSequenceReference::frame_for_time(\n+    RationalTime const& time, ErrorStatus* error_status) const\n+{\n+    if (!this->available_range().has_value() ||\n+        !this->available_range().value().contains(time))\n+    {\n+        if (error_status)\n+        {\n             *error_status = ErrorStatus(ErrorStatus::INVALID_TIME_RANGE);\n-            return 0;\n         }\n+        return 0;\n+    }\n \n-        RationalTime start = this->available_range().value().start_time();\n-        RationalTime duration_from_start = (time - start);\n-        int frame_offset = duration_from_start.to_frames(_rate);\n+    RationalTime start = this->available_range().value().start_time();\n+    RationalTime duration_from_start = (time - start);\n+    int          frame_offset        = duration_from_start.to_frames(_rate);\n \n+    if (error_status)\n+    {\n         *error_status = ErrorStatus(ErrorStatus::OK);\n-\n-        return (_start_frame + frame_offset);\n     }\n \n-    std::string\n-    ImageSequenceReference::target_url_for_image_number(int image_number, ErrorStatus* error_status) const {\n-        if (_rate == 0) {\n-            *error_status = ErrorStatus(ErrorStatus::ILLEGAL_INDEX, \"Zero rate sequence has no frames.\");\n-            return std::string();\n+    return (_start_frame + frame_offset);\n+}\n+\n+std::string\n+ImageSequenceReference::target_url_for_image_number(\n+    int image_number, ErrorStatus* error_status) const\n+{\n+    if (_rate == 0)\n+    {\n+        if (error_status)\n+        {\n+            *error_status = ErrorStatus(\n+                ErrorStatus::ILLEGAL_INDEX,\n+                \"Zero rate sequence has no frames.\");\n         }\n-        else if (!this->available_range().has_value() || this->available_range().value().duration().value() == 0) {\n-            *error_status = ErrorStatus(ErrorStatus::ILLEGAL_INDEX, \"Zero duration sequences has no frames.\");\n-            return std::string();\n+        return std::string();\n+    }\n+    else if (\n+        !this->available_range().has_value() ||\n+        this->available_range().value().duration().value() == 0)\n+    {\n+        if (error_status)\n+        {\n+            *error_status = ErrorStatus(\n+                ErrorStatus::ILLEGAL_INDEX,\n+                \"Zero duration sequences has no frames.\");\n         }\n-        else if (image_number >= this->number_of_images_in_sequence()) {\n+        return std::string();\n+    }\n+    else if (image_number >= this->number_of_images_in_sequence())\n+    {\n+        if (error_status)\n+        {\n             *error_status = ErrorStatus(ErrorStatus::ILLEGAL_INDEX);\n-            return std::string();\n         }\n-        const int file_image_num = _start_frame + (image_number * _frame_step);\n-        const bool is_negative = (file_image_num < 0);\n+        return std::string();\n+    }\n+    const int  file_image_num = _start_frame + (image_number * _frame_step);\n+    const bool is_negative    = (file_image_num < 0);\n \n-        std::string image_num_string = std::to_string(abs(file_image_num));\n+    std::string image_num_string = std::to_string(abs(file_image_num));\n \n-        std::string zero_pad = std::string();\n-        if (static_cast<int>(image_num_string.length()) <  _frame_zero_padding) {\n-            zero_pad = std::string(_frame_zero_padding - image_num_string.length(), '0');\n-        }\n+    std::string zero_pad = std::string();\n+    if (static_cast<int>(image_num_string.length()) < _frame_zero_padding)\n+    {\n+        zero_pad =\n+            std::string(_frame_zero_padding - image_num_string.length(), '0');\n+    }\n \n-        std::string sign = std::string();\n-        if (is_negative) {\n-            sign = \"-\";\n-        }\n+    std::string sign = std::string();\n+    if (is_negative)\n+    {\n+        sign = \"-\";\n+    }\n \n-        // If the base does not include a trailing slash, add it\n-        std::string path_sep = std::string();\n-        const auto target_url_base_len = _target_url_base.length();\n-        if (target_url_base_len > 0 && _target_url_base.compare(target_url_base_len - 1, 1, \"/\") != 0) {\n-            path_sep = \"/\";\n-        }\n+    // If the base does not include a trailing slash, add it\n+    std::string path_sep            = std::string();\n+    const auto  target_url_base_len = _target_url_base.length();\n+    if (target_url_base_len > 0 &&\n+        _target_url_base.compare(target_url_base_len - 1, 1, \"/\") != 0)\n+    {\n+        path_sep = \"/\";\n+    }\n \n-        std::string out_string = _target_url_base + path_sep + _name_prefix + sign + zero_pad + image_num_string + _name_suffix;\n+    std::string out_string = _target_url_base + path_sep + _name_prefix + sign +\n+                             zero_pad + image_num_string + _name_suffix;\n+    if (error_status)\n+    {\n         *error_status = ErrorStatus(ErrorStatus::OK);\n-        return out_string;\n     }\n+    return out_string;\n+}\n \n-    RationalTime\n-    ImageSequenceReference::presentation_time_for_image_number(int image_number, ErrorStatus* error_status) const {\n-        if (image_number >= this->number_of_images_in_sequence()) {\n+RationalTime\n+ImageSequenceReference::presentation_time_for_image_number(\n+    int image_number, ErrorStatus* error_status) const\n+{\n+    if (image_number >= this->number_of_images_in_sequence())\n+    {\n+        if (error_status)\n+        {\n             *error_status = ErrorStatus(ErrorStatus::ILLEGAL_INDEX);\n-            return RationalTime();\n         }\n-\n-        auto first_frame_time = this->available_range().value().start_time();\n-        auto time_multiplier = TimeTransform(first_frame_time, image_number, -1);\n-        return time_multiplier.applied_to(frame_duration());\n+        return RationalTime();\n     }\n \n-    bool ImageSequenceReference::read_from(Reader& reader) {\n+    auto first_frame_time = this->available_range().value().start_time();\n+    auto time_multiplier  = TimeTransform(first_frame_time, image_number, -1);\n+    return time_multiplier.applied_to(frame_duration());\n+}\n \n-        int64_t start_frame_value = 0;\n-        int64_t frame_step_value = 0;\n-        int64_t frame_zero_padding_value = 0;\n+bool\n+ImageSequenceReference::read_from(Reader& reader)\n+{\n \n-        auto result = reader.read(\"target_url_base\", &_target_url_base) &&\n-            reader.read(\"name_prefix\", &_name_prefix) &&\n-            reader.read(\"name_suffix\", &_name_suffix) &&\n-            reader.read(\"start_frame\", &start_frame_value) &&\n-            reader.read(\"frame_step\", &frame_step_value) &&\n-            reader.read(\"rate\", &_rate) &&\n-            reader.read(\"frame_zero_padding\", &frame_zero_padding_value);\n+    int64_t start_frame_value        = 0;\n+    int64_t frame_step_value         = 0;\n+    int64_t frame_zero_padding_value = 0;\n \n-        _start_frame = static_cast<int>(start_frame_value);\n-        _frame_step = static_cast<int>(frame_step_value);\n-        _frame_zero_padding = static_cast<int>(frame_zero_padding_value);\n+    auto result = reader.read(\"target_url_base\", &_target_url_base) &&\n+                  reader.read(\"name_prefix\", &_name_prefix) &&\n+                  reader.read(\"name_suffix\", &_name_suffix) &&\n+                  reader.read(\"start_frame\", &start_frame_value) &&\n+                  reader.read(\"frame_step\", &frame_step_value) &&\n+                  reader.read(\"rate\", &_rate) &&\n+                  reader.read(\"frame_zero_padding\", &frame_zero_padding_value);\n \n-        std::string missing_frame_policy_value;\n-        result && reader.read(\"missing_frame_policy\", &missing_frame_policy_value);\n-        if (!result) {\n-            return result;\n-        }\n+    _start_frame        = static_cast<int>(start_frame_value);\n+    _frame_step         = static_cast<int>(frame_step_value);\n+    _frame_zero_padding = static_cast<int>(frame_zero_padding_value);\n \n-        if (missing_frame_policy_value == \"error\") {\n-            _missing_frame_policy = MissingFramePolicy::error;\n-        }\n-        else if (missing_frame_policy_value == \"black\") {\n-            _missing_frame_policy = MissingFramePolicy::black;\n-        }\n-        else if (missing_frame_policy_value == \"hold\") {\n-            _missing_frame_policy = MissingFramePolicy::hold;\n-        }\n-        else {\n-            // Unrecognized value\n-            ErrorStatus error_status = ErrorStatus(ErrorStatus::JSON_PARSE_ERROR,\n-                       \"Unknown missing_frame_policy: \" + missing_frame_policy_value);\n-            reader.error(error_status);\n-            return false;\n-        }\n+    std::string missing_frame_policy_value;\n+    result&& reader.read(\"missing_frame_policy\", &missing_frame_policy_value);\n+    if (!result)\n+    {\n+        return result;\n+    }\n \n-        return result && Parent::read_from(reader);\n+    if (missing_frame_policy_value == \"error\")\n+    {\n+        _missing_frame_policy = MissingFramePolicy::error;\n+    }\n+    else if (missing_frame_policy_value == \"black\")\n+    {\n+        _missing_frame_policy = MissingFramePolicy::black;\n+    }\n+    else if (missing_frame_policy_value == \"hold\")\n+    {\n+        _missing_frame_policy = MissingFramePolicy::hold;\n+    }\n+    else\n+    {\n+        // Unrecognized value\n+        ErrorStatus error_status = ErrorStatus(\n+            ErrorStatus::JSON_PARSE_ERROR,\n+            \"Unknown missing_frame_policy: \" + missing_frame_policy_value);\n+        reader.error(error_status);\n+        return false;\n     }\n \n-    void ImageSequenceReference::write_to(Writer& writer) const {\n-        int64_t start_frame_value = static_cast<int64_t>(_start_frame);\n-        int64_t frame_step_value = static_cast<int64_t>(_frame_step);\n-        int64_t frame_zero_padding_value = static_cast<int64_t>(_frame_zero_padding);\n+    return result && Parent::read_from(reader);\n+}\n \n-        Parent::write_to(writer);\n-        writer.write(\"target_url_base\", _target_url_base);\n-        writer.write(\"name_prefix\", _name_prefix);\n-        writer.write(\"name_suffix\", _name_suffix);\n-        writer.write(\"start_frame\", start_frame_value);\n-        writer.write(\"frame_step\", frame_step_value);\n-        writer.write(\"rate\", _rate);\n-        writer.write(\"frame_zero_padding\", frame_zero_padding_value);\n+void\n+ImageSequenceReference::write_to(Writer& writer) const\n+{\n+    int64_t start_frame_value = static_cast<int64_t>(_start_frame);\n+    int64_t frame_step_value  = static_cast<int64_t>(_frame_step);\n+    int64_t frame_zero_padding_value =\n+        static_cast<int64_t>(_frame_zero_padding);\n \n-        std::string missing_frame_policy_value;\n-        switch (_missing_frame_policy)\n-        {\n+    Parent::write_to(writer);\n+    writer.write(\"target_url_base\", _target_url_base);\n+    writer.write(\"name_prefix\", _name_prefix);\n+    writer.write(\"name_suffix\", _name_suffix);\n+    writer.write(\"start_frame\", start_frame_value);\n+    writer.write(\"frame_step\", frame_step_value);\n+    writer.write(\"rate\", _rate);\n+    writer.write(\"frame_zero_padding\", frame_zero_padding_value);\n+\n+    std::string missing_frame_policy_value;\n+    switch (_missing_frame_policy)\n+    {\n         case MissingFramePolicy::error:\n             missing_frame_policy_value = \"error\";\n             break;",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/serializableObject.cpp",
        "code_diff": "@@ -1,123 +1,191 @@\n#include \"opentimelineio/serializableObject.h\"\n-#include \"opentimelineio/serialization.h\"\n #include \"opentimelineio/deserialization.h\"\n+#include \"opentimelineio/serialization.h\"\n+#include \"stringUtils.h\"\n+\n+namespace opentimelineio { namespace OPENTIMELINEIO_VERSION {\n \n-namespace opentimelineio { namespace OPENTIMELINEIO_VERSION  {\n-    \n SerializableObject::SerializableObject()\n-    : _cached_type_record(nullptr) {\n+    : _cached_type_record(nullptr)\n+{\n     _managed_ref_count = 0;\n }\n \n-SerializableObject::~SerializableObject() {\n+SerializableObject::~SerializableObject()\n+{}\n+\n+// forwarded functions\n+std::string\n+SerializableObject::Reader::fwd_type_name_for_error_message(\n+    std::type_info const& t)\n+{\n+    return type_name_for_error_message(t);\n+}\n+std::string\n+SerializableObject::Reader::fwd_type_name_for_error_message(any const& a)\n+{\n+    return type_name_for_error_message(a);\n+}\n+std::string\n+SerializableObject::Reader::fwd_type_name_for_error_message(\n+    class SerializableObject* so)\n+{\n+    return type_name_for_error_message(so);\n }\n \n-TypeRegistry::_TypeRecord const* SerializableObject::_type_record() const {\n+TypeRegistry::_TypeRecord const*\n+SerializableObject::_type_record() const\n+{\n     std::lock_guard<std::mutex> lock(_mutex);\n-    if (!_cached_type_record) {\n-        _cached_type_record = TypeRegistry::instance()._lookup_type_record(typeid(*this));\n-        if (!_cached_type_record) {\n-            fatal_error(string_printf(\"Code for C++ type %s has not been registered via \"\n-                                      \"TypeRegistry::register_type<T>()\",\n-                                      demangled_type_name(typeid(*this)).c_str()));\n+    if (!_cached_type_record)\n+    {\n+        _cached_type_record =\n+            TypeRegistry::instance()._lookup_type_record(typeid(*this));\n+        if (!_cached_type_record)\n+        {\n+            fatal_error(string_printf(\n+                \"Code for C++ type %s has not been registered via \"\n+                \"TypeRegistry::register_type<T>()\",\n+                type_name_for_error_message(typeid(*this)).c_str()));\n         }\n     }\n \n     return _cached_type_record;\n }\n \n-bool SerializableObject::_is_deletable() {\n+bool\n+SerializableObject::_is_deletable()\n+{\n     std::lock_guard<std::mutex> lock(_mutex);\n     return _managed_ref_count == 0;\n }\n \n-bool SerializableObject::possibly_delete() {\n-    if (!_is_deletable()) {\n+bool\n+SerializableObject::possibly_delete()\n+{\n+    if (!_is_deletable())\n+    {\n         return false;\n     }\n     delete this;\n     return true;\n }\n \n-bool SerializableObject::read_from(Reader& reader) {\n+bool\n+SerializableObject::read_from(Reader& reader)\n+{\n     /*\n      * Want to move everything from reader._dict into\n      * _dynamic_fields, overwriting as we go.\n      */\n-    for (auto& e: reader._dict) {\n+    for (auto& e: reader._dict)\n+    {\n         auto it = _dynamic_fields.find(e.first);\n-        if (it != _dynamic_fields.end()) {\n+        if (it != _dynamic_fields.end())\n+        {\n             it->second.swap(e.second);\n         }\n-        else {\n+        else\n+        {\n             _dynamic_fields.emplace(e.first, std::move(e.second));\n         }\n     }\n     return true;\n }\n \n-void SerializableObject::write_to(Writer& writer) const {\n-    for (auto e: _dynamic_fields) {\n+void\n+SerializableObject::write_to(Writer& writer) const\n+{\n+    for (auto e: _dynamic_fields)\n+    {\n         writer.write(e.first, e.second);\n     }\n }\n \n-bool SerializableObject::is_unknown_schema() const {\n+bool\n+SerializableObject::is_unknown_schema() const\n+{\n     return false;\n }\n \n-std::string SerializableObject::to_json_string(ErrorStatus* error_status, int indent) const {\n-    return serialize_json_to_string(any(Retainer<>(this)), error_status, indent);\n+std::string\n+SerializableObject::to_json_string(ErrorStatus* error_status, int indent) const\n+{\n+    return serialize_json_to_string(\n+        any(Retainer<>(this)), error_status, indent);\n }\n \n-bool SerializableObject::to_json_file(std::string const& file_name, ErrorStatus* error_status, int indent) const {\n-    return serialize_json_to_file(any(Retainer<>(this)), file_name, error_status, indent);\n+bool\n+SerializableObject::to_json_file(\n+    std::string const& file_name, ErrorStatus* error_status, int indent) const\n+{\n+    return serialize_json_to_file(\n+        any(Retainer<>(this)), file_name, error_status, indent);\n }\n \n-SerializableObject* SerializableObject::from_json_string(std::string const& input, ErrorStatus* error_status) {\n+SerializableObject*\n+SerializableObject::from_json_string(\n+    std::string const& input, ErrorStatus* error_status)\n+{\n     any dest;\n \n-    if (!deserialize_json_from_string(input, &dest, error_status)) {\n+    if (!deserialize_json_from_string(input, &dest, error_status))\n+    {\n         return nullptr;\n     }\n \n-    if (dest.type() != typeid(Retainer<>)) {\n-        if (!(*error_status)) {\n-            *error_status = ErrorStatus(ErrorStatus::TYPE_MISMATCH,\n-                                        string_printf(\"Expected a SerializableObject*, found object of type '%s' instead\",\n-                                                      demangled_type_name(dest.type()).c_str()));\n+    if (dest.type() != typeid(Retainer<>))\n+    {\n+        if (error_status)\n+        {\n+            *error_status = ErrorStatus(\n+                ErrorStatus::TYPE_MISMATCH,\n+                string_printf(\n+                    \"Expected a SerializableObject*, found object of type '%s' instead\",\n+                    type_name_for_error_message(dest.type()).c_str()));\n         }\n         return nullptr;\n     }\n \n     return any_cast<Retainer<>&>(dest).take_value();\n }\n \n-\n-SerializableObject* SerializableObject::from_json_file(std::string const& file_name, ErrorStatus* error_status) {\n+SerializableObject*\n+SerializableObject::from_json_file(\n+    std::string const& file_name, ErrorStatus* error_status)\n+{\n     any dest;\n \n-    if (!deserialize_json_from_file(file_name, &dest, error_status)) {\n+    if (!deserialize_json_from_file(file_name, &dest, error_status))\n+    {\n         return nullptr;\n     }\n \n-    if (dest.type() != typeid(Retainer<>)) {\n-        if (!(*error_status)) {\n-            *error_status = ErrorStatus(ErrorStatus::TYPE_MISMATCH,\n-                                        string_printf(\"Expected a SerializableObject*, found object of type '%s' instead\",\n-                                                      demangled_type_name(dest.type()).c_str()));\n+    if (dest.type() != typeid(Retainer<>))\n+    {\n+        if (error_status)\n+        {\n+            *error_status = ErrorStatus(\n+                ErrorStatus::TYPE_MISMATCH,\n+                string_printf(\n+                    \"Expected a SerializableObject*, found object of type '%s' instead\",\n+                    type_name_for_error_message(dest.type()).c_str()));\n         }\n         return nullptr;\n     }\n \n     return any_cast<Retainer<>&>(dest).take_value();\n }\n \n-std::string SerializableObject::_schema_name_for_reference() const {\n+std::string\n+SerializableObject::_schema_name_for_reference() const\n+{\n     return schema_name();\n }\n \n-void SerializableObject::_managed_retain() {\n+void\n+SerializableObject::_managed_retain()\n+{\n     {\n         std::lock_guard<std::mutex> lock(_mutex);\n         if (_managed_ref_count++ != 1 || !_external_keepalive_monitor)",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/serializableObject.cpp",
        "code_diff": "@@ -129,16 +197,20 @@\nvoid SerializableObject::_managed_retain() {\n     _external_keepalive_monitor();\n }\n \n-void SerializableObject::_managed_release() {\n+void\n+SerializableObject::_managed_release()\n+{\n     _mutex.lock();\n \n-    if (--_managed_ref_count == 0) {\n+    if (--_managed_ref_count == 0)\n+    {\n         _mutex.unlock();\n         delete this;\n         return;\n     }\n-        \n-    if (_managed_ref_count != 1 || !_external_keepalive_monitor) {\n+\n+    if (_managed_ref_count != 1 || !_external_keepalive_monitor)\n+    {\n         _mutex.unlock();\n         return;\n     }",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -1,27 +1,27 @@\n#include \"opentimelineio/serializableObject.h\"\n #include \"opentimelineio/unknownSchema.h\"\n-#include \"opentimelineio/stringUtils.h\"\n+#include \"stringUtils.h\"\n \n #define RAPIDJSON_NAMESPACE OTIO_rapidjson\n+#include <rapidjson/ostreamwrapper.h>\n+#include <rapidjson/prettywriter.h>\n #include <rapidjson/stringbuffer.h>\n #include <rapidjson/writer.h>\n-#include <rapidjson/prettywriter.h>\n-#include <rapidjson/ostreamwrapper.h>\n \n #include <fstream>\n \n #if defined(_WINDOWS)\n-#ifndef WIN32_LEAN_AND_MEAN\n-#define WIN32_LEAN_AND_MEAN\n-#endif // WIN32_LEAN_AND_MEAN\n-#ifndef NOMINMAX\n-#define NOMINMAX\n-#endif // NOMINMAX\n-#include <windows.h>\n+#    ifndef WIN32_LEAN_AND_MEAN\n+#        define WIN32_LEAN_AND_MEAN\n+#    endif // WIN32_LEAN_AND_MEAN\n+#    ifndef NOMINMAX\n+#        define NOMINMAX\n+#    endif // NOMINMAX\n+#    include <windows.h>\n #endif\n \n-namespace opentimelineio { namespace OPENTIMELINEIO_VERSION  {\n-    \n+namespace opentimelineio { namespace OPENTIMELINEIO_VERSION {\n+\n /**\n  * Base class for encoders.  Since rapidjson is templated (no virtual functions)\n  * we need to do our dynamically classed hierarchy to abstract away which writer",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -32,42 +32,44 @@\nnamespace opentimelineio { namespace OPENTIMELINEIO_VERSION  {\n  * This hierarchy is not visible outside this library, so we're not very concerned\n  * about access control within this class.\n  */\n-class Encoder {\n+class Encoder\n+{\n public:\n-    virtual ~Encoder() {\n-    }\n+    virtual ~Encoder() {}\n \n-    bool has_errored(ErrorStatus* error_status) {\n-        *error_status = _error_status;\n-        return bool(_error_status);\n-    }\n-        \n-    bool has_errored() {\n-        return bool(_error_status);\n+    bool has_errored(ErrorStatus* error_status)\n+    {\n+        if (error_status)\n+        {\n+            *error_status = _error_status;\n+        }\n+        return is_error(_error_status);\n     }\n-    \n+\n+    bool has_errored() { return is_error(_error_status); }\n \n     virtual void start_object() = 0;\n-    virtual void end_object() = 0;\n+    virtual void end_object()   = 0;\n \n     virtual void start_array(size_t) = 0;\n-    virtual void end_array() = 0;\n-\n-    virtual void write_key(std::string const& key) = 0;\n-    virtual void write_null_value() = 0;\n-    virtual void write_value(bool value) = 0;\n-    virtual void write_value(int value) = 0;\n-    virtual void write_value(int64_t value) = 0;\n-    virtual void write_value(uint64_t value) = 0;\n-    virtual void write_value(double value) = 0;\n-    virtual void write_value(std::string const& value) = 0;\n-    virtual void write_value(class RationalTime const& value) = 0;\n-    virtual void write_value(class TimeRange const& value) = 0;\n-    virtual void write_value(class TimeTransform const& value) = 0;\n+    virtual void end_array()         = 0;\n+\n+    virtual void write_key(std::string const& key)                   = 0;\n+    virtual void write_null_value()                                  = 0;\n+    virtual void write_value(bool value)                             = 0;\n+    virtual void write_value(int value)                              = 0;\n+    virtual void write_value(int64_t value)                          = 0;\n+    virtual void write_value(uint64_t value)                         = 0;\n+    virtual void write_value(double value)                           = 0;\n+    virtual void write_value(std::string const& value)               = 0;\n+    virtual void write_value(class RationalTime const& value)        = 0;\n+    virtual void write_value(class TimeRange const& value)           = 0;\n+    virtual void write_value(class TimeTransform const& value)       = 0;\n     virtual void write_value(struct SerializableObject::ReferenceId) = 0;\n \n protected:\n-    void _error(ErrorStatus const& error_status) {\n+    void _error(ErrorStatus const& error_status)\n+    {\n         _error_status = error_status;\n     }",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -81,157 +83,171 @@\nclass Encoder {\n  * The dictionary is than handed off to a CloningDecoder, to complete\n  * copying of a SerializableObject instance.\n  */\n-class CloningEncoder : public Encoder {\n+class CloningEncoder : public Encoder\n+{\n public:\n-    CloningEncoder(bool actually_clone) {\n+    CloningEncoder(bool actually_clone)\n+    {\n         using namespace std::placeholders;\n         _error_function = std::bind(&CloningEncoder::_error, this, _1);\n         _actually_clone = actually_clone;\n     }\n \n-    virtual ~CloningEncoder() {\n-    }\n+    virtual ~CloningEncoder() {}\n \n-    void write_key(std::string const& key) {\n-        if (has_errored()) {\n+    void write_key(std::string const& key)\n+    {\n+        if (has_errored())\n+        {\n             return;\n         }\n-        \n-        if (_stack.empty() || !_stack.back().is_dict) {\n-            _internal_error(\"Encoder::write_key  called while not decoding an object\");\n+\n+        if (_stack.empty() || !_stack.back().is_dict)\n+        {\n+            _internal_error(\n+                \"Encoder::write_key  called while not decoding an object\");\n             return;\n         }\n \n         _stack.back().cur_key = key;\n     }\n \n-    void _store(any&& a) {\n-        if (has_errored()) {\n+    void _store(any&& a)\n+    {\n+        if (has_errored())\n+        {\n             return;\n         }\n-        \n-        if (_stack.empty()) {\n+\n+        if (_stack.empty())\n+        {\n             _root.swap(a);\n         }\n-        else {\n+        else\n+        {\n             auto& top = _stack.back();\n-            if (top.is_dict) {\n+            if (top.is_dict)\n+            {\n                 top.dict.emplace(_stack.back().cur_key, a);\n             }\n-            else {\n+            else\n+            {\n                 top.array.emplace_back(a);\n             }\n         }\n     }\n \n-    void write_null_value() {\n-        _store(any());\n-    }\n+    void write_null_value() { _store(any()); }\n \n-    void write_value(bool value) {\n-        _store(any(value));\n-    }\n+    void write_value(bool value) { _store(any(value)); }\n \n-    void write_value(int value) {\n-        _store(any(value));\n-    }\n+    void write_value(int value) { _store(any(value)); }\n \n-    void write_value(int64_t value) {\n-        _store(any(value));\n-    }\n+    void write_value(int64_t value) { _store(any(value)); }\n \n-    void write_value(uint64_t value) {\n-        _store(any(value));\n-    }\n+    void write_value(uint64_t value) { _store(any(value)); }\n \n-    void write_value(std::string const& value) {\n-        _store(any(value));\n-    }\n+    void write_value(std::string const& value) { _store(any(value)); }\n \n-    void write_value(double value) {\n-        _store(any(value));\n-    }\n+    void write_value(double value) { _store(any(value)); }\n \n-    void write_value(RationalTime const& value) {\n-        _store(any(value));\n-    }\n+    void write_value(RationalTime const& value) { _store(any(value)); }\n \n-    void write_value(TimeRange const& value) {\n-        _store(any(value));\n-    }\n+    void write_value(TimeRange const& value) { _store(any(value)); }\n \n-    void write_value(TimeTransform const& value) {\n-        _store(any(value));\n-    }\n-    \n-    void write_value(SerializableObject::ReferenceId value) {\n+    void write_value(TimeTransform const& value) { _store(any(value)); }\n+\n+    void write_value(SerializableObject::ReferenceId value)\n+    {\n         _store(any(value));\n     }\n \n-    void start_array(size_t /* n */) {\n-        if (has_errored()) {\n+    void start_array(size_t /* n */)\n+    {\n+        if (has_errored())\n+        {\n             return;\n         }\n-        \n-        _stack.emplace_back(_DictOrArray { false /* is_dict*/ });\n+\n+        _stack.emplace_back(_DictOrArray{ false /* is_dict*/ });\n     }\n-    \n-    void start_object() {\n-        if (has_errored()) {\n+\n+    void start_object()\n+    {\n+        if (has_errored())\n+        {\n             return;\n         }\n-        \n-        _stack.emplace_back(_DictOrArray { true /* is_dict*/ });\n+\n+        _stack.emplace_back(_DictOrArray{ true /* is_dict*/ });\n     }\n \n-    void end_array() {\n-        if (has_errored()) {\n+    void end_array()\n+    {\n+        if (has_errored())\n+        {\n             return;\n         }\n \n-        if (_stack.empty()) {\n-            _internal_error(\"Encoder::end_array() called without matching start_array()\");\n+        if (_stack.empty())\n+        {\n+            _internal_error(\n+                \"Encoder::end_array() called without matching start_array()\");\n         }\n-        else {\n+        else\n+        {\n             auto& top = _stack.back();\n-            if (top.is_dict) {\n-                _internal_error(\"Encoder::end_array() called without matching start_array()\");\n+            if (top.is_dict)\n+            {\n+                _internal_error(\n+                    \"Encoder::end_array() called without matching start_array()\");\n                 _stack.pop_back();\n             }\n-            else {\n+            else\n+            {\n                 AnyVector va;\n                 va.swap(top.array);\n                 _stack.pop_back();\n                 _store(any(std::move(va)));\n             }\n-\n         }\n     }\n \n-    void end_object() {\n-        if (has_errored()) {\n+    void end_object()\n+    {\n+        if (has_errored())\n+        {\n             return;\n         }\n \n-        if (_stack.empty()) {\n-            _internal_error(\"Encoder::end_object() called without matching start_object()\");\n+        if (_stack.empty())\n+        {\n+            _internal_error(\n+                \"Encoder::end_object() called without matching start_object()\");\n         }\n-        else {\n+        else\n+        {\n             auto& top = _stack.back();\n-            if (!top.is_dict) {\n-                _internal_error(\"Encoder::end_object() called without matching start_object()\");\n+            if (!top.is_dict)\n+            {\n+                _internal_error(\n+                    \"Encoder::end_object() called without matching start_object()\");\n                 _stack.pop_back();\n             }\n-            else {\n+            else\n+            {\n                 /*\n                  * Convert back to SerializableObject* right here.\n                  */\n-                if (_actually_clone) {\n-                    SerializableObject::Reader reader(top.dict, _error_function, nullptr);\n+                if (_actually_clone)\n+                {\n+                    SerializableObject::Reader reader(\n+                        top.dict, _error_function, nullptr);\n                     _stack.pop_back();\n                     _store(reader._decode(_resolver));\n                 }\n-                else {\n+                else\n+                {\n                     AnyDictionary m;\n                     m.swap(top.dict);\n                     _stack.pop_back();",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -242,76 +258,61 @@\nclass CloningEncoder : public Encoder {\n     }\n \n private:\n-    any _root;\n-    SerializableObject::Reader::_Resolver _resolver;\n-    std::function<void (ErrorStatus const&)> _error_function;\n+    any                                     _root;\n+    SerializableObject::Reader::_Resolver   _resolver;\n+    std::function<void(ErrorStatus const&)> _error_function;\n \n-    struct _DictOrArray {\n-        _DictOrArray(bool is_dict) {\n-            this->is_dict = is_dict;\n-        }\n-        \n-        bool is_dict;\n+    struct _DictOrArray\n+    {\n+        _DictOrArray(bool is_dict) { this->is_dict = is_dict; }\n+\n+        bool          is_dict;\n         AnyDictionary dict;\n-        AnyVector array;\n-        std::string cur_key;\n+        AnyVector     array;\n+        std::string   cur_key;\n     };\n \n-    void _internal_error(std::string const& err_msg) {\n+    void _internal_error(std::string const& err_msg)\n+    {\n         _error(ErrorStatus(ErrorStatus::INTERNAL_ERROR, err_msg));\n     }\n \n-\n     friend class SerializableObject;\n     std::vector<_DictOrArray> _stack;\n-    bool _actually_clone;\n+    bool                      _actually_clone;\n };\n \n-\n template <typename RapidJSONWriterType>\n-class JSONEncoder : public Encoder {\n+class JSONEncoder : public Encoder\n+{\n public:\n     JSONEncoder(RapidJSONWriterType& writer)\n-        : _writer(writer) {\n-    }\n-    \n-    virtual ~JSONEncoder() {\n-    }\n+        : _writer(writer)\n+    {}\n \n-    void write_key(std::string const& key) {\n-        _writer.Key(key.c_str());\n-    }\n+    virtual ~JSONEncoder() {}\n \n-    void write_null_value() {\n-        _writer.Null();\n-    }\n+    void write_key(std::string const& key) { _writer.Key(key.c_str()); }\n \n-    void write_value(bool value) {\n-        _writer.Bool(value);\n-    }\n+    void write_null_value() { _writer.Null(); }\n \n-    void write_value(int value) {\n-        _writer.Int(value);\n-    }\n+    void write_value(bool value) { _writer.Bool(value); }\n \n-    void write_value(int64_t value) {\n-        _writer.Int64(value);\n-    }\n+    void write_value(int value) { _writer.Int(value); }\n \n-    void write_value(uint64_t value) {\n-        _writer.Uint64(value);\n-    }\n+    void write_value(int64_t value) { _writer.Int64(value); }\n+\n+    void write_value(uint64_t value) { _writer.Uint64(value); }\n \n-    void write_value(std::string const& value) {\n+    void write_value(std::string const& value)\n+    {\n         _writer.String(value.c_str());\n     }\n \n-    void write_value(double value) {\n-        _writer.Double(value);\n+    void write_value(double value) { _writer.Double(value); }\n \n-    }\n-\n-    void write_value(RationalTime const& value) {\n+    void write_value(RationalTime const& value)\n+    {\n         _writer.StartObject();\n \n         _writer.Key(\"OTIO_SCHEMA\");",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -322,34 +323,36 @@\nclass JSONEncoder : public Encoder {\n \n         _writer.Key(\"value\");\n         _writer.Double(value.value());\n-        \n+\n         _writer.EndObject();\n     }\n \n-    void write_value(TimeRange const& value) {\n+    void write_value(TimeRange const& value)\n+    {\n         _writer.StartObject();\n \n         _writer.Key(\"OTIO_SCHEMA\");\n         _writer.String(\"TimeRange.1\");\n \n         _writer.Key(\"duration\");\n         write_value(value.duration());\n-        \n+\n         _writer.Key(\"start_time\");\n         write_value(value.start_time());\n \n         _writer.EndObject();\n     }\n \n-    void write_value(TimeTransform const& value) {\n+    void write_value(TimeTransform const& value)\n+    {\n         _writer.StartObject();\n \n         _writer.Key(\"OTIO_SCHEMA\");\n         _writer.String(\"TimeTransform.1\");\n \n         _writer.Key(\"offset\");\n         write_value(value.offset());\n-        \n+\n         _writer.Key(\"rate\");\n         _writer.Double(value.rate());",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -358,8 +361,9 @@\nclass JSONEncoder : public Encoder {\n \n         _writer.EndObject();\n     }\n-    \n-    void write_value(SerializableObject::ReferenceId value) {\n+\n+    void write_value(SerializableObject::ReferenceId value)\n+    {\n         _writer.StartObject();\n         _writer.Key(\"OTIO_SCHEMA\");\n         _writer.String(\"SerializableObjectRef.1\");",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -368,101 +372,129 @@\nclass JSONEncoder : public Encoder {\n         _writer.EndObject();\n     }\n \n-    void start_array(size_t) {\n-        _writer.StartArray();\n-    }\n-    \n-    void start_object() {\n-        _writer.StartObject();\n-    }\n+    void start_array(size_t) { _writer.StartArray(); }\n \n-    void end_array() {\n-        _writer.EndArray();\n-    }\n+    void start_object() { _writer.StartObject(); }\n \n-    void end_object() {\n-        _writer.EndObject();\n-    }\n+    void end_array() { _writer.EndArray(); }\n+\n+    void end_object() { _writer.EndObject(); }\n \n private:\n     RapidJSONWriterType& _writer;\n };\n \n template <typename T>\n-bool _simple_any_comparison(any const& lhs, any const& rhs) {\n+bool\n+_simple_any_comparison(any const& lhs, any const& rhs)\n+{\n     return lhs.type() == typeid(T) && rhs.type() == typeid(T) &&\n-        any_cast<T const&>(lhs) == any_cast<T const&>(rhs);\n+           any_cast<T const&>(lhs) == any_cast<T const&>(rhs);\n }\n \n template <>\n-bool _simple_any_comparison<void>(any const& lhs, any const& rhs) {\n+bool\n+_simple_any_comparison<void>(any const& lhs, any const& rhs)\n+{\n     return lhs.type() == typeid(void) && rhs.type() == typeid(void);\n }\n \n template <>\n-bool _simple_any_comparison<char const*>(any const& lhs, any const& rhs) {\n-    return lhs.type() == typeid(char const*) && rhs.type() == typeid(char const*) &&\n+bool\n+_simple_any_comparison<char const*>(any const& lhs, any const& rhs)\n+{\n+    return lhs.type() == typeid(char const*) &&\n+           rhs.type() == typeid(char const*) &&\n            !strcmp(any_cast<char const*>(lhs), any_cast<char const*>(rhs));\n }\n \n-void SerializableObject::Writer::_build_dispatch_tables() {\n+void\n+SerializableObject::Writer::_build_dispatch_tables()\n+{\n     /*\n      * These are basically atomic writes to the encoder:\n      */\n-    auto& wt = _write_dispatch_table;\n+    auto& wt          = _write_dispatch_table;\n     wt[&typeid(void)] = [this](any const&) { _encoder.write_null_value(); };\n-    wt[&typeid(bool)] = [this](any const& value) { _encoder.write_value(any_cast<bool>(value)); };\n-    wt[&typeid(int64_t)] = [this](any const& value) { _encoder.write_value(any_cast<int64_t>(value)); };\n-    wt[&typeid(double)] = [this](any const& value) { _encoder.write_value(any_cast<double>(value)); };\n-    wt[&typeid(std::string)] = [this](any const& value) { _encoder.write_value(any_cast<std::string const&>(value)); };\n+    wt[&typeid(bool)] = [this](any const& value) {\n+        _encoder.write_value(any_cast<bool>(value));\n+    };\n+    wt[&typeid(int64_t)] = [this](any const& value) {\n+        _encoder.write_value(any_cast<int64_t>(value));\n+    };\n+    wt[&typeid(double)] = [this](any const& value) {\n+        _encoder.write_value(any_cast<double>(value));\n+    };\n+    wt[&typeid(std::string)] = [this](any const& value) {\n+        _encoder.write_value(any_cast<std::string const&>(value));\n+    };\n     wt[&typeid(char const*)] = [this](any const& value) {\n-        _encoder.write_value(std::string(any_cast<char const*>(value))); };\n-    wt[&typeid(RationalTime)] = [this](any const& value) { _encoder.write_value(any_cast<RationalTime const&>(value)); };\n-    wt[&typeid(TimeRange)] = [this](any const& value) { _encoder.write_value(any_cast<TimeRange const&>(value)); };\n-    wt[&typeid(TimeTransform)] = [this](any const& value) { _encoder.write_value(any_cast<TimeTransform const&>(value)); };\n+        _encoder.write_value(std::string(any_cast<char const*>(value)));\n+    };\n+    wt[&typeid(RationalTime)] = [this](any const& value) {\n+        _encoder.write_value(any_cast<RationalTime const&>(value));\n+    };\n+    wt[&typeid(TimeRange)] = [this](any const& value) {\n+        _encoder.write_value(any_cast<TimeRange const&>(value));\n+    };\n+    wt[&typeid(TimeTransform)] = [this](any const& value) {\n+        _encoder.write_value(any_cast<TimeTransform const&>(value));\n+    };\n \n     /*\n      * These next recurse back through the Writer itself:\n      */\n     wt[&typeid(SerializableObject::Retainer<>)] = [this](any const& value) {\n-        this->write(_no_key, any_cast<SerializableObject::Retainer<>>(value)); };\n+        this->write(_no_key, any_cast<SerializableObject::Retainer<>>(value));\n+    };\n \n     wt[&typeid(AnyDictionary)] = [this](any const& value) {\n-        this->write(_no_key, any_cast<AnyDictionary const&>(value)); };\n+        this->write(_no_key, any_cast<AnyDictionary const&>(value));\n+    };\n \n     wt[&typeid(AnyVector)] = [this](any const& value) {\n-        this->write(_no_key, any_cast<AnyVector const&>(value)); };\n+        this->write(_no_key, any_cast<AnyVector const&>(value));\n+    };\n \n     /*\n      * Install a backup table, using the actual type name as a key.\n      * This is to deal with type aliasing across compilation units.\n      */\n-    for (auto e: wt) {\n+    for (auto e: wt)\n+    {\n         _write_dispatch_table_by_name[e.first->name()] = e.second;\n     }\n \n-    auto& et = _equality_dispatch_table;\n-    et[&typeid(void)] = &_simple_any_comparison<void>;\n-    et[&typeid(bool)] = &_simple_any_comparison<bool>;\n-    et[&typeid(int64_t)] = &_simple_any_comparison<int64_t>;\n-    et[&typeid(double)] = &_simple_any_comparison<double>;\n-    et[&typeid(std::string)] = &_simple_any_comparison<std::string>;\n-    et[&typeid(char const*)] = &_simple_any_comparison<char const*>;\n-    et[&typeid(RationalTime)] = &_simple_any_comparison<RationalTime>;\n-    et[&typeid(TimeRange)] = &_simple_any_comparison<TimeRange>;\n+    auto& et                   = _equality_dispatch_table;\n+    et[&typeid(void)]          = &_simple_any_comparison<void>;\n+    et[&typeid(bool)]          = &_simple_any_comparison<bool>;\n+    et[&typeid(int64_t)]       = &_simple_any_comparison<int64_t>;\n+    et[&typeid(double)]        = &_simple_any_comparison<double>;\n+    et[&typeid(std::string)]   = &_simple_any_comparison<std::string>;\n+    et[&typeid(char const*)]   = &_simple_any_comparison<char const*>;\n+    et[&typeid(RationalTime)]  = &_simple_any_comparison<RationalTime>;\n+    et[&typeid(TimeRange)]     = &_simple_any_comparison<TimeRange>;\n     et[&typeid(TimeTransform)] = &_simple_any_comparison<TimeTransform>;\n-    et[&typeid(SerializableObject::ReferenceId)] = &_simple_any_comparison<SerializableObject::ReferenceId>;\n+    et[&typeid(SerializableObject::ReferenceId)] =\n+        &_simple_any_comparison<SerializableObject::ReferenceId>;\n \n     /*\n      * These next recurse back through the Writer itself:\n      */\n-    et[&typeid(AnyDictionary)] = [this](any const& lhs, any const& rhs) { return _any_dict_equals(lhs, rhs); };\n-    et[&typeid(AnyVector)] = [this](any const& lhs, any const& rhs) { return _any_array_equals(lhs, rhs); };\n-\n+    et[&typeid(AnyDictionary)] = [this](any const& lhs, any const& rhs) {\n+        return _any_dict_equals(lhs, rhs);\n+    };\n+    et[&typeid(AnyVector)] = [this](any const& lhs, any const& rhs) {\n+        return _any_array_equals(lhs, rhs);\n+    };\n }\n \n-bool SerializableObject::Writer::_any_dict_equals(any const& lhs, any const& rhs) {\n-    if (lhs.type() != typeid(AnyDictionary) || rhs.type() != typeid(AnyDictionary)) {\n+bool\n+SerializableObject::Writer::_any_dict_equals(any const& lhs, any const& rhs)\n+{\n+    if (lhs.type() != typeid(AnyDictionary) ||\n+        rhs.type() != typeid(AnyDictionary))\n+    {\n         return false;\n     }",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -471,147 +503,200 @@\nbool SerializableObject::Writer::_any_dict_equals(any const& lhs, any const& rhs\n \n     auto r_it = rd.begin();\n \n-    for (auto l_it : ld) {\n-        if (r_it == rd.end()) {\n+    for (auto l_it: ld)\n+    {\n+        if (r_it == rd.end())\n+        {\n             return false;\n         }\n \n-        if (l_it.first != r_it->first || !_any_equals(l_it.second, r_it->second)) {\n+        if (l_it.first != r_it->first ||\n+            !_any_equals(l_it.second, r_it->second))\n+        {\n             return false;\n         }\n         ++r_it;\n     }\n     return r_it == rd.end();\n }\n \n-bool SerializableObject::Writer::_any_array_equals(any const& lhs, any const& rhs) {\n-    if (lhs.type() != typeid(AnyVector) || rhs.type() != typeid(AnyVector)) {\n+bool\n+SerializableObject::Writer::_any_array_equals(any const& lhs, any const& rhs)\n+{\n+    if (lhs.type() != typeid(AnyVector) || rhs.type() != typeid(AnyVector))\n+    {\n         return false;\n     }\n \n     AnyVector const& lv = any_cast<AnyVector const&>(lhs);\n     AnyVector const& rv = any_cast<AnyVector const&>(rhs);\n \n-    if (lv.size() != rv.size()) {\n+    if (lv.size() != rv.size())\n+    {\n         return false;\n     }\n \n-    for (size_t i = 0; i < lv.size(); i++) {\n-        if (!_any_equals(lv[i], rv[i])) {\n+    for (size_t i = 0; i < lv.size(); i++)\n+    {\n+        if (!_any_equals(lv[i], rv[i]))\n+        {\n             return false;\n         }\n     }\n \n     return true;\n }\n \n-bool SerializableObject::Writer::_any_equals(any const& lhs, any const& rhs) {\n+bool\n+SerializableObject::Writer::_any_equals(any const& lhs, any const& rhs)\n+{\n     auto e = _equality_dispatch_table.find(&lhs.type());\n     return (e != _equality_dispatch_table.end()) && e->second(lhs, rhs);\n }\n \n-bool SerializableObject::Writer::write_root(any const& value, Encoder& encoder, ErrorStatus* error_status) {\n+bool\n+SerializableObject::Writer::write_root(\n+    any const& value, Encoder& encoder, ErrorStatus* error_status)\n+{\n     Writer w(encoder);\n     w.write(w._no_key, value);\n     return !encoder.has_errored(error_status);\n }\n \n-void SerializableObject::Writer::_encoder_write_key(std::string const& key) {\n-    if (&key != &_no_key) {\n+void\n+SerializableObject::Writer::_encoder_write_key(std::string const& key)\n+{\n+    if (&key != &_no_key)\n+    {\n         _encoder.write_key(key);\n     }\n }\n \n-void SerializableObject::Writer::write(std::string const& key, bool value) {\n+void\n+SerializableObject::Writer::write(std::string const& key, bool value)\n+{\n     _encoder_write_key(key);\n     _encoder.write_value(value);\n }\n \n-void SerializableObject::Writer::write(std::string const& key, int64_t value) {\n+void\n+SerializableObject::Writer::write(std::string const& key, int64_t value)\n+{\n     _encoder_write_key(key);\n     _encoder.write_value(value);\n }\n \n-void SerializableObject::Writer::write(std::string const& key, double value) {\n+void\n+SerializableObject::Writer::write(std::string const& key, double value)\n+{\n     _encoder_write_key(key);\n     _encoder.write_value(value);\n }\n \n-void SerializableObject::Writer::write(std::string const& key, std::string const& value) {\n+void\n+SerializableObject::Writer::write(\n+    std::string const& key, std::string const& value)\n+{\n     _encoder_write_key(key);\n     _encoder.write_value(value);\n }\n \n-void SerializableObject::Writer::write(std::string const& key, RationalTime value) {\n+void\n+SerializableObject::Writer::write(std::string const& key, RationalTime value)\n+{\n     _encoder_write_key(key);\n     _encoder.write_value(value);\n }\n \n-void SerializableObject::Writer::write(std::string const& key, TimeRange value) {\n+void\n+SerializableObject::Writer::write(std::string const& key, TimeRange value)\n+{\n     _encoder_write_key(key);\n     _encoder.write_value(value);\n }\n \n-void SerializableObject::Writer::write(std::string const& key, optional<RationalTime> value) {\n+void\n+SerializableObject::Writer::write(\n+    std::string const& key, optional<RationalTime> value)\n+{\n     _encoder_write_key(key);\n     value ? _encoder.write_value(*value) : _encoder.write_null_value();\n }\n \n-void SerializableObject::Writer::write(std::string const& key, optional<TimeRange> value) {\n+void\n+SerializableObject::Writer::write(\n+    std::string const& key, optional<TimeRange> value)\n+{\n     _encoder_write_key(key);\n     value ? _encoder.write_value(*value) : _encoder.write_null_value();\n }\n \n-void SerializableObject::Writer::write(std::string const& key, TimeTransform value) {\n+void\n+SerializableObject::Writer::write(std::string const& key, TimeTransform value)\n+{\n     _encoder_write_key(key);\n     _encoder.write_value(value);\n }\n \n-void SerializableObject::Writer::write(std::string const& key, SerializableObject const* value) {\n+void\n+SerializableObject::Writer::write(\n+    std::string const& key, SerializableObject const* value)\n+{\n     _encoder_write_key(key);\n-    if (!value) {\n+    if (!value)\n+    {\n         _encoder.write_null_value();\n         return;\n     }\n \n     auto e = _id_for_object.find(value);\n-    if (e != _id_for_object.end()) {\n+    if (e != _id_for_object.end())\n+    {\n #ifdef OTIO_INSTANCING_SUPPORT\n         /*\n          * We've already written this value.\n          */\n-        _encoder.write_value(SerializableObject::ReferenceId { e->second });\n+        _encoder.write_value(SerializableObject::ReferenceId{ e->second });\n #else\n         /*\n          * We're encountering the same object while it is still\n          * in the map, meaning we're in the middle of writing it out.\n          * That's a cycle, as opposed to mere instancing, which we\n          * allow so as not to break old allowed behavior.\n          */\n-        std::string s = string_printf(\"cyclically encountered object has schema %s\",\n-                                      value->schema_name().c_str());\n+        std::string s = string_printf(\n+            \"cyclically encountered object has schema %s\",\n+            value->schema_name().c_str());\n         _encoder._error(ErrorStatus(ErrorStatus::OBJECT_CYCLE, s));\n-#endif        \n+#endif\n         return;\n     }\n \n     std::string const& schema_type_name = value->_schema_name_for_reference();\n-    if (_next_id_for_type.find(schema_type_name) == _next_id_for_type.end()) {\n+    if (_next_id_for_type.find(schema_type_name) == _next_id_for_type.end())\n+    {\n         _next_id_for_type[schema_type_name] = 0;\n     }\n \n-    std::string next_id = schema_type_name + \"-\" + std::to_string(++_next_id_for_type[schema_type_name]);\n+    std::string next_id = schema_type_name + \"-\" +\n+                          std::to_string(++_next_id_for_type[schema_type_name]);\n     _id_for_object[value] = next_id;\n \n     _encoder.start_object();\n \n     _encoder.write_key(\"OTIO_SCHEMA\");\n-    \n-    if (UnknownSchema const* us = dynamic_cast<UnknownSchema const*>(value)) {\n-        _encoder.write_value(string_printf(\"%s.%d\", us->_original_schema_name.c_str(), us->_original_schema_version));\n+\n+    if (UnknownSchema const* us = dynamic_cast<UnknownSchema const*>(value))\n+    {\n+        _encoder.write_value(string_printf(\n+            \"%s.%d\",\n+            us->_original_schema_name.c_str(),\n+            us->_original_schema_version));\n     }\n-    else {\n-        _encoder.write_value(string_printf(\"%s.%d\", value->schema_name().c_str(), value->schema_version()));\n+    else\n+    {\n+        _encoder.write_value(string_printf(\n+            \"%s.%d\", value->schema_name().c_str(), value->schema_version()));\n     }\n \n #ifdef OTIO_INSTANCING_SUPPORT",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -624,43 +709,55 @@\nvoid SerializableObject::Writer::write(std::string const& key, SerializableObjec\n \n #ifndef OTIO_INSTANCING_SUPPORT\n     auto valueEntry = _id_for_object.find(value);\n-    if (valueEntry != _id_for_object.end()) {\n+    if (valueEntry != _id_for_object.end())\n+    {\n         _id_for_object.erase(valueEntry);\n     }\n-#endif    \n+#endif\n }\n \n-void SerializableObject::Writer::write(std::string const& key, AnyDictionary const& value) {\n+void\n+SerializableObject::Writer::write(\n+    std::string const& key, AnyDictionary const& value)\n+{\n     _encoder_write_key(key);\n \n     _encoder.start_object();\n \n-    for (auto e: value) {\n+    for (auto e: value)\n+    {\n         write(e.first, e.second);\n     }\n \n     _encoder.end_object();\n }\n \n-void SerializableObject::Writer::write(std::string const& key, AnyVector const& value) {\n+void\n+SerializableObject::Writer::write(\n+    std::string const& key, AnyVector const& value)\n+{\n     _encoder_write_key(key);\n \n     _encoder.start_array(value.size());\n \n-    for (auto e: value) {\n+    for (auto e: value)\n+    {\n         write(_no_key, e);\n     }\n \n     _encoder.end_array();\n }\n \n-void SerializableObject::Writer::write(std::string const& key, any const& value) {\n+void\n+SerializableObject::Writer::write(std::string const& key, any const& value)\n+{\n     std::type_info const& type = value.type();\n \n     _encoder_write_key(key);\n \n     auto e = _write_dispatch_table.find(&type);\n-    if (e == _write_dispatch_table.end()) {\n+    if (e == _write_dispatch_table.end())\n+    {\n         /*\n          * Using the address of a type_info suffers from aliasing across compilation units.\n          * If we fail on a lookup, we fallback on the by_name table, but that's slow because",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/track.cpp",
        "code_diff": "@@ -1,114 +1,159 @@\n#include \"opentimelineio/track.h\"\n #include \"opentimelineio/clip.h\"\n-#include \"opentimelineio/transition.h\"\n #include \"opentimelineio/gap.h\"\n+#include \"opentimelineio/transition.h\"\n #include \"opentimelineio/vectorIndexing.h\"\n \n-namespace opentimelineio { namespace OPENTIMELINEIO_VERSION  {\n-    \n-Track::Track(std::string const& name,\n-             optional<TimeRange> const& source_range,\n-             std::string const& kind,\n-             AnyDictionary const& metadata)\n-    : Parent( name, source_range, metadata),\n-      _kind(kind) {\n-}\n+namespace opentimelineio { namespace OPENTIMELINEIO_VERSION {\n \n-Track::~Track() {\n-}\n+Track::Track(\n+    std::string const&         name,\n+    optional<TimeRange> const& source_range,\n+    std::string const&         kind,\n+    AnyDictionary const&       metadata)\n+    : Parent(name, source_range, metadata)\n+    , _kind(kind)\n+{}\n \n-std::string Track::composition_kind() const {\n+Track::~Track()\n+{}\n+\n+std::string\n+Track::composition_kind() const\n+{\n     static std::string kind = \"Track\";\n     return kind;\n }\n \n-bool Track::read_from(Reader& reader) {\n-    return reader.read(\"kind\", &_kind) &&\n-        Parent::read_from(reader);\n+bool\n+Track::read_from(Reader& reader)\n+{\n+    return reader.read(\"kind\", &_kind) && Parent::read_from(reader);\n }\n \n-void Track::write_to(Writer& writer) const {\n+void\n+Track::write_to(Writer& writer) const\n+{\n     Parent::write_to(writer);\n     writer.write(\"kind\", _kind);\n }\n \n-static RationalTime _safe_duration(Composable* c, ErrorStatus* error_status) {\n-    if (auto item = dynamic_cast<Item*>(c)) {\n+static RationalTime\n+_safe_duration(Composable* c, ErrorStatus* error_status)\n+{\n+    if (auto item = dynamic_cast<Item*>(c))\n+    {\n         return item->duration(error_status);\n     }\n-    else if (auto transition = dynamic_cast<Transition*>(c)) {\n+    else if (auto transition = dynamic_cast<Transition*>(c))\n+    {\n         return transition->duration(error_status);\n     }\n-    else {\n-        *error_status = ErrorStatus(ErrorStatus::OBJECT_WITHOUT_DURATION,\n-                                    \"Cannot determine duration from this kind of object\", c);\n+    else\n+    {\n+        if (error_status)\n+        {\n+            *error_status = ErrorStatus(\n+                ErrorStatus::OBJECT_WITHOUT_DURATION,\n+                \"Cannot determine duration from this kind of object\",\n+                c);\n+        }\n         return RationalTime();\n     }\n }\n \n-TimeRange Track::range_of_child_at_index(int index, ErrorStatus* error_status) const {\n+TimeRange\n+Track::range_of_child_at_index(int index, ErrorStatus* error_status) const\n+{\n     index = adjusted_vector_index(index, children());\n-    if (index < 0 || index >= int(children().size())) {\n-        *error_status = ErrorStatus::ILLEGAL_INDEX;\n+    if (index < 0 || index >= int(children().size()))\n+    {\n+        if (error_status)\n+        {\n+            *error_status = ErrorStatus::ILLEGAL_INDEX;\n+        }\n         return TimeRange();\n     }\n-    \n-    Composable* child = children()[index];\n+\n+    Composable*  child          = children()[index];\n     RationalTime child_duration = _safe_duration(child, error_status);\n-    if (*error_status) {\n+    if (is_error(error_status))\n+    {\n         return TimeRange();\n     }\n-    \n+\n     RationalTime start_time(0, child_duration.rate());\n-    \n-    for (int i = 0; i < index; i++) {\n+\n+    for (int i = 0; i < index; i++)\n+    {\n         Composable* child2 = children()[i];\n-        if (!child2->overlapping()) {\n+        if (!child2->overlapping())\n+        {\n             start_time += _safe_duration(children()[i], error_status);\n         }\n-        if (*error_status) {\n+        if (is_error(error_status))\n+        {\n             return TimeRange();\n         }\n     }\n-    \n-    if (auto transition = dynamic_cast<Transition*>(child)) {\n+\n+    if (auto transition = dynamic_cast<Transition*>(child))\n+    {\n         start_time -= transition->in_offset();\n     }\n-    \n+\n     return TimeRange(start_time, child_duration);\n }\n \n-TimeRange Track::trimmed_range_of_child_at_index(int index, ErrorStatus* error_status) const {\n+TimeRange\n+Track::trimmed_range_of_child_at_index(\n+    int index, ErrorStatus* error_status) const\n+{\n     auto child_range = range_of_child_at_index(index, error_status);\n-    if (*error_status) {\n+    if (is_error(error_status))\n+    {\n         return child_range;\n     }\n-    \n+\n     auto trimmed_range = trim_child_range(child_range);\n-    if (!trimmed_range) {\n-        *error_status = ErrorStatus::INVALID_TIME_RANGE;\n+    if (!trimmed_range)\n+    {\n+        if (error_status)\n+        {\n+            *error_status = ErrorStatus::INVALID_TIME_RANGE;\n+        }\n         return TimeRange();\n     }\n-    \n+\n     return *trimmed_range;\n }\n \n-TimeRange Track::available_range(ErrorStatus* error_status) const {\n+TimeRange\n+Track::available_range(ErrorStatus* error_status) const\n+{\n     RationalTime duration;\n-    for (auto child: children()) {\n-        if (auto item = dynamic_retainer_cast<Item>(child)) {\n+    for (auto child: children())\n+    {\n+        if (auto item = dynamic_retainer_cast<Item>(child))\n+        {\n             duration += item->duration(error_status);\n-            if (*error_status) {\n+            if (is_error(error_status))\n+            {\n                 return TimeRange();\n             }\n         }\n     }\n-    \n-    if (!children().empty()) {\n-        if (auto transition = dynamic_retainer_cast<Transition>(children().front())) {\n+\n+    if (!children().empty())\n+    {\n+        if (auto transition =\n+                dynamic_retainer_cast<Transition>(children().front()))\n+        {\n             duration += transition->in_offset();\n         }\n-        if (auto transition = dynamic_retainer_cast<Transition>(children().back())) {\n+        if (auto transition =\n+                dynamic_retainer_cast<Transition>(children().back()))\n+        {\n             duration += transition->out_offset();\n         }\n     }",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/typeRegistry.cpp",
        "code_diff": "@@ -1,5 +1,4 @@\n#include \"opentimelineio/typeRegistry.h\"\n-#include \"opentimelineio/stringUtils.h\"\n \n #include \"opentimelineio/clip.h\"\n #include \"opentimelineio/composable.h\"",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/typeRegistry.cpp",
        "code_diff": "@@ -23,27 +22,35 @@\n#include \"opentimelineio/timeline.h\"\n #include \"opentimelineio/track.h\"\n #include \"opentimelineio/transition.h\"\n-#include \"opentimelineio/stack.h\"\n #include \"opentimelineio/unknownSchema.h\"\n+#include \"stringUtils.h\"\n \n #include <assert.h>\n #include <vector>\n //#include <sstream>\n //#include <iostream>\n \n-namespace opentimelineio { namespace OPENTIMELINEIO_VERSION  {\n-    \n-TypeRegistry& TypeRegistry::TypeRegistry::instance() {\n+namespace opentimelineio { namespace OPENTIMELINEIO_VERSION {\n+\n+TypeRegistry&\n+TypeRegistry::TypeRegistry::instance()\n+{\n     static TypeRegistry r;\n     return r;\n }\n \n-TypeRegistry::TypeRegistry() {\n-    register_type(UnknownSchema::Schema::name, UnknownSchema::Schema::version, &typeid(UnknownSchema),\n-                  [] () {\n-                      fatal_error(\"UnknownSchema should not be created from type registry\");\n-                      return nullptr;\n-                  }, \"UnknownSchema\");\n+TypeRegistry::TypeRegistry()\n+{\n+    register_type(\n+        UnknownSchema::Schema::name,\n+        UnknownSchema::Schema::version,\n+        &typeid(UnknownSchema),\n+        []() {\n+            fatal_error(\n+                \"UnknownSchema should not be created from type registry\");\n+            return nullptr;\n+        },\n+        \"UnknownSchema\");\n \n     register_type<Clip>();\n     register_type<Composable>();",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/typeRegistry.cpp",
        "code_diff": "@@ -66,7 +73,8 @@\nTypeRegistry::TypeRegistry() {\n     register_type<SerializableObject>();\n     register_type<SerializableObjectWithMetadata>();\n     register_type<SerializableCollection>();\n-    register_type_from_existing_type(\"SerializeableCollection\", 1, \"SerializableCollection\", nullptr);\n+    register_type_from_existing_type(\n+        \"SerializeableCollection\", 1, \"SerializableCollection\", nullptr);\n \n     register_type<Stack>();\n     register_type<TimeEffect>();",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/typeRegistry.cpp",
        "code_diff": "@@ -78,25 +86,29 @@\nTypeRegistry::TypeRegistry() {\n     /*\n      * Upgrade functions:\n      */\n-    register_upgrade_function(Marker::Schema::name, 2,\n-                              [](AnyDictionary* d) {\n-                                  (*d)[\"marked_range\"] = (*d)[\"range\"];\n-                                  d->erase(\"range\");\n-                              });\n+    register_upgrade_function(Marker::Schema::name, 2, [](AnyDictionary* d) {\n+        (*d)[\"marked_range\"] = (*d)[\"range\"];\n+        d->erase(\"range\");\n+    });\n }\n \n bool\n-TypeRegistry::register_type(std::string const& schema_name, int schema_version,\n-                            std::type_info const* type, \n-                            std::function<SerializableObject* ()> create,\n-                            std::string const& class_name)\n+TypeRegistry::register_type(\n+    std::string const&                   schema_name,\n+    int                                  schema_version,\n+    std::type_info const*                type,\n+    std::function<SerializableObject*()> create,\n+    std::string const&                   class_name)\n {\n     std::lock_guard<std::mutex> lock(_registry_mutex);\n \n-    if (!_find_type_record(schema_name)) {\n-        _TypeRecord* r = new _TypeRecord { schema_name, schema_version, class_name, create };\n+    if (!_find_type_record(schema_name))\n+    {\n+        _TypeRecord* r =\n+            new _TypeRecord{ schema_name, schema_version, class_name, create };\n         _type_records[schema_name] = r;\n-        if (type) {\n+        if (type)\n+        {\n             _type_records_by_type_name[type->name()] = r;\n         }\n         return true;",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/opentimelineio/typeRegistry.cpp",
        "code_diff": "@@ -105,33 +117,56 @@\nTypeRegistry::register_type(std::string const& schema_name, int schema_version,\n }\n \n bool\n-TypeRegistry::register_type_from_existing_type(std::string const& schema_name, int /* schema_version */,\n-                                               std::string const& existing_schema_name,\n-                                               ErrorStatus* error_status) {\n+TypeRegistry::register_type_from_existing_type(\n+    std::string const& schema_name,\n+    int /* schema_version */,\n+    std::string const& existing_schema_name,\n+    ErrorStatus*       error_status)\n+{\n     std::lock_guard<std::mutex> lock(_registry_mutex);\n-    if (auto r = _find_type_record(existing_schema_name)) {\n-        if (!_find_type_record(schema_name)) {\n-            _type_records[schema_name] = new _TypeRecord { r->schema_name, r->schema_version, r->class_name, r->create };\n+    if (auto r = _find_type_record(existing_schema_name))\n+    {\n+        if (!_find_type_record(schema_name))\n+        {\n+            _type_records[schema_name] = new _TypeRecord{\n+                r->schema_name, r->schema_version, r->class_name, r->create\n+            };\n             return true;\n         }\n \n-        *error_status = ErrorStatus(ErrorStatus::SCHEMA_ALREADY_REGISTERED, schema_name);\n+        if (error_status)\n+        {\n+            *error_status = ErrorStatus(\n+                ErrorStatus::SCHEMA_ALREADY_REGISTERED, schema_name);\n+        }\n         return false;\n     }\n \n-    *error_status = ErrorStatus(ErrorStatus::SCHEMA_NOT_REGISTERED,\n-                                string_printf(\"cannot define schema %s in terms of %s; %s has not been registered\",\n-                                              schema_name.c_str(), existing_schema_name.c_str(),\n-                                              existing_schema_name.c_str()));\n+    if (error_status)\n+    {\n+        *error_status = ErrorStatus(\n+            ErrorStatus::SCHEMA_NOT_REGISTERED,\n+            string_printf(\n+                \"cannot define schema %s in terms of %s; %s has not been registered\",\n+                schema_name.c_str(),\n+                existing_schema_name.c_str(),\n+                existing_schema_name.c_str()));\n+    }\n     return false;\n }\n \n-bool TypeRegistry::register_upgrade_function(std::string const& schema_name,\n-                                             int version_to_upgrade_to,\n-                                             std::function<void (AnyDictionary*)> upgrade_function) {\n+bool\n+TypeRegistry::register_upgrade_function(\n+    std::string const&                  schema_name,\n+    int                                 version_to_upgrade_to,\n+    std::function<void(AnyDictionary*)> upgrade_function)\n+{\n     std::lock_guard<std::mutex> lock(_registry_mutex);\n-    if (auto r = _find_type_record(schema_name)) {\n-        if (r->upgrade_functions.find(version_to_upgrade_to) == r->upgrade_functions.end()) {\n+    if (auto r = _find_type_record(schema_name))\n+    {\n+        if (r->upgrade_functions.find(version_to_upgrade_to) ==\n+            r->upgrade_functions.end())\n+        {\n             r->upgrade_functions[version_to_upgrade_to] = upgrade_function;\n             return true;\n         }",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/py-opentimelineio/opentime-bindings/opentime_rationalTime.cpp",
        "code_diff": "@@ -14,10 +14,10 @@\nstruct ErrorStatusConverter {\n     operator ErrorStatus* () {\n         return &error_status;\n     }\n-    \n+\n     ~ErrorStatusConverter() noexcept(false) {\n         namespace py = pybind11;\n-        if (error_status) {\n+        if (is_error(error_status)) {\n             throw py::value_error(error_status.details);\n         }\n     }",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/py-opentimelineio/opentimelineio-bindings/otio_utils.cpp",
        "code_diff": "@@ -7,6 +7,7 @@\n#include \"opentime/timeTransform.h\"\n #include \"opentimelineio/serializableObject.h\"\n #include \"opentimelineio/safely_typed_any.h\"\n+#include \"opentimelineio/stringUtils.h\"\n \n #include <map>\n #include <cstring>",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/py-opentimelineio/opentimelineio-bindings/otio_utils.cpp",
        "code_diff": "@@ -111,7 +112,7 @@\nAnyDictionary py_to_any_dictionary(py::object const& o) {\n     py_to_any(o, &a);\n     if (!compare_typeids(a.type(), typeid(AnyDictionary))) {\n         throw py::type_error(string_printf(\"expected an AnyDictionary (i.e. metadata); got %s instead\",\n-                                           demangled_type_name(a).c_str()));\n+                                           type_name_for_error_message(a).c_str()));\n     }\n \n     return safely_cast_any_dictionary_any(a);",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/py-opentimelineio/opentimelineio/adapters/adapter.py",
        "code_diff": "@@ -115,7 +115,7 @@\ndef read_from_file(\n         filepath,\n         media_linker_name=media_linker.MediaLinkingPolicy.ForceDefaultLinker,\n         media_linker_argument_map=None,\n-        hook_function_argument_map={},\n+        hook_function_argument_map=None,\n         **adapter_argument_map\n     ):\n         \"\"\"Execute the read_from_file function on this adapter.",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/py-opentimelineio/opentimelineio/adapters/adapter.py",
        "code_diff": "@@ -124,8 +124,9 @@\ndef read_from_file(\n         a trivial file object wrapper.\n         \"\"\"\n \n-        if media_linker_argument_map is None:\n-            media_linker_argument_map = {}\n+        media_linker_argument_map = copy.deepcopy(\n+            media_linker_argument_map or {}\n+        )\n \n         result = None",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/py-opentimelineio/opentimelineio/adapters/adapter.py",
        "code_diff": "@@ -147,11 +148,20 @@\ndef read_from_file(\n                 **adapter_argument_map\n             )\n \n-        hook_function_argument_map['adapter_arguments'] = adapter_argument_map\n-        hook_function_argument_map['media_linker_argument_map'] = \\\n+        hook_function_argument_map = copy.deepcopy(\n+            hook_function_argument_map or {}\n+        )\n+        hook_function_argument_map['adapter_arguments'] = copy.deepcopy(\n+            adapter_argument_map\n+        )\n+        hook_function_argument_map['media_linker_argument_map'] = (\n             media_linker_argument_map\n-        result = hooks.run(\"post_adapter_read\", result,\n-                           extra_args=hook_function_argument_map)\n+        )\n+        result = hooks.run(\n+            \"post_adapter_read\",\n+            result,\n+            extra_args=hook_function_argument_map\n+        )\n \n         if media_linker_name and (\n             media_linker_name != media_linker.MediaLinkingPolicy.DoNotLinkMedia",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/py-opentimelineio/opentimelineio/adapters/adapter.py",
        "code_diff": "@@ -162,23 +172,30 @@\ndef read_from_file(\n                 media_linker_argument_map\n             )\n \n-        result = hooks.run(\"post_media_linker\", result,\n-                           extra_args=media_linker_argument_map)\n+        result = hooks.run(\n+            \"post_media_linker\",\n+            result,\n+            extra_args=media_linker_argument_map\n+        )\n \n         return result\n \n     def write_to_file(\n         self,\n         input_otio,\n         filepath,\n-        hook_function_argument_map={},\n+        hook_function_argument_map=None,\n         **adapter_argument_map\n     ):\n         \"\"\"Execute the write_to_file function on this adapter.\n \n         If write_to_string exists, but not write_to_file, execute that with\n         a trivial file object wrapper.\n         \"\"\"\n+\n+        hook_function_argument_map = copy.deepcopy(\n+            hook_function_argument_map or {}\n+        )\n         hook_function_argument_map['adapter_arguments'] = copy.deepcopy(\n             adapter_argument_map\n         )",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/py-opentimelineio/opentimelineio/adapters/adapter.py",
        "code_diff": "@@ -205,8 +222,11 @@\ndef write_to_file(\n                 **adapter_argument_map\n             )\n \n-        hooks.run(\"post_adapter_write\", input_otio,\n-                  extra_args=hook_function_argument_map)\n+        hooks.run(\n+            \"post_adapter_write\",\n+            input_otio,\n+            extra_args=hook_function_argument_map\n+        )\n \n         return result",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/py-opentimelineio/opentimelineio/adapters/adapter.py",
        "code_diff": "@@ -215,7 +235,7 @@\ndef read_from_string(\n         input_str,\n         media_linker_name=media_linker.MediaLinkingPolicy.ForceDefaultLinker,\n         media_linker_argument_map=None,\n-        hook_function_argument_map={},\n+        hook_function_argument_map=None,\n         **adapter_argument_map\n     ):\n         \"\"\"Call the read_from_string function on this adapter.\"\"\"",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "src/py-opentimelineio/opentimelineio/adapters/adapter.py",
        "code_diff": "@@ -225,12 +245,22 @@\ndef read_from_string(\n             input_str=input_str,\n             **adapter_argument_map\n         )\n-        hook_function_argument_map['adapter_arguments'] = adapter_argument_map\n-        hook_function_argument_map['media_linker_argument_map'] = \\\n+\n+        hook_function_argument_map = copy.deepcopy(\n+            hook_function_argument_map or {}\n+        )\n+        hook_function_argument_map['adapter_arguments'] = copy.deepcopy(\n+            adapter_argument_map\n+        )\n+        hook_function_argument_map['media_linker_argument_map'] = copy.deepcopy(\n             media_linker_argument_map\n+        )\n \n-        result = hooks.run(\"post_adapter_read\", result,\n-                           extra_args=hook_function_argument_map)\n+        result = hooks.run(\n+            \"post_adapter_read\",\n+            result,\n+            extra_args=hook_function_argument_map\n+        )\n \n         if media_linker_name and (\n             media_linker_name != media_linker.MediaLinkingPolicy.DoNotLinkMedia",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Technical Steering Committee Charter,  GOVERNANCE,  etc.",
        "pr_number": 1057,
        "file_name": "tests/test_opentime.py",
        "code_diff": "@@ -380,7 +380,8 @@\ndef test_faulty_formatted_timecode_24(self):\n \n         This is what we're testing here. When using 24 fps for the\n         drop-frame timecode 01:00:13;23 we should get a ValueError\n-        mapping internally to the ErrorStatus 'NON_DROPFRAME_RATE'.\n+        mapping internally to the ErrorStatus\n+        'INVALID_RATE_FOR_DROP_FRAME_TIMECODE'.\n         \"\"\"\n         with self.assertRaises(ValueError):\n             otio.opentime.from_timecode('01:00:13;23', 24)",
        "comments": [],
        "commit_message": "Merge branch 'PixarAnimationStudios:main' into tsc",
        "commit_id": "f940c1c2aa12f89f32227d19fa722b1721c88d7d"
    },
    {
        "pr_title": "Add constexpr/noexcept",
        "pr_number": 1047,
        "file_name": "src/opentimelineio/imageSequenceReference.cpp",
        "code_diff": "@@ -7,7 +7,7 @@\nImageSequenceReference::ImageSequenceReference(std::string const& target_url_bas\n                       std::string const& name_suffix,\n                       int start_frame,\n                       int frame_step,\n-                      double const rate,\n+                      double rate,\n                       int frame_zero_padding,\n                       MissingFramePolicy const missing_frame_policy,\n                       optional<TimeRange> const& available_range,",
        "comments": [],
        "commit_message": "Const changes",
        "commit_id": "28b7283ca9e6dcc6b49bd485f21f61dd36edd939"
    },
    {
        "pr_title": "Add constexpr/noexcept",
        "pr_number": 1047,
        "file_name": "src/opentimelineio/imageSequenceReference.cpp",
        "code_diff": "@@ -52,7 +52,7 @@\nImageSequenceReference::ImageSequenceReference(std::string const& target_url_bas\n         return num_frames;\n     }\n \n-    int ImageSequenceReference::frame_for_time(RationalTime const time, ErrorStatus* error_status) const {\n+    int ImageSequenceReference::frame_for_time(RationalTime const& time, ErrorStatus* error_status) const {\n         if (!this->available_range().has_value() || !this->available_range().value().contains(time)) {\n             *error_status = ErrorStatus(ErrorStatus::INVALID_TIME_RANGE);\n             return 0;",
        "comments": [],
        "commit_message": "Const changes",
        "commit_id": "28b7283ca9e6dcc6b49bd485f21f61dd36edd939"
    },
    {
        "pr_title": "Add constexpr/noexcept",
        "pr_number": 1047,
        "file_name": "src/opentimelineio/imageSequenceReference.cpp",
        "code_diff": "@@ -68,7 +68,7 @@\nImageSequenceReference::ImageSequenceReference(std::string const& target_url_bas\n     }\n \n     std::string\n-    ImageSequenceReference::target_url_for_image_number(int const image_number, ErrorStatus* error_status) const {\n+    ImageSequenceReference::target_url_for_image_number(int image_number, ErrorStatus* error_status) const {\n         if (_rate == 0) {\n             *error_status = ErrorStatus(ErrorStatus::ILLEGAL_INDEX, \"Zero rate sequence has no frames.\");\n             return std::string();",
        "comments": [],
        "commit_message": "Const changes",
        "commit_id": "28b7283ca9e6dcc6b49bd485f21f61dd36edd939"
    },
    {
        "pr_title": "Python build: Use CMake FindPython",
        "pr_number": 1000,
        "file_name": "setup.py",
        "code_diff": "@@ -13,6 +13,8 @@\nimport platform\n import subprocess\n import unittest\n+import tempfile\n+import shutil\n \n from setuptools import (\n     setup,",
        "comments": [
            {
                "comment": "The linter is failing the compile checks because this line is too long. Is there an escape to allow long lines to evade the linter?",
                "position": null
            },
            {
                "comment": "Yep, `# noqa: E501` at the end of the line should do it.",
                "position": null
            },
            {
                "comment": "Thanks!",
                "position": null
            }
        ],
        "commit_message": "Run cmake --check-system-vars inside a temporary directory",
        "commit_id": "77158f37d3439b228d81c53ec6ca905f5aaae2e2",
        "fn_context": ""
    },
    {
        "pr_title": "Python build: Use CMake FindPython",
        "pr_number": 1000,
        "file_name": "setup.py",
        "code_diff": "@@ -60,6 +62,7 @@\ndef run(self):\n         # This works around the fact that we build _opentime and _otio\n         # extensions as a one-shot cmake invocation. Usually we'd build each\n         # separately using build_extension.\n+        self.announce('running OTIO build_ext', level=2)\n         self.build()\n \n     def build(self):",
        "comments": [
            {
                "comment": "The linter is failing the compile checks because this line is too long. Is there an escape to allow long lines to evade the linter?",
                "position": null
            },
            {
                "comment": "Yep, `# noqa: E501` at the end of the line should do it.",
                "position": null
            },
            {
                "comment": "Thanks!",
                "position": null
            }
        ],
        "commit_message": "Run cmake --check-system-vars inside a temporary directory",
        "commit_id": "77158f37d3439b228d81c53ec6ca905f5aaae2e2",
        "fn_context": "    def run(self):\n        # This works around the fact that we build _opentime and _otio\n        # extensions as a one-shot cmake invocation. Usually we'd build each\n        # separately using build_extension.\n        self.announce('running OTIO build_ext', level=2)\n        self.build()\n\n    def build(self):\n        self.build_temp_dir = (\n            os.environ.get(\"OTIO_CXX_BUILD_TMP_DIR\")\n            or os.path.abspath(self.build_temp)\n        )\n\n        if not os.path.exists(self.build_temp_dir):\n            os.makedirs(self.build_temp_dir)\n\n        debug = (self.debug or bool(os.environ.get(\"OTIO_CXX_DEBUG_BUILD\")))\n        self.build_config = ('Debug' if debug else 'Release')\n\n        self.cmake_preflight_check()\n        self.cmake_generate()\n        self.cmake_install()"
    },
    {
        "pr_title": "Python build: Use CMake FindPython",
        "pr_number": 1000,
        "file_name": "setup.py",
        "code_diff": "@@ -78,23 +81,7 @@\ndef build(self):\n         self.cmake_generate()\n         self.cmake_install()\n \n-    def cmake_preflight_check(self):\n-        \"\"\"\n-        Verify that CMake is greater or equal to the required version\n-        We do this so that the error message is clear if the minimum version is not met.\n-        \"\"\"\n-        proc = subprocess.Popen(\n-            [\"cmake\", \"--check-system-vars\", SOURCE_DIR],\n-            stdout=subprocess.PIPE,\n-            stderr=subprocess.PIPE,\n-            cwd=self.build_temp_dir,\n-            universal_newlines=True\n-        )\n-        _, stderr = proc.communicate()\n-        if proc.returncode != 0:\n-            raise RuntimeError(stderr.strip())\n-\n-    def cmake_generate(self):\n+    def generate_cmake_arguments(self):\n         # Use the provided build dir so setuptools will be able to locate and\n         # either install to the correct location or package.\n         install_dir = os.path.abspath(self.build_lib)",
        "comments": [
            {
                "comment": "The linter is failing the compile checks because this line is too long. Is there an escape to allow long lines to evade the linter?",
                "position": null
            },
            {
                "comment": "Yep, `# noqa: E501` at the end of the line should do it.",
                "position": null
            },
            {
                "comment": "Thanks!",
                "position": null
            }
        ],
        "commit_message": "Run cmake --check-system-vars inside a temporary directory",
        "commit_id": "77158f37d3439b228d81c53ec6ca905f5aaae2e2",
        "fn_context": "    def build(self):\n        self.build_temp_dir = (\n            os.environ.get(\"OTIO_CXX_BUILD_TMP_DIR\")\n            or os.path.abspath(self.build_temp)\n        )\n\n        if not os.path.exists(self.build_temp_dir):\n            os.makedirs(self.build_temp_dir)\n\n        debug = (self.debug or bool(os.environ.get(\"OTIO_CXX_DEBUG_BUILD\")))\n        self.build_config = ('Debug' if debug else 'Release')\n\n        self.cmake_preflight_check()\n        self.cmake_generate()\n        self.cmake_install()\n\n    def generate_cmake_arguments(self):\n        # Use the provided build dir so setuptools will be able to locate and\n        # either install to the correct location or package.\n        install_dir = os.path.abspath(self.build_lib)\n        if not install_dir.endswith(os.path.sep):\n            install_dir += os.path.sep\n\n        cmake_args = [\n            # Python_EXECUTABLE is important as it tells CMake's FindPython\n            # which Python executable to use. We absolutely want to use the\n            # interpreter that was used to execute the setup.py.\n            # See https://cmake.org/cmake/help/v3.20/module/FindPython.html#artifacts-specification # noqa: E501\n            # Also, be careful, CMake is case sensitive ;)\n            '-DPython_EXECUTABLE=' + sys.executable,\n            '-DOTIO_PYTHON_INSTALL:BOOL=ON',\n            '-DOTIO_CXX_INSTALL:BOOL=OFF',\n            '-DOTIO_SHARED_LIBS:BOOL=OFF',\n            '-DCMAKE_BUILD_TYPE=' + self.build_config,\n            '-DOTIO_PYTHON_INSTALL_DIR=' + install_dir,\n        ]\n\n        if platform.system() == \"Windows\":\n            if sys.maxsize > 2**32:\n                cmake_args += ['-A', 'x64']\n\n        cxx_coverage = bool(os.environ.get(\"OTIO_CXX_COVERAGE_BUILD\"))\n        if cxx_coverage and not os.environ.get(\"OTIO_CXX_BUILD_TMP_DIR\"):\n            raise RuntimeError(\n                \"C++ code coverage requires that both OTIO_CXX_COVERAGE_BUILD=ON \"\n                \"and OTIO_CXX_BUILD_TMP_DIR are specified as environment \"\n                \"variables, otherwise coverage cannot be generated.\"\n            )\n\n        if cxx_coverage:\n            cmake_args += ['-DOTIO_CXX_COVERAGE=1']\n\n        return cmake_args"
    },
    {
        "pr_title": "Add C++ implementation of each_child()/each_clip()",
        "pr_number": 926,
        "file_name": "src/opentimelineio/composition.cpp",
        "code_diff": "@@ -350,45 +350,51 @@\nSerializableObject::Retainer<Composable> Composition::child_at_time(\n         search_time,\n         [&range_map](Composable* child) { return range_map[child].end_time_exclusive(); },\n         error_status);\n-    if (!error_status)\n+    if (!error_status) {\n         *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n+    }\n \n     // find the last item whose start_time is before the\n     const auto last_in_range = _bisect_right(\n         search_time,\n         [&range_map](Composable* child) { return range_map[child].start_time(); },\n         error_status,\n         first_inside_range);\n-    if (!error_status)\n+    if (!error_status) {\n         *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n+    }\n \n     // limit the search to children who are in the search_range\n     std::vector<Retainer<Composable>> possible_matches;\n-    for (auto child = _children.begin() + first_inside_range; child < _children.begin() + last_in_range; ++child)\n+    for (auto child = _children.begin() + first_inside_range; child < _children.begin() + last_in_range; ++child) {\n         possible_matches.push_back(child->value);\n-    \n+    }\n     Retainer<Composable> result;\n-    for (const auto& thing : possible_matches)\n+    for (const auto& thing : possible_matches) {\n         if (range_map[thing].overlaps(search_time))\n         {\n             result = thing;\n             break;\n         }\n+    }\n \n     // if the search cannot or should not continue\n     auto composition = Retainer<Composition>(dynamic_cast<Composition*>(result.value));\n-    if (!result || shallow_search || !composition)\n+    if (!result || shallow_search || !composition) {\n         return result;\n+    }\n \n     // before you recurse, you have to transform the time into the\n     // space of the child\n     const auto child_search_time = transformed_time(search_time, composition.value, error_status);\n-    if (!error_status)\n+    if (!error_status) {\n         *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n+    }\n \n     result = composition.value->child_at_time(child_search_time, error_status, shallow_search);\n-    if (!error_status)\n+    if (!error_status) {\n         *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n+    }\n     return result;\n }",
        "comments": [
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    for (auto child = _children.begin() + first_inside_range; child < _children.begin() + last_in_range; ++child) {\r\n        possible_matches.push_back(child->value);\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    for (const auto& thing : possible_matches) {\r\n        if (range_map[thing].overlaps(search_time))\r\n        {\r\n            result = thing;\r\n            break;\r\n        }\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!result || shallow_search || !composition) {\r\n        return result;\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    for (auto child = _children.begin() + first_inside_range; child < _children.begin() + last_in_range; ++child) {\r\n        children.push_back(child->value);\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!upper_search_bound) {\r\n        upper_search_bound = optional<int64_t>(_children.size());\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        if (tgt < key_func(_children[midpoint_index])) {\r\n            upper_search_bound = midpoint_index;\r\n        } \r\n        else {\r\n            lower_search_bound = midpoint_index + 1;\r\n        }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!upper_search_bound) {\r\n        upper_search_bound = optional<int64_t>(_children.size());\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        if (key_func(_children[midpoint_index]) < tgt) {\r\n            lower_search_bound = midpoint_index + 1;\r\n        }\r\n        else {\r\n            upper_search_bound = midpoint_index;\r\n        }\r\n    }\r\n```",
                "position": null
            }
        ],
        "commit_message": "Apply suggestions from code review",
        "commit_id": "7a410d9d618f32e4161d04b1faf87eeb2c8ec559",
        "fn_context": "SerializableObject::Retainer<Composable> Composition::child_at_time(\n    RationalTime const& search_time,\n    ErrorStatus* error_status,\n    bool shallow_search) const\n{\n    auto range_map = range_of_all_children(error_status);\n    if (!error_status) {\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n    }\n\n    // find the first item whose end_time_exclusive is after the\n    const auto first_inside_range = _bisect_left(\n        search_time,\n        [&range_map](Composable* child) { return range_map[child].end_time_exclusive(); },\n        error_status);\n    if (!error_status) {\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n    }\n\n    // find the last item whose start_time is before the\n    const auto last_in_range = _bisect_right(\n        search_time,\n        [&range_map](Composable* child) { return range_map[child].start_time(); },\n        error_status,\n        first_inside_range);\n    if (!error_status) {\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n    }\n\n    // limit the search to children who are in the search_range\n    std::vector<Retainer<Composable>> possible_matches;\n    for (auto child = _children.begin() + first_inside_range; child < _children.begin() + last_in_range; ++child) {\n        possible_matches.push_back(child->value);\n    }\n    Retainer<Composable> result;\n    for (const auto& thing : possible_matches) {\n        if (range_map[thing].overlaps(search_time))\n        {\n            result = thing;\n            break;\n        }\n    }\n\n    // if the search cannot or should not continue\n    auto composition = Retainer<Composition>(dynamic_cast<Composition*>(result.value));\n    if (!result || shallow_search || !composition) {\n        return result;\n    }\n\n    // before you recurse, you have to transform the time into the\n    // space of the child\n    const auto child_search_time = transformed_time(search_time, composition.value, error_status);\n    if (!error_status) {\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n    }\n\n    result = composition.value->child_at_time(child_search_time, error_status, shallow_search);\n    if (!error_status) {\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n    }\n    return result;\n}\n\nstd::vector<SerializableObject::Retainer<Composable>> Composition::children_in_range(\n    TimeRange const& search_range,\n    ErrorStatus* error_status) const\n{\n    auto range_map = range_of_all_children(error_status);\n    if (!error_status) {\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n    }\n\n    // find the first item whose end_time_inclusive is after the\n    // start_time of the search range\n    const auto first_inside_range = _bisect_left(\n        search_range.start_time(),\n        [&range_map](Composable* child) { return range_map[child].end_time_inclusive(); },\n        error_status);\n    if (!error_status) {\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n    }\n\n    // find the last item whose start_time is before the\n    // end_time_inclusive of the search_range\n    const auto last_in_range = _bisect_right(\n        search_range.end_time_inclusive(),\n        [&range_map](Composable* child) { return range_map[child].start_time(); },\n        error_status,\n        first_inside_range);\n    if (!error_status) {\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n    }\n\n    // limit the search to children who are in the search_range\n    std::vector<Retainer<Composable>> children;\n    for (auto child = _children.begin() + first_inside_range; child < _children.begin() + last_in_range; ++child) {\n        children.push_back(child->value);\n    }\n    return children;\n}"
    },
    {
        "pr_title": "Add C++ implementation of each_child()/each_clip()",
        "pr_number": 926,
        "file_name": "src/opentimelineio/composition.cpp",
        "code_diff": "@@ -397,17 +403,19 @@\nstd::vector<SerializableObject::Retainer<Composable>> Composition::children_in_r\n     ErrorStatus* error_status) const\n {\n     auto range_map = range_of_all_children(error_status);\n-    if (!error_status)\n+    if (!error_status) {\n         *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n+    }\n \n     // find the first item whose end_time_inclusive is after the\n     // start_time of the search range\n     const auto first_inside_range = _bisect_left(\n         search_range.start_time(),\n         [&range_map](Composable* child) { return range_map[child].end_time_inclusive(); },\n         error_status);\n-    if (!error_status)\n+    if (!error_status) {\n         *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n+    }\n \n     // find the last item whose start_time is before the\n     // end_time_inclusive of the search_range",
        "comments": [
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    for (auto child = _children.begin() + first_inside_range; child < _children.begin() + last_in_range; ++child) {\r\n        possible_matches.push_back(child->value);\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    for (const auto& thing : possible_matches) {\r\n        if (range_map[thing].overlaps(search_time))\r\n        {\r\n            result = thing;\r\n            break;\r\n        }\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!result || shallow_search || !composition) {\r\n        return result;\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    for (auto child = _children.begin() + first_inside_range; child < _children.begin() + last_in_range; ++child) {\r\n        children.push_back(child->value);\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!upper_search_bound) {\r\n        upper_search_bound = optional<int64_t>(_children.size());\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        if (tgt < key_func(_children[midpoint_index])) {\r\n            upper_search_bound = midpoint_index;\r\n        } \r\n        else {\r\n            lower_search_bound = midpoint_index + 1;\r\n        }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!upper_search_bound) {\r\n        upper_search_bound = optional<int64_t>(_children.size());\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        if (key_func(_children[midpoint_index]) < tgt) {\r\n            lower_search_bound = midpoint_index + 1;\r\n        }\r\n        else {\r\n            upper_search_bound = midpoint_index;\r\n        }\r\n    }\r\n```",
                "position": null
            }
        ],
        "commit_message": "Apply suggestions from code review",
        "commit_id": "7a410d9d618f32e4161d04b1faf87eeb2c8ec559",
        "fn_context": "std::vector<SerializableObject::Retainer<Composable>> Composition::children_in_range(\n    TimeRange const& search_range,\n    ErrorStatus* error_status) const\n{\n    auto range_map = range_of_all_children(error_status);\n    if (!error_status) {\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n    }\n\n    // find the first item whose end_time_inclusive is after the\n    // start_time of the search range\n    const auto first_inside_range = _bisect_left(\n        search_range.start_time(),\n        [&range_map](Composable* child) { return range_map[child].end_time_inclusive(); },\n        error_status);\n    if (!error_status) {\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n    }\n\n    // find the last item whose start_time is before the\n    // end_time_inclusive of the search_range\n    const auto last_in_range = _bisect_right(\n        search_range.end_time_inclusive(),\n        [&range_map](Composable* child) { return range_map[child].start_time(); },\n        error_status,\n        first_inside_range);\n    if (!error_status) {\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n    }\n\n    // limit the search to children who are in the search_range\n    std::vector<Retainer<Composable>> children;\n    for (auto child = _children.begin() + first_inside_range; child < _children.begin() + last_in_range; ++child) {\n        children.push_back(child->value);\n    }\n    return children;\n}"
    },
    {
        "pr_title": "Add C++ implementation of each_child()/each_clip()",
        "pr_number": 926,
        "file_name": "src/opentimelineio/composition.cpp",
        "code_diff": "@@ -416,13 +424,15 @@\nstd::vector<SerializableObject::Retainer<Composable>> Composition::children_in_r\n         [&range_map](Composable* child) { return range_map[child].start_time(); },\n         error_status,\n         first_inside_range);\n-    if (!error_status)\n+    if (!error_status) {\n         *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n+    }\n \n     // limit the search to children who are in the search_range\n     std::vector<Retainer<Composable>> children;\n-    for (auto child = _children.begin() + first_inside_range; child < _children.begin() + last_in_range; ++child)\n+    for (auto child = _children.begin() + first_inside_range; child < _children.begin() + last_in_range; ++child) {\n         children.push_back(child->value);\n+    }\n     return children;\n }",
        "comments": [
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    for (auto child = _children.begin() + first_inside_range; child < _children.begin() + last_in_range; ++child) {\r\n        possible_matches.push_back(child->value);\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    for (const auto& thing : possible_matches) {\r\n        if (range_map[thing].overlaps(search_time))\r\n        {\r\n            result = thing;\r\n            break;\r\n        }\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!result || shallow_search || !composition) {\r\n        return result;\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    for (auto child = _children.begin() + first_inside_range; child < _children.begin() + last_in_range; ++child) {\r\n        children.push_back(child->value);\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!upper_search_bound) {\r\n        upper_search_bound = optional<int64_t>(_children.size());\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        if (tgt < key_func(_children[midpoint_index])) {\r\n            upper_search_bound = midpoint_index;\r\n        } \r\n        else {\r\n            lower_search_bound = midpoint_index + 1;\r\n        }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!upper_search_bound) {\r\n        upper_search_bound = optional<int64_t>(_children.size());\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        if (key_func(_children[midpoint_index]) < tgt) {\r\n            lower_search_bound = midpoint_index + 1;\r\n        }\r\n        else {\r\n            upper_search_bound = midpoint_index;\r\n        }\r\n    }\r\n```",
                "position": null
            }
        ],
        "commit_message": "Apply suggestions from code review",
        "commit_id": "7a410d9d618f32e4161d04b1faf87eeb2c8ec559",
        "fn_context": "std::vector<SerializableObject::Retainer<Composable>> Composition::children_in_range(\n    TimeRange const& search_range,\n    ErrorStatus* error_status) const\n{\n    auto range_map = range_of_all_children(error_status);\n    if (!error_status) {\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n    }\n\n    // find the first item whose end_time_inclusive is after the\n    // start_time of the search range\n    const auto first_inside_range = _bisect_left(\n        search_range.start_time(),\n        [&range_map](Composable* child) { return range_map[child].end_time_inclusive(); },\n        error_status);\n    if (!error_status) {\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n    }\n\n    // find the last item whose start_time is before the\n    // end_time_inclusive of the search_range\n    const auto last_in_range = _bisect_right(\n        search_range.end_time_inclusive(),\n        [&range_map](Composable* child) { return range_map[child].start_time(); },\n        error_status,\n        first_inside_range);\n    if (!error_status) {\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\n    }\n\n    // limit the search to children who are in the search_range\n    std::vector<Retainer<Composable>> children;\n    for (auto child = _children.begin() + first_inside_range; child < _children.begin() + last_in_range; ++child) {\n        children.push_back(child->value);\n    }\n    return children;\n}\n\nint64_t Composition::_bisect_right(\n    RationalTime const& tgt,\n    std::function<RationalTime(Composable*)> const& key_func,\n    ErrorStatus* error_status,\n    optional<int64_t> lower_search_bound,\n    optional<int64_t> upper_search_bound) const\n{\n    if (*lower_search_bound < 0)\n    {\n        *error_status = ErrorStatus(ErrorStatus::Outcome::INTERNAL_ERROR, \"lower_search_bound must be non-negative\");\n        return 0;\n    }\n    if (!upper_search_bound) {\n        upper_search_bound = optional<int64_t>(_children.size());\n    }\n    int64_t midpoint_index = 0;\n    while (*lower_search_bound < *upper_search_bound)\n    {\n        midpoint_index = static_cast<int64_t>(std::floor((*lower_search_bound + *upper_search_bound) / 2.0));\n\n        if (tgt < key_func(_children[midpoint_index])) {\n            upper_search_bound = midpoint_index;\n        } \n        else {\n            lower_search_bound = midpoint_index + 1;\n        }\n    }\n\n    return *lower_search_bound;\n}"
    },
    {
        "pr_title": "Add C++ implementation of each_child()/each_clip()",
        "pr_number": 926,
        "file_name": "src/opentimelineio/composition.cpp",
        "code_diff": "@@ -438,17 +448,20 @@\nint64_t Composition::_bisect_right(\n         *error_status = ErrorStatus(ErrorStatus::Outcome::INTERNAL_ERROR, \"lower_search_bound must be non-negative\");\n         return 0;\n     }\n-    if (!upper_search_bound)\n+    if (!upper_search_bound) {\n         upper_search_bound = optional<int64_t>(_children.size());\n+    }\n     int64_t midpoint_index = 0;\n     while (*lower_search_bound < *upper_search_bound)\n     {\n         midpoint_index = static_cast<int64_t>(std::floor((*lower_search_bound + *upper_search_bound) / 2.0));\n \n-        if (tgt < key_func(_children[midpoint_index]))\n+        if (tgt < key_func(_children[midpoint_index])) {\n             upper_search_bound = midpoint_index;\n-        else\n+        } \n+        else {\n             lower_search_bound = midpoint_index + 1;\n+        }\n     }\n \n     return *lower_search_bound;",
        "comments": [
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    for (auto child = _children.begin() + first_inside_range; child < _children.begin() + last_in_range; ++child) {\r\n        possible_matches.push_back(child->value);\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    for (const auto& thing : possible_matches) {\r\n        if (range_map[thing].overlaps(search_time))\r\n        {\r\n            result = thing;\r\n            break;\r\n        }\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!result || shallow_search || !composition) {\r\n        return result;\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!error_status) {\r\n        *error_status = ErrorStatus(ErrorStatus::INTERNAL_ERROR, \"one or more invalid children encountered\");\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    for (auto child = _children.begin() + first_inside_range; child < _children.begin() + last_in_range; ++child) {\r\n        children.push_back(child->value);\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!upper_search_bound) {\r\n        upper_search_bound = optional<int64_t>(_children.size());\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        if (tgt < key_func(_children[midpoint_index])) {\r\n            upper_search_bound = midpoint_index;\r\n        } \r\n        else {\r\n            lower_search_bound = midpoint_index + 1;\r\n        }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n    if (!upper_search_bound) {\r\n        upper_search_bound = optional<int64_t>(_children.size());\r\n    }\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        if (key_func(_children[midpoint_index]) < tgt) {\r\n            lower_search_bound = midpoint_index + 1;\r\n        }\r\n        else {\r\n            upper_search_bound = midpoint_index;\r\n        }\r\n    }\r\n```",
                "position": null
            }
        ],
        "commit_message": "Apply suggestions from code review",
        "commit_id": "7a410d9d618f32e4161d04b1faf87eeb2c8ec559",
        "fn_context": "int64_t Composition::_bisect_right(\n    RationalTime const& tgt,\n    std::function<RationalTime(Composable*)> const& key_func,\n    ErrorStatus* error_status,\n    optional<int64_t> lower_search_bound,\n    optional<int64_t> upper_search_bound) const\n{\n    if (*lower_search_bound < 0)\n    {\n        *error_status = ErrorStatus(ErrorStatus::Outcome::INTERNAL_ERROR, \"lower_search_bound must be non-negative\");\n        return 0;\n    }\n    if (!upper_search_bound) {\n        upper_search_bound = optional<int64_t>(_children.size());\n    }\n    int64_t midpoint_index = 0;\n    while (*lower_search_bound < *upper_search_bound)\n    {\n        midpoint_index = static_cast<int64_t>(std::floor((*lower_search_bound + *upper_search_bound) / 2.0));\n\n        if (tgt < key_func(_children[midpoint_index])) {\n            upper_search_bound = midpoint_index;\n        } \n        else {\n            lower_search_bound = midpoint_index + 1;\n        }\n    }\n\n    return *lower_search_bound;\n}\n\nint64_t Composition::_bisect_left(\n    RationalTime const& tgt,\n    std::function<RationalTime(Composable*)> const& key_func,\n    ErrorStatus* error_status,\n    optional<int64_t> lower_search_bound,\n    optional<int64_t> upper_search_bound) const\n{\n    if (*lower_search_bound < 0)\n    {\n        *error_status = ErrorStatus(ErrorStatus::Outcome::INTERNAL_ERROR, \"lower_search_bound must be non-negative\");\n        return 0;\n    }\n    if (!upper_search_bound) {\n        upper_search_bound = optional<int64_t>(_children.size());\n    }\n    int64_t midpoint_index = 0;\n    while (*lower_search_bound < *upper_search_bound)\n    {\n        midpoint_index = static_cast<int64_t>(std::floor((*lower_search_bound + *upper_search_bound) / 2.0));\n\n        if (key_func(_children[midpoint_index]) < tgt) {\n            lower_search_bound = midpoint_index + 1;\n        }\n        else {\n            upper_search_bound = midpoint_index;\n        }\n    }\n\n    return *lower_search_bound;\n}"
    },
    {
        "pr_title": "Enable xges unit tests on all platform/python versions",
        "pr_number": 894,
        "file_name": "contrib/opentimelineio_contrib/adapters/tests/tests_xges_adapter.py",
        "code_diff": "@@ -23,13 +23,12 @@\n#\n \n import os\n-import platform\n-import sys\n import tempfile\n import unittest\n from fractions import Fraction\n from xml.etree import ElementTree\n \n+from builtins import int\n import opentimelineio as otio\n import opentimelineio.test_utils as otio_test_utils\n from opentimelineio.schema import (",
        "comments": [],
        "commit_message": "Run xges tests everywhere\n\nFixing minor confusion between int and longs",
        "commit_id": "77d851fa669e884e6528916fd8bc74d1944946bb"
    },
    {
        "pr_title": "Enable xges unit tests on all platform/python versions",
        "pr_number": 894,
        "file_name": "contrib/opentimelineio_contrib/adapters/tests/tests_xges_adapter.py",
        "code_diff": "@@ -352,7 +351,7 @@\ndef assertOtioHasAttrPath(self, otio_obj, attr_path):\n         attr_str = \"\"\n         val = otio_obj\n         for attr_name in attr_path:\n-            if type(attr_name) is int:\n+            if isinstance(attr_name, int):\n                 if not hasattr(val, \"__getitem__\"):\n                     raise AssertionError(\n                         \"{}{} is not a list\".format(",
        "comments": [],
        "commit_message": "Run xges tests everywhere\n\nFixing minor confusion between int and longs",
        "commit_id": "77d851fa669e884e6528916fd8bc74d1944946bb"
    },
    {
        "pr_title": "Enable xges unit tests on all platform/python versions",
        "pr_number": 894,
        "file_name": "contrib/opentimelineio_contrib/adapters/xges.py",
        "code_diff": "@@ -38,6 +38,7 @@\nfrom urlparse import urlparse\n     from urlparse import parse_qs\n \n+from builtins import int\n from fractions import Fraction\n from xml.etree import ElementTree\n from xml.dom import minidom",
        "comments": [],
        "commit_message": "Run xges tests everywhere\n\nFixing minor confusion between int and longs",
        "commit_id": "77d851fa669e884e6528916fd8bc74d1944946bb"
    },
    {
        "pr_title": "Enable xges unit tests on all platform/python versions",
        "pr_number": 894,
        "file_name": "contrib/opentimelineio_contrib/adapters/xges.py",
        "code_diff": "@@ -3526,7 +3527,7 @@\ndef __init__(self, position=0, metadatas=None):\n \n     def set_color_from_argb(self, argb):\n         \"\"\"Set the color of the marker using the AARRGGBB hex value\"\"\"\n-        if type(argb) is not int:\n+        if not isinstance(argb, int):\n             _wrong_type_for_arg(argb, \"int\", \"argb\")\n         if argb < 0 or argb > 0xffffffff:\n             raise InvalidValueError(",
        "comments": [],
        "commit_message": "Run xges tests everywhere\n\nFixing minor confusion between int and longs",
        "commit_id": "77d851fa669e884e6528916fd8bc74d1944946bb"
    },
    {
        "pr_title": "Enable xges unit tests on all platform/python versions",
        "pr_number": 894,
        "file_name": "contrib/opentimelineio_contrib/adapters/xges.py",
        "code_diff": "@@ -3669,15 +3670,8 @@\ndef add(self, marker):\n \n     def markers_at_position(self, position):\n         \"\"\"Return a list of markers with the given position\"\"\"\n-        if type(position) is not int:\n-            # TODO: remove below once python2 has ended\n-            # currently in python2, can receive either an int or\n-            # a long\n-            if isinstance(position, numbers.Integral):\n-                position = int(position)\n-                # may still be an int if the position is too big\n-            if type(position) is not int:\n-                _wrong_type_for_arg(position, \"int\", \"position\")\n+        if not isinstance(position, int):\n+            _wrong_type_for_arg(position, \"int\", \"position\")\n         return [mrk for mrk in self.markers if mrk.position == position]\n \n     def __getitem__(self, index):",
        "comments": [],
        "commit_message": "Run xges tests everywhere\n\nFixing minor confusion between int and longs",
        "commit_id": "77d851fa669e884e6528916fd8bc74d1944946bb"
    },
    {
        "pr_title": "CMake: Fix message() types",
        "pr_number": 879,
        "file_name": "setup.py",
        "code_diff": "@@ -29,6 +29,14 @@\nimport distutils\n \n \n+# XXX: If there is a better way to find the value of --prefix, please notify\n+#      the maintainers of OpenTimelineIO.\n+_dist = distutils.dist.Distribution()\n+_dist.parse_config_files()\n+_dist.parse_command_line()\n+PREFIX = _dist.get_option_dict('install').get('prefix', [None, None])[1]\n+\n+\n class _Ctx(object):\n     pass",
        "comments": [],
        "commit_message": "Merge branch 'master' into cmake-typos-2021-02-09",
        "commit_id": "0008d11e3bb47799a4b573de9ed042e4d9981e2b"
    },
    {
        "pr_title": "CMake: Fix message() types",
        "pr_number": 879,
        "file_name": "setup.py",
        "code_diff": "@@ -77,21 +85,30 @@\ndef cmake_generate():\n         '-DCMAKE_BUILD_TYPE=' + ('Debug' if _ctx.debug else 'Release')\n     ]\n \n-    if \"--user\" in sys.argv:\n-        cmake_args += [\n-            '-DOTIO_PYTHON_INSTALL_DIR=' + _ctx.install_usersite,\n-            '-DCMAKE_INSTALL_PREFIX=' + os.path.join(_ctx.install_usersite,\n-                                                     \"opentimelineio\", \"cxx-libs\"),\n-            '-DOTIO_PYTHON_PACKAGE_DIR=' + os.path.join(_ctx.install_usersite,\n-                                                        \"opentimelineio\")\n-        ]\n-    else:\n-        cmake_args += [\n-            '-DOTIO_PYTHON_INSTALL_DIR=' + get_python_lib(),\n-            '-DCMAKE_INSTALL_PREFIX=' + os.path.join(get_python_lib(),\n-                                                     \"opentimelineio\", \"cxx-libs\"),\n-            '-DOTIO_PYTHON_PACKAGE_DIR=' + get_python_lib()\n-        ]\n+    python_inst_dir = get_python_lib()\n+\n+    if PREFIX:\n+        # XXX: is there a better way to find this?  This is the suffix from\n+        # where it would have been installed pasted onto the PREFIX as passed\n+        # in by --prefix.\n+        python_inst_dir = (\n+            distutils.sysconfig.get_python_lib().replace(sys.prefix, PREFIX)\n+        )\n+    elif \"--user\" in sys.argv:\n+        python_inst_dir = _ctx.install_usersite\n+\n+    # install the C++ into the opentimelineio/cxx-sdk directory under the\n+    # python installation\n+    cmake_install_prefix = os.path.join(\n+        python_inst_dir,\n+        \"opentimelineio\",\n+        \"cxx-sdk\"\n+    )\n+\n+    cmake_args += [\n+        '-DOTIO_PYTHON_INSTALL_DIR=' + python_inst_dir,\n+        '-DCMAKE_INSTALL_PREFIX=' + cmake_install_prefix,\n+    ]\n \n     if platform.system() == \"Windows\":\n         if sys.maxsize > 2**32:",
        "comments": [],
        "commit_message": "Merge branch 'master' into cmake-typos-2021-02-09",
        "commit_id": "0008d11e3bb47799a4b573de9ed042e4d9981e2b"
    },
    {
        "pr_title": "add windows and mac builds to ci",
        "pr_number": 873,
        "file_name": "contrib/opentimelineio_contrib/adapters/tests/tests_xges_adapter.py",
        "code_diff": "@@ -24,6 +24,7 @@\nimport os\n import platform\n+import sys\n import tempfile\n import unittest\n from fractions import Fraction",
        "comments": [],
        "commit_message": "Also disable xges for python < 3.",
        "commit_id": "215de621348cdf09409e0ccc88ab519142e9b721"
    },
    {
        "pr_title": "detect cycles",
        "pr_number": 848,
        "file_name": "src/opentimelineio/serialization.cpp",
        "code_diff": "@@ -556,16 +556,26 @@\nvoid SerializableObject::Writer::write(std::string const& key, SerializableObjec\n         return;\n     }\n \n-#ifdef OTIO_INSTANCING_SUPPORT\n     auto e = _id_for_object.find(value);\n     if (e != _id_for_object.end()) {\n+#ifdef OTIO_INSTANCING_SUPPORT\n         /*\n          * We've already written this value.\n          */\n         _encoder.write_value(SerializableObject::ReferenceId { e->second });\n+#else\n+        /*\n+         * We're encountering the same object while it is still\n+         * in the map, meaning we're in the middle of writing it out.\n+         * That's a cycle, as opposed to mere instancing, which we\n+         * allow so as not to break old allowed behavior.\n+         */\n+        std::string s = string_printf(\"cyclically encountered object has schema %s\",\n+                                      value->schema_name().c_str());\n+        _encoder._error(ErrorStatus(ErrorStatus::OBJECT_CYCLE, s));\n+#endif        \n         return;\n     }\n-#endif\n \n     std::string const& schema_type_name = value->_schema_name_for_reference();\n     if (_next_id_for_type.find(schema_type_name) == _next_id_for_type.end()) {",
        "comments": [],
        "commit_message": "detect cycles",
        "commit_id": "8481e6ba3388c0d6cd59d89125d0b2b1dc77d913"
    },
    {
        "pr_title": "xges: Add support for image sequences",
        "pr_number": 792,
        "file_name": "contrib/opentimelineio_contrib/adapters/tests/tests_xges_adapter.py",
        "code_diff": "@@ -47,6 +47,8 @@\nXGES_EXAMPLE_PATH = os.path.join(SAMPLE_DATA_DIR, \"xges_example.xges\")\n XGES_TIMING_PATH = os.path.join(SAMPLE_DATA_DIR, \"xges_timing_example.xges\")\n XGES_NESTED_PATH = os.path.join(SAMPLE_DATA_DIR, \"xges_nested_example.xges\")\n+IMAGE_SEQUENCE_EXAMPLE_PATH = os.path.join(\n+    SAMPLE_DATA_DIR, \"image_sequence_example.otio\")\n \n SCHEMA = otio.schema.schemadef.module_from_name(\"xges\")\n # TODO: remove once python2 has ended:",
        "comments": [],
        "commit_message": "xges: Add support for image sequences",
        "commit_id": "51603d815de3c4bf2395d54cee4b40239140f1e6"
    },
    {
        "pr_title": "xges: Add support for image sequences",
        "pr_number": 792,
        "file_name": "contrib/opentimelineio_contrib/adapters/xges.py",
        "code_diff": "@@ -24,8 +24,20 @@\n\"\"\"OpenTimelineIO GStreamer Editing Services XML Adapter.\"\"\"\n import re\n+import os\n import warnings\n import numbers\n+try:\n+    from urllib.parse import quote\n+    from urllib.parse import unquote\n+    from urllib.parse import urlparse\n+    from urllib.parse import parse_qs\n+except ImportError:  # python2\n+    from urllib import quote\n+    from urllib import unquote\n+    from urlparse import urlparse\n+    from urlparse import parse_qs\n+\n from fractions import Fraction\n from xml.etree import ElementTree\n from xml.dom import minidom",
        "comments": [],
        "commit_message": "xges: Add support for image sequences",
        "commit_id": "51603d815de3c4bf2395d54cee4b40239140f1e6"
    },
    {
        "pr_title": "xges: Add support for image sequences",
        "pr_number": 792,
        "file_name": "contrib/opentimelineio_contrib/adapters/xges.py",
        "code_diff": "@@ -957,6 +969,33 @@\ndef _create_otio_gap(self, gst_duration):\n             self._to_rational_time(gst_duration))\n         return otio.schema.Gap(source_range=source_range)\n \n+    def _otio_image_sequence_from_url(self, ref_url):\n+\n+        # TODO: Add support for missing policy\n+        params = {}\n+        fname, ext = os.path.splitext(unquote(os.path.basename(ref_url.path)))\n+        index_format = re.findall(r\"%\\d+d\", fname)\n+        if index_format:\n+            params[\"frame_zero_padding\"] = int(index_format[-1][2:-1])\n+            fname = fname[0:-len(index_format[-1])]\n+\n+        url_params = parse_qs(ref_url.query)\n+        if \"framerate\" in url_params:\n+            rate = params[\"rate\"] = float(Fraction(url_params[\"framerate\"][-1]))\n+            if \"start-index\" in url_params and \"stop-index\" in url_params:\n+                start = int(url_params[\"start-index\"][-1])\n+                stop = int(url_params[\"stop-index\"][-1])\n+                params[\"available_range\"] = otio.opentime.TimeRange(\n+                    otio.opentime.RationalTime(int(start), rate),\n+                    otio.opentime.RationalTime(int(stop - start), rate),\n+                )\n+        else:\n+            rate = params[\"rate\"] = float(30)\n+\n+        return otio.schema.ImageSequenceReference(\n+            \"file://\" + os.path.dirname(ref_url.path),\n+            fname, ext, **params)\n+\n     def _otio_reference_from_id(self, asset_id):\n         \"\"\"\n         Create a new otio.schema.Reference from the given 'asset_id'",
        "comments": [],
        "commit_message": "xges: Add support for image sequences",
        "commit_id": "51603d815de3c4bf2395d54cee4b40239140f1e6"
    },
    {
        "pr_title": "xges: Add support for image sequences",
        "pr_number": 792,
        "file_name": "contrib/opentimelineio_contrib/adapters/xges.py",
        "code_diff": "@@ -979,10 +1018,15 @@\ndef _otio_reference_from_id(self, asset_id):\n                 start_time=self._to_rational_time(0),\n                 duration=self._to_rational_time(duration)\n             )\n-        otio_ref = otio.schema.ExternalReference(\n-            target_url=asset_id,\n-            available_range=available_range\n-        )\n+\n+        ref_url = urlparse(asset_id)\n+        if ref_url.scheme == \"imagesequence\":\n+            otio_ref = self._otio_image_sequence_from_url(ref_url)\n+        else:\n+            otio_ref = otio.schema.ExternalReference(\n+                target_url=asset_id,\n+                available_range=available_range\n+            )\n         self._add_properties_and_metadatas_to_otio(otio_ref, asset)\n         return otio_ref",
        "comments": [],
        "commit_message": "xges: Add support for image sequences",
        "commit_id": "51603d815de3c4bf2395d54cee4b40239140f1e6"
    },
    {
        "pr_title": "xges: Add support for image sequences",
        "pr_number": 792,
        "file_name": "contrib/opentimelineio_contrib/adapters/xges.py",
        "code_diff": "@@ -1277,9 +1321,37 @@\ndef _serialize_external_reference_to_ressource(\n         a new xges <asset> under the xges 'ressources' corresponding to a\n         uri clip asset. If the asset already exists, it is not created.\n         \"\"\"\n-        asset_id = reference.target_url\n+        if isinstance(reference, otio.schema.ImageSequenceReference):\n+            base_url = urlparse(reference.target_url_base)\n+            asset_id = \"imagesequence:\" + base_url.path\n+            if not base_url.path.endswith(\"/\"):\n+                asset_id += \"/\"\n+            asset_id += quote(\n+                reference.name_prefix + \"%0\"\n+                + str(reference.frame_zero_padding)\n+                + \"d\" + reference.name_suffix)\n+\n+            params = []\n+            if reference.rate:\n+                rate = reference.rate.as_integer_ratio()\n+                params.append(\"rate=%i/%i\" % (rate[0], rate[1]))\n+\n+            if reference.available_range:\n+                params.append(\n+                    \"start-index=%i\" %\n+                    int(reference.available_range.start_time.value))\n+                params.append(\n+                    \"stop-index=%i\" % (\n+                        reference.available_range.start_time.value\n+                        + reference.available_range.duration.value))\n+\n+            if params:\n+                asset_id += '?'\n+                asset_id += '&'.join(params)\n+        else:\n+            asset_id = reference.target_url\n         if self._asset_exists(asset_id, ressources, \"GESUriClip\"):\n-            return\n+            return asset_id\n         properties = self._get_element_properties(reference)\n         if properties.get_typed(\"duration\", \"guint64\") is None:\n             a_range = reference.available_range",
        "comments": [],
        "commit_message": "xges: Add support for image sequences",
        "commit_id": "51603d815de3c4bf2395d54cee4b40239140f1e6"
    },
    {
        "pr_title": "xges: Add support for image sequences",
        "pr_number": 792,
        "file_name": "contrib/opentimelineio_contrib/adapters/xges.py",
        "code_diff": "@@ -1297,6 +1369,7 @@\ndef _serialize_external_reference_to_ressource(\n                 \"id\": asset_id, \"extractable-type-name\": \"GESUriClip\"})\n         self._add_properties_and_metadatas_to_element(\n             asset, reference, properties=properties)\n+        return asset_id\n \n     @classmethod\n     def _get_effect_bin_desc(cls, otio_effect):",
        "comments": [],
        "commit_message": "xges: Add support for image sequences",
        "commit_id": "51603d815de3c4bf2395d54cee4b40239140f1e6"
    },
    {
        "pr_title": "AAF Reader Metadata",
        "pr_number": 739,
        "file_name": "contrib/opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -341,7 +341,7 @@\ndef _transcribe(item, parents, edit_rate, indent=0):\n                                        aaf2.essence.DigitalImageDescriptor)\n                                 and sourcemob.descriptor.locator):\n                             locator = sourcemob.descriptor.locator[0]\n-                        elif \"Locator\" in sourcemob.descriptor.keys():\n+                        elif \"Locator\" in sourcemob.descriptor:\n                             locator = sourcemob.descriptor[\"Locator\"].value[0]\n \n                         if locator:",
        "comments": [
            {
                "comment": "usually stored where? This looks like the comment got truncated?",
                "position": null
            },
            {
                "comment": "Can a MasterMob be referenced multiple times? If so, is there an opportunity to cache the result of this _transcribe? If that seems tricky, we could just leave a TODO or file an issue for later.",
                "position": null
            },
            {
                "comment": "I'm slightly confused by the variable name `clipMetadata` since it is going into the media reference. Did it come from the clip?\r\n",
                "position": null
            },
            {
                "comment": "Since the code is modifying the dictionary later, it should be deep copied here. That way the modification doesn't accidentally get included if/when this same mastermob_child is used elsewhere.",
                "position": null
            },
            {
                "comment": "We tend to avoid leaving people's names/logins in the open source code base.",
                "position": null
            },
            {
                "comment": "This should have an `_` prefix since it is for internal use only.",
                "position": null
            },
            {
                "comment": "Hm, the metadata will be assigned to the clip as well as the media reference. From my perspective it represents the metadata for a given clip.",
                "position": null
            },
            {
                "comment": "Yes, I can change that. I still think the mastermob_child will not be used anywhere else, but better safe than sorry, it should not be expensive. Just pointing out: I kept the original behavior, just added an intermediate step storing the result in a variable. ",
                "position": null
            },
            {
                "comment": "sorry, that was from my tools-tree change. I'll remove that.",
                "position": null
            },
            {
                "comment": "ok.",
                "position": null
            },
            {
                "comment": "Usually there is one mastermob associated with a SourceClip as far as my experience goes. I don't think caching the master-mob will gain us much. But I can just test it with some of our large sequences and see if I get the same master-mob-id several times. ",
                "position": null
            },
            {
                "comment": "That line is from the old comment, the updated comment right above covers everything. ",
                "position": null
            },
            {
                "comment": "(But I followed the debug flag just right above, which is also internal as far as I can see. That should be _debug then as well?)",
                "position": null
            },
            {
                "comment": "We've tried to follow that convention for functions. (e.g. `_transcode`) Maybe we should do that for module-level variables as well? @ssteinbach do you have an opinion?\r\n\r\nYou could argue that the toggles to turn this on/off are useful from outside the module - e.g. when debugging an application that uses this module.",
                "position": null
            },
            {
                "comment": "Yeah.  @meshula we should add these notes to the in progress style guide.  Some stuff:\r\n\r\n- I would vote to remove the `transcribe_debug` variable entirely (and any if statements that were only switched by that)\r\n- if its going to exist it should be all caps (module level constant) + prefixed by a `_` (not intended to pollute the namespace)\r\n- I think josh was commenting on the `transcribe` function, which if its for internal usage should also be prefixed by a `_`, `_transcribe()` to not pollute the namespace and signal its intent.\r\n- finally, we're using snake case a la pep8, so the argument should be `always_print` rather than `alwaysPrint`.\r\n\r\nsee also: https://www.python.org/dev/peps/pep-0008/#function-and-variable-names\r\n\r\n> Function and Variable Names\r\nFunction names should be lowercase, with words separated by underscores as necessary to improve readability.\r\n>\r\n> Variable names follow the same convention as function names.\r\n>\r\n> mixedCase is allowed only in contexts where that's already the prevailing style (e.g. threading.py), to retain backwards compatibility.\r\n\r\n\r\nAlso:\r\n>Constants\r\n>Constants are usually defined on a module level and written in all capital letters with underscores separating words. Examples include MAX_OVERFLOW and TOTAL",
                "position": null
            },
            {
                "comment": "Just to clarify, I prefer to remove debug code - it decreases test coverage, usually requires special knowledge to turn on, and for python at least is rarely as useful as using a dedicated debugger like `pdb`.  Unless its exposed as top level interface functionality to users.  Open to counter point on that one though; what do the others think?",
                "position": null
            },
            {
                "comment": "I think its probably a good idea to make sure this adapter also adheres more closely to pep8:\r\n\r\n```suggestion\r\n        child_mastermob, composition_user_metadata = _find_mastermob_for_sourceclip(item)\r\n```",
                "position": null
            },
            {
                "comment": "do we need these transcribe log calls?",
                "position": null
            },
            {
                "comment": "```suggestion\r\n            clip_metadata = copy.deepcopy(mastermob_child.metadata.get(\"AAF\", {}))\r\n```",
                "position": null
            },
            {
                "comment": "`not in` is a check against `keys()`, don't need the extra call\r\n\r\n```suggestion\r\n                if \"UserComments\" not in clipMetadata:\r\n```",
                "position": null
            },
            {
                "comment": "does it make sense to apply it to the media reference then?  Or is the media reference a reference to the AAF file?",
                "position": null
            },
            {
                "comment": "if we're in here:\r\n```suggestion\r\ndef _transcribe(item, parents, edit_rate, indent=0):\r\n```",
                "position": null
            },
            {
                "comment": "If we're going to include these `trancribe_log` calls for each type, I'd rather we use a consistent format, so:\r\n\r\n`transcribe_log(\"Gap\", item, indent)`\r\n\r\n...and then inside `transcribe_log()` it unrolls the name into the message `\"Creating {} for {}\".format(typestr, _encoded_name(item)), indent)`\r\n\r\n...but the broader problem with these kinds of logs from adapters is that we don't have a consistent framework for it.  @reinecke just flagging this since I know its something you've thought about in the past.\r\nand then have transcribe_log format the item name and string consistently (unless there is a lot of helpful variation in these strings)",
                "position": null
            },
            {
                "comment": "I am sorry, but I disagree in this point strongly. The AAF-object-structure is hard to understand and you need some debug tools in place to manage the complexity of _transcribe(). I actually use this flag in conjunction with pdb. I activate the flag and then do a set_trace() in the part of the traversal-tree I am interested in. So If I set it in SourceClip, I can see right away what traversal happened until this point, giving me some important context of how to introspect my current state in relation to the traversal. ",
                "position": null
            },
            {
                "comment": "yes, I'll change that. ",
                "position": null
            },
            {
                "comment": "will update",
                "position": null
            },
            {
                "comment": "Also just to clarify, I agree with your point in most (almost all) cases, but there are some exceptions where a complex, 'deep' structure requires some debug-bandaid-code put in place, so whoever wants to do a deep dive into that code has already something in place and does not require to scaffold the code from ground up. IMHO such situations are exceptions but should outweigh the desire of a perfect code-coverage.   ",
                "position": null
            },
            {
                "comment": "I like the idea of turning it on via an adapter argument. We will be improving this adapter for a long time to come, and this sort of diagnostic feature is akin to the simplify=False option that is already an adapter argument.",
                "position": null
            },
            {
                "comment": "Oh, and I guess I wasn't clear. It was mostly the function `transcribe_log()` that I was suggesting should be `_transcribe_log()`",
                "position": null
            },
            {
                "comment": "yes, I like the idea to have it be an adapter argument. And yes, I totally did not realize you meant the method. I'll fix that.",
                "position": null
            },
            {
                "comment": "yeah, good point. It started as a more generic log, but since it's now specific to the _transcribe() method, this is a good idea.",
                "position": null
            },
            {
                "comment": "Ah, okay - this makes sense to me now.",
                "position": null
            },
            {
                "comment": "The original intent of the mastermob list was meant to avoid re-transcribing the same mastermobs again and again. I was under the (somewhat naive) impression that this would be common. \r\n\r\nIn your testing so far, is the adapter faster, slower, or about the same as before this change? If it is the same or faster with a few real world AAFs then, we can leave this until another time when we do some more extensive performance testing. If it is slower, then we should look deeper since folks are already unhappy with the performance.",
                "position": null
            },
            {
                "comment": "I would not have removed the first pass if it would have made the adapter slower. The reason (besides simplifying the code) is a 30-40% speed gain (also listed in the original commit-description), tested on three large sequences from three different shows of ours.",
                "position": null
            },
            {
                "comment": "Faster is great! Thanks for checking.",
                "position": null
            },
            {
                "comment": "This is an adapter argument now - does that satisfy your concern @ssteinbach ?",
                "position": null
            }
        ],
        "commit_message": "Addressing some final review comments",
        "commit_id": "57f6f6c591e47bb8070c3c10ce24ad2ca5dd9cf5",
        "fn_context": "def _transcribe(item, parents, edit_rate, indent=0):\n    result = None\n    metadata = {}\n\n    # First lets grab some standard properties that are present on\n    # many types of AAF objects...\n    metadata[\"Name\"] = _get_name(item)\n    metadata[\"ClassName\"] = _get_class_name(item)\n\n    # Some AAF objects (like TimelineMobSlot) have an edit rate\n    # which should be used for all of the object's children.\n    # We will pass it on to any recursive calls to _transcribe()\n    if hasattr(item, \"edit_rate\"):\n        edit_rate = float(item.edit_rate)\n\n    if isinstance(item, aaf2.components.Component):\n        metadata[\"Length\"] = item.length\n\n    if isinstance(item, aaf2.core.AAFObject):\n        for prop in item.properties():\n            if hasattr(prop, 'name') and hasattr(prop, 'value'):\n                key = str(prop.name)\n                value = prop.value\n                metadata[key] = _transcribe_property(value)\n\n    # Now we will use the item's class to determine which OTIO type\n    # to transcribe into. Note that the order of this if/elif/... chain\n    # is important, because the class hierarchy of AAF objects is more\n    # complex than OTIO.\n\n    if isinstance(item, aaf2.content.ContentStorage):\n        msg = \"Creating SerializableCollection for {}\".format(_encoded_name(item))\n        _transcribe_log(msg, indent)\n        result = otio.schema.SerializableCollection()\n\n        for mob in item.compositionmobs():\n            _transcribe_log(\"compositionmob traversal\", indent)\n            child = _transcribe(mob, parents + [item], edit_rate, indent + 2)\n            _add_child(result, child, mob)\n\n    elif isinstance(item, aaf2.mobs.Mob):\n        _transcribe_log(\"Creating Timeline for {}\".format(_encoded_name(item)), indent)\n        result = otio.schema.Timeline()\n\n        for slot in item.slots:\n            track = _transcribe(slot, parents + [item], edit_rate, indent + 2)\n            _add_child(result.tracks, track, slot)\n\n            # Use a heuristic to find the starting timecode from\n            # this track and use it for the Timeline's global_start_time\n            start_time = _find_timecode_track_start(track)\n            if start_time:\n                result.global_start_time = start_time\n\n    elif isinstance(item, aaf2.components.SourceClip):\n        _transcribe_log(\"Creating SourceClip for {}\".format(\n            _encoded_name(item)), indent)\n        result = otio.schema.Clip()\n\n        # Evidently the last mob is the one with the timecode\n        mobs = _find_timecode_mobs(item)\n\n        # Get the Timecode start and length values\n        last_mob = mobs[-1] if mobs else None\n        timecode_info = _extract_timecode_info(last_mob) if last_mob else None\n\n        source_start = int(metadata.get(\"StartTime\", \"0\"))\n        source_length = item.length\n        media_start = source_start\n        media_length = item.length\n\n        if timecode_info:\n            media_start, media_length = timecode_info\n            source_start += media_start\n\n        # The goal here is to find a source range. Actual editorial opinions are\n        # found on SourceClips in the CompositionMobs. To figure out whether this\n        # clip is directly in the CompositionMob, we detect if our parent mobs\n        # are only CompositionMobs. If they were anything else - a MasterMob, a\n        # SourceMob, we would know that this is in some indirect relationship.\n        parent_mobs = filter(lambda parent: isinstance(parent, aaf2.mobs.Mob), parents)\n        is_directly_in_composition = all(\n            isinstance(mob, aaf2.mobs.CompositionMob)\n            for mob in parent_mobs\n        )\n        if is_directly_in_composition:\n            result.source_range = otio.opentime.TimeRange(\n                otio.opentime.RationalTime(source_start, edit_rate),\n                otio.opentime.RationalTime(source_length, edit_rate)\n            )\n\n        # The goal here is to find an available range. Media ranges are stored\n        # in the related MasterMob, and there should only be one - hence the name\n        # \"Master\" mob. Somewhere down our chain (either a child or our parents)\n        # is a MasterMob.\n        # There are some special cases where the masterMob could be:\n        # 1) For SourceClips in the CompositionMob, the mob the SourceClip is\n        #    referencing can be our MasterMob.\n        # 2) If the source clip is referencing another CompositionMob,\n        #    drill down to see if the composition holds the MasterMob\n        # 3) For everything else, it is a previously encountered parent. Find the\n        #    MasterMob in our chain, and then extract the information from that.\n\n        child_mastermob, composition_user_metadata = \\\n            _find_mastermob_for_sourceclip(item)\n\n        if composition_user_metadata:\n            metadata['UserComments'] = composition_user_metadata\n\n        parent_mastermobs = [\n            parent for parent in parents\n            if isinstance(parent, aaf2.mobs.MasterMob)\n        ]\n        parent_mastermob = parent_mastermobs[0] if len(parent_mastermobs) > 1 else None\n\n        if child_mastermob:\n            _transcribe_log(\"[found child_mastermob]\", indent)\n        elif parent_mastermob:\n            _transcribe_log(\"[found parent_mastermob]\", indent)\n        else:\n            _transcribe_log(\"[found no mastermob]\", indent)\n\n        mastermob = child_mastermob or parent_mastermob or None\n\n        if mastermob:\n            # Get target path\n            mastermob_child = _transcribe(mastermob, list(), edit_rate, indent)\n\n            target_path = (mastermob_child.metadata.get(\"AAF\", {})\n                                                   .get(\"UserComments\", {})\n                                                   .get(\"UNC Path\"))\n            if not target_path:\n                # retrieve locator form the MasterMob's Essence\n                for mobslot in mastermob.slots:\n                    if isinstance(mobslot.segment, aaf2.components.SourceClip):\n                        sourcemob = mobslot.segment.mob\n                        locator = None\n                        # different essences store locators in different places\n                        if (isinstance(sourcemob.descriptor,\n                                       aaf2.essence.DigitalImageDescriptor)\n                                and sourcemob.descriptor.locator):\n                            locator = sourcemob.descriptor.locator[0]\n                        elif \"Locator\" in sourcemob.descriptor:\n                            locator = sourcemob.descriptor[\"Locator\"].value[0]\n\n                        if locator:\n                            target_path = locator[\"URLString\"].value\n\n            # if we have target path, create an ExternalReference, otherwise\n            # create an MissingReference.\n            if target_path:\n                if not target_path.startswith(\"file://\"):\n                    target_path = \"file://\" + target_path\n                target_path = target_path.replace(\"\\\\\", \"/\")\n                media = otio.schema.ExternalReference(target_url=target_path)\n            else:\n                media = otio.schema.MissingReference()\n\n            media.available_range = otio.opentime.TimeRange(\n                otio.opentime.RationalTime(media_start, edit_rate),\n                otio.opentime.RationalTime(media_length, edit_rate)\n            )\n\n            # Copy the metadata from the master into the media_reference\n            clip_metadata = copy.deepcopy(mastermob_child.metadata.get(\"AAF\", {}))\n\n            # If the composition was holding UserComments and the current masterMob has\n            # no UserComments, use the ones from the CompositionMob. But if the\n            # masterMob has any, prefer them over the compositionMob, since the\n            # masterMob is the ultimate authority for a source clip.\n            if composition_user_metadata:\n                if \"UserComments\" not in clip_metadata:\n                    clip_metadata['UserComments'] = composition_user_metadata\n\n            media.metadata[\"AAF\"] = clip_metadata\n\n            result.media_reference = media\n\n    elif isinstance(item, aaf2.components.Transition):\n        _transcribe_log(\"Creating Transition for {}\".format(\n            _encoded_name(item)), indent)\n        result = otio.schema.Transition()\n\n        # Does AAF support anything else?\n        result.transition_type = otio.schema.TransitionTypes.SMPTE_Dissolve\n\n        # Extract value and time attributes of both ControlPoints used for\n        # creating AAF Transition objects\n        varying_value = None\n        for param in item.getvalue('OperationGroup').parameters:\n            if isinstance(param, aaf2.misc.VaryingValue):\n                varying_value = param\n                break\n\n        if varying_value is not None:\n            for control_point in varying_value.getvalue('PointList'):\n                value = control_point.value\n                time = control_point.time\n                metadata.setdefault('PointList', []).append({'Value': value,\n                                                             'Time': time})\n\n        in_offset = int(metadata.get(\"CutPoint\", \"0\"))\n        out_offset = item.length - in_offset\n        result.in_offset = otio.opentime.RationalTime(in_offset, edit_rate)\n        result.out_offset = otio.opentime.RationalTime(out_offset, edit_rate)\n\n    elif isinstance(item, aaf2.components.Filler):\n        _transcribe_log(\"Creating Gap for {}\".format(_encoded_name(item)), indent)\n        result = otio.schema.Gap()\n\n        length = item.length\n        result.source_range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(0, edit_rate),\n            otio.opentime.RationalTime(length, edit_rate)\n        )\n\n    elif isinstance(item, aaf2.components.NestedScope):\n        msg = \"Creating Stack for NestedScope for {}\".format(_encoded_name(item))\n        _transcribe_log(msg, indent)\n        # TODO: Is this the right class?\n        result = otio.schema.Stack()\n\n        for slot in item.slots:\n            child = _transcribe(slot, parents + [item], edit_rate, indent + 2)\n            _add_child(result, child, slot)\n\n    elif isinstance(item, aaf2.components.Sequence):\n        msg = \"Creating Track for Sequence for {}\".format(_encoded_name(item))\n        _transcribe_log(msg, indent)\n        result = otio.schema.Track()\n\n        for component in item.components:\n            child = _transcribe(component, parents + [item], edit_rate, indent + 2)\n            _add_child(result, child, component)\n\n    elif isinstance(item, aaf2.components.OperationGroup):\n        msg = \"Creating operationGroup for {}\".format(_encoded_name(item))\n        _transcribe_log(msg, indent)\n        result = _transcribe_operation_group(item, parents, metadata,\n                                             edit_rate, indent + 2)\n\n    elif isinstance(item, aaf2.mobslots.TimelineMobSlot):\n        msg = \"Creating Track for TimelineMobSlot for {}\".format(_encoded_name(item))\n        _transcribe_log(msg, indent)\n        result = otio.schema.Track()\n\n        child = _transcribe(item.segment, parents + [item], edit_rate, indent + 2)\n\n        _add_child(result, child, item.segment)\n\n    elif isinstance(item, aaf2.mobslots.MobSlot):\n        msg = \"Creating Track for MobSlot for {}\".format(_encoded_name(item))\n        _transcribe_log(msg, indent)\n        result = otio.schema.Track()\n\n        child = _transcribe(item.segment, parents + [item], edit_rate, indent + 2)\n        _add_child(result, child, item.segment)\n\n    elif isinstance(item, aaf2.components.Timecode):\n        pass\n\n    elif isinstance(item, aaf2.components.Pulldown):\n        pass\n\n    elif isinstance(item, aaf2.components.EdgeCode):\n        pass\n\n    elif isinstance(item, aaf2.components.ScopeReference):\n        msg = \"Creating Gap for ScopedReference for {}\".format(_encoded_name(item))\n        _transcribe_log(msg, indent)\n        # TODO: is this like FILLER?\n\n        result = otio.schema.Gap()\n\n        length = item.length\n        result.source_range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(0, edit_rate),\n            otio.opentime.RationalTime(length, edit_rate)\n        )\n\n    elif isinstance(item, aaf2.components.DescriptiveMarker):\n\n        # Markers come in on their own separate Track.\n        # TODO: We should consolidate them onto the same track(s) as the clips\n        # result = otio.schema.Marker()\n        pass\n\n    elif isinstance(item, aaf2.components.Selector):\n        msg = \"Transcribe selector for  {}\".format(_encoded_name(item))\n        _transcribe_log(msg, indent)\n        # If you mute a clip in media composer, it becomes one of these in the\n        # AAF.\n        result = _transcribe(item.getvalue(\"Selected\"),\n                             parents + [item], edit_rate, indent + 2)\n\n        alternates = [\n            _transcribe(alt, parents + [item], edit_rate, indent + 2)\n            for alt in item.getvalue(\"Alternates\")\n        ]\n\n        # muted case -- if there is only one item its muted, otherwise its\n        # a multi cam thing\n        if alternates and len(alternates) == 1:\n            metadata['muted_clip'] = True\n            result.name = str(alternates[0].name) + \"_MUTED\"\n\n        metadata['alternates'] = alternates\n\n    # @TODO: There are a bunch of other AAF object types that we will\n    # likely need to add support for. I'm leaving this code here to help\n    # future efforts to extract the useful information out of these.\n\n    # elif isinstance(item, aaf.storage.File):\n    #     self.extendChildItems([item.header])\n\n    # elif isinstance(item, aaf.storage.Header):\n    #     self.extendChildItems([item.storage()])\n    #     self.extendChildItems([item.dictionary()])\n\n    # elif isinstance(item, aaf.dictionary.Dictionary):\n    #     l = []\n    #     l.append(DummyItem(list(item.class_defs()), 'ClassDefs'))\n    #     l.append(DummyItem(list(item.codec_defs()), 'CodecDefs'))\n    #     l.append(DummyItem(list(item.container_defs()), 'ContainerDefs'))\n    #     l.append(DummyItem(list(item.data_defs()), 'DataDefs'))\n    #     l.append(DummyItem(list(item.interpolation_defs()),\n    #        'InterpolationDefs'))\n    #     l.append(DummyItem(list(item.klvdata_defs()), 'KLVDataDefs'))\n    #     l.append(DummyItem(list(item.operation_defs()), 'OperationDefs'))\n    #     l.append(DummyItem(list(item.parameter_defs()), 'ParameterDefs'))\n    #     l.append(DummyItem(list(item.plugin_defs()), 'PluginDefs'))\n    #     l.append(DummyItem(list(item.taggedvalue_defs()), 'TaggedValueDefs'))\n    #     l.append(DummyItem(list(item.type_defs()), 'TypeDefs'))\n    #     self.extendChildItems(l)\n    #\n    #     elif isinstance(item, pyaaf.AxSelector):\n    #         self.extendChildItems(list(item.EnumAlternateSegments()))\n    #\n    #     elif isinstance(item, pyaaf.AxScopeReference):\n    #         #print item, item.GetRelativeScope(),item.GetRelativeSlot()\n    #         pass\n    #\n    #     elif isinstance(item, pyaaf.AxEssenceGroup):\n    #         segments = []\n    #\n    #         for i in xrange(item.CountChoices()):\n    #             choice = item.GetChoiceAt(i)\n    #             segments.append(choice)\n    #         self.extendChildItems(segments)\n    #\n    #     elif isinstance(item, pyaaf.AxProperty):\n    #         self.properties['Value'] = str(item.GetValue())\n\n    elif isinstance(item, collections.Iterable):\n        msg = \"Creating SerializableCollection for Iterable for {}\".format(\n            _encoded_name(item))\n        _transcribe_log(msg, indent)\n\n        result = otio.schema.SerializableCollection()\n        for child in item:\n            result.append(_transcribe(child, parents + [item], edit_rate, indent + 2))\n    else:\n        # For everything else, we just ignore it.\n        # To see what is being ignored, turn on the debug flag\n        if debug:\n            print(\"SKIPPING: {}: {} -- {}\".format(type(item), item, result))\n\n    # Did we get anything? If not, we're done\n    if result is None:\n        return None\n\n    # Okay, now we've turned the AAF thing into an OTIO result\n    # There's a bit more we can do before we're ready to return the result.\n\n    # If we didn't get a name yet, use the one we have in metadata\n    if not result.name:\n        result.name = metadata[\"Name\"]\n\n    # Attach the AAF metadata\n    if not result.metadata:\n        result.metadata.clear()\n    result.metadata[\"AAF\"] = metadata\n\n    # Double check that we got the length we expected\n    if isinstance(result, otio.core.Item):\n        length = metadata.get(\"Length\")\n        if (\n                length\n                and result.source_range is not None\n                and result.source_range.duration.value != length\n        ):\n            raise AAFAdapterError(\n                \"Wrong duration? {} should be {} in {}\".format(\n                    result.source_range.duration.value,\n                    length,\n                    result\n                )\n            )\n\n    # Did we find a Track?\n    if isinstance(result, otio.schema.Track):\n        # Try to figure out the kind of Track it is\n        if hasattr(item, 'media_kind'):\n            media_kind = str(item.media_kind)\n            result.metadata[\"AAF\"][\"MediaKind\"] = media_kind\n            if media_kind == \"Picture\":\n                result.kind = otio.schema.TrackKind.Video\n            elif media_kind in (\"SoundMasterTrack\", \"Sound\"):\n                result.kind = otio.schema.TrackKind.Audio\n            else:\n                # Timecode, Edgecode, others?\n                result.kind = \"\"\n\n    # Done!\n    return result"
    },
    {
        "pr_title": "AAF Reader Metadata",
        "pr_number": 739,
        "file_name": "contrib/opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -370,7 +370,7 @@\ndef _transcribe(item, parents, edit_rate, indent=0):\n             # masterMob has any, prefer them over the compositionMob, since the\n             # masterMob is the ultimate authority for a source clip.\n             if composition_user_metadata:\n-                if \"UserComments\" not in clip_metadata.keys():\n+                if \"UserComments\" not in clip_metadata:\n                     clip_metadata['UserComments'] = composition_user_metadata\n \n             media.metadata[\"AAF\"] = clip_metadata",
        "comments": [
            {
                "comment": "usually stored where? This looks like the comment got truncated?",
                "position": null
            },
            {
                "comment": "Can a MasterMob be referenced multiple times? If so, is there an opportunity to cache the result of this _transcribe? If that seems tricky, we could just leave a TODO or file an issue for later.",
                "position": null
            },
            {
                "comment": "I'm slightly confused by the variable name `clipMetadata` since it is going into the media reference. Did it come from the clip?\r\n",
                "position": null
            },
            {
                "comment": "Since the code is modifying the dictionary later, it should be deep copied here. That way the modification doesn't accidentally get included if/when this same mastermob_child is used elsewhere.",
                "position": null
            },
            {
                "comment": "We tend to avoid leaving people's names/logins in the open source code base.",
                "position": null
            },
            {
                "comment": "This should have an `_` prefix since it is for internal use only.",
                "position": null
            },
            {
                "comment": "Hm, the metadata will be assigned to the clip as well as the media reference. From my perspective it represents the metadata for a given clip.",
                "position": null
            },
            {
                "comment": "Yes, I can change that. I still think the mastermob_child will not be used anywhere else, but better safe than sorry, it should not be expensive. Just pointing out: I kept the original behavior, just added an intermediate step storing the result in a variable. ",
                "position": null
            },
            {
                "comment": "sorry, that was from my tools-tree change. I'll remove that.",
                "position": null
            },
            {
                "comment": "ok.",
                "position": null
            },
            {
                "comment": "Usually there is one mastermob associated with a SourceClip as far as my experience goes. I don't think caching the master-mob will gain us much. But I can just test it with some of our large sequences and see if I get the same master-mob-id several times. ",
                "position": null
            },
            {
                "comment": "That line is from the old comment, the updated comment right above covers everything. ",
                "position": null
            },
            {
                "comment": "(But I followed the debug flag just right above, which is also internal as far as I can see. That should be _debug then as well?)",
                "position": null
            },
            {
                "comment": "We've tried to follow that convention for functions. (e.g. `_transcode`) Maybe we should do that for module-level variables as well? @ssteinbach do you have an opinion?\r\n\r\nYou could argue that the toggles to turn this on/off are useful from outside the module - e.g. when debugging an application that uses this module.",
                "position": null
            },
            {
                "comment": "Yeah.  @meshula we should add these notes to the in progress style guide.  Some stuff:\r\n\r\n- I would vote to remove the `transcribe_debug` variable entirely (and any if statements that were only switched by that)\r\n- if its going to exist it should be all caps (module level constant) + prefixed by a `_` (not intended to pollute the namespace)\r\n- I think josh was commenting on the `transcribe` function, which if its for internal usage should also be prefixed by a `_`, `_transcribe()` to not pollute the namespace and signal its intent.\r\n- finally, we're using snake case a la pep8, so the argument should be `always_print` rather than `alwaysPrint`.\r\n\r\nsee also: https://www.python.org/dev/peps/pep-0008/#function-and-variable-names\r\n\r\n> Function and Variable Names\r\nFunction names should be lowercase, with words separated by underscores as necessary to improve readability.\r\n>\r\n> Variable names follow the same convention as function names.\r\n>\r\n> mixedCase is allowed only in contexts where that's already the prevailing style (e.g. threading.py), to retain backwards compatibility.\r\n\r\n\r\nAlso:\r\n>Constants\r\n>Constants are usually defined on a module level and written in all capital letters with underscores separating words. Examples include MAX_OVERFLOW and TOTAL",
                "position": null
            },
            {
                "comment": "Just to clarify, I prefer to remove debug code - it decreases test coverage, usually requires special knowledge to turn on, and for python at least is rarely as useful as using a dedicated debugger like `pdb`.  Unless its exposed as top level interface functionality to users.  Open to counter point on that one though; what do the others think?",
                "position": null
            },
            {
                "comment": "I think its probably a good idea to make sure this adapter also adheres more closely to pep8:\r\n\r\n```suggestion\r\n        child_mastermob, composition_user_metadata = _find_mastermob_for_sourceclip(item)\r\n```",
                "position": null
            },
            {
                "comment": "do we need these transcribe log calls?",
                "position": null
            },
            {
                "comment": "```suggestion\r\n            clip_metadata = copy.deepcopy(mastermob_child.metadata.get(\"AAF\", {}))\r\n```",
                "position": null
            },
            {
                "comment": "`not in` is a check against `keys()`, don't need the extra call\r\n\r\n```suggestion\r\n                if \"UserComments\" not in clipMetadata:\r\n```",
                "position": null
            },
            {
                "comment": "does it make sense to apply it to the media reference then?  Or is the media reference a reference to the AAF file?",
                "position": null
            },
            {
                "comment": "if we're in here:\r\n```suggestion\r\ndef _transcribe(item, parents, edit_rate, indent=0):\r\n```",
                "position": null
            },
            {
                "comment": "If we're going to include these `trancribe_log` calls for each type, I'd rather we use a consistent format, so:\r\n\r\n`transcribe_log(\"Gap\", item, indent)`\r\n\r\n...and then inside `transcribe_log()` it unrolls the name into the message `\"Creating {} for {}\".format(typestr, _encoded_name(item)), indent)`\r\n\r\n...but the broader problem with these kinds of logs from adapters is that we don't have a consistent framework for it.  @reinecke just flagging this since I know its something you've thought about in the past.\r\nand then have transcribe_log format the item name and string consistently (unless there is a lot of helpful variation in these strings)",
                "position": null
            },
            {
                "comment": "I am sorry, but I disagree in this point strongly. The AAF-object-structure is hard to understand and you need some debug tools in place to manage the complexity of _transcribe(). I actually use this flag in conjunction with pdb. I activate the flag and then do a set_trace() in the part of the traversal-tree I am interested in. So If I set it in SourceClip, I can see right away what traversal happened until this point, giving me some important context of how to introspect my current state in relation to the traversal. ",
                "position": null
            },
            {
                "comment": "yes, I'll change that. ",
                "position": null
            },
            {
                "comment": "will update",
                "position": null
            },
            {
                "comment": "Also just to clarify, I agree with your point in most (almost all) cases, but there are some exceptions where a complex, 'deep' structure requires some debug-bandaid-code put in place, so whoever wants to do a deep dive into that code has already something in place and does not require to scaffold the code from ground up. IMHO such situations are exceptions but should outweigh the desire of a perfect code-coverage.   ",
                "position": null
            },
            {
                "comment": "I like the idea of turning it on via an adapter argument. We will be improving this adapter for a long time to come, and this sort of diagnostic feature is akin to the simplify=False option that is already an adapter argument.",
                "position": null
            },
            {
                "comment": "Oh, and I guess I wasn't clear. It was mostly the function `transcribe_log()` that I was suggesting should be `_transcribe_log()`",
                "position": null
            },
            {
                "comment": "yes, I like the idea to have it be an adapter argument. And yes, I totally did not realize you meant the method. I'll fix that.",
                "position": null
            },
            {
                "comment": "yeah, good point. It started as a more generic log, but since it's now specific to the _transcribe() method, this is a good idea.",
                "position": null
            },
            {
                "comment": "Ah, okay - this makes sense to me now.",
                "position": null
            },
            {
                "comment": "The original intent of the mastermob list was meant to avoid re-transcribing the same mastermobs again and again. I was under the (somewhat naive) impression that this would be common. \r\n\r\nIn your testing so far, is the adapter faster, slower, or about the same as before this change? If it is the same or faster with a few real world AAFs then, we can leave this until another time when we do some more extensive performance testing. If it is slower, then we should look deeper since folks are already unhappy with the performance.",
                "position": null
            },
            {
                "comment": "I would not have removed the first pass if it would have made the adapter slower. The reason (besides simplifying the code) is a 30-40% speed gain (also listed in the original commit-description), tested on three large sequences from three different shows of ours.",
                "position": null
            },
            {
                "comment": "Faster is great! Thanks for checking.",
                "position": null
            },
            {
                "comment": "This is an adapter argument now - does that satisfy your concern @ssteinbach ?",
                "position": null
            }
        ],
        "commit_message": "Addressing some final review comments",
        "commit_id": "57f6f6c591e47bb8070c3c10ce24ad2ca5dd9cf5",
        "fn_context": "def _transcribe(item, parents, edit_rate, indent=0):\n    result = None\n    metadata = {}\n\n    # First lets grab some standard properties that are present on\n    # many types of AAF objects...\n    metadata[\"Name\"] = _get_name(item)\n    metadata[\"ClassName\"] = _get_class_name(item)\n\n    # Some AAF objects (like TimelineMobSlot) have an edit rate\n    # which should be used for all of the object's children.\n    # We will pass it on to any recursive calls to _transcribe()\n    if hasattr(item, \"edit_rate\"):\n        edit_rate = float(item.edit_rate)\n\n    if isinstance(item, aaf2.components.Component):\n        metadata[\"Length\"] = item.length\n\n    if isinstance(item, aaf2.core.AAFObject):\n        for prop in item.properties():\n            if hasattr(prop, 'name') and hasattr(prop, 'value'):\n                key = str(prop.name)\n                value = prop.value\n                metadata[key] = _transcribe_property(value)\n\n    # Now we will use the item's class to determine which OTIO type\n    # to transcribe into. Note that the order of this if/elif/... chain\n    # is important, because the class hierarchy of AAF objects is more\n    # complex than OTIO.\n\n    if isinstance(item, aaf2.content.ContentStorage):\n        msg = \"Creating SerializableCollection for {}\".format(_encoded_name(item))\n        _transcribe_log(msg, indent)\n        result = otio.schema.SerializableCollection()\n\n        for mob in item.compositionmobs():\n            _transcribe_log(\"compositionmob traversal\", indent)\n            child = _transcribe(mob, parents + [item], edit_rate, indent + 2)\n            _add_child(result, child, mob)\n\n    elif isinstance(item, aaf2.mobs.Mob):\n        _transcribe_log(\"Creating Timeline for {}\".format(_encoded_name(item)), indent)\n        result = otio.schema.Timeline()\n\n        for slot in item.slots:\n            track = _transcribe(slot, parents + [item], edit_rate, indent + 2)\n            _add_child(result.tracks, track, slot)\n\n            # Use a heuristic to find the starting timecode from\n            # this track and use it for the Timeline's global_start_time\n            start_time = _find_timecode_track_start(track)\n            if start_time:\n                result.global_start_time = start_time\n\n    elif isinstance(item, aaf2.components.SourceClip):\n        _transcribe_log(\"Creating SourceClip for {}\".format(\n            _encoded_name(item)), indent)\n        result = otio.schema.Clip()\n\n        # Evidently the last mob is the one with the timecode\n        mobs = _find_timecode_mobs(item)\n\n        # Get the Timecode start and length values\n        last_mob = mobs[-1] if mobs else None\n        timecode_info = _extract_timecode_info(last_mob) if last_mob else None\n\n        source_start = int(metadata.get(\"StartTime\", \"0\"))\n        source_length = item.length\n        media_start = source_start\n        media_length = item.length\n\n        if timecode_info:\n            media_start, media_length = timecode_info\n            source_start += media_start\n\n        # The goal here is to find a source range. Actual editorial opinions are\n        # found on SourceClips in the CompositionMobs. To figure out whether this\n        # clip is directly in the CompositionMob, we detect if our parent mobs\n        # are only CompositionMobs. If they were anything else - a MasterMob, a\n        # SourceMob, we would know that this is in some indirect relationship.\n        parent_mobs = filter(lambda parent: isinstance(parent, aaf2.mobs.Mob), parents)\n        is_directly_in_composition = all(\n            isinstance(mob, aaf2.mobs.CompositionMob)\n            for mob in parent_mobs\n        )\n        if is_directly_in_composition:\n            result.source_range = otio.opentime.TimeRange(\n                otio.opentime.RationalTime(source_start, edit_rate),\n                otio.opentime.RationalTime(source_length, edit_rate)\n            )\n\n        # The goal here is to find an available range. Media ranges are stored\n        # in the related MasterMob, and there should only be one - hence the name\n        # \"Master\" mob. Somewhere down our chain (either a child or our parents)\n        # is a MasterMob.\n        # There are some special cases where the masterMob could be:\n        # 1) For SourceClips in the CompositionMob, the mob the SourceClip is\n        #    referencing can be our MasterMob.\n        # 2) If the source clip is referencing another CompositionMob,\n        #    drill down to see if the composition holds the MasterMob\n        # 3) For everything else, it is a previously encountered parent. Find the\n        #    MasterMob in our chain, and then extract the information from that.\n\n        child_mastermob, composition_user_metadata = \\\n            _find_mastermob_for_sourceclip(item)\n\n        if composition_user_metadata:\n            metadata['UserComments'] = composition_user_metadata\n\n        parent_mastermobs = [\n            parent for parent in parents\n            if isinstance(parent, aaf2.mobs.MasterMob)\n        ]\n        parent_mastermob = parent_mastermobs[0] if len(parent_mastermobs) > 1 else None\n\n        if child_mastermob:\n            _transcribe_log(\"[found child_mastermob]\", indent)\n        elif parent_mastermob:\n            _transcribe_log(\"[found parent_mastermob]\", indent)\n        else:\n            _transcribe_log(\"[found no mastermob]\", indent)\n\n        mastermob = child_mastermob or parent_mastermob or None\n\n        if mastermob:\n            # Get target path\n            mastermob_child = _transcribe(mastermob, list(), edit_rate, indent)\n\n            target_path = (mastermob_child.metadata.get(\"AAF\", {})\n                                                   .get(\"UserComments\", {})\n                                                   .get(\"UNC Path\"))\n            if not target_path:\n                # retrieve locator form the MasterMob's Essence\n                for mobslot in mastermob.slots:\n                    if isinstance(mobslot.segment, aaf2.components.SourceClip):\n                        sourcemob = mobslot.segment.mob\n                        locator = None\n                        # different essences store locators in different places\n                        if (isinstance(sourcemob.descriptor,\n                                       aaf2.essence.DigitalImageDescriptor)\n                                and sourcemob.descriptor.locator):\n                            locator = sourcemob.descriptor.locator[0]\n                        elif \"Locator\" in sourcemob.descriptor:\n                            locator = sourcemob.descriptor[\"Locator\"].value[0]\n\n                        if locator:\n                            target_path = locator[\"URLString\"].value\n\n            # if we have target path, create an ExternalReference, otherwise\n            # create an MissingReference.\n            if target_path:\n                if not target_path.startswith(\"file://\"):\n                    target_path = \"file://\" + target_path\n                target_path = target_path.replace(\"\\\\\", \"/\")\n                media = otio.schema.ExternalReference(target_url=target_path)\n            else:\n                media = otio.schema.MissingReference()\n\n            media.available_range = otio.opentime.TimeRange(\n                otio.opentime.RationalTime(media_start, edit_rate),\n                otio.opentime.RationalTime(media_length, edit_rate)\n            )\n\n            # Copy the metadata from the master into the media_reference\n            clip_metadata = copy.deepcopy(mastermob_child.metadata.get(\"AAF\", {}))\n\n            # If the composition was holding UserComments and the current masterMob has\n            # no UserComments, use the ones from the CompositionMob. But if the\n            # masterMob has any, prefer them over the compositionMob, since the\n            # masterMob is the ultimate authority for a source clip.\n            if composition_user_metadata:\n                if \"UserComments\" not in clip_metadata:\n                    clip_metadata['UserComments'] = composition_user_metadata\n\n            media.metadata[\"AAF\"] = clip_metadata\n\n            result.media_reference = media\n\n    elif isinstance(item, aaf2.components.Transition):\n        _transcribe_log(\"Creating Transition for {}\".format(\n            _encoded_name(item)), indent)\n        result = otio.schema.Transition()\n\n        # Does AAF support anything else?\n        result.transition_type = otio.schema.TransitionTypes.SMPTE_Dissolve\n\n        # Extract value and time attributes of both ControlPoints used for\n        # creating AAF Transition objects\n        varying_value = None\n        for param in item.getvalue('OperationGroup').parameters:\n            if isinstance(param, aaf2.misc.VaryingValue):\n                varying_value = param\n                break\n\n        if varying_value is not None:\n            for control_point in varying_value.getvalue('PointList'):\n                value = control_point.value\n                time = control_point.time\n                metadata.setdefault('PointList', []).append({'Value': value,\n                                                             'Time': time})\n\n        in_offset = int(metadata.get(\"CutPoint\", \"0\"))\n        out_offset = item.length - in_offset\n        result.in_offset = otio.opentime.RationalTime(in_offset, edit_rate)\n        result.out_offset = otio.opentime.RationalTime(out_offset, edit_rate)\n\n    elif isinstance(item, aaf2.components.Filler):\n        _transcribe_log(\"Creating Gap for {}\".format(_encoded_name(item)), indent)\n        result = otio.schema.Gap()\n\n        length = item.length\n        result.source_range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(0, edit_rate),\n            otio.opentime.RationalTime(length, edit_rate)\n        )\n\n    elif isinstance(item, aaf2.components.NestedScope):\n        msg = \"Creating Stack for NestedScope for {}\".format(_encoded_name(item))\n        _transcribe_log(msg, indent)\n        # TODO: Is this the right class?\n        result = otio.schema.Stack()\n\n        for slot in item.slots:\n            child = _transcribe(slot, parents + [item], edit_rate, indent + 2)\n            _add_child(result, child, slot)\n\n    elif isinstance(item, aaf2.components.Sequence):\n        msg = \"Creating Track for Sequence for {}\".format(_encoded_name(item))\n        _transcribe_log(msg, indent)\n        result = otio.schema.Track()\n\n        for component in item.components:\n            child = _transcribe(component, parents + [item], edit_rate, indent + 2)\n            _add_child(result, child, component)\n\n    elif isinstance(item, aaf2.components.OperationGroup):\n        msg = \"Creating operationGroup for {}\".format(_encoded_name(item))\n        _transcribe_log(msg, indent)\n        result = _transcribe_operation_group(item, parents, metadata,\n                                             edit_rate, indent + 2)\n\n    elif isinstance(item, aaf2.mobslots.TimelineMobSlot):\n        msg = \"Creating Track for TimelineMobSlot for {}\".format(_encoded_name(item))\n        _transcribe_log(msg, indent)\n        result = otio.schema.Track()\n\n        child = _transcribe(item.segment, parents + [item], edit_rate, indent + 2)\n\n        _add_child(result, child, item.segment)\n\n    elif isinstance(item, aaf2.mobslots.MobSlot):\n        msg = \"Creating Track for MobSlot for {}\".format(_encoded_name(item))\n        _transcribe_log(msg, indent)\n        result = otio.schema.Track()\n\n        child = _transcribe(item.segment, parents + [item], edit_rate, indent + 2)\n        _add_child(result, child, item.segment)\n\n    elif isinstance(item, aaf2.components.Timecode):\n        pass\n\n    elif isinstance(item, aaf2.components.Pulldown):\n        pass\n\n    elif isinstance(item, aaf2.components.EdgeCode):\n        pass\n\n    elif isinstance(item, aaf2.components.ScopeReference):\n        msg = \"Creating Gap for ScopedReference for {}\".format(_encoded_name(item))\n        _transcribe_log(msg, indent)\n        # TODO: is this like FILLER?\n\n        result = otio.schema.Gap()\n\n        length = item.length\n        result.source_range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(0, edit_rate),\n            otio.opentime.RationalTime(length, edit_rate)\n        )\n\n    elif isinstance(item, aaf2.components.DescriptiveMarker):\n\n        # Markers come in on their own separate Track.\n        # TODO: We should consolidate them onto the same track(s) as the clips\n        # result = otio.schema.Marker()\n        pass\n\n    elif isinstance(item, aaf2.components.Selector):\n        msg = \"Transcribe selector for  {}\".format(_encoded_name(item))\n        _transcribe_log(msg, indent)\n        # If you mute a clip in media composer, it becomes one of these in the\n        # AAF.\n        result = _transcribe(item.getvalue(\"Selected\"),\n                             parents + [item], edit_rate, indent + 2)\n\n        alternates = [\n            _transcribe(alt, parents + [item], edit_rate, indent + 2)\n            for alt in item.getvalue(\"Alternates\")\n        ]\n\n        # muted case -- if there is only one item its muted, otherwise its\n        # a multi cam thing\n        if alternates and len(alternates) == 1:\n            metadata['muted_clip'] = True\n            result.name = str(alternates[0].name) + \"_MUTED\"\n\n        metadata['alternates'] = alternates\n\n    # @TODO: There are a bunch of other AAF object types that we will\n    # likely need to add support for. I'm leaving this code here to help\n    # future efforts to extract the useful information out of these.\n\n    # elif isinstance(item, aaf.storage.File):\n    #     self.extendChildItems([item.header])\n\n    # elif isinstance(item, aaf.storage.Header):\n    #     self.extendChildItems([item.storage()])\n    #     self.extendChildItems([item.dictionary()])\n\n    # elif isinstance(item, aaf.dictionary.Dictionary):\n    #     l = []\n    #     l.append(DummyItem(list(item.class_defs()), 'ClassDefs'))\n    #     l.append(DummyItem(list(item.codec_defs()), 'CodecDefs'))\n    #     l.append(DummyItem(list(item.container_defs()), 'ContainerDefs'))\n    #     l.append(DummyItem(list(item.data_defs()), 'DataDefs'))\n    #     l.append(DummyItem(list(item.interpolation_defs()),\n    #        'InterpolationDefs'))\n    #     l.append(DummyItem(list(item.klvdata_defs()), 'KLVDataDefs'))\n    #     l.append(DummyItem(list(item.operation_defs()), 'OperationDefs'))\n    #     l.append(DummyItem(list(item.parameter_defs()), 'ParameterDefs'))\n    #     l.append(DummyItem(list(item.plugin_defs()), 'PluginDefs'))\n    #     l.append(DummyItem(list(item.taggedvalue_defs()), 'TaggedValueDefs'))\n    #     l.append(DummyItem(list(item.type_defs()), 'TypeDefs'))\n    #     self.extendChildItems(l)\n    #\n    #     elif isinstance(item, pyaaf.AxSelector):\n    #         self.extendChildItems(list(item.EnumAlternateSegments()))\n    #\n    #     elif isinstance(item, pyaaf.AxScopeReference):\n    #         #print item, item.GetRelativeScope(),item.GetRelativeSlot()\n    #         pass\n    #\n    #     elif isinstance(item, pyaaf.AxEssenceGroup):\n    #         segments = []\n    #\n    #         for i in xrange(item.CountChoices()):\n    #             choice = item.GetChoiceAt(i)\n    #             segments.append(choice)\n    #         self.extendChildItems(segments)\n    #\n    #     elif isinstance(item, pyaaf.AxProperty):\n    #         self.properties['Value'] = str(item.GetValue())\n\n    elif isinstance(item, collections.Iterable):\n        msg = \"Creating SerializableCollection for Iterable for {}\".format(\n            _encoded_name(item))\n        _transcribe_log(msg, indent)\n\n        result = otio.schema.SerializableCollection()\n        for child in item:\n            result.append(_transcribe(child, parents + [item], edit_rate, indent + 2))\n    else:\n        # For everything else, we just ignore it.\n        # To see what is being ignored, turn on the debug flag\n        if debug:\n            print(\"SKIPPING: {}: {} -- {}\".format(type(item), item, result))\n\n    # Did we get anything? If not, we're done\n    if result is None:\n        return None\n\n    # Okay, now we've turned the AAF thing into an OTIO result\n    # There's a bit more we can do before we're ready to return the result.\n\n    # If we didn't get a name yet, use the one we have in metadata\n    if not result.name:\n        result.name = metadata[\"Name\"]\n\n    # Attach the AAF metadata\n    if not result.metadata:\n        result.metadata.clear()\n    result.metadata[\"AAF\"] = metadata\n\n    # Double check that we got the length we expected\n    if isinstance(result, otio.core.Item):\n        length = metadata.get(\"Length\")\n        if (\n                length\n                and result.source_range is not None\n                and result.source_range.duration.value != length\n        ):\n            raise AAFAdapterError(\n                \"Wrong duration? {} should be {} in {}\".format(\n                    result.source_range.duration.value,\n                    length,\n                    result\n                )\n            )\n\n    # Did we find a Track?\n    if isinstance(result, otio.schema.Track):\n        # Try to figure out the kind of Track it is\n        if hasattr(item, 'media_kind'):\n            media_kind = str(item.media_kind)\n            result.metadata[\"AAF\"][\"MediaKind\"] = media_kind\n            if media_kind == \"Picture\":\n                result.kind = otio.schema.TrackKind.Video\n            elif media_kind in (\"SoundMasterTrack\", \"Sound\"):\n                result.kind = otio.schema.TrackKind.Audio\n            else:\n                # Timecode, Edgecode, others?\n                result.kind = \"\"\n\n    # Done!\n    return result"
    },
    {
        "pr_title": "Implementation of post adapter write hook",
        "pr_number": 730,
        "file_name": "src/py-opentimelineio/opentimelineio/adapters/adapter.py",
        "code_diff": "@@ -30,6 +30,7 @@\nimport inspect\n import collections\n+import copy\n \n from .. import (\n     core,",
        "comments": [],
        "commit_message": "* Deep copy `adapter_argument_map` into `hook_function_argument_map['adapter_arguments']` to avoid modifying the original arguments implicitly.",
        "commit_id": "c8e2a809c99ca4a2684d08e5c3122cd66bf7edb7"
    },
    {
        "pr_title": "Addressing a handful of lint issues.",
        "pr_number": 702,
        "file_name": "contrib/opentimelineio_contrib/adapters/tests/test_hls_playlist_adapter.py",
        "code_diff": "@@ -260,8 +260,8 @@\ndef test_media_roundtrip(self):\n         os.remove(media_pl_tmp_path)\n \n         # Strip newline chars\n-        reference_lines = [l.strip('\\n') for l in reference_lines]\n-        adapter_out_lines = [l.strip('\\n') for l in adapter_out_lines]\n+        reference_lines = [line.strip('\\n') for line in reference_lines]\n+        adapter_out_lines = [line.strip('\\n') for line in adapter_out_lines]\n \n         # Compare the lines\n         self.assertEqual(reference_lines, adapter_out_lines)",
        "comments": [],
        "commit_message": "Addressing a handful of lint issues.",
        "commit_id": "12608261649036175a59c00f5c7b94cc533b18a5"
    },
    {
        "pr_title": "Toggle 'complete track name' display on double click in otioview",
        "pr_number": 690,
        "file_name": "src/opentimelineview/track_widgets.py",
        "code_diff": "@@ -36,6 +36,7 @@\nEFFECT_HEIGHT = (1.0 / 3.0) * TRACK_HEIGHT\n HIGHLIGHT_WIDTH = 5\n TRACK_NAME_WIDGET_WIDTH = 100.0\n+SHORT_NAME_LENGTH = 7\n CURRENT_ZOOM_LEVEL = 1.0",
        "comments": [
            {
                "comment": "```suggestion\r\n        self.track_name = 'Track' if not track.name else track.name\r\n```\r\nShould be the same but also covers the case where `track.name` is `None`",
                "position": null
            },
            {
                "comment": "lets break the 7 out to be a top level constant, maybe `SHORT_NAME_LENGTH`",
                "position": null
            }
        ],
        "commit_message": "Minor refactoring",
        "commit_id": "0b19217843e2ec40dd7d32ba247d661930f16b81",
        "fn_context": ""
    },
    {
        "pr_title": "Indicate Empty track in otioview and display track name",
        "pr_number": 677,
        "file_name": "src/opentimelineview/timeline_widget.py",
        "code_diff": "@@ -468,10 +468,8 @@\ndef wheelEvent(self, event):\n         # inverse the effect of the zoom\n         items_to_scale = [\n             i for i in self.scene().items()\n-            if isinstance(i, track_widgets.BaseItem) or\n-            isinstance(i, track_widgets.Marker) or\n-            isinstance(i, ruler_widget.Ruler) or\n-            isinstance(i, track_widgets.TimeSlider)\n+            if (isinstance(i, (track_widgets.BaseItem, track_widgets.Marker,\n+                               ruler_widget.Ruler, track_widgets.TimeSlider)))\n         ]\n \n         for item in items_to_scale:",
        "comments": [
            {
                "comment": "I _believe_ that `isinstance` takes a tuple of classes to check in one shot, so you can do:\r\n```python\r\nisinstance(i, (track_widgets.BaseItem, track_widgets.Marker, ...))\r\n```\r\nand its the same as this list",
                "position": null
            },
            {
                "comment": "see below comment about passing tuples of classes to `isinstance`",
                "position": null
            }
        ],
        "commit_message": "display item count in tooltip and some minor changes",
        "commit_id": "831dac8a4f5ceb682cc2bb1a86e75fc952d9872a",
        "fn_context": "    def wheelEvent(self, event):\n        scale_by = 1.0 + float(event.delta()) / 1000\n        self.scale(scale_by, 1)\n        zoom_level = 1.0 / self.matrix().m11()\n        track_widgets.CURRENT_ZOOM_LEVEL = zoom_level\n\n        # some items we do want to keep the same visual size. So we need to\n        # inverse the effect of the zoom\n        items_to_scale = [\n            i for i in self.scene().items()\n            if (isinstance(i, (track_widgets.BaseItem, track_widgets.Marker,\n                               ruler_widget.Ruler, track_widgets.TimeSlider)))\n        ]\n\n        for item in items_to_scale:\n            item.counteract_zoom(zoom_level)\n\n    def _get_first_item(self):\n        newXpos = 0\n        newYpos = track_widgets.TIME_SLIDER_HEIGHT\n\n        newPosition = QtCore.QPointF(newXpos, newYpos)\n\n        return self.scene().itemAt(newPosition, QtGui.QTransform())"
    },
    {
        "pr_title": "Indicate Empty track in otioview and display track name",
        "pr_number": 677,
        "file_name": "src/opentimelineview/track_widgets.py",
        "code_diff": "@@ -39,7 +39,6 @@\nclass BaseItem(QtWidgets.QGraphicsRectItem):\n-    x_value = 0.0\n \n     def __init__(self, item, timeline_range, *args, **kwargs):\n         super(BaseItem, self).__init__(*args, **kwargs)",
        "comments": [
            {
                "comment": "Looking at the code, I suspect that you intend `x_value` to be an instance rather than class variable.  These are typically assigned in the constructor (`__init__`) method rather than at the class level, because when assigned at the class level they are share for all instance of the class.  For basic types (like floats) in practice this means they're still instance specific because of the rules around how those pointers work, but its still idiomatic and better practice to assign all of these in the constructor rather than the class.  This helps signals their intent.  (assuming you mean this to be a member variable, of course).",
                "position": null
            }
        ],
        "commit_message": "display item count in tooltip and some minor changes",
        "commit_id": "831dac8a4f5ceb682cc2bb1a86e75fc952d9872a",
        "fn_context": "    def __init__(self, item, timeline_range, *args, **kwargs):\n        super(BaseItem, self).__init__(*args, **kwargs)\n        self.item = item\n        self.timeline_range = timeline_range\n\n        # List of otio.core.SerializableObject\n        # it excludes decorator widgets as QLabel ...\n        self._otio_sub_items = list()\n\n        self.setFlags(QtWidgets.QGraphicsItem.ItemIsSelectable)\n        self.setBrush(\n            QtGui.QBrush(QtGui.QColor(180, 180, 180, 255))\n        )\n\n        pen = QtGui.QPen()\n        pen.setWidth(0)\n        pen.setCosmetic(True)\n        self.setPen(pen)\n\n        self.source_in_label = QtWidgets.QGraphicsSimpleTextItem(self)\n        self.source_out_label = QtWidgets.QGraphicsSimpleTextItem(self)\n        self.source_name_label = QtWidgets.QGraphicsSimpleTextItem(self)\n\n        self._add_markers()\n        self._add_effects()\n        self._set_labels()\n        self._set_tooltip()\n\n        self.x_value = 0.0"
    },
    {
        "pr_title": "Indicate Empty track in otioview and display track name",
        "pr_number": 677,
        "file_name": "src/opentimelineview/track_widgets.py",
        "code_diff": "@@ -69,6 +68,8 @@\ndef __init__(self, item, timeline_range, *args, **kwargs):\n         self._set_labels()\n         self._set_tooltip()\n \n+        self.x_value = 0.0\n+\n     def paint(self, *args, **kwargs):\n         new_args = [args[0],\n                     QtWidgets.QStyleOptionGraphicsItem()] + list(args[2:])",
        "comments": [
            {
                "comment": "Looking at the code, I suspect that you intend `x_value` to be an instance rather than class variable.  These are typically assigned in the constructor (`__init__`) method rather than at the class level, because when assigned at the class level they are share for all instance of the class.  For basic types (like floats) in practice this means they're still instance specific because of the rules around how those pointers work, but its still idiomatic and better practice to assign all of these in the constructor rather than the class.  This helps signals their intent.  (assuming you mean this to be a member variable, of course).",
                "position": null
            }
        ],
        "commit_message": "display item count in tooltip and some minor changes",
        "commit_id": "831dac8a4f5ceb682cc2bb1a86e75fc952d9872a",
        "fn_context": "    def __init__(self, item, timeline_range, *args, **kwargs):\n        super(BaseItem, self).__init__(*args, **kwargs)\n        self.item = item\n        self.timeline_range = timeline_range\n\n        # List of otio.core.SerializableObject\n        # it excludes decorator widgets as QLabel ...\n        self._otio_sub_items = list()\n\n        self.setFlags(QtWidgets.QGraphicsItem.ItemIsSelectable)\n        self.setBrush(\n            QtGui.QBrush(QtGui.QColor(180, 180, 180, 255))\n        )\n\n        pen = QtGui.QPen()\n        pen.setWidth(0)\n        pen.setCosmetic(True)\n        self.setPen(pen)\n\n        self.source_in_label = QtWidgets.QGraphicsSimpleTextItem(self)\n        self.source_out_label = QtWidgets.QGraphicsSimpleTextItem(self)\n        self.source_name_label = QtWidgets.QGraphicsSimpleTextItem(self)\n\n        self._add_markers()\n        self._add_effects()\n        self._set_labels()\n        self._set_tooltip()\n\n        self.x_value = 0.0\n\n    def paint(self, *args, **kwargs):\n        new_args = [args[0],\n                    QtWidgets.QStyleOptionGraphicsItem()] + list(args[2:])\n        super(BaseItem, self).paint(*new_args, **kwargs)"
    },
    {
        "pr_title": "Add List of Supported Formats to Conform.py Help Text",
        "pr_number": 676,
        "file_name": "examples/conform.py",
        "code_diff": "@@ -58,9 +58,8 @@\ndef parse_args():\n         'input',\n         type=str,\n         required=True,\n-        help='Timeline file(s) to read. Any format supported by OTIO will'\n-        ' work: fcp_xml, fcpx_xml, EDL, AAF, and more via community '\n-        'developed adapters.'\n+        help='Timeline file(s) to read. Supported formats: {adapters}'\n+             ''.format(adapters=otio.adapters.available_adapter_names())\n     )\n     parser.add_argument(\n         '-f',",
        "comments": [
            {
                "comment": "You can take this a step further if you'd like and list the available adapters, using this function:\r\nhttps://github.com/PixarAnimationStudios/OpenTimelineIO/blob/9d010f2966523505b5c954d803f4f982f5e4edb5/src/py-opentimelineio/opentimelineio/console/otioconvert.py#L36\r\n\r\nAnd if you want to put the file suffixes in the message, you can use this function:\r\nhttps://github.com/PixarAnimationStudios/OpenTimelineIO/blob/9d010f2966523505b5c954d803f4f982f5e4edb5/src/py-opentimelineio/opentimelineio/adapters/__init__.py#L52\r\n\r\nIts a great idea to put that information into the file input argument!  That upgrade could go into the real scripts (otioconvert, otioview, etc.) if you were interested in making the fix.",
                "position": null
            },
            {
                "comment": "On it! Thank you for the info!",
                "position": null
            }
        ],
        "commit_message": "Change help text to get list of supported format from otio.adapters.available_adapter_names()",
        "commit_id": "0d857086cbb4fc39c1303947c61318aa6e523ea5",
        "fn_context": "def parse_args():\n    \"\"\" parse arguments out of sys.argv \"\"\"\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\n        'input',\n        type=str,\n        required=True,\n        help='Timeline file(s) to read. Supported formats: {adapters}'\n             ''.format(adapters=otio.adapters.available_adapter_names())\n    )\n    parser.add_argument(\n        '-f',\n        '--folder',\n        type=str,\n        required=True,\n        help='Folder to look for media in.'\n    )\n    parser.add_argument(\n        '-o',\n        '--output',\n        type=str,\n        required=True,\n        help=\"Timeline file to write out.\"\n    )\n    return parser.parse_args()"
    },
    {
        "pr_title": "Implement OTIO to SVG Adapter",
        "pr_number": 649,
        "file_name": "src/py-opentimelineio/opentimelineio/adapters/svg.py",
        "code_diff": "@@ -261,7 +261,8 @@\ndef draw_labeled_rect(self, rect, stroke_width=2.0,\n                    {\n                        \"width\": \"{:.8f}\".format(svg_rect.width),\n                        \"height\": \"{:.8f}\".format(svg_rect.height),\n-                       \"style\": \"fill:{};stroke-width:{:.8f};stroke:{};opacity:1;\".format(\n+                       \"style\": \"fill:{};stroke-width:{:.8f};\"\n+                                \"stroke:{};opacity:1;\".format(\n                            fill_color.svg_color(),\n                            stroke_width,\n                            stroke_color.svg_color())",
        "comments": [
            {
                "comment": "Use of globals is frowned upon. Perhaps consider creating either a config object to carry around these values or refactor the functions into a class.",
                "position": null
            },
            {
                "comment": "This looks like a debugging print left in?",
                "position": null
            },
            {
                "comment": "In `write_to_file`, `input_otio` is expected to be a `SerializableObject`, the below `isinstance` checks shouldn't be needed (you can discard the code branch for handling strings as `input_otio`)",
                "position": null
            },
            {
                "comment": "I'd suggest not using `**kwargs` for these and make the `width` and `height` args explicit.\r\nAlso, since this adapter is creating an in-memory string and then writing the whole thing to disk, you could implement `write_to_string` instead and OTIO will automatically provide a `write_to_file` implementation. This gives users the flexibility to manage file I/O on their own (very useful for cloud applications).\r\nFor an example of these ideas, check out the EDL adapter's `write_to_string` (note that there is no `write_to_file` implementation: https://github.com/PixarAnimationStudios/OpenTimelineIO/blob/27adbc3c5f62aec8a2cd47b990a7f0507f48a9f2/src/py-opentimelineio/opentimelineio/adapters/cmx_3600.py#L877",
                "position": null
            },
            {
                "comment": "These functions and `svg_color` may be best bundled together into a `Color` class (maybe a `namedtuple` subclass?). This will centralize all the logic about these together.",
                "position": null
            },
            {
                "comment": "This is very brittle, if the font gets changed for whatever reason, this array will become invalid. This also will break for characters outside the ASCII range, like \u00e9. That said, I'm not sure if there is a smarter library for querying this info in the standard library.\r\nIf we are hard-coding font info, it may be useful to create something like a `Font` class that bundles together this data.\r\n\r\nIt could look like:\r\n\r\n```python\r\nclass Font:\r\n    def __init__(self, name, character_widths):\r\n        self.name = name\r\n        self._character_widths = character_widths\r\n\r\n    def width_for_character(self, char):\r\n        try:\r\n            return self._character_widths[ord(char)]\r\n        except IndexError:\r\n            raise ValueError(\"Unsupported character {}\".format(char))\r\n\r\nROBOTO = Font(\"Roboto\", ...char width list here...)\r\n```\r\n\r\nThis abstracts the behavior in a way where future people can either implement alternate fonts, or add in usage for a library that can provide the important information.",
                "position": null
            },
            {
                "comment": "Consider using ETree to build XML: https://docs.python.org/3/library/xml.etree.elementtree.html#building-xml-documents\r\n\r\nThis lets you build elements using code like `ET.Element(\"rect\", {\"x\": str(svg_rect.origin.x), \"y\": str(svg_rect.origin.y), \"another attribute\": some_string_value})`. These can be composed into the SVG document hierarchy and serialized to a string using `ET.tostring`. This means that you don't have to worry about writing code for cases like when \"<\" is used.\r\n\r\nIn `draw_text` this could be especially helpful (note the auto-conversion to escaped character sequences):\r\n```python\r\nelem = ET.Element(\"text\", {\"x\": \"1\", \"y\": \"1\"})\r\nelem.text = \"<something unexpect\u00e9d>\"\r\nET.tostring(elem)\r\n'<text x=\"1\" y=\"1\">&lt;something unexpect&#233;d&gt;</text>'\r\n```\r\n\r\n",
                "position": null
            },
            {
                "comment": "Throughout this code there are a lot of [\"magic number\"](https://en.wikipedia.org/wiki/Magic_number_(programming)#Unnamed_numerical_constants) usages here (`9.0` and `4.5`). This should be avoided.",
                "position": null
            },
            {
                "comment": "I think I'd chosen these numbers according to what worked well at the time. Would moving these to variables with appropriate names be the correct way?",
                "position": null
            },
            {
                "comment": "I had a Color class eariler, but since we just need to generate random colors and no other custom methods @ssteinbach suggested not using a class.\r\nhttps://github.com/PixarAnimationStudios/OpenTimelineIO/pull/649#discussion_r389167101",
                "position": null
            },
            {
                "comment": "One of the things he mentioned in that previous comment was \"not using any custom methods\". Personally, I think between `color_distance`, `random_color`, and `svg_color`, there is probably enough to justify a class here.\r\n\r\n@ssteinbach what do you think?",
                "position": null
            },
            {
                "comment": "Yeah, mostly it's about just giving names for the values. If someone needs to tune them later they'll be unaware of what they mean.\r\nIt may be something like (I'm actually not sure if these are the right names):\r\n```python\r\narrow_height = 9.0\r\narrow_width = (arrow_height / 2)\r\n```\r\n\r\nIt's okay to have hard-coded values, we just want them to be named so we know their meaning in this context.",
                "position": null
            },
            {
                "comment": "Roboto isn't available by default on a lot of systems. For instance, when I view one of the sample SVGs in chrome I see this:\r\n![image](https://user-images.githubusercontent.com/1649087/119546237-8e710a80-bd48-11eb-981f-672b19f92386.png)\r\n\r\nShould this be using a font stack that includes some of the more universally available fonts (e.x. Helvetica)?\r\n",
                "position": null
            },
            {
                "comment": "I would strongly recommend not introducing any dependencies for font handling because there aren't really any lightweight libraries that I'm aware of, at all. They all chain react in a lot of secondary modules, and I'm not thrilled imagining a worst case scenario of dependabot noticing a dozen new minor dependency changes every week.\r\n\r\nIf it's possible to eliminate the need to measure text at all, that would be ideal.",
                "position": null
            },
            {
                "comment": "name it arrowhead_length or similar? and use arrowhead_length * 0.5 as appropriate.",
                "position": null
            },
            {
                "comment": "If the font measurement is being used for text layout, it looks like SVG has ways to control this without having to explicitly know the font metrics: https://stackoverflow.com/questions/5546346/how-to-place-and-center-text-in-an-svg-rectangle\r\n\r\nThis may eliminate the need for a `Font` class and the `LCARS_CHAR_SIZE_ARRAY`",
                "position": null
            },
            {
                "comment": "I haven't been able to find a way to use system fonts, or local fonts in an SVG. Here, Roboto will be pulled from the Google Fonts API. If you open the svg in a browser it should work fine.",
                "position": null
            },
            {
                "comment": "Unfortunately, that doesn't seem to be the case. This screenshot is from Chrome on Mac pointed at this URL: https://raw.githubusercontent.com/PixarAnimationStudios/OpenTimelineIO/7010dc09797dc1f4b6ee922344307f99eaea2f0a/tests/sample_data/transition.svg",
                "position": null
            },
            {
                "comment": "Oh :(\r\nI'll look into this. ",
                "position": null
            },
            {
                "comment": "Helvetica isn't universally available :)\r\n\r\nThe only right answer is to embed a font. https://developer.mozilla.org/en-US/docs/Web/SVG/Tutorial/SVG_fonts\r\n\r\nEmbedding also solves the metrics problem noted above, as they can be fetched from the embed information.",
                "position": null
            },
            {
                "comment": "I think this may not actually be an answer:\r\n>SVG Fonts are currently supported only in Safari and Android Browser.\r\nInternet Explorer hasn't considered implementing this, the functionality has been removed from Chrome 38 (and Opera 25) and Firefox has postponed its implementation indefinitely to concentrate on WOFF. Other tools however like the Adobe SVG Viewer plugin, Batik and parts of Inkscape support SVG font embedding.",
                "position": null
            },
            {
                "comment": "The Google fonts thing seems to work on chrome on Android (Not sure if it's because Roboto is the default android font \ud83d\ude05 ). But it worked on chrome on Ubuntu, AFAIR (from the screenshots I posted in this PR).",
                "position": null
            },
            {
                "comment": "Yeah, it's most likely working on those platforms because you have Roboto installed on them.\r\n\r\nThis looks like a decent article on the topic: https://vecta.io/blog/using-fonts-in-svg\r\n\r\nNote this tidbit:\r\n> If you're using SVG just as a static image using <img> tags, it is not allowed to access to externals link sources and in this case, the specified @import Google font files URL.\r\n\r\nI think what you'll likely want to do is use something like:\r\n`font-family=\"Roboto, Helvetica, Arial, sans-serif\"`",
                "position": null
            },
            {
                "comment": "omg it's like the 20th century html font dance. If that works, it satisfies \"i don't have Helvetica\"., and just leaves us needing to avoid embedding our own guestimate font metrics",
                "position": null
            },
            {
                "comment": "I've removed this font metrics stuff. Please take a look. The tests pass on my end. Can you please rerun the actions?\r\n\r\nPS: I'm working on all the comments, one by one.",
                "position": null
            },
            {
                "comment": "Relaunched action, seems to be successful again. Looks like there was a transient 504 fetching stuff from pypi",
                "position": null
            },
            {
                "comment": "`font-family=\"Roboto, Helvetica, Arial, sans-serif\"` This didn't work. Using just a single font family is what seems to work. So I've used Arial for now.",
                "position": null
            },
            {
                "comment": "What happens if you just use \"sans-serif\"? That is, in theory the most agnostic. It's odd the precedence list didn't work, I tried switching to that using a find/replace and it seemed to work on chrome for Mac. What happened when you tried it?",
                "position": null
            },
            {
                "comment": "This monkeypatching behavior is pretty wacky. Can you describe what the intent is here?",
                "position": null
            },
            {
                "comment": "Consider making these properties:\r\n```\r\n@property\r\ndef f(self):\r\n    return self.value[1]\r\n```\r\n\r\nThen the computed property can accessed as if it were an attribute:\r\n`green_value = my_color.g`",
                "position": null
            },
            {
                "comment": "this fails if `config=None` - default args should only be used where there are reasonable defaults.",
                "position": null
            },
            {
                "comment": "Since you're only using `image_height` off the config object, I suggest not passing the whole object in and instead taking `image_height` directly. Ideally, you avoid passing any data except exactly what you need.",
                "position": null
            },
            {
                "comment": "Same notes about `config` from `convert_point_to_svg_coordinates`",
                "position": null
            },
            {
                "comment": "This structure seems to be a mix of adapter configuration and actual intermediate state data. Since you have a collection of functions that you non-optionally pass instances of this class into, you may consider making them be methods on the class.",
                "position": null
            },
            {
                "comment": "Same note, `config=None` is an invalid state, config should not be an optional kwarg.",
                "position": null
            },
            {
                "comment": "Same note, `config=None` is an invalid state, config should not be an optional kwarg.",
                "position": null
            },
            {
                "comment": "Same note, `config=None` is an invalid state, config should not be an optional kwarg.",
                "position": null
            },
            {
                "comment": "Same note, `config=None` is an invalid state, config should not be an optional kwarg.",
                "position": null
            },
            {
                "comment": "Same note, `config=None` is an invalid state, config should not be an optional kwarg.",
                "position": null
            },
            {
                "comment": "Same note, `config=None` is an invalid state, config should not be an optional kwarg.",
                "position": null
            },
            {
                "comment": "Same note, `config=None` is an invalid state, config should not be an optional kwarg.",
                "position": null
            },
            {
                "comment": "Same note, `config=None` is an invalid state, config should not be an optional kwarg.",
                "position": null
            },
            {
                "comment": "Same note, `config=None` is an invalid state, config should not be an optional kwarg.",
                "position": null
            },
            {
                "comment": "Consider adding a method to the `Point` class to generate the SVG point string (this could handle any float rounding you may want to do as well. Then the building of this points list could be simplified as something like:\r\n\r\n```python\r\n\" \".join(p.svg_point_string() for p in [triangle_tip, triangle_pt_1, triangle_pt_2])\r\n```\r\n\r\nThis is also more expressive for readers of the code because it reads as \"make a space-delimited list of the svg point string from the list of points\".",
                "position": null
            },
            {
                "comment": "I noticed that the usages of `Color.svg_color` throughout the file tend to look like this:\r\n```python\r\nColor.svg_color(stroke_color)\r\n```\r\n\r\nWhy is this a `staticmethod`? If the user has to pass in a `Color` instance, why not make it an instance method and have that instance just be `self`?\r\n\r\nThen the usages become more concise:\r\n```python\r\nstroke_color.svg_color()\r\n```",
                "position": null
            },
            {
                "comment": "All the usage of `OrderedDict` makes the code heavier and less readable. Is the ordering in these dictionaries really important to the output?",
                "position": null
            },
            {
                "comment": "This is a kind of non-standard pattern. Can you add comments in the code as to why you're doing this?",
                "position": null
            },
            {
                "comment": "I suggest breaking these \u00fcberexpressions up into several small expressions with intermediate variable assignments to make them clearer.\r\n\r\nAlso note that most the math is exactly the same in these two code branches. I'd pull as much of the calculation as possible out of the conditionals so it is really clear what is changed base on the conditional.",
                "position": null
            },
            {
                "comment": "when using `0` as a starting value, you don't need to provide it. Also, rather than assigning the unused iteration number to `i`, it is convention to use `_` to call out it is an unused value. So this expression could read:\r\n```python\r\nfor _ in range(100):\r\n```",
                "position": null
            },
            {
                "comment": "In python, zero length strings test as [truthy](https://www.freecodecamp.org/news/truthy-and-falsy-values-in-python/), so `len(track_name) == 0` is the same as `not track_name`. Also for reader clarity, I'd invert the logic on this so that it reads more like \"use the track name if set, otherwise default to 'Track'\":\r\n```python\r\ntrack_text = track.name if track.name else \"Track\"\r\n```",
                "position": null
            },
            {
                "comment": "This might be silly, but I had to use None because config was the last argument and since the first argument has a default value I had to give a default value to config.",
                "position": null
            },
            {
                "comment": "ETree did not preserve attribut le ordering before python3.8.\r\nI didn't go deep into what is actually going on here. But I found a solution on stackoverflow that replacee the dict that ETree uses for attributes with an ordered dict. So the attribute order when we convert the tree to a string is preserved.",
                "position": null
            },
            {
                "comment": "Yes, that makes sense. There was no specific reason for static. I'll make the change. ",
                "position": null
            },
            {
                "comment": "Python 3 produces a byte string with the tostring() method, but python 2 gives an str object. This pattern here is to check for this case. I'll add a comment.",
                "position": null
            },
            {
                "comment": "Ahh, makes sense. It seems like in that case it would always throw the same exception, why are there two different exception types handled?\r\n\r\nIn the case of bytes (py 3) vs. bytestring (py 2), using `isinstance(xmlstr, bytes)` might be the more explicit way to branch. Either way, I'd still include some comments about it. ",
                "position": null
            },
            {
                "comment": "In that case, since this isn't an API in usage, I'd put `config` as a positional argument in from of `point` in that case. Either way, I think this note is superseded by another about avoiding passing the entire config object in.",
                "position": null
            },
            {
                "comment": "Hrm, this is quite a bit hackier than I'd prefer - it assumes knowledge about the implementation and usage of `SubElement` beyond what's possible. It would be much better if we could avert dependency on preserving the ordering.",
                "position": null
            },
            {
                "comment": "I agree coercing behavior (through monkeypatching or anything else) is a \"code smell,\" i.e. a pattern that points to a more basic underlying problem.\r\n\r\nI'm trying to follow the attribute ordering conversation, could someone help me out here? Earlier in the thread there's comments that it may or may not be important and/or a problem to preserve attribute ordering. The reason to use ETree is for transmogrification into svg, because ETree is idiomatic python for treating XML, yes? If the idiomatic XML library for Python doesn't map onto SVG naturally, it's probably not the right solution? At the end of the day it's a fairly trivial thing we are representing, so I'm starting to wonder if we're going to spend time coercing the proverbial square peg into the round hole to satisfy idiom, at the expense of just writing the simplest needed code to solve the problem. \r\n\r\nAnyway, if someone could help me understand the issue a bit, I'd really appreciate it.",
                "position": null
            },
            {
                "comment": "I'm not writing this code, but my understanding of Python is improving as a result of reading this review ;)",
                "position": null
            },
            {
                "comment": "ETree doesn't preserve the order of attributes, when serializing or deserializing XML, before python 3.8. Currently I am just checking string equality in the tests, which fails in python3.8. \r\nSomething like this was happeing: `<svg width=\"2043.3636363636365\" height=\"65.6\">` != `<svg  height=\"65.6\" width=\"2043.3636363636365\">`\r\nThis is why I did this hacky stuff to replace dict with ordered dict, to force it to maintain attribute order in python versions before 3.8.\r\n\r\nAs you and Eric mentioned, this approach seems wrong, and I should look for a different way to test.",
                "position": null
            },
            {
                "comment": "I thought, what if xmlstr was not a bytestring or string for some reason. But that doesn't make sense now :sweat_smile:. I should just be checking for UnicodeDecodeError.\r\n\r\npy3 gives bytestring, and py2 gives str. Will `isinstance(xmlstr, bytes)` work for bytestring?",
                "position": null
            },
            {
                "comment": "Ah, got it. Looks like ETree was never designed with testing in mind.  This SO thread https://stackoverflow.com/questions/7905380/testing-equivalence-of-xml-etree-elementtree#24349916 is filled with bad ideas and terrible advice!",
                "position": null
            },
            {
                "comment": "Oh. I was thinking of using etree to parse and check equality of each attribute. That's what the SO thread says right? What would be a better way?",
                "position": null
            },
            {
                "comment": "Well, the thread says a lot of unhelpful things like \"XML is text so use tostring()\". My comment that it was not designed with testing in mind comes from the point that testing the nodes for equality directly results in apparent failures even for trivial examples.\r\n\r\nParsing and checking equality is also in the thread, and to me it seems like the only solution, so, yes. ",
                "position": null
            },
            {
                "comment": "And, to close the loop on why use ETree for XML - I recommended that as opposed to using string concatenation to generate the XML because it is important to make sure text from OTIO files is escaped properly. Basically it's the [Bobby Tables](https://xkcd.com/327/) thing.",
                "position": null
            },
            {
                "comment": "Are you sure about `bytestring`?\r\n\r\n```\r\n$ python3\r\nPython 3.8.6 (default, Nov 12 2020, 12:54:53)\r\n[Clang 12.0.0 (clang-1200.0.32.21)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> bytestring\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nNameError: name 'bytestring' is not defined\r\n>>> from xml.etree.ElementTree import Element, SubElement, tostring\r\n>>> elem = Element(\"rootElem\")\r\n>>> xmlstr = tostring(elem, encoding='utf-8', method='xml')\r\n>>> type(xmlstr)\r\n<class 'bytes'>\r\n>>>\r\n$ python2\r\nPython 2.7.17 (default, Oct 24 2019, 12:57:47)\r\n[GCC 4.2.1 Compatible Apple LLVM 11.0.0 (clang-1100.0.33.8)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from xml.etree.ElementTree import Element, SubElement, tostring\r\n>>> elem = Element(\"rootElem\")\r\n>>> xmlstr = tostring(elem, encoding='utf-8', method='xml')\r\n>>> type(xmlstr)\r\n<type 'str'>\r\n```\r\n\r\nIn Python 2, `str` is often called \"byte string\" because each character is a byte. This term is used to differentiate between the newer `unicode` type in python 2. In python 3, `str` is a unicode string (roughly equivalent to a python 2 `unicode`) and `bytes` is used in place of anywhere a string of data is needed. So, often binary bytes in python 2 were `str` instances, but in python 3 the more explicit `bytes` type is used.",
                "position": null
            },
            {
                "comment": "Ohh. I didn't know str was called byte string. When I printed the result from py3 it gave me something like `b'abcd'`.",
                "position": null
            },
            {
                "comment": "escaping XML is tricky enough that I buy that argument :)",
                "position": null
            }
        ],
        "commit_message": "Lint fix",
        "commit_id": "7d1b96c8c90fd75e35a250c8142b3c3d01a9a195",
        "fn_context": "    def draw_labeled_rect(self, rect, stroke_width=2.0,\n                          stroke_color=COLORS['black'],\n                          fill_color=COLORS['white'],\n                          label='',\n                          label_size=10.0):\n        svg_rect = convert_rect_to_svg_coordinates(rect, self.image_height)\n        g_elem = SubElement(self.svg_elem, \"g\",\n                            {\n                                \"transform\": \"translate({:.8f},{:.8f})\".format(\n                                    svg_rect.origin.x,\n                                    svg_rect.origin.y)\n                            })\n        SubElement(g_elem, \"rect\",\n                   {\n                       \"width\": \"{:.8f}\".format(svg_rect.width),\n                       \"height\": \"{:.8f}\".format(svg_rect.height),\n                       \"style\": \"fill:{};stroke-width:{:.8f};\"\n                                \"stroke:{};opacity:1;\".format(\n                           fill_color.svg_color(),\n                           stroke_width,\n                           stroke_color.svg_color())\n                   })\n        sub_svg_elem = SubElement(g_elem, \"svg\",\n                                  {\n                                      \"width\": \"{:.8f}\".format(svg_rect.width),\n                                      \"height\": \"{:.8f}\".format(svg_rect.height)\n                                  })\n        text_elem = SubElement(sub_svg_elem, \"text\",\n                               {\n                                   \"x\": \"50%\",\n                                   \"y\": \"50%\",\n                                   \"font-size\": \"{:.8f}\".format(label_size),\n                                   \"font-family\": self.font_family,\n                                   \"style\":\n                                       \"stroke:{};stroke-width:{:.8f};\"\n                                       \"fill:{};opacity:{:.8f};\".format(\n                                           COLORS['black'].svg_color(),\n                                           stroke_width / 4.0,\n                                           COLORS['black'].svg_color(),\n                                           COLORS['black'].a),\n                                   \"alignment-baseline\": \"middle\",\n                                   \"text-anchor\": \"middle\"})\n        text_elem.text = label"
    },
    {
        "pr_title": "Prevent FD leak in RVsession tests",
        "pr_number": 638,
        "file_name": "contrib/opentimelineio_contrib/adapters/tests/test_rvsession.py",
        "code_diff": "@@ -469,17 +469,18 @@\n)\n class RVSessionAdapterReadTest(unittest.TestCase):\n     def setUp(self):\n-        self.fd, self.tmp_path = tempfile.mkstemp(suffix=\".rv\", text=True)\n+        fd, self.tmp_path = tempfile.mkstemp(suffix=\".rv\", text=True)\n+\n+        # Close file descriptor to avoid leak. We only need the tmp_path.\n+        os.close(fd)\n \n     def tearDown(self):\n-        os.close(self.fd)\n         os.unlink(self.tmp_path)\n \n     def test_basic_rvsession_read(self):\n         timeline = otio.adapters.read_from_file(SCREENING_EXAMPLE_PATH)\n \n         otio.adapters.write_to_file(timeline, self.tmp_path)\n-        self.assertTrue(os.path.exists(self.tmp_path))\n \n         with open(self.tmp_path) as fo:\n             test_data = fo.read()",
        "comments": [
            {
                "comment": "I think you should be closing the fd here to avoid the issue of re-opening an already open file in windows.",
                "position": null
            },
            {
                "comment": "You can omit this assertion because the `open` on the next line will exercise that for you.",
                "position": null
            },
            {
                "comment": "Since you're checking the content below, this zero size assertion isn't necessary.",
                "position": null
            }
        ],
        "commit_message": "* Close tmp file descriptor in `setUp()` to avoid trouble in Windows\n* Removed redundant assertions as suggested in PR comments",
        "commit_id": "2854450626a4d27da886e8f65c75b082395bd6e0",
        "fn_context": "    def setUp(self):\n        fd, self.tmp_path = tempfile.mkstemp(suffix=\".rv\", text=True)\n\n        # Close file descriptor to avoid leak. We only need the tmp_path.\n        os.close(fd)\n\n    def tearDown(self):\n        os.unlink(self.tmp_path)\n\n    def test_basic_rvsession_read(self):\n        timeline = otio.adapters.read_from_file(SCREENING_EXAMPLE_PATH)\n\n        otio.adapters.write_to_file(timeline, self.tmp_path)\n\n        with open(self.tmp_path) as fo:\n            test_data = fo.read()\n\n        with open(BASELINE_PATH) as fo:\n            baseline_data = fo.read()\n\n        self.maxDiff = None\n        self.assertMultiLineEqual(baseline_data, test_data)"
    },
    {
        "pr_title": "Unifies ALE to the EDL adapter by storing the cdl data in the metadata",
        "pr_number": 621,
        "file_name": "contrib/opentimelineio_contrib/adapters/ale.py",
        "code_diff": "@@ -120,26 +120,25 @@\ndef _parse_data_line(line, columns, fps):\n         # If available, collect cdl values in the same way we do for CMX EDL\n         cdl = {}\n \n-        if 'CDL' in metadata:\n+        if metadata.get('CDL'):\n             cdl = _cdl_values_from_metadata(metadata['CDL'])\n             if cdl:\n-                metadata.pop('CDL')\n+                del metadata['CDL']\n \n         # If we have more specific metadata, let's use them\n-        if 'ASC_SOP' in metadata:\n+        if metadata.get('ASC_SOP'):\n             cdl = _cdl_values_from_metadata(metadata['ASC_SOP'])\n \n             if cdl:\n-                metadata.pop('ASC_SOP')\n+                del metadata['ASC_SOP']\n \n-        if 'ASC_SAT' in metadata:\n-            if metadata['ASC_SAT']:\n-                try:\n-                    asc_sat_value = float(metadata['ASC_SAT'])\n-                    cdl.update(asc_sat=asc_sat_value)\n-                    metadata.pop('ASC_SAT')\n-                except ValueError:\n-                    pass\n+        if metadata.get('ASC_SAT'):\n+            try:\n+                asc_sat_value = float(metadata['ASC_SAT'])\n+                cdl.update(asc_sat=asc_sat_value)\n+                del metadata['ASC_SAT']\n+            except ValueError:\n+                pass\n \n         if cdl:\n             clip.metadata['cdl'] = cdl",
        "comments": [
            {
                "comment": "`unicode` and `str` are just merged to `str` in python3 so the `isinstance` check is problematic there. In what cases will this be a non-string type? Perhaps we can get away with something like an `is None` check here?",
                "position": null
            },
            {
                "comment": "Oh I see!\r\nThis is basically just me trying to do type checking in python, I don't know in which cases this value will not be a string, but I have no guarantee that it will always be - that's why I put that check in place as a \"defence\" mechanism - the idea being that the sooner and you raise an Exception, the better (and also explicit is always better than implicit).\r\nI guess it could become a check for `None`, but it would also become a little bit less explicit.\r\nThe alternative would be adding a check like this  https://stackoverflow.com/a/32632271 ?\r\nIf we feel safe enough about `metadata['ASC_SOP']` and `metadata['CDL']` always being strings or `None`, we could just actually check for `None`. What do you think?\r\n\r\nPS: linting fixed!",
                "position": null
            },
            {
                "comment": "It is slightly more pythonic to use `del metadata['CDL']` instead of `pop`",
                "position": null
            },
            {
                "comment": "You could combine these two ifs into one like this: `if metadata.get('ASC_SAT')` since `get` will return `None` if the key is missing.",
                "position": null
            },
            {
                "comment": "You can use `isinstance(foo, (type(\"\"), type(u\"\")))` to get around the issue.",
                "position": null
            },
            {
                "comment": "Yup, I generally use the `in` keyword because it should perform in constant time since it performs an hashing function, while I don't know if `get()` internally tries to get the key and catches a KeyError (which is generally slower). But yeah, no harm in just using `get` I guess!",
                "position": null
            },
            {
                "comment": "Cool! Makes sense since we don't need the returned index",
                "position": null
            },
            {
                "comment": "@jminor Nice trick!",
                "position": null
            }
        ],
        "commit_message": "Slight reformat as requested by jminor.",
        "commit_id": "c5be8ea4c3ec72b5cdcfb00a7610464af7220e8c",
        "fn_context": "def _parse_data_line(line, columns, fps):\n    row = line.split(\"\\t\")\n\n    if len(row) < len(columns):\n        # Fill in blanks for any missing fields in this row\n        row.extend([\"\"] * (len(columns) - len(row)))\n\n    if len(row) > len(columns):\n        raise ALEParseError(\"Too many values on row: \" + line)\n\n    try:\n\n        # Gather all the columns into a dictionary\n        # For expected columns, like Name, Start, etc. we will pop (remove)\n        # those from metadata, leaving the rest alone.\n        metadata = dict(zip(columns, row))\n\n        clip = otio.schema.Clip()\n        clip.name = metadata.pop(\"Name\", None)\n\n        # When looking for Start, Duration and End, they might be missing\n        # or blank. Treat None and \"\" as the same via: get(k,\"\")!=\"\"\n        # To have a valid source range, you need Start and either Duration\n        # or End. If all three are provided, we check to make sure they match.\n        if metadata.get(\"Start\", \"\") != \"\":\n            value = metadata.pop(\"Start\")\n            try:\n                start = otio.opentime.from_timecode(value, fps)\n            except (ValueError, TypeError):\n                raise ALEParseError(\"Invalid Start timecode: {}\".format(value))\n            duration = None\n            end = None\n            if metadata.get(\"Duration\", \"\") != \"\":\n                value = metadata.pop(\"Duration\")\n                try:\n                    duration = otio.opentime.from_timecode(value, fps)\n                except (ValueError, TypeError):\n                    raise ALEParseError(\"Invalid Duration timecode: {}\".format(\n                        value\n                    ))\n            if metadata.get(\"End\", \"\") != \"\":\n                value = metadata.pop(\"End\")\n                try:\n                    end = otio.opentime.from_timecode(value, fps)\n                except (ValueError, TypeError):\n                    raise ALEParseError(\"Invalid End timecode: {}\".format(\n                        value\n                    ))\n            if duration is None:\n                duration = end - start\n            if end is None:\n                end = start + duration\n            if end != start + duration:\n                raise ALEParseError(\n                    \"Inconsistent Start, End, Duration: \" + line\n                )\n            clip.source_range = otio.opentime.TimeRange(\n                start,\n                duration\n            )\n\n        if metadata.get(\"Source File\"):\n            source = metadata.pop(\"Source File\")\n            clip.media_reference = otio.schema.ExternalReference(\n                target_url=source\n            )\n\n        # If available, collect cdl values in the same way we do for CMX EDL\n        cdl = {}\n\n        if metadata.get('CDL'):\n            cdl = _cdl_values_from_metadata(metadata['CDL'])\n            if cdl:\n                del metadata['CDL']\n\n        # If we have more specific metadata, let's use them\n        if metadata.get('ASC_SOP'):\n            cdl = _cdl_values_from_metadata(metadata['ASC_SOP'])\n\n            if cdl:\n                del metadata['ASC_SOP']\n\n        if metadata.get('ASC_SAT'):\n            try:\n                asc_sat_value = float(metadata['ASC_SAT'])\n                cdl.update(asc_sat=asc_sat_value)\n                del metadata['ASC_SAT']\n            except ValueError:\n                pass\n\n        if cdl:\n            clip.metadata['cdl'] = cdl\n\n        # We've pulled out the key/value pairs that we treat specially.\n        # Put the remaining key/values into clip.metadata[\"ALE\"]\n        clip.metadata[\"ALE\"] = metadata\n\n        return clip\n    except Exception as ex:\n        raise ALEParseError(\"Error parsing line: {}\\n{}\".format(\n            line, repr(ex)\n        ))\n\ndef _cdl_values_from_metadata(asc_sop_string):\n\n    if not isinstance(asc_sop_string, (type(''), type(u''))):\n        return {}\n\n    asc_sop_values = ASC_SOP_REGEX.findall(asc_sop_string)\n\n    cdl_data = {}\n\n    if len(asc_sop_values) >= 9:\n\n        cdl_data.update(\n            asc_sop={\n                'slope': [float(v) for v in asc_sop_values[:3]],\n                'offset': [float(v) for v in asc_sop_values[3:6]],\n                'power': [float(v) for v in asc_sop_values[6:9]]\n            })\n\n        if len(asc_sop_values) == 10:\n            cdl_data.update(asc_sat=float(asc_sop_values[9]))\n\n    return cdl_data"
    },
    {
        "pr_title": "Timecode rate is ignored",
        "pr_number": 612,
        "file_name": "src/opentime/rationalTime.cpp",
        "code_diff": "@@ -194,12 +194,9 @@\nRationalTime::to_timecode(\n \n     *error_status = ErrorStatus();\n \n-    double value_in_target_rate = _value;\n-    if (rate != _rate) {\n-        value_in_target_rate = this->value_rescaled_to(rate);\n-    }\n+    double frames_in_target_rate = this->value_rescaled_to(rate);\n     \n-    if (value_in_target_rate < 0) {\n+    if (frames_in_target_rate < 0) {\n         *error_status = ErrorStatus(ErrorStatus::NEGATIVE_VALUE);\n         return std::string();\n     }",
        "comments": [
            {
                "comment": "The goal of this is to convert the time to a frame count. I think the logic would be clearer if rather than using `value` and `rate` from the input `RationalTime` in this method, we instead used `RationalTime::to_frames` to get the frame count and never let the member `_rate` from the `RationalTime` instance enter this function's calculations. This clarifies the distinction between `RationalTime` `rate` (which is an arbitrary time division used to accurately express a time in seconds), and fps (which is a scheme of recording samples).\r\n\r\nThis is a bit pedantic, but it's akin to the distinction between pixels and little squares.",
                "position": null
            },
            {
                "comment": "As discussed, since we'll be looking more at this more in #613 we should keep it as-is t limit impact",
                "position": null
            }
        ],
        "commit_message": "Rename variable for clarity and drop extra check.",
        "commit_id": "5ecba56732dc6a5bb82725da0d460bf2e031c098",
        "fn_context": "std::string\nRationalTime::to_timecode(\n        double rate,\n        IsDropFrameRate drop_frame,\n        ErrorStatus* error_status\n) const {\n\n    *error_status = ErrorStatus();\n\n    double frames_in_target_rate = this->value_rescaled_to(rate);\n    \n    if (frames_in_target_rate < 0) {\n        *error_status = ErrorStatus(ErrorStatus::NEGATIVE_VALUE);\n        return std::string();\n    }\n        \n    if (!is_valid_timecode_rate(rate)) {\n        *error_status = ErrorStatus(ErrorStatus::INVALID_TIMECODE_RATE);\n        return std::string();\n    }\n\n    bool rate_is_dropframe = is_dropframe_rate(rate);\n    if (drop_frame == IsDropFrameRate::ForceYes and not rate_is_dropframe) {\n        *error_status = ErrorStatus(\n                ErrorStatus::INVALID_RATE_FOR_DROP_FRAME_TIMECODE\n        );\n        return std::string();\n    }\n\n    if (drop_frame != IsDropFrameRate::InferFromRate) {\n        if (drop_frame == IsDropFrameRate::ForceYes) {\n            rate_is_dropframe = true;\n        }\n        else {\n            rate_is_dropframe = false;\n        }\n    }\n\n    // extra math for dropframes stuff\n    int dropframes = 0;\n    char div = ':';\n    if (!rate_is_dropframe)\n    {\n        if (std::round(rate) == 24) {\n            rate = 24.0;\n        }\n    }\n    else {\n        if ((rate == 29.97) or (rate == 30000/1001.0)) {\n            dropframes = 2;\n        }\n        else if(rate == 59.94) {\n            dropframes = 4;\n        }\n        div = ';';\n    }\n\n    // Number of frames in an hour\n    int frames_per_hour = static_cast<int>(std::round(rate * 60 * 60));\n    // Number of frames in a day - timecode rolls over after 24 hours\n    int frames_per_24_hours = frames_per_hour * 24;\n    // Number of frames per ten minutes\n    int frames_per_10_minutes = static_cast<int>(std::round(rate * 60 * 10));\n    // Number of frames per minute is the round of the framerate * 60 minus\n    // the number of dropped frames\n    int frames_per_minute = static_cast<int>(\n            (std::round(rate) * 60) - dropframes);\n\n    // If the number of frames is more than 24 hours, roll over clock\n    double value = std::fmod(frames_in_target_rate, frames_per_24_hours);\n\n    if (rate_is_dropframe) {\n        int ten_minute_chunks = static_cast<int>(std::floor(value/frames_per_10_minutes));\n        int frames_over_ten_minutes = static_cast<int>(std::fmod(value, frames_per_10_minutes));\n\n        if (frames_over_ten_minutes > dropframes) {\n            value += (dropframes * 9 * ten_minute_chunks) +\n                dropframes * std::floor((frames_over_ten_minutes - dropframes) / frames_per_minute);\n        }\n        else {\n            value += dropframes * 9 * ten_minute_chunks;\n        }\n    }\n\n    int nominal_fps = static_cast<int>(std::ceil(rate));\n\n    // compute the fields\n    int frames = static_cast<int>(std::fmod(value, nominal_fps));\n    int seconds_total = static_cast<int>(std::floor(value / nominal_fps));\n    int seconds = static_cast<int>(std::fmod(seconds_total, 60));\n    int minutes = static_cast<int>(std::fmod(std::floor(seconds_total / 60), 60));\n    int hours = static_cast<int>(std::floor(std::floor(seconds_total / 60) / 60));\n\n    return string_printf(\"%02d:%02d:%02d%c%02d\", hours, minutes, seconds, div, frames);\n}"
    },
    {
        "pr_title": "xges: Effects and Markers Support",
        "pr_number": 609,
        "file_name": "contrib/opentimelineio_contrib/adapters/tests/tests_xges_adapter.py",
        "code_diff": "@@ -1655,14 +1655,15 @@\ndef test_timing(self):\n     def _make_nested_project(self):\n         xges_el = XgesElement()\n         xges_el.add_video_track()\n+        xges_el.add_audio_track()\n         xges_el.add_layer()\n         asset = xges_el.add_asset(\"file:///example.xges\", \"GESTimeline\")\n         xges_el.add_clip(\n-            7, 2, 1, \"GESUriClip\", 4, \"file:///example.xges\")\n+            70, 20, 10, \"GESUriClip\", 6, \"file:///example.xges\")\n         sub_xges_el = XgesElement()\n         sub_xges_el.add_video_track()\n         sub_xges_el.add_layer()\n-        sub_xges_el.add_clip(5, 4, 3, \"GESUriClip\", 4)\n+        sub_xges_el.add_clip(50, 40, 30, \"GESUriClip\", 6)\n         asset.append(sub_xges_el.ges)\n         return xges_el",
        "comments": [
            {
                "comment": "s/with//",
                "position": null
            },
            {
                "comment": "thanks. Changed it",
                "position": null
            }
        ],
        "commit_message": "xges: fix handling of sub-stacks\n\nPreviously, the method would assume that the asset-id stored in the otio\nmetadata by the xges adapter was sufficient to determine whether the\nsub-project found in the otio stack was equivalent. However, the otio\nobject may be edited in between, which can lead to different\nsub-projects. Although, not always.\nAlso, there was a bug in the previous code that meant a None asset id\nwas returned by _serialize_stack_to_ressource if the asset id already\nexisted.\nInstead, we now explicitly test whether the corresponding sub-projects are\nequivalent, and share the asset id only if they are.\n\nAdded a test for these cases.",
        "commit_id": "ebd79bc3bc05873a2904583ca047e36b2e709f65",
        "fn_context": "    def _make_nested_project(self):\n        xges_el = XgesElement()\n        xges_el.add_video_track()\n        xges_el.add_audio_track()\n        xges_el.add_layer()\n        asset = xges_el.add_asset(\"file:///example.xges\", \"GESTimeline\")\n        xges_el.add_clip(\n            70, 20, 10, \"GESUriClip\", 6, \"file:///example.xges\")\n        sub_xges_el = XgesElement()\n        sub_xges_el.add_video_track()\n        sub_xges_el.add_layer()\n        sub_xges_el.add_clip(50, 40, 30, \"GESUriClip\", 6)\n        asset.append(sub_xges_el.ges)\n        return xges_el\n\n    def test_nested_projects_and_stacks(self):\n        xges_el = self._make_nested_project()\n        timeline = xges_el.get_otio_timeline()\n        test_tree = OtioTestTree(\n            self, type_tests={\n                Track: [OtioTest.none_source],\n                Clip: [OtioTest.has_ex_ref]},\n            base=OtioTestNode(\n                Stack, tests=[OtioTest.none_source], children=[\n                    OtioTestNode(\n                        Track, tests=[OtioTest.is_video],\n                        children=[\n                            OtioTestNode(\n                                Gap,\n                                tests=[OtioTest.duration(70)]),\n                            OtioTestNode(\n                                Stack,\n                                tests=[OtioTest.range(10, 20)],\n                                children=[\n                                    OtioTestNode(\n                                        Track,\n                                        tests=[OtioTest.is_video],\n                                        children=[\n                                            OtioTestNode(Gap, tests=[\n                                                OtioTest.duration(50)]),\n                                            OtioTestNode(Clip, tests=[\n                                                OtioTest.range(30, 40)])\n                                        ]),\n                                    OtioTestNode(\n                                        Track,\n                                        tests=[OtioTest.is_audio],\n                                        children=[\n                                            OtioTestNode(Gap, tests=[\n                                                OtioTest.duration(50)]),\n                                            OtioTestNode(Clip, tests=[\n                                                OtioTest.range(30, 40)])\n                                        ])\n                                ])\n                        ]),\n                    OtioTestNode(\n                        Track, tests=[OtioTest.is_audio],\n                        children=[\n                            OtioTestNode(\n                                Gap,\n                                tests=[OtioTest.duration(70)]),\n                            OtioTestNode(\n                                Stack,\n                                tests=[OtioTest.range(10, 20)],\n                                children=[\n                                    OtioTestNode(\n                                        Track,\n                                        tests=[OtioTest.is_video],\n                                        children=[\n                                            OtioTestNode(Gap, tests=[\n                                                OtioTest.duration(50)]),\n                                            OtioTestNode(Clip, tests=[\n                                                OtioTest.range(30, 40)])\n                                        ]),\n                                    OtioTestNode(\n                                        Track,\n                                        tests=[OtioTest.is_audio],\n                                        children=[\n                                            OtioTestNode(Gap, tests=[\n                                                OtioTest.duration(50)]),\n                                            OtioTestNode(Clip, tests=[\n                                                OtioTest.range(30, 40)])\n                                        ])\n                                ])\n                        ])\n                ]))\n        test_tree.test_compare(timeline.tracks)\n        self._xges_has_nested_clip(timeline, 70, 20, 10, 6, 50, 40, 30, 6)"
    },
    {
        "pr_title": "xges: Effects and Markers Support",
        "pr_number": 609,
        "file_name": "contrib/opentimelineio_contrib/adapters/tests/tests_xges_adapter.py",
        "code_diff": "@@ -1671,30 +1672,155 @@\ndef test_nested_projects_and_stacks(self):\n         timeline = xges_el.get_otio_timeline()\n         test_tree = OtioTestTree(\n             self, type_tests={\n-                Track: [OtioTest.none_source, OtioTest.is_video],\n+                Track: [OtioTest.none_source],\n                 Clip: [OtioTest.has_ex_ref]},\n             base=OtioTestNode(\n                 Stack, tests=[OtioTest.none_source], children=[\n-                    OtioTestNode(Track, children=[\n-                        OtioTestNode(Gap, tests=[OtioTest.duration(7)]),\n-                        OtioTestNode(\n-                            Stack, tests=[OtioTest.range(1, 2)],\n-                            children=[\n-                                OtioTestNode(Track, children=[\n-                                    OtioTestNode(Gap, tests=[\n-                                        OtioTest.duration(5)]),\n-                                    OtioTestNode(Clip, tests=[\n-                                        OtioTest.range(3, 4)])\n+                    OtioTestNode(\n+                        Track, tests=[OtioTest.is_video],\n+                        children=[\n+                            OtioTestNode(\n+                                Gap,\n+                                tests=[OtioTest.duration(70)]),\n+                            OtioTestNode(\n+                                Stack,\n+                                tests=[OtioTest.range(10, 20)],\n+                                children=[\n+                                    OtioTestNode(\n+                                        Track,\n+                                        tests=[OtioTest.is_video],\n+                                        children=[\n+                                            OtioTestNode(Gap, tests=[\n+                                                OtioTest.duration(50)]),\n+                                            OtioTestNode(Clip, tests=[\n+                                                OtioTest.range(30, 40)])\n+                                        ]),\n+                                    OtioTestNode(\n+                                        Track,\n+                                        tests=[OtioTest.is_audio],\n+                                        children=[\n+                                            OtioTestNode(Gap, tests=[\n+                                                OtioTest.duration(50)]),\n+                                            OtioTestNode(Clip, tests=[\n+                                                OtioTest.range(30, 40)])\n+                                        ])\n                                 ])\n-                            ])\n-                    ])\n+                        ]),\n+                    OtioTestNode(\n+                        Track, tests=[OtioTest.is_audio],\n+                        children=[\n+                            OtioTestNode(\n+                                Gap,\n+                                tests=[OtioTest.duration(70)]),\n+                            OtioTestNode(\n+                                Stack,\n+                                tests=[OtioTest.range(10, 20)],\n+                                children=[\n+                                    OtioTestNode(\n+                                        Track,\n+                                        tests=[OtioTest.is_video],\n+                                        children=[\n+                                            OtioTestNode(Gap, tests=[\n+                                                OtioTest.duration(50)]),\n+                                            OtioTestNode(Clip, tests=[\n+                                                OtioTest.range(30, 40)])\n+                                        ]),\n+                                    OtioTestNode(\n+                                        Track,\n+                                        tests=[OtioTest.is_audio],\n+                                        children=[\n+                                            OtioTestNode(Gap, tests=[\n+                                                OtioTest.duration(50)]),\n+                                            OtioTestNode(Clip, tests=[\n+                                                OtioTest.range(30, 40)])\n+                                        ])\n+                                ])\n+                        ])\n                 ]))\n         test_tree.test_compare(timeline.tracks)\n-        self._xges_has_nested_clip(timeline, 7, 2, 1, 5, 4, 3)\n+        self._xges_has_nested_clip(timeline, 70, 20, 10, 6, 50, 40, 30, 6)\n+\n+    def test_nested_projects_and_stacks_edited(self):\n+        xges_el = self._make_nested_project()\n+        timeline = xges_el.get_otio_timeline()\n+        # Previous test will assert the correct structure\n+\n+        # Change the gap before the video sub-stack to 30 seconds\n+        timeline.tracks[0][0].source_range = _tm_range_from_secs(0, 30)\n+\n+        # The sub-project should be the same, but we now have two\n+        # different clips referencing the same sub-project\n+\n+        # Now have an audio clip, with the new start time\n+        first_top_clip, _ = self._xges_has_nested_clip(\n+            timeline, 30, 20, 10, 4, 50, 40, 30, 6)\n+        # And the video clip, with the old start time\n+        second_top_clip, _ = self._xges_has_nested_clip(\n+            timeline, 70, 20, 10, 2, 50, 40, 30, 6)\n+        # They both reference the same project\n+        first_id = self.assertXgesHasAttr(first_top_clip, \"asset-id\")\n+        self.assertXgesAttrEqual(second_top_clip, \"asset-id\", first_id)\n+\n+        # Restore the timing\n+        timeline.tracks[0][0].source_range = _tm_range_from_secs(0, 70)\n+        # Change the video sub-stack to reference an earlier point\n+        timeline.tracks[0][1].source_range = _tm_range_from_secs(0, 10)\n+\n+        # The sub-project should be the same, but we now have two\n+        # different clips referencing the same sub-project\n+\n+        # Now have a video clip, with the new duration and inpoint\n+        first_top_clip, _ = self._xges_has_nested_clip(\n+            timeline, 70, 10, 0, 4, 50, 40, 30, 6)\n+        # And an audio clip, with the old start time\n+        second_top_clip, _ = self._xges_has_nested_clip(\n+            timeline, 70, 20, 10, 2, 50, 40, 30, 6)\n+        # They both reference the same project\n+        first_id = self.assertXgesHasAttr(first_top_clip, \"asset-id\")\n+        self.assertXgesAttrEqual(second_top_clip, \"asset-id\", first_id)\n+\n+        # Restore the timing\n+        timeline.tracks[0][1].source_range = _tm_range_from_secs(10, 20)\n+        # Change the content of the video sub-stack by reducing the gap\n+        timeline.tracks[0][1][0][0].source_range = _tm_range_from_secs(0, 20)\n+        timeline.tracks[0][1][1][0].source_range = _tm_range_from_secs(0, 20)\n+\n+        # The sub-project should now be different, so we should have two\n+        # separate assets\n+        first_top_clip, _ = self._xges_has_nested_clip(\n+            timeline, 70, 20, 10, 4, 20, 40, 30, 6)\n+        second_top_clip, _ = self._xges_has_nested_clip(\n+            timeline, 70, 20, 10, 2, 50, 40, 30, 6)\n+        # They now reference different projects\n+        first_id = self.assertXgesHasAttr(first_top_clip, \"asset-id\")\n+        second_id = self.assertXgesHasAttr(second_top_clip, \"asset-id\")\n+        self.assertNotEqual(first_id, second_id)\n+\n+        # Restore the stack's timing\n+        timeline.tracks[0][1][0][0].source_range = _tm_range_from_secs(0, 50)\n+        timeline.tracks[0][1][1][0].source_range = _tm_range_from_secs(0, 50)\n+        # Change the content of the video sub-stack by referencing\n+        # different times for its clip\n+        timeline.tracks[0][1][0][1].source_range = _tm_range_from_secs(10, 60)\n+        timeline.tracks[0][1][1][1].source_range = _tm_range_from_secs(10, 60)\n+\n+        # The sub-project should now be different, so we should have two\n+        # separate assets\n+        first_top_clip, _ = self._xges_has_nested_clip(\n+            timeline, 70, 20, 10, 4, 50, 60, 10, 6)\n+        second_top_clip, _ = self._xges_has_nested_clip(\n+            timeline, 70, 20, 10, 2, 50, 40, 30, 6)\n+        # They now reference different projects\n+        first_id = self.assertXgesHasAttr(first_top_clip, \"asset-id\")\n+        second_id = self.assertXgesHasAttr(second_top_clip, \"asset-id\")\n+        self.assertNotEqual(first_id, second_id)\n \n     def _xges_has_nested_clip(\n-            self, timeline, top_start, top_duration, top_inpoint,\n-            orig_start, orig_duration, orig_inpoint, effect_names=None):\n+            self, timeline,\n+            top_start, top_duration, top_inpoint, top_track_types,\n+            orig_start, orig_duration, orig_inpoint, orig_track_types,\n+            effect_names=None):\n+        \"\"\"Returns the top clip and nested clip\"\"\"\n         if effect_names is None:\n             effect_names = []",
        "comments": [
            {
                "comment": "s/with//",
                "position": null
            },
            {
                "comment": "thanks. Changed it",
                "position": null
            }
        ],
        "commit_message": "xges: fix handling of sub-stacks\n\nPreviously, the method would assume that the asset-id stored in the otio\nmetadata by the xges adapter was sufficient to determine whether the\nsub-project found in the otio stack was equivalent. However, the otio\nobject may be edited in between, which can lead to different\nsub-projects. Although, not always.\nAlso, there was a bug in the previous code that meant a None asset id\nwas returned by _serialize_stack_to_ressource if the asset id already\nexisted.\nInstead, we now explicitly test whether the corresponding sub-projects are\nequivalent, and share the asset id only if they are.\n\nAdded a test for these cases.",
        "commit_id": "ebd79bc3bc05873a2904583ca047e36b2e709f65",
        "fn_context": "    def test_nested_projects_and_stacks(self):\n        xges_el = self._make_nested_project()\n        timeline = xges_el.get_otio_timeline()\n        test_tree = OtioTestTree(\n            self, type_tests={\n                Track: [OtioTest.none_source],\n                Clip: [OtioTest.has_ex_ref]},\n            base=OtioTestNode(\n                Stack, tests=[OtioTest.none_source], children=[\n                    OtioTestNode(\n                        Track, tests=[OtioTest.is_video],\n                        children=[\n                            OtioTestNode(\n                                Gap,\n                                tests=[OtioTest.duration(70)]),\n                            OtioTestNode(\n                                Stack,\n                                tests=[OtioTest.range(10, 20)],\n                                children=[\n                                    OtioTestNode(\n                                        Track,\n                                        tests=[OtioTest.is_video],\n                                        children=[\n                                            OtioTestNode(Gap, tests=[\n                                                OtioTest.duration(50)]),\n                                            OtioTestNode(Clip, tests=[\n                                                OtioTest.range(30, 40)])\n                                        ]),\n                                    OtioTestNode(\n                                        Track,\n                                        tests=[OtioTest.is_audio],\n                                        children=[\n                                            OtioTestNode(Gap, tests=[\n                                                OtioTest.duration(50)]),\n                                            OtioTestNode(Clip, tests=[\n                                                OtioTest.range(30, 40)])\n                                        ])\n                                ])\n                        ]),\n                    OtioTestNode(\n                        Track, tests=[OtioTest.is_audio],\n                        children=[\n                            OtioTestNode(\n                                Gap,\n                                tests=[OtioTest.duration(70)]),\n                            OtioTestNode(\n                                Stack,\n                                tests=[OtioTest.range(10, 20)],\n                                children=[\n                                    OtioTestNode(\n                                        Track,\n                                        tests=[OtioTest.is_video],\n                                        children=[\n                                            OtioTestNode(Gap, tests=[\n                                                OtioTest.duration(50)]),\n                                            OtioTestNode(Clip, tests=[\n                                                OtioTest.range(30, 40)])\n                                        ]),\n                                    OtioTestNode(\n                                        Track,\n                                        tests=[OtioTest.is_audio],\n                                        children=[\n                                            OtioTestNode(Gap, tests=[\n                                                OtioTest.duration(50)]),\n                                            OtioTestNode(Clip, tests=[\n                                                OtioTest.range(30, 40)])\n                                        ])\n                                ])\n                        ])\n                ]))\n        test_tree.test_compare(timeline.tracks)\n        self._xges_has_nested_clip(timeline, 70, 20, 10, 6, 50, 40, 30, 6)\n\n    def test_nested_projects_and_stacks_edited(self):\n        xges_el = self._make_nested_project()\n        timeline = xges_el.get_otio_timeline()\n        # Previous test will assert the correct structure\n\n        # Change the gap before the video sub-stack to 30 seconds\n        timeline.tracks[0][0].source_range = _tm_range_from_secs(0, 30)\n\n        # The sub-project should be the same, but we now have two\n        # different clips referencing the same sub-project\n\n        # Now have an audio clip, with the new start time\n        first_top_clip, _ = self._xges_has_nested_clip(\n            timeline, 30, 20, 10, 4, 50, 40, 30, 6)\n        # And the video clip, with the old start time\n        second_top_clip, _ = self._xges_has_nested_clip(\n            timeline, 70, 20, 10, 2, 50, 40, 30, 6)\n        # They both reference the same project\n        first_id = self.assertXgesHasAttr(first_top_clip, \"asset-id\")\n        self.assertXgesAttrEqual(second_top_clip, \"asset-id\", first_id)\n\n        # Restore the timing\n        timeline.tracks[0][0].source_range = _tm_range_from_secs(0, 70)\n        # Change the video sub-stack to reference an earlier point\n        timeline.tracks[0][1].source_range = _tm_range_from_secs(0, 10)\n\n        # The sub-project should be the same, but we now have two\n        # different clips referencing the same sub-project\n\n        # Now have a video clip, with the new duration and inpoint\n        first_top_clip, _ = self._xges_has_nested_clip(\n            timeline, 70, 10, 0, 4, 50, 40, 30, 6)\n        # And an audio clip, with the old start time\n        second_top_clip, _ = self._xges_has_nested_clip(\n            timeline, 70, 20, 10, 2, 50, 40, 30, 6)\n        # They both reference the same project\n        first_id = self.assertXgesHasAttr(first_top_clip, \"asset-id\")\n        self.assertXgesAttrEqual(second_top_clip, \"asset-id\", first_id)\n\n        # Restore the timing\n        timeline.tracks[0][1].source_range = _tm_range_from_secs(10, 20)\n        # Change the content of the video sub-stack by reducing the gap\n        timeline.tracks[0][1][0][0].source_range = _tm_range_from_secs(0, 20)\n        timeline.tracks[0][1][1][0].source_range = _tm_range_from_secs(0, 20)\n\n        # The sub-project should now be different, so we should have two\n        # separate assets\n        first_top_clip, _ = self._xges_has_nested_clip(\n            timeline, 70, 20, 10, 4, 20, 40, 30, 6)\n        second_top_clip, _ = self._xges_has_nested_clip(\n            timeline, 70, 20, 10, 2, 50, 40, 30, 6)\n        # They now reference different projects\n        first_id = self.assertXgesHasAttr(first_top_clip, \"asset-id\")\n        second_id = self.assertXgesHasAttr(second_top_clip, \"asset-id\")\n        self.assertNotEqual(first_id, second_id)\n\n        # Restore the stack's timing\n        timeline.tracks[0][1][0][0].source_range = _tm_range_from_secs(0, 50)\n        timeline.tracks[0][1][1][0].source_range = _tm_range_from_secs(0, 50)\n        # Change the content of the video sub-stack by referencing\n        # different times for its clip\n        timeline.tracks[0][1][0][1].source_range = _tm_range_from_secs(10, 60)\n        timeline.tracks[0][1][1][1].source_range = _tm_range_from_secs(10, 60)\n\n        # The sub-project should now be different, so we should have two\n        # separate assets\n        first_top_clip, _ = self._xges_has_nested_clip(\n            timeline, 70, 20, 10, 4, 50, 60, 10, 6)\n        second_top_clip, _ = self._xges_has_nested_clip(\n            timeline, 70, 20, 10, 2, 50, 40, 30, 6)\n        # They now reference different projects\n        first_id = self.assertXgesHasAttr(first_top_clip, \"asset-id\")\n        second_id = self.assertXgesHasAttr(second_top_clip, \"asset-id\")\n        self.assertNotEqual(first_id, second_id)\n\n    def _xges_has_nested_clip(\n            self, timeline,\n            top_start, top_duration, top_inpoint, top_track_types,\n            orig_start, orig_duration, orig_inpoint, orig_track_types,\n            effect_names=None):\n        \"\"\"Returns the top clip and nested clip\"\"\"\n        if effect_names is None:\n            effect_names = []\n\n        if effect_names and str is not bytes:\n            # TODO: remove the str is not bytes check once python2 has\n            # ended. Python2 does not have assertWarns\n            # TODO: warning is for the fact that we do not yet have a\n            # smart way to convert effect names into bin-descriptions\n            # Should be removed once this is sorted\n            with self.assertWarns(UserWarning):\n                ges_el = self._get_xges_from_otio_timeline(timeline)\n        else:\n            ges_el = self._get_xges_from_otio_timeline(timeline)\n        if orig_track_types == 6:\n            self.assertXgesTrackTypes(ges_el, 2, 4)\n        else:\n            self.assertXgesTrackTypes(ges_el, top_track_types)\n        top_clip = self.assertXgesClip(\n            ges_el, {\n                \"start\": top_start, \"duration\": top_duration,\n                \"inpoint\": top_inpoint, \"type-name\": \"GESUriClip\",\n                \"track-types\": top_track_types})\n        effects = self.assertXgesNumClipEffects(\n            top_clip, len(effect_names))\n        for effect, name in zip(effects, effect_names):\n            self.assertXgesAttrEqual(effect, \"asset-id\", name)\n\n        ges_el = self.assertXgesClipIsSubproject(ges_el, top_clip)\n        self.assertXgesNumClips(ges_el, 1)\n        orig_clip = self.assertXgesClip(\n            ges_el, {\n                \"start\": orig_start, \"duration\": orig_duration,\n                \"inpoint\": orig_inpoint, \"type-name\": \"GESUriClip\",\n                \"track-types\": orig_track_types})\n        self.assertXgesNumClipEffects(orig_clip, 0)\n        self.assertXgesClipHasAsset(ges_el, orig_clip)\n        return top_clip, orig_clip"
    },
    {
        "pr_title": "xges: Effects and Markers Support",
        "pr_number": 609,
        "file_name": "contrib/opentimelineio_contrib/adapters/tests/tests_xges_adapter.py",
        "code_diff": "@@ -1708,12 +1834,15 @@\ndef _xges_has_nested_clip(\n                 ges_el = self._get_xges_from_otio_timeline(timeline)\n         else:\n             ges_el = self._get_xges_from_otio_timeline(timeline)\n-        self.assertXgesTrackTypes(ges_el, 4)\n+        if orig_track_types == 6:\n+            self.assertXgesTrackTypes(ges_el, 2, 4)\n+        else:\n+            self.assertXgesTrackTypes(ges_el, top_track_types)\n         top_clip = self.assertXgesClip(\n             ges_el, {\n                 \"start\": top_start, \"duration\": top_duration,\n                 \"inpoint\": top_inpoint, \"type-name\": \"GESUriClip\",\n-                \"track-types\": 4})\n+                \"track-types\": top_track_types})\n         effects = self.assertXgesNumClipEffects(\n             top_clip, len(effect_names))\n         for effect, name in zip(effects, effect_names):",
        "comments": [
            {
                "comment": "s/with//",
                "position": null
            },
            {
                "comment": "thanks. Changed it",
                "position": null
            }
        ],
        "commit_message": "xges: fix handling of sub-stacks\n\nPreviously, the method would assume that the asset-id stored in the otio\nmetadata by the xges adapter was sufficient to determine whether the\nsub-project found in the otio stack was equivalent. However, the otio\nobject may be edited in between, which can lead to different\nsub-projects. Although, not always.\nAlso, there was a bug in the previous code that meant a None asset id\nwas returned by _serialize_stack_to_ressource if the asset id already\nexisted.\nInstead, we now explicitly test whether the corresponding sub-projects are\nequivalent, and share the asset id only if they are.\n\nAdded a test for these cases.",
        "commit_id": "ebd79bc3bc05873a2904583ca047e36b2e709f65",
        "fn_context": "    def _xges_has_nested_clip(\n            self, timeline,\n            top_start, top_duration, top_inpoint, top_track_types,\n            orig_start, orig_duration, orig_inpoint, orig_track_types,\n            effect_names=None):\n        \"\"\"Returns the top clip and nested clip\"\"\"\n        if effect_names is None:\n            effect_names = []\n\n        if effect_names and str is not bytes:\n            # TODO: remove the str is not bytes check once python2 has\n            # ended. Python2 does not have assertWarns\n            # TODO: warning is for the fact that we do not yet have a\n            # smart way to convert effect names into bin-descriptions\n            # Should be removed once this is sorted\n            with self.assertWarns(UserWarning):\n                ges_el = self._get_xges_from_otio_timeline(timeline)\n        else:\n            ges_el = self._get_xges_from_otio_timeline(timeline)\n        if orig_track_types == 6:\n            self.assertXgesTrackTypes(ges_el, 2, 4)\n        else:\n            self.assertXgesTrackTypes(ges_el, top_track_types)\n        top_clip = self.assertXgesClip(\n            ges_el, {\n                \"start\": top_start, \"duration\": top_duration,\n                \"inpoint\": top_inpoint, \"type-name\": \"GESUriClip\",\n                \"track-types\": top_track_types})\n        effects = self.assertXgesNumClipEffects(\n            top_clip, len(effect_names))\n        for effect, name in zip(effects, effect_names):\n            self.assertXgesAttrEqual(effect, \"asset-id\", name)\n\n        ges_el = self.assertXgesClipIsSubproject(ges_el, top_clip)\n        self.assertXgesNumClips(ges_el, 1)\n        orig_clip = self.assertXgesClip(\n            ges_el, {\n                \"start\": orig_start, \"duration\": orig_duration,\n                \"inpoint\": orig_inpoint, \"type-name\": \"GESUriClip\",\n                \"track-types\": orig_track_types})\n        self.assertXgesNumClipEffects(orig_clip, 0)\n        self.assertXgesClipHasAsset(ges_el, orig_clip)\n        return top_clip, orig_clip"
    },
    {
        "pr_title": "xges: Effects and Markers Support",
        "pr_number": 609,
        "file_name": "contrib/opentimelineio_contrib/adapters/tests/tests_xges_adapter.py",
        "code_diff": "@@ -1725,9 +1854,10 @@\ndef _xges_has_nested_clip(\n             ges_el, {\n                 \"start\": orig_start, \"duration\": orig_duration,\n                 \"inpoint\": orig_inpoint, \"type-name\": \"GESUriClip\",\n-                \"track-types\": 4})\n+                \"track-types\": orig_track_types})\n         self.assertXgesNumClipEffects(orig_clip, 0)\n         self.assertXgesClipHasAsset(ges_el, orig_clip)\n+        return top_clip, orig_clip\n \n     def test_effect_stack(self):\n         timeline = Timeline()",
        "comments": [
            {
                "comment": "s/with//",
                "position": null
            },
            {
                "comment": "thanks. Changed it",
                "position": null
            }
        ],
        "commit_message": "xges: fix handling of sub-stacks\n\nPreviously, the method would assume that the asset-id stored in the otio\nmetadata by the xges adapter was sufficient to determine whether the\nsub-project found in the otio stack was equivalent. However, the otio\nobject may be edited in between, which can lead to different\nsub-projects. Although, not always.\nAlso, there was a bug in the previous code that meant a None asset id\nwas returned by _serialize_stack_to_ressource if the asset id already\nexisted.\nInstead, we now explicitly test whether the corresponding sub-projects are\nequivalent, and share the asset id only if they are.\n\nAdded a test for these cases.",
        "commit_id": "ebd79bc3bc05873a2904583ca047e36b2e709f65",
        "fn_context": "    def _xges_has_nested_clip(\n            self, timeline,\n            top_start, top_duration, top_inpoint, top_track_types,\n            orig_start, orig_duration, orig_inpoint, orig_track_types,\n            effect_names=None):\n        \"\"\"Returns the top clip and nested clip\"\"\"\n        if effect_names is None:\n            effect_names = []\n\n        if effect_names and str is not bytes:\n            # TODO: remove the str is not bytes check once python2 has\n            # ended. Python2 does not have assertWarns\n            # TODO: warning is for the fact that we do not yet have a\n            # smart way to convert effect names into bin-descriptions\n            # Should be removed once this is sorted\n            with self.assertWarns(UserWarning):\n                ges_el = self._get_xges_from_otio_timeline(timeline)\n        else:\n            ges_el = self._get_xges_from_otio_timeline(timeline)\n        if orig_track_types == 6:\n            self.assertXgesTrackTypes(ges_el, 2, 4)\n        else:\n            self.assertXgesTrackTypes(ges_el, top_track_types)\n        top_clip = self.assertXgesClip(\n            ges_el, {\n                \"start\": top_start, \"duration\": top_duration,\n                \"inpoint\": top_inpoint, \"type-name\": \"GESUriClip\",\n                \"track-types\": top_track_types})\n        effects = self.assertXgesNumClipEffects(\n            top_clip, len(effect_names))\n        for effect, name in zip(effects, effect_names):\n            self.assertXgesAttrEqual(effect, \"asset-id\", name)\n\n        ges_el = self.assertXgesClipIsSubproject(ges_el, top_clip)\n        self.assertXgesNumClips(ges_el, 1)\n        orig_clip = self.assertXgesClip(\n            ges_el, {\n                \"start\": orig_start, \"duration\": orig_duration,\n                \"inpoint\": orig_inpoint, \"type-name\": \"GESUriClip\",\n                \"track-types\": orig_track_types})\n        self.assertXgesNumClipEffects(orig_clip, 0)\n        self.assertXgesClipHasAsset(ges_el, orig_clip)\n        return top_clip, orig_clip\n\n    def test_effect_stack(self):\n        timeline = Timeline()\n        effect_names = [\"agingtv\", \"videobalance\"]\n        for name in effect_names:\n            timeline.tracks.effects.append(Effect(effect_name=name))\n        track = Track()\n        track.kind = TrackKind.Video\n        timeline.tracks.append(track)\n        track.append(_make_clip(start=20, duration=50))\n        self._xges_has_nested_clip(\n            timeline, 0, 50, 0, 4, 0, 50, 20, 4, effect_names)"
    },
    {
        "pr_title": "xges: Effects and Markers Support",
        "pr_number": 609,
        "file_name": "contrib/opentimelineio_contrib/adapters/tests/tests_xges_adapter.py",
        "code_diff": "@@ -1737,27 +1867,27 @@\ndef test_effect_stack(self):\n         track = Track()\n         track.kind = TrackKind.Video\n         timeline.tracks.append(track)\n-        track.append(_make_clip(start=2, duration=5))\n+        track.append(_make_clip(start=20, duration=50))\n         self._xges_has_nested_clip(\n-            timeline, 0, 5, 0, 0, 5, 2, effect_names)\n+            timeline, 0, 50, 0, 4, 0, 50, 20, 4, effect_names)\n \n     def test_source_range_stack(self):\n         timeline = Timeline()\n         track = Track()\n         track.kind = TrackKind.Video\n         timeline.tracks.append(track)\n-        track.append(_make_clip(start=2, duration=5))\n-        timeline.tracks.source_range = _tm_range_from_secs(1, 3)\n-        self._xges_has_nested_clip(timeline, 0, 3, 1, 0, 5, 2)\n+        track.append(_make_clip(start=20, duration=50))\n+        timeline.tracks.source_range = _tm_range_from_secs(10, 30)\n+        self._xges_has_nested_clip(timeline, 0, 30, 10, 4, 0, 50, 20, 4)\n \n     def test_source_range_track(self):\n         timeline = Timeline()\n         track = Track()\n         track.kind = TrackKind.Video\n         timeline.tracks.append(track)\n-        track.append(_make_clip(start=2, duration=5))\n-        track.source_range = _tm_range_from_secs(1, 3)\n-        self._xges_has_nested_clip(timeline, 0, 3, 1, 0, 5, 2)\n+        track.append(_make_clip(start=20, duration=50))\n+        track.source_range = _tm_range_from_secs(10, 30)\n+        self._xges_has_nested_clip(timeline, 0, 30, 10, 4, 0, 50, 20, 4)\n \n     def test_double_track(self):\n         timeline = Timeline()",
        "comments": [
            {
                "comment": "s/with//",
                "position": null
            },
            {
                "comment": "thanks. Changed it",
                "position": null
            }
        ],
        "commit_message": "xges: fix handling of sub-stacks\n\nPreviously, the method would assume that the asset-id stored in the otio\nmetadata by the xges adapter was sufficient to determine whether the\nsub-project found in the otio stack was equivalent. However, the otio\nobject may be edited in between, which can lead to different\nsub-projects. Although, not always.\nAlso, there was a bug in the previous code that meant a None asset id\nwas returned by _serialize_stack_to_ressource if the asset id already\nexisted.\nInstead, we now explicitly test whether the corresponding sub-projects are\nequivalent, and share the asset id only if they are.\n\nAdded a test for these cases.",
        "commit_id": "ebd79bc3bc05873a2904583ca047e36b2e709f65",
        "fn_context": "    def test_effect_stack(self):\n        timeline = Timeline()\n        effect_names = [\"agingtv\", \"videobalance\"]\n        for name in effect_names:\n            timeline.tracks.effects.append(Effect(effect_name=name))\n        track = Track()\n        track.kind = TrackKind.Video\n        timeline.tracks.append(track)\n        track.append(_make_clip(start=20, duration=50))\n        self._xges_has_nested_clip(\n            timeline, 0, 50, 0, 4, 0, 50, 20, 4, effect_names)\n\n    def test_source_range_stack(self):\n        timeline = Timeline()\n        track = Track()\n        track.kind = TrackKind.Video\n        timeline.tracks.append(track)\n        track.append(_make_clip(start=20, duration=50))\n        timeline.tracks.source_range = _tm_range_from_secs(10, 30)\n        self._xges_has_nested_clip(timeline, 0, 30, 10, 4, 0, 50, 20, 4)\n\n    def test_source_range_track(self):\n        timeline = Timeline()\n        track = Track()\n        track.kind = TrackKind.Video\n        timeline.tracks.append(track)\n        track.append(_make_clip(start=20, duration=50))\n        track.source_range = _tm_range_from_secs(10, 30)\n        self._xges_has_nested_clip(timeline, 0, 30, 10, 4, 0, 50, 20, 4)\n\n    def test_double_track(self):\n        timeline = Timeline()\n        track1 = Track()\n        track1.kind = TrackKind.Video\n        timeline.tracks.append(track1)\n        track2 = Track()\n        track2.kind = TrackKind.Video\n        track1.append(_make_clip(start=40, duration=90))\n        track1.append(track2)\n        track2.append(_make_clip(start=20, duration=50))\n        self._xges_has_nested_clip(timeline, 90, 50, 0, 4, 0, 50, 20, 4)"
    },
    {
        "pr_title": "xges: Effects and Markers Support",
        "pr_number": 609,
        "file_name": "contrib/opentimelineio_contrib/adapters/xges.py",
        "code_diff": "@@ -1045,7 +1045,10 @@\ndef __init__(self, input_otio=None):\n         else:\n             self.timeline = None\n         self.all_names = set()\n+        # map track types to a track id\n         self.track_id_for_type = {}\n+        # map from a sub-<ges> element to an asset id\n+        self.sub_projects = {}\n \n     @staticmethod\n     def _rat_to_gstclocktime(rat_time):",
        "comments": [],
        "commit_message": "xges: fix handling of sub-stacks\n\nPreviously, the method would assume that the asset-id stored in the otio\nmetadata by the xges adapter was sufficient to determine whether the\nsub-project found in the otio stack was equivalent. However, the otio\nobject may be edited in between, which can lead to different\nsub-projects. Although, not always.\nAlso, there was a bug in the previous code that meant a None asset id\nwas returned by _serialize_stack_to_ressource if the asset id already\nexisted.\nInstead, we now explicitly test whether the corresponding sub-projects are\nequivalent, and share the asset id only if they are.\n\nAdded a test for these cases.",
        "commit_id": "ebd79bc3bc05873a2904583ca047e36b2e709f65"
    },
    {
        "pr_title": "[RodeoFX] Fix Marker __repr__ and __str__ #592",
        "pr_number": 596,
        "file_name": "src/py-opentimelineio/opentimelineio/schema/marker.py",
        "code_diff": "@@ -4,8 +4,9 @@\n@add_method(_otio.Marker)\n def __str__(self):\n-    return 'Marker(\"{}\", {}, {})'.format(\n+    return 'Marker(\"{}\", \"{}\", {}, {})'.format(\n         self.name,\n+        self.color,\n         self.marked_range,\n         self.metadata\n     )",
        "comments": [],
        "commit_message": "Include color attribute in Marker __repr__ and __str__, add tests",
        "commit_id": "2df4c05507b1d187e9bdb33b184f7d515b27fe33"
    },
    {
        "pr_title": "Fix warnings for Visual Studio 2019",
        "pr_number": 577,
        "file_name": "src/opentimelineio/deserialization.cpp",
        "code_diff": "@@ -14,7 +14,7 @@\nnamespace opentimelineio { namespace OPENTIMELINEIO_VERSION  {\n     \n class JSONDecoder : public OTIO_rapidjson::BaseReaderHandler<OTIO_rapidjson::UTF8<>, JSONDecoder> {\n public:\n-    JSONDecoder(std::function<int ()> line_number_function)\n+    JSONDecoder(std::function<size_t ()> line_number_function)\n         : _line_number_function {line_number_function} {\n         using namespace std::placeholders;\n         _error_function = std::bind(&JSONDecoder::_error, this, _1);",
        "comments": [],
        "commit_message": "Fix warnings for Visual Studio 2019",
        "commit_id": "628940edb49948565718c6946d7ba534946f6ec4"
    },
    {
        "pr_title": "Fix warnings for Visual Studio 2019",
        "pr_number": 577,
        "file_name": "src/opentimelineio/deserialization.cpp",
        "code_diff": "@@ -121,7 +121,7 @@\nclass JSONDecoder : public OTIO_rapidjson::BaseReaderHandler<OTIO_rapidjson::UTF\n             else {\n                 // when we end a dictionary, we immediately convert it\n                 // to the type it really represents, if it is a schema object.\n-                SerializableObject::Reader reader(top.dict, _error_function, nullptr, _line_number_function());\n+                SerializableObject::Reader reader(top.dict, _error_function, nullptr, static_cast<int>(_line_number_function()));\n                 _stack.pop_back();                \n                 store(reader._decode(_resolver));\n             }",
        "comments": [],
        "commit_message": "Fix warnings for Visual Studio 2019",
        "commit_id": "628940edb49948565718c6946d7ba534946f6ec4"
    },
    {
        "pr_title": "Fix warnings for Visual Studio 2019",
        "pr_number": 577,
        "file_name": "src/opentimelineio/deserialization.cpp",
        "code_diff": "@@ -185,7 +185,7 @@\nclass JSONDecoder : public OTIO_rapidjson::BaseReaderHandler<OTIO_rapidjson::UTF\n \n     std::vector<_DictOrArray> _stack;\n     std::function<void (ErrorStatus const&)> _error_function;\n-    std::function<int ()> _line_number_function;\n+    std::function<size_t ()> _line_number_function;\n \n     SerializableObject::Reader::_Resolver _resolver;\n };",
        "comments": [],
        "commit_message": "Fix warnings for Visual Studio 2019",
        "commit_id": "628940edb49948565718c6946d7ba534946f6ec4"
    },
    {
        "pr_title": "Fix warnings for Visual Studio 2019",
        "pr_number": 577,
        "file_name": "src/opentimelineio/deserialization.cpp",
        "code_diff": "@@ -309,12 +309,12 @@\nbool SerializableObject::Reader::_fetch(std::string const& key, double* dest) {\n         return true;\n     }\n     else if (e->second.type() == typeid(int)) {\n-        *dest = any_cast<int>(e->second);\n+        *dest = static_cast<double>(any_cast<int>(e->second));\n         _dict.erase(e);\n         return true;\n     }\n     else if (e->second.type() == typeid(int64_t)) {\n-        *dest = any_cast<int64_t>(e->second);\n+        *dest = static_cast<double>(any_cast<int64_t>(e->second));\n         _dict.erase(e);\n         return true;\n     }",
        "comments": [],
        "commit_message": "Fix warnings for Visual Studio 2019",
        "commit_id": "628940edb49948565718c6946d7ba534946f6ec4"
    },
    {
        "pr_title": "Fix warnings for Visual Studio 2019",
        "pr_number": 577,
        "file_name": "src/opentimelineio/stringUtils.cpp",
        "code_diff": "@@ -1,5 +1,5 @@\n#include \"opentimelineio/serializableObject.h\"\n-#if defined(__GNUC__) or defined(__clang__)\n+#if defined(__GNUC__) || defined(__clang__)\n #include <cstdlib>\n #include <memory>\n #include <cxxabi.h>",
        "comments": [],
        "commit_message": "Fix warnings for Visual Studio 2019",
        "commit_id": "628940edb49948565718c6946d7ba534946f6ec4"
    },
    {
        "pr_title": "Fix warnings for Visual Studio 2019",
        "pr_number": 577,
        "file_name": "src/opentimelineio/stringUtils.cpp",
        "code_diff": "@@ -9,7 +9,7 @@\nnamespace opentimelineio { namespace OPENTIMELINEIO_VERSION  {\n     \n-#if defined(__GNUC__) or defined(__clang__)\n+#if defined(__GNUC__) || defined(__clang__)\n std::string cxxabi_demangled_type_name(const char* name) {\n     int status = -4; // some arbitrary value to eliminate the compiler warning",
        "comments": [],
        "commit_message": "Fix warnings for Visual Studio 2019",
        "commit_id": "628940edb49948565718c6946d7ba534946f6ec4"
    },
    {
        "pr_title": "Example RV OTIO reading application plugin",
        "pr_number": 565,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/example_otio_reader_plugin.py",
        "code_diff": "@@ -61,9 +61,9 @@\ndef __init__(self):\n \n     def incoming_source_path(self, event):\n         \"\"\"\n-        Detects if otio file is being loaded, and replaces with\n-        empty movie proc containing an otioFile tag. This will be\n-        replaced in after loading.\n+        Detects if a file supported by otio is being loaded, and replaces\n+        it with an empty movie proc containing an otioFile tag. This will be\n+        replaced in expand_sources().\n         \"\"\"\n         event.reject()",
        "comments": [],
        "commit_message": "Cleaner list comprehension for source RV paths.",
        "commit_id": "2785a654f4c7f8ad9787373b9a05032b60526a17"
    },
    {
        "pr_title": "Example RV OTIO reading application plugin",
        "pr_number": 565,
        "file_name": "contrib/opentimelineio_contrib/application_plugins/rv/example_otio_reader/example_otio_reader_plugin.py",
        "code_diff": "@@ -129,8 +129,8 @@\ndef expand_sources(self):\n                     continue\n \n                 # get the source file name\n-                paths = [info.get('file', '') for info in\n-                         commands.sourceMediaInfoList(src)]\n+                paths = [info['file'] for info in\n+                         commands.sourceMediaInfoList(src) if 'file' in info]\n                 for info_path in paths:\n                     # Looking for: 'blank,otioFile=/foo.otio.movieproc'\n                     parts = info_path.split(\"=\", 1)",
        "comments": [
            {
                "comment": "In line 132 there is a default value of `''` used for paths.  It won't throw an exception here (since line 138 will throw it out), but I think for clarity you could add a:\r\n```python\r\nif not info_path:\r\n  continue\r\n```\r\nto skip those cases altogether.",
                "position": 136
            },
            {
                "comment": "Good catch! I'll fix that.",
                "position": 136
            }
        ],
        "commit_message": "Cleaner list comprehension for source RV paths.",
        "commit_id": "2785a654f4c7f8ad9787373b9a05032b60526a17",
        "fn_context": "    def expand_sources(self):\n        \"\"\"\n        Expand any movie movieproc otioFile sources.\n        \"\"\"\n        # disable caching for load speed\n        cache_mode = commands.cacheMode()\n        commands.setCacheMode(commands.CacheOff)\n\n        try:\n            # find sources with a movieproc with an otioFile=foo.otio tag\n            default_inputs, _ = commands.nodeConnections('defaultSequence')\n            for src in commands.nodesOfType('RVSource'):\n                src_group = commands.nodeGroup(src)\n                if src_group not in default_inputs:\n                    # not in default sequence, already processed\n                    continue\n\n                # get the source file name\n                paths = [info['file'] for info in\n                         commands.sourceMediaInfoList(src) if 'file' in info]\n                for info_path in paths:\n                    # Looking for: 'blank,otioFile=/foo.otio.movieproc'\n                    parts = info_path.split(\"=\", 1)\n                    itype = parts[0]\n                    if not itype.startswith('blank,otioFile'):\n                        continue\n                    # remove the .movieproc extension\n                    path, _ = os.path.splitext(parts[1])\n\n                    # remove temp movieproc source from current view, and all\n                    # the default views\n                    _remove_source_from_views(src_group)\n\n                    result = otio_reader.read_otio_file(path)\n                    commands.setViewNode(result)\n                    break\n        finally:\n            # turn cache mode back on and go back to sleep\n            commands.setCacheMode(cache_mode)\n            self.mode = Mode.sleeping"
    },
    {
        "pr_title": "AAF reader: global_start_timecode fixes",
        "pr_number": 558,
        "file_name": "contrib/opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -32,8 +32,8 @@\nimport sys\n import numbers\n import copy\n-from collections import Iterable\n-from fractions import Fraction\n+import collections\n+import fractions\n import opentimelineio as otio\n \n lib_path = os.environ.get(\"OTIO_AAF_PYTHON_LIB\")",
        "comments": [
            {
                "comment": "Is the default value here needed? Would PhysicalTrackNumber ever be missing?",
                "position": null
            },
            {
                "comment": "I don't expect it would be. We should probably be able to assume the same for `EditRate`, `Segment`, and `Start` too, right? I also noticed that defaulting `Start` to a string `\"0\"` didn't match the files I looked like that had integer values.\r\n\r\nI can do something like:\r\n```python\r\ntry:\r\n    edit_rate = Fraction(aaf_metadata.get[\"EditRate\"])\r\n    start = aaf_metadata[\"Segment\"][\"Start\"]\r\n    physical_track_number = aaf_metadata[\"PhysicalTrackNumber\"]\r\nexcept KeyError:\r\n    continue\r\n```\r\n\r\nThat way we'll do a best-effort without totally dying if we're missing something.",
                "position": null
            },
            {
                "comment": "Since we still have a ways to go in understanding AAF deeply, I think we should throw an exception if we find something we don't expect. That isn't the best thing for the user at that moment, but it forces us to stop, figure out what we misunderstood, and then fix it. So maybe\r\n```\r\n...\r\nexcept KeyError:\r\n  raise AAFAdapterError(\"Timecode missing PhysicalTrackNumber\")\r\n```",
                "position": null
            },
            {
                "comment": "I'm guessing the style in this file is to do the \"from\" style imports but I think I'd prefer it if moving forward we could leave the namespaces in... I still think thats more readable, especially for stuff like this.",
                "position": null
            },
            {
                "comment": "updated code to conform to style guidelines.",
                "position": null
            }
        ],
        "commit_message": "Adjusted from x import y imports so that they retain module namespacing",
        "commit_id": "3decd5958dba48408d5595a8241f569bd7d4c8d3",
        "fn_context": ""
    },
    {
        "pr_title": "AAF reader: global_start_timecode fixes",
        "pr_number": 558,
        "file_name": "contrib/opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -470,7 +470,7 @@\ndef _transcribe(item, parents, editRate, masterMobs):\n     #     elif isinstance(item, pyaaf.AxProperty):\n     #         self.properties['Value'] = str(item.GetValue())\n \n-    elif isinstance(item, Iterable):\n+    elif isinstance(item, collections.Iterable):\n         result = otio.schema.SerializableCollection()\n         for child in item:\n             result.append(",
        "comments": [
            {
                "comment": "Is the default value here needed? Would PhysicalTrackNumber ever be missing?",
                "position": null
            },
            {
                "comment": "I don't expect it would be. We should probably be able to assume the same for `EditRate`, `Segment`, and `Start` too, right? I also noticed that defaulting `Start` to a string `\"0\"` didn't match the files I looked like that had integer values.\r\n\r\nI can do something like:\r\n```python\r\ntry:\r\n    edit_rate = Fraction(aaf_metadata.get[\"EditRate\"])\r\n    start = aaf_metadata[\"Segment\"][\"Start\"]\r\n    physical_track_number = aaf_metadata[\"PhysicalTrackNumber\"]\r\nexcept KeyError:\r\n    continue\r\n```\r\n\r\nThat way we'll do a best-effort without totally dying if we're missing something.",
                "position": null
            },
            {
                "comment": "Since we still have a ways to go in understanding AAF deeply, I think we should throw an exception if we find something we don't expect. That isn't the best thing for the user at that moment, but it forces us to stop, figure out what we misunderstood, and then fix it. So maybe\r\n```\r\n...\r\nexcept KeyError:\r\n  raise AAFAdapterError(\"Timecode missing PhysicalTrackNumber\")\r\n```",
                "position": null
            },
            {
                "comment": "I'm guessing the style in this file is to do the \"from\" style imports but I think I'd prefer it if moving forward we could leave the namespaces in... I still think thats more readable, especially for stuff like this.",
                "position": null
            },
            {
                "comment": "updated code to conform to style guidelines.",
                "position": null
            }
        ],
        "commit_message": "Adjusted from x import y imports so that they retain module namespacing",
        "commit_id": "3decd5958dba48408d5595a8241f569bd7d4c8d3",
        "fn_context": "def _transcribe(item, parents, editRate, masterMobs):\n    result = None\n    metadata = {}\n\n    # First lets grab some standard properties that are present on\n    # many types of AAF objects...\n    metadata[\"Name\"] = _get_name(item)\n    metadata[\"ClassName\"] = _get_class_name(item)\n\n    # Some AAF objects (like TimelineMobSlot) have an edit rate\n    # which should be used for all of the object's children.\n    # We will pass it on to any recursive calls to _transcribe()\n    if hasattr(item, \"edit_rate\"):\n        editRate = float(item.edit_rate)\n\n    if isinstance(item, aaf2.components.Component):\n        metadata[\"Length\"] = item.length\n\n    if isinstance(item, aaf2.core.AAFObject):\n        for prop in item.properties():\n            if hasattr(prop, 'name') and hasattr(prop, 'value'):\n                key = str(prop.name)\n                value = prop.value\n                metadata[key] = _transcribe_property(value)\n\n    # Now we will use the item's class to determine which OTIO type\n    # to transcribe into. Note that the order of this if/elif/... chain\n    # is important, because the class hierarchy of AAF objects is more\n    # complex than OTIO.\n\n    if isinstance(item, aaf2.content.ContentStorage):\n        result = otio.schema.SerializableCollection()\n\n        # Gather all the Master Mobs, so we can find them later by MobID\n        # when we parse the SourceClips in the composition\n        if masterMobs is None:\n            masterMobs = {}\n        for mob in item.mastermobs():\n            child = _transcribe(mob, parents + [item], editRate, masterMobs)\n            if child is not None:\n                mobID = child.metadata.get(\"AAF\", {}).get(\"MobID\")\n                masterMobs[mobID] = child\n\n        for mob in item.compositionmobs():\n            child = _transcribe(mob, parents + [item], editRate, masterMobs)\n            _add_child(result, child, mob)\n\n    elif isinstance(item, aaf2.mobs.Mob):\n        result = otio.schema.Timeline()\n\n        for slot in item.slots:\n            track = _transcribe(slot, parents + [item], editRate, masterMobs)\n            _add_child(result.tracks, track, slot)\n\n            # Use a heuristic to find the starting timecode from\n            # this track and use it for the Timeline's global_start_time\n            start_time = _find_timecode_track_start(track)\n            if start_time:\n                result.global_start_time = start_time\n\n    elif isinstance(item, aaf2.components.SourceClip):\n        result = otio.schema.Clip()\n\n        # Evidently the last mob is the one with the timecode\n        mobs = _find_timecode_mobs(item)\n        # Get the Timecode start and length values\n        last_mob = mobs[-1] if mobs else None\n        timecode_info = _extract_timecode_info(last_mob) if last_mob else None\n\n        source_start = int(metadata.get(\"StartTime\", \"0\"))\n        source_length = item.length\n        media_start = source_start\n        media_length = item.length\n\n        if timecode_info:\n            media_start, media_length = timecode_info\n            source_start += media_start\n\n        # The goal here is to find a source range. Actual editorial opinions are\n        # found on SourceClips in the CompositionMobs. To figure out whether this\n        # clip is directly in the CompositionMob, we detect if our parent mobs\n        # are only CompositionMobs. If they were anything else - a MasterMob, a\n        # SourceMob, we would know that this is in some indirect relationship.\n        parent_mobs = filter(lambda parent: isinstance(parent, aaf2.mobs.Mob), parents)\n        is_directly_in_composition = all(\n            isinstance(mob, aaf2.mobs.CompositionMob)\n            for mob in parent_mobs\n        )\n        if is_directly_in_composition:\n            result.source_range = otio.opentime.TimeRange(\n                otio.opentime.RationalTime(source_start, editRate),\n                otio.opentime.RationalTime(source_length, editRate)\n            )\n\n        # The goal here is to find an available range. Media ranges are stored\n        # in the related MasterMob, and there should only be one - hence the name\n        # \"Master\" mob. Somewhere down our chain (either a child or our parents)\n        # is a MasterMob. For SourceClips in the CompositionMob, it is our child.\n        # For everything else, it is a previously encountered parent. Find the\n        # MasterMob in our chain, and then extract the information from that.\n        child_mastermob = (\n            item.mob if isinstance(item.mob, aaf2.mobs.MasterMob) else None\n        )\n        parent_mastermobs = [\n            parent for parent in parents\n            if isinstance(parent, aaf2.mobs.MasterMob)\n        ]\n        parent_mastermob = parent_mastermobs[0] if len(parent_mastermobs) > 1 else None\n        mastermob = child_mastermob or parent_mastermob or None\n\n        if mastermob:\n            media = otio.schema.MissingReference()\n            media.available_range = otio.opentime.TimeRange(\n                otio.opentime.RationalTime(media_start, editRate),\n                otio.opentime.RationalTime(media_length, editRate)\n            )\n            # copy the metadata from the master into the media_reference\n            mastermob_child = masterMobs.get(str(mastermob.mob_id))\n            media.metadata[\"AAF\"] = mastermob_child.metadata.get(\"AAF\", {})\n            result.media_reference = media\n\n    elif isinstance(item, aaf2.components.Transition):\n        result = otio.schema.Transition()\n\n        # Does AAF support anything else?\n        result.transition_type = otio.schema.TransitionTypes.SMPTE_Dissolve\n\n        # Extract value and time attributes of both ControlPoints used for\n        # creating AAF Transition objects\n        varying_value = None\n        for param in item.getvalue('OperationGroup').parameters:\n            if isinstance(param, aaf2.misc.VaryingValue):\n                varying_value = param\n                break\n\n        if varying_value is not None:\n            for control_point in varying_value.getvalue('PointList'):\n                value = control_point.value\n                time = control_point.time\n                metadata.setdefault('PointList', []).append({'Value': value,\n                                                             'Time': time})\n\n        in_offset = int(metadata.get(\"CutPoint\", \"0\"))\n        out_offset = item.length - in_offset\n        result.in_offset = otio.opentime.RationalTime(in_offset, editRate)\n        result.out_offset = otio.opentime.RationalTime(out_offset, editRate)\n\n    elif isinstance(item, aaf2.components.Filler):\n        result = otio.schema.Gap()\n\n        length = item.length\n        result.source_range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(0, editRate),\n            otio.opentime.RationalTime(length, editRate)\n        )\n\n    elif isinstance(item, aaf2.components.NestedScope):\n        # TODO: Is this the right class?\n        result = otio.schema.Stack()\n\n        for slot in item.slots:\n            child = _transcribe(slot, parents + [item], editRate, masterMobs)\n            _add_child(result, child, slot)\n\n    elif isinstance(item, aaf2.components.Sequence):\n        result = otio.schema.Track()\n\n        for component in item.components:\n            child = _transcribe(component, parents + [item], editRate, masterMobs)\n            _add_child(result, child, component)\n\n    elif isinstance(item, aaf2.components.OperationGroup):\n        result = _transcribe_operation_group(\n            item, parents, metadata, editRate, masterMobs\n        )\n\n    elif isinstance(item, aaf2.mobslots.TimelineMobSlot):\n        result = otio.schema.Track()\n\n        child = _transcribe(item.segment, parents + [item], editRate, masterMobs)\n        _add_child(result, child, item.segment)\n\n    elif isinstance(item, aaf2.mobslots.MobSlot):\n        result = otio.schema.Track()\n\n        child = _transcribe(item.segment, parents + [item], editRate, masterMobs)\n        _add_child(result, child, item.segment)\n\n    elif isinstance(item, aaf2.components.Timecode):\n        pass\n\n    elif isinstance(item, aaf2.components.Pulldown):\n        pass\n\n    elif isinstance(item, aaf2.components.EdgeCode):\n        pass\n\n    elif isinstance(item, aaf2.components.ScopeReference):\n        # TODO: is this like FILLER?\n\n        result = otio.schema.Gap()\n\n        length = item.length\n        result.source_range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(0, editRate),\n            otio.opentime.RationalTime(length, editRate)\n        )\n\n    elif isinstance(item, aaf2.components.DescriptiveMarker):\n\n        # Markers come in on their own separate Track.\n        # TODO: We should consolidate them onto the same track(s) as the clips\n        # result = otio.schema.Marker()\n        pass\n\n    elif isinstance(item, aaf2.components.Selector):\n        # If you mute a clip in media composer, it becomes one of these in the\n        # AAF.\n        result = _transcribe(\n            item.getvalue(\"Selected\"),\n            parents + [item],\n            editRate,\n            masterMobs\n        )\n\n        alternates = [\n            _transcribe(alt, parents + [item], editRate, masterMobs)\n            for alt in item.getvalue(\"Alternates\")\n        ]\n\n        # muted case -- if there is only one item its muted, otherwise its\n        # a multi cam thing\n        if alternates and len(alternates) == 1:\n            metadata['muted_clip'] = True\n            result.name = str(alternates[0].name) + \"_MUTED\"\n\n        metadata['alternates'] = alternates\n\n    # @TODO: There are a bunch of other AAF object types that we will\n    # likely need to add support for. I'm leaving this code here to help\n    # future efforts to extract the useful information out of these.\n\n    # elif isinstance(item, aaf.storage.File):\n    #     self.extendChildItems([item.header])\n\n    # elif isinstance(item, aaf.storage.Header):\n    #     self.extendChildItems([item.storage()])\n    #     self.extendChildItems([item.dictionary()])\n\n    # elif isinstance(item, aaf.dictionary.Dictionary):\n    #     l = []\n    #     l.append(DummyItem(list(item.class_defs()), 'ClassDefs'))\n    #     l.append(DummyItem(list(item.codec_defs()), 'CodecDefs'))\n    #     l.append(DummyItem(list(item.container_defs()), 'ContainerDefs'))\n    #     l.append(DummyItem(list(item.data_defs()), 'DataDefs'))\n    #     l.append(DummyItem(list(item.interpolation_defs()),\n    #        'InterpolationDefs'))\n    #     l.append(DummyItem(list(item.klvdata_defs()), 'KLVDataDefs'))\n    #     l.append(DummyItem(list(item.operation_defs()), 'OperationDefs'))\n    #     l.append(DummyItem(list(item.parameter_defs()), 'ParameterDefs'))\n    #     l.append(DummyItem(list(item.plugin_defs()), 'PluginDefs'))\n    #     l.append(DummyItem(list(item.taggedvalue_defs()), 'TaggedValueDefs'))\n    #     l.append(DummyItem(list(item.type_defs()), 'TypeDefs'))\n    #     self.extendChildItems(l)\n    #\n    #     elif isinstance(item, pyaaf.AxSelector):\n    #         self.extendChildItems(list(item.EnumAlternateSegments()))\n    #\n    #     elif isinstance(item, pyaaf.AxScopeReference):\n    #         #print item, item.GetRelativeScope(),item.GetRelativeSlot()\n    #         pass\n    #\n    #     elif isinstance(item, pyaaf.AxEssenceGroup):\n    #         segments = []\n    #\n    #         for i in xrange(item.CountChoices()):\n    #             choice = item.GetChoiceAt(i)\n    #             segments.append(choice)\n    #         self.extendChildItems(segments)\n    #\n    #     elif isinstance(item, pyaaf.AxProperty):\n    #         self.properties['Value'] = str(item.GetValue())\n\n    elif isinstance(item, collections.Iterable):\n        result = otio.schema.SerializableCollection()\n        for child in item:\n            result.append(\n                _transcribe(\n                    child,\n                    parents + [item],\n                    editRate,\n                    masterMobs\n                )\n            )\n    else:\n        # For everything else, we just ignore it.\n        # To see what is being ignored, turn on the debug flag\n        if debug:\n            print(\"SKIPPING: {}: {} -- {}\".format(type(item), item, result))\n\n    # Did we get anything? If not, we're done\n    if result is None:\n        return None\n\n    # Okay, now we've turned the AAF thing into an OTIO result\n    # There's a bit more we can do before we're ready to return the result.\n\n    # If we didn't get a name yet, use the one we have in metadata\n    if not result.name:\n        result.name = metadata[\"Name\"]\n\n    # Attach the AAF metadata\n    if not result.metadata:\n        result.metadata.clear()\n    result.metadata[\"AAF\"] = metadata\n\n    # Double check that we got the length we expected\n    if isinstance(result, otio.core.Item):\n        length = metadata.get(\"Length\")\n        if (\n                length\n                and result.source_range is not None\n                and result.source_range.duration.value != length\n        ):\n            raise AAFAdapterError(\n                \"Wrong duration? {} should be {} in {}\".format(\n                    result.source_range.duration.value,\n                    length,\n                    result\n                )\n            )\n\n    # Did we find a Track?\n    if isinstance(result, otio.schema.Track):\n        # Try to figure out the kind of Track it is\n        if hasattr(item, 'media_kind'):\n            media_kind = str(item.media_kind)\n            result.metadata[\"AAF\"][\"MediaKind\"] = media_kind\n            if media_kind == \"Picture\":\n                result.kind = otio.schema.TrackKind.Video\n            elif media_kind in (\"SoundMasterTrack\", \"Sound\"):\n                result.kind = otio.schema.TrackKind.Audio\n            else:\n                # Timecode, Edgecode, others?\n                result.kind = \"\"\n\n    # Done!\n    return result"
    },
    {
        "pr_title": "Fix installing in user prefix when --user is specified",
        "pr_number": 545,
        "file_name": "setup.py",
        "code_diff": "@@ -20,6 +20,7 @@\nimport setuptools.command.build_ext\n import setuptools.command.build_py\n from setuptools.command.install import install\n+from distutils.sysconfig import get_python_lib\n from distutils.version import LooseVersion\n import distutils",
        "comments": [
            {
                "comment": "I know this is far from being elegant, but it looks like setuptools is not providing the information in a clean way.",
                "position": 23
            },
            {
                "comment": "To keep platform agnostic, this line should change to:\r\n`os.path.join(_ctx.install_usersite, \"opentimelineio\", \"cxx-libs\")`",
                "position": null
            },
            {
                "comment": "do these slashes still work on windows?  or do we need to switch to an `os.path.sep.join` kind of thing?",
                "position": 23
            }
        ],
        "commit_message": "setup: Fix the way we get the site-packages directory\n\nUsing `get_python_lib()` seems to be the correct way to get the\nsite-packages directory for installation",
        "commit_id": "5804949093abe5154b4d0765a28a8456995e92af",
        "fn_context": ""
    },
    {
        "pr_title": "xges: Fix tracks ordering",
        "pr_number": 538,
        "file_name": "contrib/opentimelineio_contrib/adapters/xges.py",
        "code_diff": "@@ -307,7 +307,8 @@\ndef to_otio(self):\n     def _add_layers(self, timeline, otio_timeline, all_names):\n         for layer in timeline.findall(\"./layer\"):\n             tracks = self._build_tracks_from_layer_clips(layer, all_names)\n-            otio_timeline.tracks.extend(tracks)\n+            for track in tracks:\n+                otio_timeline.tracks.insert(0, track)\n \n     def _get_clips_for_type(self, clips, track_type):\n         if not clips:",
        "comments": [],
        "commit_message": "xges: Fix tracks ordering\n\nThe tracks should be loaded in reverse order for the XGES format.\n\nAlso in GES, layers (tracks in otio terms) do not have a track type\n(tracks are global to the timeline which formalizes the outputed\ntypes), thus we should try to keep audio and video tracks in the same\norder during deserialization.",
        "commit_id": "480795efcb7e360d153de38c5404ee57fa7fa873"
    },
    {
        "pr_title": "fcpx: Decode output of ffprobe before using it as string",
        "pr_number": 537,
        "file_name": "contrib/opentimelineio_contrib/adapters/tests/test_fcpx_adapter.py",
        "code_diff": "@@ -1,7 +1,23 @@\nimport os\n+import subprocess\n+import sys\n import unittest\n import opentimelineio as otio\n import opentimelineio.test_utils as otio_test_utils\n+from opentimelineio_contrib.adapters.fcpx_xml import format_name\n+\n+try:\n+    # Python 3.3 forward includes the mock module\n+    from unittest import mock\n+    could_import_mock = True\n+except ImportError:\n+    # Fallback for older python (not included in standard library)\n+    try:\n+        import mock\n+        could_import_mock = True\n+    except ImportError:\n+        # Mock appears to not be installed\n+        could_import_mock = False\n \n SAMPLE_LIBRARY_XML = os.path.join(\n     os.path.dirname(__file__),",
        "comments": [],
        "commit_message": "fcpx: Decode output of ffprobe before using it as string\n\nGetting this stack trace otherwise:\n\n```\nTraceback (most recent call last):\n  File \"gesotioformatter.py\", line 44, in do_save_to_uri\n  File \"/home/thiblahute/devel/gstreamer/gst-build/subprojects/OpenTimelineIO/opentimelineio/adapters/__init__.py\", line 194, in write_to_file\n    **adapter_argument_map\n  File \"/home/thiblahute/devel/gstreamer/gst-build/subprojects/OpenTimelineIO/opentimelineio/adapters/adapter.py\", line 179, in write_to_file\n    result = self.write_to_string(input_otio, **adapter_argument_map)\n  File \"/home/thiblahute/devel/gstreamer/gst-build/subprojects/OpenTimelineIO/opentimelineio/adapters/adapter.py\", line 243, in write_to_string\n    **adapter_argument_map\n  File \"/home/thiblahute/devel/gstreamer/gst-build/subprojects/OpenTimelineIO/opentimelineio/plugins/python_plugin.py\", line 128, in _execute_function\n    return (getattr(self.module(), func_name)(**kwargs))\n  File \"/home/thiblahute/devel/gstreamer/gst-build/subprojects/OpenTimelineIO/opentimelineio_contrib/adapters/fcpx_xml.py\", line 1182, in write_to_string\n    return FcpxOtio(input_otio).to_xml()\n  File \"/home/thiblahute/devel/gstreamer/gst-build/subprojects/OpenTimelineIO/opentimelineio_contrib/adapters/fcpx_xml.py\", line 193, in to_xml\n    top_sequence = self._stack_to_sequence(project.tracks)\n  File \"/home/thiblahute/devel/gstreamer/gst-build/subprojects/OpenTimelineIO/opentimelineio_contrib/adapters/fcpx_xml.py\", line 265, in _stack_to_sequence\n    self._track_for_spine(track, idx, spine, compound_clip)\n  File \"/home/thiblahute/devel/gstreamer/gst-build/subprojects/OpenTimelineIO/opentimelineio_contrib/adapters/fcpx_xml.py\", line 279, in _track_for_spine\n    compound=compound\n  File \"/home/thiblahute/devel/gstreamer/gst-build/subprojects/OpenTimelineIO/opentimelineio_contrib/adapters/fcpx_xml.py\", line 379, in _element_for_item\n    asset_id = self._add_asset(item, compound_only=compound)\n  File \"/home/thiblahute/devel/gstreamer/gst-build/subprojects/OpenTimelineIO/opentimelineio_contrib/adapters/fcpx_xml.py\", line 560, in _add_asset\n    format_element = self._find_or_create_format_from(clip)\n  File \"/home/thiblahute/devel/gstreamer/gst-build/subprojects/OpenTimelineIO/opentimelineio_contrib/adapters/fcpx_xml.py\", line 556, in _find_or_create_format_from\n    format_element.set(\"name\", self._clip_format_name(clip))\n  File \"/home/thiblahute/devel/gstreamer/gst-build/subprojects/OpenTimelineIO/opentimelineio_contrib/adapters/fcpx_xml.py\", line 537, in _clip_format_name\n    clip.media_reference.target_url\n  File \"/home/thiblahute/devel/gstreamer/gst-build/subprojects/OpenTimelineIO/opentimelineio_contrib/adapters/fcpx_xml.py\", line 95, in format_name\n    if \"1920\" in frame_size:\nTypeError: a bytes-like object is required, not 'str'\n```\n\nAnd add a unit test (the mock avalaibility logic comes from\ntest_detection_plugin.py)",
        "commit_id": "dd8f4893f47d148c91705741e24942d0f5ee6069"
    },
    {
        "pr_title": "Add support for more elements and XMLs without libraries",
        "pr_number": 515,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -24,156 +24,299 @@\n\"\"\"OpenTimelineIO Final Cut Pro 7 XML Adapter.\"\"\"\n \n-import os\n-import math\n-import functools\n import collections\n+import functools\n+import itertools\n+import math\n+import os\n+import re\n from xml.etree import cElementTree\n from xml.dom import minidom\n \n-# deal with renaming of default library from python 2 / 3\n+# urlparse's name changes in Python 3\n try:\n+    # Python 2.7\n     import urlparse as urllib_parse\n except ImportError:\n+    # Python 3\n     import urllib.parse as urllib_parse\n \n-import opentimelineio as otio\n+# Same with the ABC classes from collections\n+try:\n+    # Python 3\n+    from collections.abc import Mapping\n+except ImportError:\n+    # Python 2.7\n+    from collections import Mapping\n+\n+from opentimelineio import (\n+    core,\n+    opentime,\n+    schema,\n+)\n+\n+# namespace to use for metadata\n+META_NAMESPACE = 'fcp_xml'\n \n-META_NAMESPACE = 'fcp_xml'  # namespace to use for metadata\n+# Regex to match identifiers like clipitem-22\n+ID_RE = re.compile(r\"^(?P<tag>[a-zA-Z]*)-(?P<id>\\d*)$\")\n \n \n # ---------\n # utilities\n # ---------\n \n+\n+class _Context(Mapping):\n+    \"\"\"\n+    An inherited value context.\n+\n+    In FCP XML there is a concept of inheritance down the element heirarchy.\n+    For instance, a ``clip`` element may not specify the ``rate`` locally, but\n+    instead inherit it from the parent ``track`` element.\n+\n+    This object models that as a stack of elements. When a value needs to be\n+    queried from the context, it will be gathered by walking from the top of\n+    the stack until the value is found.\n+\n+    For example, to find the ``rate`` element as an immediate child most\n+    appropriate to the current context, you would do something like::\n+        ``my_current_context[\"./rate\"]``\n+\n+    This object can be thought of as immutable. You get a new context when you\n+    push an element. This prevents inadvertant tampering with parent contexts\n+    that may be used at levels above.\n+\n+    This DOES NOT support ``id`` attribute dereferencing, please make sure to\n+    do that prior to using this structure.\n+\n+    .. seealso:: https://developer.apple.com/library/archive/documentation\\\n+            /AppleApplications/Reference/FinalCutPro_XML/Basics/Basics.html#\\\n+            //apple_ref/doc/uid/TP30001154-TPXREF102\n+    \"\"\"\n+\n+    def __init__(self, element=None, parent_elements=None):\n+        if parent_elements is not None:\n+            self.elements = parent_elements[:]\n+        else:\n+            self.elements = []\n+\n+        if element is not None:\n+            self.elements.append(element)\n+\n+    def _all_keys(self):\n+        \"\"\"\n+        Returns a set of all the keys available in the context stack.\n+        \"\"\"\n+        return set(\n+            itertools.chain.fromiterable(e.keys() for e in self.elements)\n+        )\n+\n+    def __getitem__(self, key):\n+        # Walk down the contexts until the item is found\n+        for element in reversed(self.elements):\n+            found_element = element.find(key)\n+            if found_element is not None:\n+                return found_element\n+\n+        raise KeyError(key)\n+\n+    def __iter__(self):\n+        # This is unlikely to be used, so we'll do it the expensive way\n+        return iter(self._all_keys)\n+\n+    def __len__(self):\n+        # This is unlikely to be used, so we'll do it the expensive way\n+        return len(self._all_keys)\n+\n+    def context_pushing_element(self, element):\n+        \"\"\"\n+        Pushes an element to the top of the stack.\n+\n+        :param element: Element to push to the stack.\n+        :return: The new context with the provided element pushed to the top\n+            of the stack.\n+        :raises: :class:`ValueError` if the element is already in the stack.\n+        \"\"\"\n+        for context_element in self.elements:\n+            if context_element == element:\n+                raise ValueError(\n+                    \"element {} already in context\".format(element)\n+                )\n+\n+        return _Context(element, self.elements)\n+\n+\n def _url_to_path(url):\n     parsed = urllib_parse.urlparse(url)\n     return parsed.path\n \n \n-def _populate_element_map(elem, element_map):\n-    if 'id' in elem.attrib and list(elem):\n-        element_map[elem.tag].setdefault(elem.attrib['id'], elem)\n-    for sub_elem in elem:\n-        _populate_element_map(sub_elem, element_map)\n+def _bool_value(element):\n+    \"\"\"\n+    Given an xml element, returns the tag text converted to a bool.\n \n+    :param element: The element to fetch the value from.\n \n-def _resolved_backreference(elem, tag, element_map):\n-    if 'id' in elem.attrib:\n-        elem = element_map[tag].setdefault(elem.attrib['id'], elem)\n+    :return: A boolean.\n+    \"\"\"\n+    return (element.text.lower() == \"true\")\n \n-    return elem\n \n+def _element_identification_string(element):\n+    \"\"\"\n+    Gets a string that will hopefully help in identifing an element when there\n+    is an error.\n+    \"\"\"\n+    info_string = \"tag: {}\".format(element.tag)\n+    try:\n+        elem_id = element.attrib[\"id\"]\n+        info_string += \" id: {}\".format(elem_id)\n+    except KeyError:\n+        pass\n \n-def _populate_backreference_map(item, br_map):\n-    if isinstance(item, otio.core.MediaReference):\n-        tag = 'file'\n-    elif isinstance(item, otio.schema.Track):\n-        tag = 'sequence'\n-    else:\n-        tag = None\n+    return info_string\n \n-    if isinstance(item, otio.schema.ExternalReference):\n-        item_hash = hash(str(item.target_url))\n-    elif isinstance(item, otio.schema.MissingReference):\n-        item_hash = 'missing_ref'\n-    else:\n-        item_hash = hash(id(item))\n-\n-    # skip unspecified tags\n-    if tag is not None:\n-        br_map[tag].setdefault(\n-            item_hash,\n-            1 if not br_map[tag] else max(br_map[tag].values()) + 1\n-        )\n-\n-    # populate children\n-    if isinstance(item, otio.schema.Timeline):\n-        for sub_item in item.tracks:\n-            _populate_backreference_map(sub_item, br_map)\n-    elif isinstance(\n-        item,\n-        (otio.schema.Clip, otio.schema.Gap, otio.schema.Transition)\n-    ):\n-        pass\n+\n+def _name_from_element(element):\n+    \"\"\"\n+    Fetches the name from the ``name`` element child of the provided element.\n+    If no element exists, returns ``None``.\n+\n+    :param element: The element to find the name for.\n+\n+    :return: The name string or ``None``\n+    \"\"\"\n+    name_elem = element.find(\"./name\")\n+    if name_elem is not None:\n+        return name_elem.text\n+\n+    return None\n+\n+\n+def _rate_for_element(element):\n+    \"\"\"\n+    Takes an FCP rate element and returns a rate to use with otio.\n+\n+    :param element: An FCP rate element.\n+\n+    :return: The float rate.\n+    \"\"\"\n+    # rate is encoded as a timebase (int) which can be drop-frame\n+    base = float(element.find(\"./timebase\").text)\n+    if _bool_value(element.find(\"./ntsc\")):\n+        base *= 1000.0 / 1001\n+\n+    return base\n+\n+\n+def _rate_from_context(context):\n+    \"\"\"\n+    Given the context object, gets the appropriate rate.\n+\n+    :param context: The :class:`_Context` instance to find the rate in.\n+\n+    :return: The rate value or ``None`` if no rate is available in the context.\n+    \"\"\"\n+    try:\n+        rate_element = context[\"./rate\"]\n+    except KeyError:\n+        return None\n+\n+    return _rate_for_element(rate_element)\n+\n+\n+def _time_from_timecode_element(tc_element, context=None):\n+    \"\"\"\n+    Given a timecode xml element, returns the time that represents.\n+\n+    .. todo:: Non Drop-Frame timecode is not yet supported by OTIO.\n+\n+    :param tc_element: The ``timecode`` element.\n+    :param context: The context dict under which this timecode is being gotten.\n+\n+    :return: The :class:`opentime.RationalTime` representation of the\n+        timecode.\n+    \"\"\"\n+    if context is not None:\n+        local_context = context.context_pushing_element(tc_element)\n     else:\n-        for sub_item in item:\n-            _populate_backreference_map(sub_item, br_map)\n+        local_context = _Context(tc_element)\n \n+    # Resolve the rate\n+    rate = _rate_from_context(local_context)\n \n-def _backreference_build(tag):\n-    # We can also encode these back-references if an item is accessed multiple\n-    # times. To do this we store an id attribute on the element. For back-\n-    # references we then only need to return an empty element of that type with\n-    # the id we logged before\n+    # Try using the display format and frame number\n+    frame = tc_element.find(\"./frame\")\n \n-    def singleton_decorator(func):\n-        @functools.wraps(func)\n-        def wrapper(item, *args, **kwargs):\n-            br_map = args[-1]\n-            if isinstance(item, otio.schema.ExternalReference):\n-                item_hash = hash(str(item.target_url))\n-            elif isinstance(item, otio.schema.MissingReference):\n-                item_hash = 'missing_ref'\n-            else:\n-                item_hash = item.__hash__()\n-            item_id = br_map[tag].get(item_hash, None)\n-            if item_id is not None:\n-                return cElementTree.Element(\n-                    tag,\n-                    id='{}-{}'.format(tag, item_id)\n-                )\n-            item_id = br_map[tag].setdefault(\n-                item_hash,\n-                1 if not br_map[tag] else max(br_map[tag].values()) + 1\n-            )\n-            elem = func(item, *args, **kwargs)\n-            elem.attrib['id'] = '{}-{}'.format(tag, item_id)\n-            return elem\n+    # Use frame number, if available\n+    if frame is not None:\n+        frame_num = int(frame.text)\n+        return opentime.RationalTime(frame_num, rate)\n \n-        return wrapper\n+    # If a TC string is provided, parse rate from it\n+    tc_string_element = tc_element.find(\"./string\")\n+    if tc_string_element is None:\n+        raise ValueError(\"Timecode element missing required elements\")\n \n-    return singleton_decorator\n+    tc_string = tc_string_element.text\n \n+    return opentime.from_timecode(tc_string, rate)\n \n-def _insert_new_sub_element(into_parent, tag, attrib=None, text=''):\n-    elem = cElementTree.SubElement(into_parent, tag, **attrib or {})\n-    elem.text = text\n-    return elem\n \n+def _track_kind_from_element(media_element):\n+    \"\"\"\n+    Given an FCP XML media sub-element, returns an appropriate\n+    :class:`schema.TrackKind` value corresponding to that media type.\n \n-def _get_top_level_tracks(elem):\n-    top_level_tracks = elem.findall('./sequence')\n-    for sub_elem in elem:\n-        if sub_elem.tag in ('sequence', 'clip'):\n-            continue\n-        top_level_tracks.extend(_get_top_level_tracks(sub_elem))\n-    return top_level_tracks\n+    :param media_element: An XML element that is a child of the ``media`` tag.\n \n+    :return: The corresponding :class`schema.TrackKind` value.\n+    :raises: :class:`ValueError` When the media type is unsupported.\n+    \"\"\"\n+    element_tag = media_element.tag.lower()\n+    if element_tag == \"audio\":\n+        return schema.TrackKind.Audio\n+    elif element_tag == \"video\":\n+        return schema.TrackKind.Video\n \n-def _make_pretty_string(tree_e):\n-    # most of the parsing in this adapter is done with cElementTree because it\n-    # is simpler and faster. However, the string representation it returns is\n-    # far from elegant. Therefor we feed it through minidom to provide an xml\n-    # with indentations.\n-    string = cElementTree.tostring(tree_e, encoding=\"UTF-8\", method=\"xml\")\n-    dom = minidom.parseString(string)\n-    return dom.toprettyxml(indent='    ')\n+    raise ValueError(\"Unsupported media kind: {}\".format(media_element.tag))\n \n \n def _is_primary_audio_channel(track):\n-    # audio may be structured in stereo where each channel occupies a separate\n-    # track. Some xml logic combines these into a single track upon import.\n-    # Here we check whether we`re dealing with the first audio channel\n-    return track.attrib.get('currentExplodedTrackIndex', '0') == '0' or \\\n-        track.attrib.get('totalExplodedTrackCount', '1') == '1'\n+    \"\"\"\n+    Determines whether or not this is the \"primary\" audio track.\n+\n+    audio may be structured in stereo where each channel occupies a separate\n+    track. This importer keeps stereo pairs ganged together as a single track.\n+\n+    :param track: An XML track element.\n+\n+    :return: A boolean ``True`` if this is the first track.\n+    \"\"\"\n+    exploded_index = track.attrib.get('currentExplodedTrackIndex', '0')\n+    exploded_count = track.attrib.get('totalExplodedTrackCount', '1')\n+\n+    return (exploded_index == '0' or exploded_count == '1')\n+\n+\n+def _transition_cut_point(transition_item, context):\n+    \"\"\"\n+    Returns the end time at which the transition progresses from one clip to\n+    the next.\n \n+    :param transition_item: The XML element for the transition.\n+    :param context: The context dictionary applying to this transition.\n \n-def _get_transition_cut_point(transition_item, element_map):\n+    :return: The :class:`opentime.RationalTime` the transition cuts at.\n+    \"\"\"\n     alignment = transition_item.find('./alignment').text\n     start = int(transition_item.find('./start').text)\n     end = int(transition_item.find('./end').text)\n-    rate = _parse_rate(transition_item, element_map)\n+\n+    # start/end time is in the parent context's rate\n+    local_context = context.context_pushing_element(transition_item)\n+    rate = _rate_from_context(local_context)\n \n     if alignment in ('end', 'end-black'):\n         value = end",
        "comments": [],
        "commit_message": "Merge branch 'master' of https://github.com/PixarAnimationStudios/OpenTimelineIO into fcpx_enhancements",
        "commit_id": "858f308f6f6d2d5a89158e1739642a50fc7241f3"
    },
    {
        "pr_title": "Add support for more elements and XMLs without libraries",
        "pr_number": 515,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -184,373 +327,1030 @@\ndef _get_transition_cut_point(transition_item, element_map):\n     else:\n         value = int((start + end) / 2)\n \n-    return otio.opentime.RationalTime(value, rate)\n-\n-\n-# -----------------------\n-# parsing single track\n-# -----------------------\n+    return opentime.RationalTime(value, rate)\n \n-def _parse_rate(elem, element_map):\n-    elem = _resolved_backreference(elem, 'all_elements', element_map)\n \n-    rate = elem.find('./rate')\n-    # rate is encoded as a timebase (int) which can be drop-frame\n-    base = float(rate.find('./timebase').text)\n-    if rate.find('./ntsc').text == 'TRUE':\n-        base *= .999\n-    return base\n+def _xml_tree_to_dict(node, ignore_tags=None, omit_timing=True):\n+    \"\"\"\n+    Translates the tree under a provided node mapping to a dictionary/list\n+    representation. XML tag attributes are placed in the dictionary with an\n+    ``@`` prefix.\n \n+    .. note:: In addition to the provided ignore tags, this filters a subset of\n+    timing metadata such as ``frame`` and ``string`` elements within timecode\n+    elements.\n \n-def _parse_media_reference(file_e, element_map):\n-    file_e = _resolved_backreference(file_e, 'file', element_map)\n+    .. warning:: This scheme does not allow for leaf elements to have\n+    attributes.  for the moment this doesn't seem to be an issue.\n \n-    url = file_e.find('./pathurl').text\n-    file_rate = _parse_rate(file_e, element_map)\n-    timecode_rate = _parse_rate(file_e.find('./timecode'), element_map)\n+    :param node: The root xml element to express childeren of in the\n+        dictionary.\n+    :param ignore_tags: A collection of tagnames to skip when converting.\n+    :param omit_timing: If ``True``, omits timing-specific tags.\n \n-    frame_e = file_e.find('./timecode/frame')\n-    if frame_e is not None:\n-        start_time = otio.opentime.RationalTime(\n-            int(frame_e.text),\n-            timecode_rate)\n-    else:\n-        start_time = otio.opentime.from_timecode(\n-            file_e.find('./timecode/string').text,\n-            timecode_rate)\n-\n-    duration_e = file_e.find('./duration')\n-    if duration_e is not None:\n-        duration = otio.opentime.RationalTime(\n-            int(duration_e.text),\n-            file_rate)\n+    :return: The dictionary representation.\n+    \"\"\"\n+    if node.tag == \"timecode\":\n+        additional_ignore_tags = {\"frame\", \"string\"}\n     else:\n-        duration = otio.opentime.RationalTime(0, file_rate)\n+        additional_ignore_tags = tuple()\n \n-    available_range = otio.opentime.TimeRange(\n-        start_time=start_time,\n-        duration=duration\n-    )\n+    out_dict = collections.OrderedDict()\n \n-    return otio.schema.ExternalReference(\n-        target_url=url.strip(),\n-        available_range=available_range\n+    # Handle the attributes\n+    out_dict.update(\n+        collections.OrderedDict(\n+            (\"@{}\".format(k), v) for k, v in node.attrib.items()\n+        )\n     )\n \n+    # Now traverse the child tags\n+    encountered_tags = set()\n+    list_tags = set()\n+    for info_node in node:\n+        # Skip tags we were asked to omit\n+        node_tag = info_node.tag\n+        if ignore_tags and node_tag in ignore_tags:\n+            continue\n \n-def _parse_clip_item_without_media(clip_item, track_rate,\n-                                   transition_offsets, element_map):\n-    markers = clip_item.findall('./marker')\n-    rate = _parse_rate(clip_item, element_map)\n+        # Skip some special case tags related to timing information\n+        if node_tag in additional_ignore_tags:\n+            continue\n \n-    # transition offsets are provided in timeline rate. If they deviate they\n-    # need to be rescaled to clip item rate\n-    context_transition_offsets = [\n-        transition_offsets[0].rescaled_to(rate),\n-        transition_offsets[1].rescaled_to(rate)\n-    ]\n+        # If there are children, make this a sub-dictionary by recursing\n+        if len(info_node):\n+            node_value = _xml_tree_to_dict(info_node)\n+        else:\n+            node_value = info_node.text\n+\n+        # If we've seen this node before, then treat it as a list\n+        if node_tag in list_tags:\n+            # We've established that this tag is a list, append to that\n+            out_dict[node_tag].append(node_value)\n+        elif node_tag in encountered_tags:\n+            # This appears to be a list we didn't know about, convert\n+            out_dict[node_tag] = [\n+                out_dict[node_tag], node_value\n+            ]\n+            list_tags.add(node_tag)\n+        else:\n+            # Store the value\n+            out_dict[node_tag] = node_value\n+            encountered_tags.add(node_tag)\n \n-    in_value = int(float(clip_item.find('./in').text))\n-    in_frame = in_value + int(round(context_transition_offsets[0].value))\n+    return out_dict\n \n-    out_value = int(float(clip_item.find('./out').text))\n-    out_frame = out_value - int(round(context_transition_offsets[1].value))\n \n-    source_range = otio.opentime.TimeRange(\n-        start_time=otio.opentime.RationalTime(in_frame, track_rate),\n-        duration=otio.opentime.RationalTime(\n-            out_frame - in_frame,\n-            track_rate\n-        )\n-    )\n+def _dict_to_xml_tree(data_dict, tag):\n+    \"\"\"\n+    Given a dictionary, returns an element tree storing the data. This is the\n+    inverse of :func:`_xml_tree_to_dict`.\n \n-    name_item = clip_item.find('name')\n-    if name_item is not None:\n-        name = name_item.text\n-    else:\n-        name = None\n+    Any key/value pairs in the dictionary heirarchy where the key is prefixed\n+    with ``@`` will be treated as attributes on the containing element.\n+\n+    .. note:: This will automatically omit some kinds of metadata it should\n+    be up to the xml building functions to manage (such as timecode and id).\n \n-    clip = otio.schema.Clip(name=name, source_range=source_range)\n-    clip.markers.extend(\n-        [_parse_marker(m, rate) for m in markers]\n+    :param data_dict: The dictionary to turn into an XML tree.\n+    :param tag: The tag name to use for the top-level element.\n+\n+    :return: The top element for the dictionary\n+    \"\"\"\n+    top_attributes = collections.OrderedDict(\n+        (k[1:], v) for k, v in data_dict.items()\n+        if k != \"@id\" and k.startswith(\"@\")\n     )\n-    return clip\n+    top_element = cElementTree.Element(tag, **top_attributes)\n+\n+    def elements_for_value(python_value, element_tag):\n+        \"\"\" Creates a list of appropriate XML elements given a value. \"\"\"\n+        if isinstance(python_value, dict):\n+            element = _dict_to_xml_tree(python_value, element_tag)\n+            return [element]\n+        elif isinstance(python_value, list):\n+            return itertools.chain.from_iterable(\n+                elements_for_value(item, element_tag) for item in python_value\n+            )\n+        else:\n+            element = cElementTree.Element(element_tag)\n+            if python_value is not None:\n+                element.text = str(python_value)\n+            return [element]\n+\n+    # Drop timecode, rate, and link elements from roundtripping because they\n+    # may become stale with timeline updates.\n+    default_ignore_keys = {\"timecode\", \"rate\", \"link\"}\n+    specific_ignore_keys = {\"samplecharacteristics\": {\"timecode\"}}\n+    ignore_keys = specific_ignore_keys.get(tag, default_ignore_keys)\n+\n+    # push the elements into the tree\n+    for key, value in data_dict.items():\n+        if key in ignore_keys:\n+            continue\n \n+        # We already handled the attributes\n+        if key.startswith(\"@\"):\n+            continue\n \n-def _parse_clip_item(clip_item, transition_offsets, element_map):\n-    markers = clip_item.findall('./marker')\n+        elements = elements_for_value(value, key)\n+        top_element.extend(elements)\n \n-    media_reference = _parse_media_reference(\n-        clip_item.find('./file'),\n-        element_map\n-    )\n-    item_rate = _parse_rate(clip_item, element_map)\n+    return top_element\n \n-    # transition offsets are provided in timeline rate. If they deviate they\n-    # need to be rescaled to clip item rate\n-    context_transition_offsets = [\n-        transition_offsets[0].rescaled_to(item_rate),\n-        transition_offsets[1].rescaled_to(item_rate)\n-    ]\n \n-    in_value = int(float(clip_item.find('./in').text))\n-    in_frame = in_value + int(round(context_transition_offsets[0].value))\n+def _element_with_item_metadata(tag, item):\n+    \"\"\"\n+    Given a tag name, gets the FCP XML metadata dict and creates a tree of XML\n+    with that metadata under a top element with the provided tag.\n \n-    out_value = int(float(clip_item.find('./out').text))\n-    out_frame = out_value - int(round(context_transition_offsets[1].value))\n-    timecode = media_reference.available_range.start_time\n+    :param tag: The XML tag for the root element.\n+    :param item: An otio object with a metadata dict.\n+    \"\"\"\n+    item_meta = item.metadata.get(META_NAMESPACE)\n+    if item_meta:\n+        return _dict_to_xml_tree(item_meta, tag)\n \n-    # source_start in xml is taken relative to the start of the media, whereas\n-    # we want the absolute start time, taking into account the timecode\n-    start_time = otio.opentime.RationalTime(in_frame, item_rate) + timecode\n+    return cElementTree.Element(tag)\n \n-    source_range = otio.opentime.TimeRange(\n-        start_time=start_time.rescaled_to(item_rate),\n-        duration=otio.opentime.RationalTime(out_frame - in_frame, item_rate)\n-    )\n \n-    # get the clip name from the media reference if not defined on the clip\n-    name_item = clip_item.find('name')\n-    if name_item is not None:\n-        name = name_item.text\n-    else:\n-        url_path = _url_to_path(media_reference.target_url)\n-        name = os.path.basename(url_path)\n+def _get_or_create_subelement(parent_element, tag):\n+    \"\"\"\n+    Given an element and tag name, either gets the direct child of parent with\n+    that tag name or creates a new subelement with that tag and returns it.\n+\n+    :param parent_element: The element to get or create the subelement from.\n+    :param tag: The tag for the subelement.\n+    \"\"\"\n+    sub_element = parent_element.find(tag)\n+    if sub_element is None:\n+        sub_element = cElementTree.SubElement(parent_element, tag)\n+\n+    return sub_element\n \n-    clip = otio.schema.Clip(\n-        name=name,\n-        media_reference=media_reference,\n-        source_range=source_range\n-    )\n \n-    clip.markers.extend([_parse_marker(m, item_rate) for m in markers])\n+def _make_pretty_string(tree_e):\n+    # most of the parsing in this adapter is done with cElementTree because it\n+    # is simpler and faster. However, the string representation it returns is\n+    # far from elegant. Therefor we feed it through minidom to provide an xml\n+    # with indentations.\n+    string = cElementTree.tostring(tree_e, encoding=\"UTF-8\", method=\"xml\")\n+    dom = minidom.parseString(string)\n+    return dom.toprettyxml(indent='    ')\n \n-    return clip\n \n+def marker_for_element(marker_element, rate):\n+    \"\"\"\n+    Creates an :class:`schema.Marker` for the provided element.\n \n-def _parse_transition_item(transition_item, element_map):\n-    rate = _parse_rate(transition_item, element_map)\n-    start = otio.opentime.RationalTime(\n-        int(transition_item.find('./start').text),\n-        rate\n-    )\n-    end = otio.opentime.RationalTime(\n-        int(transition_item.find('./end').text),\n-        rate\n+    :param marker_element: The XML element for the marker.\n+    :param rate: The rate for the object the marker is attached to.\n+\n+    :return: The :class:`schema.Marker` instance.\n+    \"\"\"\n+    # TODO: The spec doc indicates that in and out are required, but doesn't\n+    #       say they have to be locally specified, so is it possible they\n+    #       could be inherited?\n+    marker_in = opentime.RationalTime(\n+        float(marker_element.find(\"./in\").text), rate\n     )\n-    cut_point = _get_transition_cut_point(transition_item, element_map)\n-    metadata = {\n-        META_NAMESPACE: {\n-            'effectid': transition_item.find('./effect/effectid').text,\n-        }\n-    }\n+    marker_out_value = float(marker_element.find(\"./out\").text)\n+    if marker_out_value > 0:\n+        marker_out = opentime.RationalTime(\n+            marker_out_value, rate\n+        )\n+        marker_duration = (marker_out - marker_in)\n+    else:\n+        marker_duration = opentime.RationalTime(rate=rate)\n+\n+    marker_range = opentime.TimeRange(marker_in, marker_duration)\n+\n+    md_dict = _xml_tree_to_dict(marker_element, {\"in\", \"out\", \"name\"})\n+    metadata = {META_NAMESPACE: md_dict} if md_dict else None\n \n-    transition = otio.schema.Transition(\n-        name=transition_item.find('./effect/name').text,\n-        transition_type=otio.schema.TransitionTypes.SMPTE_Dissolve,\n-        in_offset=cut_point - start,\n-        out_offset=end - cut_point,\n+    return schema.Marker(\n+        name=_name_from_element(marker_element),\n+        marked_range=marker_range,\n         metadata=metadata\n     )\n-    return transition\n \n \n-def _parse_track_item(track_item, transition_offsets, element_map):\n-    track = _parse_track(track_item.find('./sequence'), element_map)\n-    source_rate = _parse_rate(track_item.find('./sequence'), element_map)\n+def markers_from_element(element, context=None):\n+    \"\"\"\n+    Given an element, returns the list of markers attached to it.\n \n-    # transition offsets are provided in timeline rate. If they deviate they\n-    # need to be rescaled to clip item rate\n-    context_transition_offsets = [\n-        transition_offsets[0].rescaled_to(source_rate),\n-        transition_offsets[1].rescaled_to(source_rate)\n-    ]\n+    :param element: An element with one or more ``marker`` child elements.\n+    :param context: The context for this element.\n \n-    in_value = int(track_item.find('./in').text)\n-    in_frame = in_value + int(round(context_transition_offsets[0].value, 0))\n+    :return: A :class:`list` of :class:`schema.Marker` instances attached\n+        to the provided element.\n+    \"\"\"\n+    if context is not None:\n+        local_context = context.context_pushing_element(element)\n+    else:\n+        local_context = _Context(element)\n+    rate = _rate_from_context(local_context)\n \n-    out_value = int(track_item.find('./out').text)\n-    out_frame = out_value - int(round(context_transition_offsets[1].value))\n+    return [marker_for_element(e, rate) for e in element.iterfind(\"./marker\")]\n \n-    track.source_range = otio.opentime.TimeRange(\n-        start_time=otio.opentime.RationalTime(in_frame, source_rate),\n-        duration=otio.opentime.RationalTime(out_frame - in_frame, source_rate)\n-    )\n-    return track\n \n+class FCP7XMLParser:\n+    \"\"\"\n+    Implements parsing of an FCP XML file into an OTIO timeline.\n+\n+    Parsing FCP XML elements include two concepts that require carrying state:\n+        1. Inheritance\n+        2. The id Attribute\n+\n+    .. seealso:: https://developer.apple.com/library/archive/documentation/\\\n+            AppleApplications/Reference/FinalCutPro_XML/Basics/Basics.html\\\n+            #//apple_ref/doc/uid/TP30001154-TPXREF102\n+\n+    Inheritance is implemented using a _Context object that is pushed down\n+    through layers of parsing. A given parsing method is passed the element to\n+    parse into an otio object along with the context that element exists under\n+    (e.x. a track element parsing method is given the track element and the\n+    sequence context for that track).\n+\n+    The id attribute dereferencing is handled through a lookup table stored on\n+    parser instances and using the ``_derefed_`` methods to take an element and\n+    find dereference elements.\n+    \"\"\"\n+\n+    _etree = None\n+    \"\"\" The root etree for the FCP XML. \"\"\"\n+\n+    _id_map = None\n+    \"\"\" A mapping of id to the first element encountered with that id. \"\"\"\n+\n+    def __init__(self, element_tree):\n+        \"\"\"\n+        Constructor, must be init with an xml etree.\n+        \"\"\"\n+        self._etree = element_tree\n+\n+        self._id_map = {}\n+\n+    def _derefed_element(self, element):\n+        \"\"\"\n+        Given an element, dereferences it by it's id attribute if needed. If\n+        the element has an id attribute and it's our first time encountering\n+        it, store the id.\n+        \"\"\"\n+        if element is None:\n+            return element\n+\n+        try:\n+            elem_id = element.attrib[\"id\"]\n+        except KeyError:\n+            return element\n+\n+        return self._id_map.setdefault(elem_id, element)\n+\n+    def _derefed_iterfind(self, element, path):\n+        \"\"\"\n+        Given an elemnt, finds elements with the provided path below and\n+        returns an iterator of the dereferenced versions of those.\n+\n+        :param element: The XML etree element.\n+        :param path: The path to find subelements.\n+\n+        :return: iterator of subelements dereferenced by id.\n+        \"\"\"\n+        return (\n+            self._derefed_element(e) for e in element.iterfind(path)\n+        )\n \n-def _parse_item(track_item, track_rate, transition_offsets, element_map):\n-    # depending on the content of the clip-item, we return either a clip, a\n-    # stack or a transition.\n-    if track_item.tag == 'transitionitem':\n-        return _parse_transition_item(track_item, element_map)\n+    def top_level_sequences(self):\n+        \"\"\"\"\n+        Returns a list of timelines for the top-level sequences in the file.\n+        \"\"\"\n+        context = _Context()\n \n-    file_e = track_item.find('./file')\n-    if file_e is not None:\n-        file_e = _resolved_backreference(file_e, 'file', element_map)\n+        # If the tree has just sequences at the top level, this will catch them\n+        top_iter = self._derefed_iterfind(self._etree, \"./sequence\")\n \n-    if file_e is not None:\n-        if file_e.find('./pathurl') is None:\n-            return _parse_clip_item_without_media(\n-                track_item, track_rate, transition_offsets, element_map)\n+        # If there is a project or bin at the top level, this should cath them\n+        project_and_bin_iter = self._derefed_iterfind(\n+            self._etree, \".//children/sequence\"\n+        )\n+\n+        # Make an iterator that will exhaust both the above\n+        sequence_iter = itertools.chain(top_iter, project_and_bin_iter)\n+\n+        return [self.timeline_for_sequence(s, context) for s in sequence_iter]\n+\n+    def timeline_for_sequence(self, sequence_element, context):\n+        \"\"\"\n+        Returns either an :class`schema.Timeline` parsed from a sequence\n+        element.\n+\n+        :param sequence_element: The sequence element.\n+        :param context: The context dictionary.\n+\n+        :return: The appropriate OTIO object for the element.\n+        \"\"\"\n+        local_context = context.context_pushing_element(sequence_element)\n+\n+        name = _name_from_element(sequence_element)\n+        parsed_tags = {\"name\", \"media\", \"marker\", \"duration\"}\n+        md_dict = _xml_tree_to_dict(sequence_element, parsed_tags)\n+\n+        sequence_timecode = self._derefed_element(\n+            sequence_element.find(\"./timecode\")\n+        )\n+        if sequence_timecode is not None:\n+            seq_start_time = _time_from_timecode_element(\n+                sequence_timecode, local_context\n+            )\n         else:\n-            return _parse_clip_item(\n-                track_item, transition_offsets, element_map)\n-    elif track_item.find('./sequence') is not None:\n-        return _parse_track_item(\n-            track_item, transition_offsets, element_map)\n+            seq_start_time = None\n \n-    raise TypeError(\n-        'Type of clip item is not supported {item_id}'.format(\n-            item_id=track_item.attrib['id']\n+        media_element = self._derefed_element(sequence_element.find(\"./media\"))\n+        if media_element is None:\n+            tracks = None\n+        else:\n+            # Reach down into the media block and escalate metadata to the\n+            # sequence\n+            for media_type in media_element:\n+                media_info_dict = _xml_tree_to_dict(media_type, {\"track\"})\n+                if media_info_dict:\n+                    media_dict = md_dict.setdefault(\n+                        \"media\", collections.OrderedDict()\n+                    )\n+                    media_dict[media_type.tag] = media_info_dict\n+\n+            tracks = self.stack_for_element(media_element, local_context)\n+            tracks.name = name\n+\n+        # TODO: Should we be parsing the duration tag and pad out a track with\n+        #       gap to match?\n+\n+        timeline = schema.Timeline(\n+            name=name,\n+            global_start_time=seq_start_time,\n+            metadata={META_NAMESPACE: md_dict} if md_dict else {},\n+        )\n+        timeline.tracks = tracks\n+\n+        # Push the sequence markers onto the top stack\n+        markers = markers_from_element(sequence_element, context)\n+        timeline.tracks.markers.extend(markers)\n+\n+        return timeline\n+\n+    def stack_for_element(self, element, context):\n+        \"\"\"\n+        Given an element, parses out track information as a stack.\n+\n+        :param element: The element under which to find the tracks (typically\n+            a ``media`` element.\n+        :param context: The current parser context.\n+\n+        :return: A :class:`schema.Stack` of the tracks.\n+        \"\"\"\n+        # Determine the context\n+        local_context = context.context_pushing_element(element)\n+\n+        tracks = []\n+        media_type_elements = self._derefed_iterfind(element, \"./\")\n+        for media_type_element in media_type_elements:\n+            try:\n+                track_kind = _track_kind_from_element(media_type_element)\n+            except ValueError:\n+                # Unexpected element\n+                continue\n+\n+            is_audio = (track_kind == schema.TrackKind.Audio)\n+            track_elements = self._derefed_iterfind(\n+                media_type_element, \"./track\"\n+            )\n+            for track_element in track_elements:\n+                if is_audio and not _is_primary_audio_channel(track_element):\n+                    continue\n+\n+                tracks.append(\n+                    self.track_for_element(\n+                        track_element, track_kind, local_context\n+                    )\n+                )\n+\n+        markers = markers_from_element(element, context)\n+\n+        stack = schema.Stack(\n+            children=tracks,\n+            markers=markers,\n+            name=_name_from_element(element),\n+        )\n+\n+        return stack\n+\n+    def track_for_element(self, track_element, track_kind, context):\n+        \"\"\"\n+        Given a track element, constructs the OTIO track.\n+\n+        :param track_element: The track XML element.\n+        :param track_kind: The :class:`schema.TrackKind` for the track.\n+        :param context: The context dict for this track.\n+        \"\"\"\n+        local_context = context.context_pushing_element(track_element)\n+        name_element = track_element.find(\"./name\")\n+        track_name = (name_element.text if name_element is not None else None)\n+\n+        timeline_item_tags = {\"clipitem\", \"generatoritem\", \"transitionitem\"}\n+\n+        md_dict = _xml_tree_to_dict(track_element, timeline_item_tags)\n+        track_metadata = {META_NAMESPACE: md_dict} if md_dict else None\n+\n+        track = schema.Track(\n+            name=track_name,\n+            kind=track_kind,\n+            metadata=track_metadata,\n         )\n-    )\n \n+        # Iterate through and parse track items\n+        track_rate = _rate_from_context(local_context)\n+        current_timeline_time = opentime.RationalTime(0, track_rate)\n+        head_transition_element = None\n+        for i, item_element in enumerate(track_element):\n+            if item_element.tag not in timeline_item_tags:\n+                continue\n+\n+            item_element = self._derefed_element(item_element)\n+\n+            # Do a lookahead to try and find the tail transition item\n+            try:\n+                tail_transition_element = track_element[i + 1]\n+                if tail_transition_element.tag != \"transitionitem\":\n+                    tail_transition_element = None\n+                else:\n+                    tail_transition_element = self._derefed_element(\n+                        tail_transition_element\n+                    )\n+            except IndexError:\n+                tail_transition_element = None\n+\n+            track_item, item_range = self.item_and_timing_for_element(\n+                item_element,\n+                head_transition_element,\n+                tail_transition_element,\n+                local_context,\n+            )\n+\n+            # Insert gap between timeline cursor and the new item if needed.\n+            if current_timeline_time < item_range.start_time:\n+                gap_duration = (item_range.start_time - current_timeline_time)\n+                gap_range = opentime.TimeRange(\n+                    duration=gap_duration.rescaled_to(track_rate)\n+                )\n+                track.append(schema.Gap(source_range=gap_range))\n+\n+            # Add the item and advance the timeline cursor\n+            track.append(track_item)\n+            current_timeline_time = item_range.end_time_exclusive()\n \n-def _parse_top_level_track(track_e, kind, rate, element_map):\n-    track = otio.schema.Track(kind=kind)\n-    track_items = [\n-        item for item in track_e if item.tag in ('clipitem', 'transitionitem')\n-    ]\n+            # Stash the element for the next iteration if it's a transition\n+            if item_element.tag == \"transitionitem\":\n+                head_transition_element = item_element\n \n-    if not track_items:\n         return track\n \n-    trackname_e = track_e.find('./name')\n-    if trackname_e is not None:\n-        track.name = trackname_e.text\n+    def media_reference_for_file_element(self, file_element, context):\n+        \"\"\"\n+        Given a file XML element, returns the\n+        :class`schema.ExternalReference`.\n \n-    last_clip_end = otio.opentime.RationalTime(rate=rate)\n-    for track_item in track_items:\n-        clip_item_index = list(track_e).index(track_item)\n-        start = otio.opentime.RationalTime(\n-            int(track_item.find('./start').text),\n-            rate\n+        :param file_element: The file xml element.\n+        :param context: The parent context dictionary.\n+\n+        :return: An :class:`schema.ExternalReference`.\n+        \"\"\"\n+        local_context = context.context_pushing_element(file_element)\n+        media_ref_rate = _rate_from_context(local_context)\n+\n+        name = _name_from_element(file_element)\n+\n+        # Get the full metadata\n+        metadata_ignore_keys = {\"duration\", \"name\", \"pathurl\"}\n+        md_dict = _xml_tree_to_dict(file_element, metadata_ignore_keys)\n+        metadata_dict = {META_NAMESPACE: md_dict} if md_dict else None\n+\n+        # Determine the file path\n+        path_element = file_element.find(\"./pathurl\")\n+        if path_element is not None:\n+            path = path_element.text\n+        else:\n+            path = None\n+\n+        # Find the timing\n+        timecode_element = file_element.find(\"./timecode\")\n+        if timecode_element is not None:\n+            start_time = _time_from_timecode_element(timecode_element)\n+            start_time = start_time.rescaled_to(media_ref_rate)\n+        else:\n+            start_time = opentime.RationalTime(0, media_ref_rate)\n+\n+        duration_element = file_element.find(\"./duration\")\n+        if duration_element is not None:\n+            duration = opentime.RationalTime(\n+                float(duration_element.text), media_ref_rate\n+            )\n+            available_range = opentime.TimeRange(start_time, duration)\n+        elif timecode_element is not None:\n+            available_range = opentime.TimeRange(\n+                start_time,\n+                opentime.RationalTime(0, media_ref_rate),\n+            )\n+        else:\n+            available_range = None\n+\n+        if path is None:\n+            media_reference = schema.MissingReference(\n+                name=name,\n+                available_range=available_range,\n+                metadata=metadata_dict,\n+            )\n+        else:\n+            media_reference = schema.ExternalReference(\n+                target_url=path,\n+                available_range=available_range,\n+                metadata=metadata_dict,\n+            )\n+            media_reference.name = name\n+\n+        return media_reference\n+\n+    def media_reference_for_effect_element(self, effect_element):\n+        \"\"\"\n+        Given an effect element, returns a generator reference.\n+\n+        :param effect_element: The effect for the generator.\n+\n+        :return: An :class:`schema.GeneratorReference` instance.\n+        \"\"\"\n+        name = _name_from_element(effect_element)\n+        md_dict = _xml_tree_to_dict(effect_element, {\"name\"})\n+\n+        return schema.GeneratorReference(\n+            name=name,\n+            metadata=({META_NAMESPACE: md_dict} if md_dict else None)\n         )\n-        end = otio.opentime.RationalTime(\n-            int(track_item.find('./end').text),\n-            rate\n+\n+    def item_and_timing_for_element(\n+        self, item_element, head_transition, tail_transition, context\n+    ):\n+        \"\"\"\n+        Given a track item, returns a tuple with the appropriate OpenTimelineIO\n+        schema item as the first element and an\n+        :class:`opentime.TimeRange`of theresolved timeline range the clip\n+        occupies.\n+\n+        :param item_element: The track item XML node.\n+        :param head_transition: The xml element for the transition immediately\n+            before or ``None``.\n+        :param tail_transition: The xml element for the transition immediately\n+            after or ``None``.\n+        :param context: The context dictionary.\n+\n+        :return: An :class:`core.Item` subclass instance and\n+            :class:`opentime.TimeRange` for the item.\n+        \"\"\"\n+        parent_rate = _rate_from_context(context)\n+\n+        # Establish the start/end time in the timeline\n+        start_value = int(item_element.find(\"./start\").text)\n+        end_value = int(item_element.find(\"./end\").text)\n+\n+        if start_value == -1:\n+            # determine based on the cut point of the head transition\n+            start = _transition_cut_point(head_transition, context)\n+\n+            # This offset is needed to determing how much to advance from the\n+            # clip media's in time. Duration accounts for this offset for the\n+            # out time.\n+            transition_rate = _rate_from_context(\n+                context.context_pushing_element(head_transition)\n+            )\n+            start_offset = start - opentime.RationalTime(\n+                int(head_transition.find('./start').text), transition_rate\n+            )\n+        else:\n+            start = opentime.RationalTime(start_value, parent_rate)\n+            start_offset = opentime.RationalTime()\n+\n+        if end_value == -1:\n+            # determine based on the cut point of the tail transition\n+            end = _transition_cut_point(tail_transition, context)\n+        else:\n+            end = opentime.RationalTime(end_value, parent_rate)\n+\n+        item_range = opentime.TimeRange(start, (end - start))\n+\n+        # Get the metadata dictionary for the item\n+        item_metadata_ignore_keys = {\n+            \"name\",\n+            \"start\",\n+            \"end\",\n+            \"in\",\n+            \"out\",\n+            \"duration\",\n+            \"file\",\n+            \"marker\",\n+            \"effect\",\n+            \"rate\",\n+            \"sequence\",\n+        }\n+        metadata_dict = _xml_tree_to_dict(\n+            item_element, item_metadata_ignore_keys\n         )\n \n-        # start time and end time on the timeline can be set to -1. This means\n-        # that there is a transition at that end of the clip-item. So the time\n-        # on the timeline has to be taken from that object.\n-        transition_offsets = [\n-            otio.opentime.RationalTime(rate=rate),\n-            otio.opentime.RationalTime(rate=rate)\n-        ]\n-\n-        if track_item.tag == 'clipitem':\n-            if start.value == -1:\n-                in_transition = list(track_e)[clip_item_index - 1]\n-                start = _get_transition_cut_point(in_transition, element_map)\n-                transition_offsets[0] = start - otio.opentime.RationalTime(\n-                    int(in_transition.find('./start').text),\n-                    _parse_rate(in_transition, element_map)\n-                )\n-            if end.value == -1:\n-                out_transition = list(track_e)[clip_item_index + 1]\n-                end = _get_transition_cut_point(out_transition, element_map)\n-                transition_offsets[1] = otio.opentime.RationalTime(\n-                    int(out_transition.find('./end').text),\n-                    _parse_rate(out_transition, element_map)\n-                ) - end\n-\n-        # see if we need to add a gap before this clip-item\n-        gap_time = start - last_clip_end\n-        last_clip_end = end\n-        if gap_time.value > 0:\n-            gap_range = otio.opentime.TimeRange(\n-                duration=gap_time.rescaled_to(rate)\n+        # deserialize the item\n+        if item_element.tag in {\"clipitem\", \"generatoritem\"}:\n+            item = self.clip_for_element(\n+                item_element, item_range, start_offset, context\n             )\n-            track.append(otio.schema.Gap(source_range=gap_range))\n+        elif item_element.tag == \"transitionitem\":\n+            item = self.transition_for_element(item_element, context)\n+        else:\n+            name = \"unknown-{}\".format(item_element.tag)\n+            item = core.Item(name=name, source_range=item_range)\n \n-        # finally add the track-item itself\n-        track.append(\n-            _parse_item(track_item, rate, transition_offsets, element_map)\n+        if metadata_dict:\n+            item.metadata.setdefault(META_NAMESPACE, {}).update(metadata_dict)\n+\n+        return (item, item_range)\n+\n+    def clip_for_element(\n+        self, clipitem_element, item_range, start_offset, context\n+    ):\n+        \"\"\"\n+        Given a clipitem xml element, returns an :class:`schema.Clip`.\n+\n+        :param clipitem_element: The element to create a clip for.\n+        :param item_range: The time range in the timeline the clip occupies.\n+        :param start_offset: The amount by which the ``in`` time of the clip\n+            source should be advanced (usually due to a transition).\n+        :param context: The parent context for the clip.\n+\n+        :return: The :class:`schema.Clip` instance.\n+        \"\"\"\n+        local_context = context.context_pushing_element(clipitem_element)\n+\n+        name = _name_from_element(clipitem_element)\n+\n+        file_element = self._derefed_element(clipitem_element.find(\"./file\"))\n+        sequence_element = self._derefed_element(\n+            clipitem_element.find(\"./sequence\")\n         )\n+        if clipitem_element.tag == \"generatoritem\":\n+            generator_effect_element = clipitem_element.find(\n+                \"./effect[effecttype='generator']\"\n+            )\n+        else:\n+            generator_effect_element = None\n+\n+        media_start_time = opentime.RationalTime()\n+        if sequence_element is not None:\n+            item = self.stack_for_element(sequence_element, local_context)\n+            # TODO: is there an applicable media start time we should be\n+            #       using from nested sequences?\n+        elif file_element is not None or generator_effect_element is not None:\n+            if file_element is not None:\n+                media_reference = self.media_reference_for_file_element(\n+                    file_element, local_context\n+                )\n+                # See if there is a start offset\n+                timecode_element = file_element.find(\"./timecode\")\n+                if timecode_element is not None:\n+                    media_start_time = _time_from_timecode_element(\n+                        timecode_element\n+                    )\n+            elif generator_effect_element is not None:\n+                media_reference = self.media_reference_for_effect_element(\n+                    generator_effect_element\n+                )\n \n-    return track\n+            item = schema.Clip(\n+                name=name,\n+                media_reference=media_reference,\n+            )\n+        else:\n+            raise TypeError(\n+                'Type of clip item is not supported {}'.format(\n+                    _element_identification_string(clipitem_element)\n+                )\n+            )\n \n+        # Add the markers\n+        markers = markers_from_element(clipitem_element, context)\n+        item.markers.extend(markers)\n \n-def _parse_marker(marker, rate):\n-    marker_range = otio.opentime.TimeRange(\n-        start_time=otio.opentime.RationalTime(\n-            int(marker.find('./in').text), rate\n+        # Find the in time (source time relative to media start)\n+        clip_rate = _rate_from_context(local_context)\n+        in_value = float(clipitem_element.find('./in').text)\n+        in_time = opentime.RationalTime(in_value, clip_rate)\n+\n+        # Offset the \"in\" time by the start offset of the media\n+        soure_start_time = in_time + media_start_time + start_offset\n+        duration = item_range.duration\n+\n+        # Source Range is the item range expressed in the clip's rate (for now)\n+        source_range = opentime.TimeRange(\n+            soure_start_time.rescaled_to(clip_rate),\n+            duration.rescaled_to(clip_rate),\n         )\n-    )\n-    metadata = {META_NAMESPACE: {'comment': marker.find('./comment').text}}\n-    return otio.schema.Marker(\n-        name=marker.find('./name').text,\n-        marked_range=marker_range,\n-        metadata=metadata\n-    )\n \n+        item.source_range = source_range\n+\n+        # Parse the filters\n+        filter_iter = self._derefed_iterfind(clipitem_element, \"./filter\")\n+        for filter_element in filter_iter:\n+            item.effects.append(\n+                self.effect_from_filter_element(filter_element)\n+            )\n+\n+        return item\n \n-def _parse_track(track, element_map):\n-    track = _resolved_backreference(track, 'sequence', element_map)\n+    def effect_from_filter_element(self, filter_element):\n+        \"\"\"\n+        Given a filter element, creates an :class:`schema.Effect`.\n \n-    track_rate = _parse_rate(track, element_map)\n+        :param filter_element: The ``filter`` element containing the effect.\n \n-    video_tracks = track.findall('./media/video/track')\n-    audio_tracks = track.findall('./media/audio/track')\n-    markers = track.findall('./marker')\n+        :return: The effect instance.\n+        \"\"\"\n+        effect_element = filter_element.find(\"./effect\")\n \n-    stack = otio.schema.Stack(name=track.find('./name').text)\n+        if effect_element is None:\n+            raise ValueError(\n+                \"could not find effect in filter: {}\".format(filter_element)\n+            )\n+\n+        name = effect_element.find(\"./name\").text\n+\n+        effect_metadata = _xml_tree_to_dict(effect_element, {\"name\"})\n \n-    stack.extend(\n-        _parse_top_level_track(\n-            t,\n-            otio.schema.TrackKind.Video, track_rate, element_map\n+        return schema.Effect(\n+            name,\n+            metadata={META_NAMESPACE: effect_metadata},\n         )\n-        for t in video_tracks\n-    )\n-    stack.extend(\n-        _parse_top_level_track(\n-            t,\n-            otio.schema.TrackKind.Audio,\n-            track_rate, element_map\n-        )\n-        for t in audio_tracks\n-        if _is_primary_audio_channel(t)\n-    )\n-    stack.markers.extend(_parse_marker(m, track_rate) for m in markers)\n \n-    return stack\n+    def transition_for_element(self, item_element, context):\n+        \"\"\"\n+        Creates an OTIO transition for the provided transition element.\n \n+        :param item_element: The element to create a transition for.\n+        :param context: The parent context for the element.\n \n-def _parse_timeline(track, element_map):\n-    track = _resolved_backreference(track, 'sequence', element_map)\n-    track_rate = _parse_rate(track, element_map)\n-    timeline = otio.schema.Timeline(name=track.find('./name').text)\n-    timeline.global_start_time = otio.opentime.RationalTime(0, track_rate)\n-    timeline.tracks = _parse_track(track, element_map)\n-    return timeline\n+        :return: The :class:`schema.Transition` instance.\n+        \"\"\"\n+        # start and end times are in the parent's rate\n+        rate = _rate_from_context(context)\n+        start = opentime.RationalTime(\n+            int(item_element.find('./start').text),\n+            rate\n+        )\n+        end = opentime.RationalTime(\n+            int(item_element.find('./end').text),\n+            rate\n+        )\n+        cut_point = _transition_cut_point(item_element, context)\n \n+        transition = schema.Transition(\n+            name=item_element.find('./effect/name').text,\n+            transition_type=schema.TransitionTypes.SMPTE_Dissolve,\n+            in_offset=cut_point - start,\n+            out_offset=end - cut_point,\n+        )\n \n-def _parse_collection(tracks, element_map):\n-    collection = otio.schema.SerializableCollection(name='tracks')\n-    collection.extend([_parse_timeline(s, element_map) for s in tracks])\n-    return collection\n+        return transition\n \n \n # ------------------------\n # building single track\n # ------------------------\n \n-def _build_rate(time):\n-    rate = math.ceil(time.rate)\n+\n+def _backreference_for_item(item, tag, br_map):\n+    \"\"\"\n+    Given an item, determines what the id in the backreference map should be.\n+    If the item is already tracked in the map, it will be returned, otherwise\n+    a new id will be minted.\n+\n+    .. note:: ``br_map`` may be mutated by this function. ``br_map`` is\n+    intended to be an opaque data structure and only accessed through this\n+    function, the structure of data in br_map may change.\n+\n+    :param item: The :class:`core.SerializableObject` to create an id for.\n+    :param tag: The tag name that will be used for object in xml.\n+    :param br_map: The dictionary containing backreference information\n+        generated so far.\n+\n+    :return: A 2-tuple of (id_string, is_new_id) where the ``id_string`` is\n+        the value for the xml id attribute and ``is_new_id`` is ``True`` when\n+        this is the first time that id was encountered.\n+    \"\"\"\n+    # br_map is structured as a dictionary with tags as keys, and dictionaries\n+    # of hash to id int as values.\n+\n+    def id_string(id_int):\n+        return \"{}-{}\".format(tag, id_int)\n+\n+    # Determine how to uniquely identify the referenced item\n+    if isinstance(item, schema.ExternalReference):\n+        item_hash = hash(str(item.target_url))\n+    else:\n+        # TODO: This may become a performance issue. It means that every\n+        #       non-ref object is serialized to json and hashed each time it's\n+        #       encountered.\n+        item_hash = hash(\n+            core.json_serializer.serialize_json_to_string(item)\n+        )\n+\n+    is_new_id = False\n+    item_id = br_map.get(tag, {}).get(item_hash)\n+    if item_id is not None:\n+        return (id_string(item_id), is_new_id)\n+\n+    # This is a new id, figure out what it should be.\n+    is_new_id = True\n+\n+    # Attempt to preserve the ID from the input metadata.\n+    preferred_id = None\n+    orig_id_string = item.metadata.get(META_NAMESPACE, {}).get(\"@id\")\n+    if orig_id_string is not None:\n+        orig_id_match = ID_RE.match(orig_id_string)\n+        if orig_id_match is not None:\n+            match_groups = orig_id_match.groupdict()\n+            orig_tagname = match_groups[\"tag\"]\n+            if orig_tagname == tag:\n+                preferred_id = int(match_groups[\"id\"])\n+\n+    # Generate an id by finding the lowest value in a contiguous range not\n+    # colliding with an existing value\n+    tag_id_map = br_map.setdefault(tag, {})\n+    existing_ids = set(tag_id_map.values())\n+    if preferred_id is not None and preferred_id not in existing_ids:\n+        item_id = preferred_id\n+    else:\n+        # Make a range from 1 including the ID after the largest assigned\n+        # (hence the +2 since range is non-inclusive on the upper bound)\n+        max_assigned_id = max(existing_ids) if existing_ids else 0\n+        max_possible_id = (max_assigned_id + 2)\n+        possible_ids = set(range(1, max_possible_id))\n+\n+        # Select the lowest unassigned ID\n+        item_id = min(possible_ids.difference(existing_ids))\n+\n+    # Store the created id\n+    tag_id_map[item_hash] = item_id\n+\n+    return (id_string(item_id), is_new_id)\n+\n+\n+def _backreference_build(tag):\n+    \"\"\"\n+    A decorator for functions creating XML elements to implement the id system\n+    described in FCP XML.\n+\n+    This wrapper determines if the otio item is equivalent to one encountered\n+    before with the provided tag name. If the item hasn't been encountered then\n+    the wrapped function will be invoked and the XML element from that function\n+    will have the ``id`` attribute set and be stored in br_map.\n+    If the item is equivalent to a previously provided item, the wrapped\n+    function won't be invoked and a simple tag with the previous instance's id\n+    will be returned instead.\n+\n+    The wrapped function must:\n+        - Have the otio item as the first positional argument.\n+        - Have br_map (backreference map, a dictionary) as the last positional\n+        arg. br_map stores the state for encountered items.\n+\n+    :param tag: The xml tag of the element the wrapped function generates.\n+    \"\"\"\n+    # We can also encode these back-references if an item is accessed multiple\n+    # times. To do this we store an id attribute on the element. For back-\n+    # references we then only need to return an empty element of that type with\n+    # the id we logged before\n+\n+    def singleton_decorator(func):\n+        @functools.wraps(func)\n+        def wrapper(item, *args, **kwargs):\n+            br_map = args[-1]\n+\n+            item_id, id_is_new = _backreference_for_item(item, tag, br_map)\n+\n+            # if the item exists in the map already, we should use the\n+            # abbreviated XML element referring to the original\n+            if not id_is_new:\n+                return cElementTree.Element(tag, id=item_id)\n+\n+            # This is the first time for this unique item, it needs it's full\n+            # XML. Get the element generated by the wrapped function and add\n+            # the id attribute.\n+            elem = func(item, *args, **kwargs)\n+            elem.attrib[\"id\"] = item_id\n+\n+            return elem\n+\n+        return wrapper\n+\n+    return singleton_decorator\n+\n+\n+def _append_new_sub_element(parent, tag, attrib=None, text=None):\n+    \"\"\"\n+    Creates a sub-element with the provided tag, attributes, and text.\n+\n+    This is a convenience because the :class:`SubElement` constructor does not\n+    provide the ability to set ``text``.\n+\n+    :param parent: The parent element.\n+    :param tag: The tag string for the element.\n+    :param attrib: An optional dictionary of attributes for the element.\n+    :param text: Optional text value for the element.\n+\n+    :return: The new XML element.\n+    \"\"\"\n+    elem = cElementTree.SubElement(parent, tag, **attrib or {})\n+    if text is not None:\n+        elem.text = text\n+\n+    return elem\n+\n+\n+def _build_rate(fps):\n+    \"\"\"\n+    Given a framerate, makes a ``rate`` xml tree.\n+\n+    :param fps: The framerate.\n+    :return: The fcp xml ``rate`` tree.\n+    \"\"\"\n+    rate = math.ceil(fps)\n \n     rate_e = cElementTree.Element('rate')\n-    _insert_new_sub_element(rate_e, 'timebase', text=str(int(rate)))\n-    _insert_new_sub_element(\n+    _append_new_sub_element(rate_e, 'timebase', text=str(int(rate)))\n+    _append_new_sub_element(\n         rate_e,\n         'ntsc',\n-        text='FALSE' if rate == time.rate else 'TRUE'\n+        text='FALSE' if rate == fps else 'TRUE'\n     )\n     return rate_e\n \n \n+def _build_timecode(time, fps, drop_frame=False, additional_metadata=None):\n+    \"\"\"\n+    Makes a timecode xml element tree.\n+\n+    .. warning:: The drop_frame parameter is currently ignored and\n+        auto-determined by rate. This is because the underlying otio timecode\n+        conversion assumes DFTC based on rate.\n+\n+    :param time: The :class:`opentime.RationalTime` for the timecode.\n+    :param fps: The framerate for the timecode.\n+    :param drop_frame: If True, generates drop-frame timecode.\n+    :param additional_metadata: A dictionary with other metadata items like\n+        ``field``, ``reel``, ``source``, and ``format``. It is assumed this\n+        dictionary is of the form generated by :func:`_xml_tree_to_dict` when\n+        the file was read originally.\n+\n+    :return: The ``timecode`` element.\n+    \"\"\"\n+    if additional_metadata:\n+        # Only allow legal child items for the timecode element\n+        filtered = {\n+            k: v for k, v in additional_metadata.items()\n+            if k in {\"field\", \"reel\", \"source\", \"format\"}\n+        }\n+        tc_element = _dict_to_xml_tree(filtered, \"timecode\")\n+    else:\n+        tc_element = cElementTree.Element(\"timecode\")\n+\n+    tc_element.append(_build_rate(fps))\n+    rate_is_not_ntsc = (tc_element.find('./rate/ntsc').text == \"FALSE\")\n+    if drop_frame and rate_is_not_ntsc:\n+        tc_fps = fps * (1000 / 1001.0)\n+    else:\n+        tc_fps = fps\n+\n+    # Get the time values\n+    tc_time = opentime.RationalTime(time.value_rescaled_to(fps), tc_fps)\n+    tc_string = opentime.to_timecode(tc_time, tc_fps, drop_frame)\n+\n+    _append_new_sub_element(tc_element, \"string\", text=tc_string)\n+\n+    frame_number = int(round(time.value))\n+    _append_new_sub_element(\n+        tc_element, \"frame\", text=\"{:.0f}\".format(frame_number)\n+    )\n+\n+    drop_frame = (\";\" in tc_string)\n+    display_format = \"DF\" if drop_frame else \"NDF\"\n+    _append_new_sub_element(tc_element, \"displayformat\", text=display_format)\n+\n+    return tc_element\n+\n+\n def _build_item_timings(\n     item_e,\n     item,",
        "comments": [],
        "commit_message": "Merge branch 'master' of https://github.com/PixarAnimationStudios/OpenTimelineIO into fcpx_enhancements",
        "commit_id": "858f308f6f6d2d5a89158e1739642a50fc7241f3"
    },
    {
        "pr_title": "Add support for more elements and XMLs without libraries",
        "pr_number": 515,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -562,95 +1362,130 @@\ndef _build_item_timings(\n     # media. But xml regards the source in point from the start of the media.\n     # So we subtract the media timecode.\n     item_rate = item.source_range.start_time.rate\n-    source_start = (item.source_range.start_time - timecode) \\\n-        .rescaled_to(item_rate)\n-    source_end = (item.source_range.end_time_exclusive() - timecode) \\\n-        .rescaled_to(item_rate)\n+    source_start = (item.source_range.start_time - timecode)\n+    source_start = source_start.rescaled_to(item_rate)\n+\n+    source_end = (item.source_range.end_time_exclusive() - timecode)\n+    source_end = source_end.rescaled_to(item_rate)\n+\n     start = '{:.0f}'.format(timeline_range.start_time.value)\n     end = '{:.0f}'.format(timeline_range.end_time_exclusive().value)\n \n+    item_e.append(_build_rate(item_rate))\n+\n     if transition_offsets[0] is not None:\n         start = '-1'\n         source_start -= transition_offsets[0]\n     if transition_offsets[1] is not None:\n         end = '-1'\n         source_end += transition_offsets[1]\n \n-    _insert_new_sub_element(\n+    _append_new_sub_element(\n         item_e, 'duration',\n         text='{:.0f}'.format(item.source_range.duration.value)\n     )\n-    _insert_new_sub_element(item_e, 'start', text=start)\n-    _insert_new_sub_element(item_e, 'end', text=end)\n-    _insert_new_sub_element(\n+    _append_new_sub_element(item_e, 'start', text=start)\n+    _append_new_sub_element(item_e, 'end', text=end)\n+    _append_new_sub_element(\n         item_e,\n         'in',\n         text='{:.0f}'.format(source_start.value)\n     )\n-    _insert_new_sub_element(\n+    _append_new_sub_element(\n         item_e,\n         'out',\n         text='{:.0f}'.format(source_end.value)\n     )\n \n \n @_backreference_build('file')\n-def _build_empty_file(media_ref, source_start, br_map):\n-    file_e = cElementTree.Element('file')\n-    file_e.append(_build_rate(source_start))\n-    file_media_e = _insert_new_sub_element(file_e, 'media')\n-    _insert_new_sub_element(file_media_e, 'video')\n+def _build_empty_file(media_ref, parent_range, br_map):\n+    file_e = _element_with_item_metadata(\"file\", media_ref)\n+    _append_new_sub_element(file_e, \"name\", text=media_ref.name)\n+\n+    if media_ref.available_range is not None:\n+        available_range = media_ref.available_range\n+    else:\n+        available_range = opentime.TimeRange(\n+            opentime.RationalTime(0, parent_range.start_time.rate),\n+            parent_range.duration,\n+        )\n+\n+    ref_rate = available_range.start_time.rate\n+    file_e.append(_build_rate(ref_rate))\n+\n+    # Only provide a duration if one came from the media, don't invent one.\n+    # For example, Slugs have no duration specified.\n+    if media_ref.available_range:\n+        duration = available_range.duration.rescaled_to(ref_rate)\n+        _append_new_sub_element(\n+            file_e,\n+            'duration',\n+            text='{:.0f}'.format(duration.value),\n+        )\n+\n+    # timecode\n+    ref_tc_metadata = media_ref.metadata.get(META_NAMESPACE, {}).get(\n+        \"timecode\"\n+    )\n+    tc_element = _build_timecode_from_metadata(\n+        available_range.start_time, ref_tc_metadata\n+    )\n+    file_e.append(tc_element)\n+\n+    file_media_e = _get_or_create_subelement(file_e, \"media\")\n+    if file_media_e.find(\"video\") is None:\n+        _append_new_sub_element(file_media_e, \"video\")\n \n     return file_e\n \n \n @_backreference_build('file')\n def _build_file(media_reference, br_map):\n-    file_e = cElementTree.Element('file')\n+    file_e = _element_with_item_metadata(\"file\", media_reference)\n \n     available_range = media_reference.available_range\n     url_path = _url_to_path(media_reference.target_url)\n \n-    _insert_new_sub_element(file_e, 'name', text=os.path.basename(url_path))\n-    file_e.append(_build_rate(available_range.start_time))\n-    _insert_new_sub_element(\n+    file_name = (\n+        media_reference.name if media_reference.name\n+        else os.path.basename(url_path)\n+    )\n+    _append_new_sub_element(file_e, 'name', text=file_name)\n+    _append_new_sub_element(file_e, 'pathurl', text=media_reference.target_url)\n+\n+    # timing info\n+    file_e.append(_build_rate(available_range.start_time.rate))\n+    _append_new_sub_element(\n         file_e, 'duration',\n         text='{:.0f}'.format(available_range.duration.value)\n     )\n-    _insert_new_sub_element(file_e, 'pathurl', text=media_reference.target_url)\n \n     # timecode\n-    timecode = available_range.start_time\n-    timecode_e = _insert_new_sub_element(file_e, 'timecode')\n-    timecode_e.append(_build_rate(timecode))\n-    # TODO: This assumes the rate on the start_time is the framerate\n-    _insert_new_sub_element(\n-        timecode_e,\n-        'string',\n-        text=otio.opentime.to_timecode(timecode, rate=timecode.rate)\n-    )\n-    _insert_new_sub_element(\n-        timecode_e,\n-        'frame',\n-        text='{:.0f}'.format(timecode.value)\n+    ref_tc_metadata = media_reference.metadata.get(META_NAMESPACE, {}).get(\n+        \"timecode\"\n     )\n-    display_format = (\n-        'DF' if (\n-            math.ceil(timecode.rate) == 30 and math.ceil(timecode.rate) != timecode.rate\n-        ) else 'NDF'\n+    tc_element = _build_timecode_from_metadata(\n+        available_range.start_time, ref_tc_metadata\n     )\n-    _insert_new_sub_element(timecode_e, 'displayformat', text=display_format)\n+    file_e.append(tc_element)\n \n     # we need to flag the file reference with the content types, otherwise it\n     # will not get recognized\n-    file_media_e = _insert_new_sub_element(file_e, 'media')\n-    content_types = []\n-    if not os.path.splitext(url_path)[1].lower() in ('.wav', '.aac', '.mp3'):\n-        content_types.append('video')\n-    content_types.append('audio')\n+    # TODO: We should use a better method for this. Perhaps pre-walk the\n+    #       timeline and find all the track kinds this media is present in?\n+    if not file_e.find(\"media\"):\n+        file_media_e = _get_or_create_subelement(file_e, \"media\")\n+\n+        audio_exts = {'.wav', '.aac', '.mp3', '.aif', '.aiff', '.m4a'}\n+        has_video = (os.path.splitext(url_path)[1].lower() not in audio_exts)\n+        if has_video and file_media_e.find(\"video\") is None:\n+            _append_new_sub_element(file_media_e, \"video\")\n \n-    for kind in content_types:\n-        _insert_new_sub_element(file_media_e, kind)\n+        # TODO: This is assuming all files have an audio track. Not sure what\n+        # the implications of that are.\n+        if file_media_e.find(\"audio\") is None:\n+            _append_new_sub_element(file_media_e, \"audio\")\n \n     return file_e",
        "comments": [],
        "commit_message": "Merge branch 'master' of https://github.com/PixarAnimationStudios/OpenTimelineIO into fcpx_enhancements",
        "commit_id": "858f308f6f6d2d5a89158e1739642a50fc7241f3"
    },
    {
        "pr_title": "Add support for more elements and XMLs without libraries",
        "pr_number": 515,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -659,119 +1494,198 @@\ndef _build_transition_item(\n     transition_item,\n     timeline_range,\n     transition_offsets,\n-    br_map\n+    br_map,\n ):\n-    transition_e = cElementTree.Element('transitionitem')\n-    _insert_new_sub_element(\n+    transition_e = _element_with_item_metadata(\n+        \"transitionitem\", transition_item\n+    )\n+    _append_new_sub_element(\n         transition_e,\n         'start',\n         text='{:.0f}'.format(timeline_range.start_time.value)\n     )\n-    _insert_new_sub_element(\n+    _append_new_sub_element(\n         transition_e,\n         'end',\n         text='{:.0f}'.format(timeline_range.end_time_exclusive().value)\n     )\n \n-    if not transition_item.in_offset.value:\n-        _insert_new_sub_element(transition_e, 'alignment', text='start-black')\n-    elif not transition_item.out_offset.value:\n-        _insert_new_sub_element(transition_e, 'alignment', text='end-black')\n-    else:\n-        _insert_new_sub_element(transition_e, 'alignment', text='center')\n-    # todo support 'start' and 'end' alignment\n-\n-    transition_e.append(_build_rate(timeline_range.start_time))\n-\n-    effectid = transition_item.metadata.get(META_NAMESPACE, {}).get(\n-        'effectid',\n-        'Cross Dissolve'\n-    )\n-\n-    effect_e = _insert_new_sub_element(transition_e, 'effect')\n-    _insert_new_sub_element(effect_e, 'name', text=transition_item.name)\n-    _insert_new_sub_element(effect_e, 'effectid', text=effectid)\n-    _insert_new_sub_element(effect_e, 'effecttype', text='transition')\n-    _insert_new_sub_element(effect_e, 'mediatype', text='video')\n+    # Only add an alignment if it didn't already come in from the metadata dict\n+    if transition_e.find(\"alignment\") is None:\n+        # default center aligned\n+        alignment = \"center\"\n+        if not transition_item.in_offset.value:\n+            alignment = 'start-black'\n+        elif not transition_item.out_offset.value:\n+            alignment = 'end-black'\n+\n+        _append_new_sub_element(transition_e, 'alignment', text=alignment)\n+        # todo support 'start' and 'end' alignment\n+\n+    transition_e.append(_build_rate(timeline_range.start_time.rate))\n+\n+    # Only add an effect if it didn't already come in from the metadata dict\n+    if not transition_e.find(\"./effect\"):\n+        try:\n+            effectid = transition_item.metadata[META_NAMESPACE][\"effectid\"]\n+        except KeyError:\n+            effectid = \"Cross Dissolve\"\n+\n+        effect_e = _append_new_sub_element(transition_e, 'effect')\n+        _append_new_sub_element(effect_e, 'name', text=transition_item.name)\n+        _append_new_sub_element(effect_e, 'effectid', text=effectid)\n+        _append_new_sub_element(effect_e, 'effecttype', text='transition')\n+        _append_new_sub_element(effect_e, 'mediatype', text='video')\n \n     return transition_e\n \n \n-def _build_clip_item_without_media(clip_item, timeline_range,\n-                                   transition_offsets, br_map):\n-    clip_item_e = cElementTree.Element('clipitem', frameBlend='FALSE')\n-    start_time = clip_item.source_range.start_time\n+@_backreference_build(\"clipitem\")\n+def _build_clip_item_without_media(\n+    clip_item,\n+    timeline_range,\n+    transition_offsets,\n+    br_map,\n+):\n+    # TODO: Does this need to be a separate function or could it be unified\n+    #       with _build_clip_item?\n+    clip_item_e = _element_with_item_metadata(\"clipitem\", clip_item)\n+    if \"frameBlend\" not in clip_item_e.attrib:\n+        clip_item_e.attrib[\"frameBlend\"] = \"FALSE\"\n+\n+    if clip_item.media_reference.available_range:\n+        media_start_time = clip_item.media_reference.available_range.start_time\n+    else:\n+        media_start_time = opentime.RationalTime(\n+            0, timeline_range.start_time.rate\n+        )\n \n-    _insert_new_sub_element(clip_item_e, 'name', text=clip_item.name)\n-    clip_item_e.append(_build_rate(start_time))\n+    _append_new_sub_element(clip_item_e, 'name', text=clip_item.name)\n     clip_item_e.append(\n-        _build_empty_file(clip_item.media_reference, start_time, br_map)\n+        _build_empty_file(\n+            clip_item.media_reference, timeline_range, br_map\n+        )\n     )\n     clip_item_e.extend([_build_marker(m) for m in clip_item.markers])\n-    timecode = otio.opentime.RationalTime(0, timeline_range.start_time.rate)\n \n     _build_item_timings(\n         clip_item_e,\n         clip_item,\n         timeline_range,\n         transition_offsets,\n-        timecode\n+        media_start_time,\n     )\n \n     return clip_item_e\n \n \n+@_backreference_build(\"clipitem\")\n def _build_clip_item(clip_item, timeline_range, transition_offsets, br_map):\n-    clip_item_e = cElementTree.Element('clipitem', frameBlend='FALSE')\n+    is_generator = isinstance(\n+        clip_item.media_reference, schema.GeneratorReference\n+    )\n+\n+    tagname = \"generatoritem\" if is_generator else \"clipitem\"\n+    clip_item_e = _element_with_item_metadata(tagname, clip_item)\n+    if \"frameBlend\" not in clip_item_e.attrib:\n+        clip_item_e.attrib[\"frameBlend\"] = \"FALSE\"\n+\n+    if is_generator:\n+        clip_item_e.append(_build_generator_effect(clip_item, br_map))\n+    else:\n+        clip_item_e.append(_build_file(clip_item.media_reference, br_map))\n \n     # set the clip name from the media reference if not defined on the clip\n     if clip_item.name is not None:\n         name = clip_item.name\n+    elif is_generator:\n+        name = clip_item.media_reference.name\n     else:\n         url_path = _url_to_path(clip_item.media_reference.target_url)\n         name = os.path.basename(url_path)\n \n-    _insert_new_sub_element(\n-        clip_item_e,\n-        'name',\n-        text=name\n-    )\n-    clip_item_e.append(_build_file(clip_item.media_reference, br_map))\n+    _append_new_sub_element(clip_item_e, 'name', text=name)\n+\n     if clip_item.media_reference.available_range:\n         clip_item_e.append(\n-            _build_rate(clip_item.source_range.start_time)\n+            _build_rate(clip_item.source_range.start_time.rate)\n         )\n     clip_item_e.extend(_build_marker(m) for m in clip_item.markers)\n \n     if clip_item.media_reference.available_range:\n         timecode = clip_item.media_reference.available_range.start_time\n-\n-        _build_item_timings(\n-            clip_item_e,\n-            clip_item,\n-            timeline_range,\n-            transition_offsets,\n-            timecode\n+    else:\n+        timecode = opentime.RationalTime(\n+            0, clip_item.source_range.start_time.rate\n         )\n \n+    _build_item_timings(\n+        clip_item_e,\n+        clip_item,\n+        timeline_range,\n+        transition_offsets,\n+        timecode\n+    )\n+\n     return clip_item_e\n \n \n+def _build_generator_effect(clip_item, br_map):\n+    \"\"\"\n+    Builds an effect element for the generator ref on the provided clip item.\n+\n+    :param clip_item: a clip with a :class:`schema.GeneratorReference` as\n+        its ``media_reference``.\n+    :param br_map: The backreference map.\n+    \"\"\"\n+    # Since we don't support effects in a standard way, just try and build\n+    # based on the metadata provided at deserialization so we can roundtrip\n+    generator_ref = clip_item.media_reference\n+    try:\n+        fcp_xml_effect_info = generator_ref.metadata[META_NAMESPACE]\n+    except KeyError:\n+        return _build_empty_file(\n+            generator_ref,\n+            clip_item.source_range,\n+            br_map,\n+        )\n+\n+    # Get the XML Tree built from the metadata\n+    effect_element = _dict_to_xml_tree(fcp_xml_effect_info, \"effect\")\n+\n+    # Validate the metadata and make sure it contains the required elements\n+    for required in (\"effectid\", \"effecttype\", \"mediatype\", \"effectcategory\"):\n+        if effect_element.find(required) is None:\n+            return _build_empty_file(\n+                generator_ref,\n+                clip_item.source_range,\n+                br_map,\n+            )\n+\n+    # Add the name\n+    _append_new_sub_element(effect_element, \"name\", text=generator_ref.name)\n+\n+    return effect_element\n+\n+\n+@_backreference_build(\"clipitem\")\n def _build_track_item(track, timeline_range, transition_offsets, br_map):\n-    clip_item_e = cElementTree.Element('clipitem', frameBlend='FALSE')\n+    clip_item_e = _element_with_item_metadata(\"clipitem\", track)\n+    if \"frameBlend\" not in clip_item_e.attrib:\n+        clip_item_e.attrib[\"frameBlend\"] = \"FALSE\"\n \n-    _insert_new_sub_element(\n+    _append_new_sub_element(\n         clip_item_e,\n         'name',\n         text=os.path.basename(track.name)\n     )\n \n-    track_e = _build_track(track, timeline_range, br_map)\n+    track_e = _build_sequence_for_stack(track, timeline_range, br_map)\n \n-    clip_item_e.append(_build_rate(track.source_range.start_time))\n+    clip_item_e.append(_build_rate(track.source_range.start_time.rate))\n     clip_item_e.extend([_build_marker(m) for m in track.markers])\n     clip_item_e.append(track_e)\n-    timecode = otio.opentime.RationalTime(0, timeline_range.start_time.rate)\n+    timecode = opentime.RationalTime(0, timeline_range.start_time.rate)\n \n     _build_item_timings(\n         clip_item_e,",
        "comments": [],
        "commit_message": "Merge branch 'master' of https://github.com/PixarAnimationStudios/OpenTimelineIO into fcpx_enhancements",
        "commit_id": "858f308f6f6d2d5a89158e1739642a50fc7241f3"
    },
    {
        "pr_title": "Add support for more elements and XMLs without libraries",
        "pr_number": 515,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -785,17 +1699,17 @@\ndef _build_track_item(track, timeline_range, transition_offsets, br_map):\n \n \n def _build_item(item, timeline_range, transition_offsets, br_map):\n-    if isinstance(item, otio.schema.Transition):\n+    if isinstance(item, schema.Transition):\n         return _build_transition_item(\n             item,\n             timeline_range,\n             transition_offsets,\n             br_map\n         )\n-    elif isinstance(item, otio.schema.Clip):\n+    elif isinstance(item, schema.Clip):\n         if isinstance(\n             item.media_reference,\n-            otio.schema.MissingReference\n+            schema.MissingReference\n         ):\n             return _build_clip_item_without_media(\n                 item,",
        "comments": [],
        "commit_message": "Merge branch 'master' of https://github.com/PixarAnimationStudios/OpenTimelineIO into fcpx_enhancements",
        "commit_id": "858f308f6f6d2d5a89158e1739642a50fc7241f3"
    },
    {
        "pr_title": "Add support for more elements and XMLs without libraries",
        "pr_number": 515,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -810,7 +1724,7 @@\ndef _build_item(item, timeline_range, transition_offsets, br_map):\n                 transition_offsets,\n                 br_map\n             )\n-    elif isinstance(item, otio.schema.Stack):\n+    elif isinstance(item, schema.Stack):\n         return _build_track_item(\n             item,\n             timeline_range,",
        "comments": [],
        "commit_message": "Merge branch 'master' of https://github.com/PixarAnimationStudios/OpenTimelineIO into fcpx_enhancements",
        "commit_id": "858f308f6f6d2d5a89158e1739642a50fc7241f3"
    },
    {
        "pr_title": "Add support for more elements and XMLs without libraries",
        "pr_number": 515,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -822,30 +1736,30 @@\ndef _build_item(item, timeline_range, transition_offsets, br_map):\n \n \n def _build_top_level_track(track, track_rate, br_map):\n-    track_e = cElementTree.Element('track')\n+    track_e = _element_with_item_metadata(\"track\", track)\n \n     for n, item in enumerate(track):\n-        if isinstance(item, otio.schema.Gap):\n+        if isinstance(item, schema.Gap):\n             continue\n \n         transition_offsets = [None, None]\n         previous_item = track[n - 1] if n > 0 else None\n         next_item = track[n + 1] if n + 1 < len(track) else None\n-        if not isinstance(item, otio.schema.Transition):\n+        if not isinstance(item, schema.Transition):\n             # find out if this item has any neighboring transition\n-            if isinstance(previous_item, otio.schema.Transition):\n+            if isinstance(previous_item, schema.Transition):\n                 if previous_item.out_offset.value:\n                     transition_offsets[0] = previous_item.in_offset\n                 else:\n                     transition_offsets[0] = None\n-            if isinstance(next_item, otio.schema.Transition):\n+            if isinstance(next_item, schema.Transition):\n                 if next_item.in_offset.value:\n                     transition_offsets[1] = next_item.out_offset\n                 else:\n                     transition_offsets[1] = None\n \n         timeline_range = track.range_of_child_at_index(n)\n-        timeline_range = otio.opentime.TimeRange(\n+        timeline_range = opentime.TimeRange(\n             timeline_range.start_time.rescaled_to(track_rate),\n             timeline_range.duration.rescaled_to(track_rate)\n         )",
        "comments": [],
        "commit_message": "Merge branch 'master' of https://github.com/PixarAnimationStudios/OpenTimelineIO into fcpx_enhancements",
        "commit_id": "858f308f6f6d2d5a89158e1739642a50fc7241f3"
    },
    {
        "pr_title": "Add support for more elements and XMLs without libraries",
        "pr_number": 515,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -857,60 +1771,125 @@\ndef _build_top_level_track(track, track_rate, br_map):\n \n \n def _build_marker(marker):\n-    marker_e = cElementTree.Element('marker')\n+    marker_e = _element_with_item_metadata(\"marker\", marker)\n \n-    comment = marker.metadata.get(META_NAMESPACE, {}).get('comment', '')\n     marked_range = marker.marked_range\n \n-    _insert_new_sub_element(marker_e, 'comment', text=comment)\n-    _insert_new_sub_element(marker_e, 'name', text=marker.name)\n-    _insert_new_sub_element(\n+    _append_new_sub_element(marker_e, 'name', text=marker.name)\n+    _append_new_sub_element(\n         marker_e, 'in',\n         text='{:.0f}'.format(marked_range.start_time.value)\n     )\n-    _insert_new_sub_element(marker_e, 'out', text='-1')\n+    _append_new_sub_element(marker_e, 'out', text='-1')\n \n     return marker_e\n \n \n+def _build_timecode_from_metadata(time, tc_metadata=None):\n+    \"\"\"\n+    Makes a timecode element with the given time and (if available)\n+    ```timecode`` metadata stashed on input.\n+\n+    :param time: The :class:`opentime.RationalTime` to encode.\n+    :param tc_metadata: The xml dict for the ``timecode`` element populated\n+        on read.\n+\n+    :return: A timecode element.\n+    \"\"\"\n+    if tc_metadata is None:\n+        tc_metadata = {}\n+\n+    try:\n+        # Parse the rate in the preserved metadata, if available\n+        tc_rate = _rate_for_element(\n+            _dict_to_xml_tree(tc_metadata[\"rate\"], \"rate\")\n+        )\n+    except KeyError:\n+        # Default to the rate in the start time\n+        tc_rate = time.rate\n+\n+    drop_frame = (tc_metadata.get(\"displayformat\", \"NDF\") == \"DF\")\n+\n+    return _build_timecode(\n+        time,\n+        tc_rate,\n+        drop_frame,\n+        additional_metadata=tc_metadata,\n+    )\n+\n+\n+@_backreference_build('sequence')\n+def _build_sequence_for_timeline(timeline, timeline_range, br_map):\n+    sequence_e = _element_with_item_metadata(\"sequence\", timeline)\n+\n+    _add_stack_elements_to_sequence(\n+        timeline.tracks, sequence_e, timeline_range, br_map\n+    )\n+\n+    # In the case of timelines, use the timeline name rather than the stack\n+    # name.\n+    if timeline.name:\n+        sequence_e.find('./name').text = timeline.name\n+\n+    # Add the sequence global start\n+    if timeline.global_start_time is not None:\n+        seq_tc_metadata = timeline.metadata.get(META_NAMESPACE, {}).get(\n+            \"timecode\"\n+        )\n+        tc_element = _build_timecode_from_metadata(\n+            timeline.global_start_time, seq_tc_metadata\n+        )\n+        sequence_e.append(tc_element)\n+\n+    return sequence_e\n+\n+\n @_backreference_build('sequence')\n-def _build_track(stack, timeline_range, br_map):\n-    track_e = cElementTree.Element('sequence')\n-    _insert_new_sub_element(track_e, 'name', text=stack.name)\n-    _insert_new_sub_element(\n-        track_e, 'duration',\n+def _build_sequence_for_stack(stack, timeline_range, br_map):\n+    sequence_e = _element_with_item_metadata(\"sequence\", stack)\n+\n+    _add_stack_elements_to_sequence(stack, sequence_e, timeline_range, br_map)\n+\n+    return sequence_e\n+\n+\n+def _add_stack_elements_to_sequence(stack, sequence_e, timeline_range, br_map):\n+    _append_new_sub_element(sequence_e, 'name', text=stack.name)\n+    _append_new_sub_element(\n+        sequence_e, 'duration',\n         text='{:.0f}'.format(timeline_range.duration.value)\n     )\n-    track_e.append(_build_rate(timeline_range.start_time))\n+    sequence_e.append(_build_rate(timeline_range.start_time.rate))\n     track_rate = timeline_range.start_time.rate\n \n-    media_e = _insert_new_sub_element(track_e, 'media')\n-    video_e = _insert_new_sub_element(media_e, 'video')\n-    audio_e = _insert_new_sub_element(media_e, 'audio')\n+    media_e = _get_or_create_subelement(sequence_e, \"media\")\n+    video_e = _get_or_create_subelement(media_e, 'video')\n+    audio_e = _get_or_create_subelement(media_e, 'audio')\n \n     for track in stack:\n-        if track.kind == otio.schema.TrackKind.Video:\n-            video_e.append(_build_top_level_track(track, track_rate, br_map))\n-        elif track.kind == otio.schema.TrackKind.Audio:\n-            audio_e.append(_build_top_level_track(track, track_rate, br_map))\n+        track_elements = _build_top_level_track(track, track_rate, br_map)\n+        if track.kind == schema.TrackKind.Video:\n+            video_e.append(track_elements)\n+        elif track.kind == schema.TrackKind.Audio:\n+            audio_e.append(track_elements)\n \n     for marker in stack.markers:\n-        track_e.append(_build_marker(marker))\n-\n-    return track_e\n+        sequence_e.append(_build_marker(marker))\n \n \n def _build_collection(collection, br_map):\n     tracks = []\n     for item in collection:\n-        if not isinstance(item, otio.schema.Timeline):\n+        if not isinstance(item, schema.Timeline):\n             continue\n \n-        timeline_range = otio.opentime.TimeRange(\n+        timeline_range = opentime.TimeRange(\n             start_time=item.global_start_time,\n             duration=item.duration()\n         )\n-        tracks.append(_build_track(item.tracks, timeline_range, br_map))\n+        tracks.append(\n+            _build_sequence_for_timeline(item, timeline_range, br_map)\n+        )\n \n     return tracks",
        "comments": [],
        "commit_message": "Merge branch 'master' of https://github.com/PixarAnimationStudios/OpenTimelineIO into fcpx_enhancements",
        "commit_id": "858f308f6f6d2d5a89158e1739642a50fc7241f3"
    },
    {
        "pr_title": "Add support for more elements and XMLs without libraries",
        "pr_number": 515,
        "file_name": "opentimelineio_contrib/adapters/aaf_adapter/aaf_writer.py",
        "code_diff": "@@ -33,6 +33,7 @@\nimport opentimelineio as otio\n import os\n import copy\n+import re\n \n \n AAF_PARAMETERDEF_PAN = aaf2.auid.AUID(\"e4962322-2267-11d3-8a4c-0050040ef7d2\")",
        "comments": [],
        "commit_message": "Merge branch 'master' of https://github.com/PixarAnimationStudios/OpenTimelineIO into fcpx_enhancements",
        "commit_id": "858f308f6f6d2d5a89158e1739642a50fc7241f3"
    },
    {
        "pr_title": "Add support for more elements and XMLs without libraries",
        "pr_number": 515,
        "file_name": "opentimelineio_contrib/adapters/aaf_adapter/aaf_writer.py",
        "code_diff": "@@ -52,6 +53,10 @@\nclass AAFAdapterError(otio.exceptions.OTIOError):\n     pass\n \n \n+class AAFValidationError(AAFAdapterError):\n+    pass\n+\n+\n class AAFFileTranscriber(object):\n     \"\"\"\n     AAFFileTranscriber",
        "comments": [],
        "commit_message": "Merge branch 'master' of https://github.com/PixarAnimationStudios/OpenTimelineIO into fcpx_enhancements",
        "commit_id": "858f308f6f6d2d5a89158e1739642a50fc7241f3"
    },
    {
        "pr_title": "Add support for more elements and XMLs without libraries",
        "pr_number": 515,
        "file_name": "opentimelineio_contrib/adapters/aaf_adapter/aaf_writer.py",
        "code_diff": "@@ -132,68 +137,40 @@\ndef track_transcriber(self, otio_track):\n def validate_metadata(timeline):\n     \"\"\"Print a check of necessary metadata requirements for an otio timeline.\"\"\"\n \n-    errors = []\n-\n-    def _check(otio_child, keys_path):\n-        keys = keys_path.split(\".\")\n-        value = otio_child.metadata\n-        try:\n-            for key in keys:\n-                value = value[key]\n-        except KeyError:\n-            print(\"{}({}) is missing required metadata {}\".format(\n-                  otio_child.name, type(otio_child), keys_path))\n-            errors.append((otio_child, keys_path))\n-\n-    def _check_equal(a, b, msg):\n-        if a != b:\n-            print(msg + \": {} vs. {}\".format(a, b))\n-            errors.append(msg)\n-\n-    # TODO: Check for existence of media_reference and media_reference.available_range\n-\n-    # Edit rate conformity\n-    edit_rate = timeline.duration().rate\n-    for otio_child in timeline.each_child():\n-        msg = \"{}({}) edit rate mismatch in \".format(otio_child.name, type(otio_child))\n-        if isinstance(otio_child, otio.schema.Gap):\n-            _check_equal(edit_rate,\n-                         otio_child.visible_range().duration.rate,\n-                         msg + \"visible_range().duration.rate\")\n-            _check_equal(edit_rate,\n-                         otio_child.duration().rate,\n-                         msg + \"duration().rate\")\n-        elif isinstance(otio_child, otio.schema.Clip):\n-            _check_equal(edit_rate,\n-                         otio_child.visible_range().duration.rate,\n-                         msg + \"visible_range().duration.rate\")\n-            _check_equal(edit_rate,\n-                         otio_child.duration().rate,\n-                         msg + \"duration().rate\")\n-            _check_equal(edit_rate,\n-                         otio_child.media_reference.available_range.duration.rate,\n-                         msg + \"media_reference.available_range.duration.rate\")\n-            _check_equal(edit_rate,\n-                         otio_child.media_reference.available_range.start_time.rate,\n-                         msg + \"media_reference.available_range.start_time.rate\")\n-        elif isinstance(otio_child, otio.schema.Transition):\n-            _check_equal(edit_rate,\n-                         otio_child.duration().rate,\n-                         msg + \"duration().rate\")\n-\n-    # Required metadata\n-    for otio_child in timeline.each_child():\n-        if isinstance(otio_child, otio.schema.Transition):\n-            _check(otio_child, \"AAF.PointList\")\n-            _check(otio_child, \"AAF.OperationGroup\")\n-            _check(otio_child, \"AAF.OperationGroup.Operation\")\n-            _check(otio_child,\n-                   \"AAF.OperationGroup.Operation.DataDefinition.Name\")\n-            _check(otio_child, \"AAF.OperationGroup.Operation.Description\")\n-            _check(otio_child, \"AAF.OperationGroup.Operation.Name\")\n-            _check(otio_child, \"AAF.CutPoint\")\n-\n-    return errors\n+    all_checks = [__check(timeline, \"duration().rate\")]\n+    edit_rate = __check(timeline, \"duration().rate\").value\n+\n+    for child in timeline.each_child():\n+        checks = []\n+        if isinstance(child, otio.schema.Gap):\n+            checks = [\n+                __check(child, \"duration().rate\").equals(edit_rate)\n+            ]\n+        if isinstance(child, otio.schema.Clip):\n+            checks = [\n+                __check(child, \"duration().rate\").equals(edit_rate),\n+                __check(child, \"media_reference.available_range.duration.rate\"\n+                        ).equals(edit_rate),\n+                __check(child, \"media_reference.available_range.start_time.rate\"\n+                        ).equals(edit_rate)\n+            ]\n+        if isinstance(child, otio.schema.Transition):\n+            checks = [\n+                __check(child, \"duration().rate\").equals(edit_rate),\n+                __check(child, \"metadata['AAF']['PointList']\"),\n+                __check(child, \"metadata['AAF']['OperationGroup']['Operation']\"\n+                        \"['DataDefinition']['Name']\"),\n+                __check(child, \"metadata['AAF']['OperationGroup']['Operation']\"\n+                        \"['Description']\"),\n+                __check(child, \"metadata['AAF']['OperationGroup']['Operation']\"\n+                        \"['Name']\"),\n+                __check(child, \"metadata['AAF']['CutPoint']\")\n+            ]\n+        all_checks.extend(checks)\n+\n+    if any(check.errors for check in all_checks):\n+        raise AAFValidationError(\"\\n\" + \"\\n\".join(\n+            sum([check.errors for check in all_checks], [])))\n \n \n def _gather_clip_mob_ids(input_otio,",
        "comments": [],
        "commit_message": "Merge branch 'master' of https://github.com/PixarAnimationStudios/OpenTimelineIO into fcpx_enhancements",
        "commit_id": "858f308f6f6d2d5a89158e1739642a50fc7241f3"
    },
    {
        "pr_title": "Add support for more elements and XMLs without libraries",
        "pr_number": 515,
        "file_name": "opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -32,7 +32,10 @@\nimport tempfile\n \n import opentimelineio as otio\n-from opentimelineio_contrib.adapters.aaf_adapter.aaf_writer import AAFAdapterError\n+from opentimelineio_contrib.adapters.aaf_adapter.aaf_writer import (\n+    AAFAdapterError,\n+    AAFValidationError\n+)\n \n SAMPLE_DATA_DIR = os.path.join(os.path.dirname(__file__), \"sample_data\")\n SIMPLE_EXAMPLE_PATH = os.path.join(",
        "comments": [],
        "commit_message": "Merge branch 'master' of https://github.com/PixarAnimationStudios/OpenTimelineIO into fcpx_enhancements",
        "commit_id": "858f308f6f6d2d5a89158e1739642a50fc7241f3"
    },
    {
        "pr_title": "Add support for more elements and XMLs without libraries",
        "pr_number": 515,
        "file_name": "opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -71,6 +74,10 @@\nSAMPLE_DATA_DIR,\n     \"misc_speed_effects.aaf\"\n )\n+PRECHECK_FAIL_OTIO = os.path.join(\n+    SAMPLE_DATA_DIR,\n+    \"precheckfail.otio\"\n+)\n LINEAR_SPEED_EFFECTS_EXAMPLE_PATH = os.path.join(\n     SAMPLE_DATA_DIR,\n     \"linear_speed_effects.aaf\"",
        "comments": [],
        "commit_message": "Merge branch 'master' of https://github.com/PixarAnimationStudios/OpenTimelineIO into fcpx_enhancements",
        "commit_id": "858f308f6f6d2d5a89158e1739642a50fc7241f3"
    },
    {
        "pr_title": "Add support for more elements and XMLs without libraries",
        "pr_number": 515,
        "file_name": "tests/test_fcp7_xml_adapter.py",
        "code_diff": "@@ -25,41 +25,905 @@\n\"\"\"Test final cut pro xml.\"\"\"\n \n # python\n+import json\n import os\n import tempfile\n import unittest\n-import collections\n from xml.etree import cElementTree\n \n-import opentimelineio as otio\n-from opentimelineio.exceptions import CannotComputeAvailableRangeError\n-import opentimelineio.test_utils as otio_test_utils\n+from opentimelineio import (\n+    adapters,\n+    opentime,\n+    schema,\n+    test_utils,\n+)\n \n SAMPLE_DATA_DIR = os.path.join(os.path.dirname(__file__), \"sample_data\")\n FCP7_XML_EXAMPLE_PATH = os.path.join(SAMPLE_DATA_DIR, \"premiere_example.xml\")\n SIMPLE_XML_PATH = os.path.join(SAMPLE_DATA_DIR, \"sample_just_track.xml\")\n HIERO_XML_PATH = os.path.join(SAMPLE_DATA_DIR, \"hiero_xml_export.xml\")\n+FILTER_XML_EXAMPLE_PATH = os.path.join(\n+    SAMPLE_DATA_DIR, \"premiere_example_filter.xml\"\n+)\n+FILTER_JSON_EXAMPLE_PATH = os.path.join(\n+    SAMPLE_DATA_DIR, \"premiere_example_filter.json\"\n+)\n \n \n-class AdaptersFcp7XmlTest(unittest.TestCase, otio_test_utils.OTIOAssertions):\n+class TestFcp7XmlUtilities(unittest.TestCase, test_utils.OTIOAssertions):\n+    adapter = adapters.from_name('fcp_xml').module()\n+\n+    def test_xml_tree_to_dict(self):\n+        self.maxDiff = None\n+\n+        with open(FILTER_JSON_EXAMPLE_PATH) as f:\n+            ref_dict = json.load(f)\n+\n+        tree = cElementTree.parse(FILTER_XML_EXAMPLE_PATH)\n+        filter_element = tree.getroot()\n+        xml_dict = self.adapter._xml_tree_to_dict(filter_element)\n+\n+        self.assertEqual(xml_dict, ref_dict)\n+\n+        out_xml = self.adapter._dict_to_xml_tree(xml_dict, \"filter\")\n+        out_xml_string = self.adapter._make_pretty_string(out_xml)\n+\n+        with open(FILTER_XML_EXAMPLE_PATH) as f:\n+            orig_xml_string = f.read()\n+\n+        self.assertEqual(out_xml_string.strip(), orig_xml_string.strip())\n+\n+        # validate empty tag handling\n+        empty_element = cElementTree.fromstring(\n+            \"<top><empty/></top>\"\n+        )\n+        empty_element_dict = self.adapter._xml_tree_to_dict(empty_element)\n+        self.assertIsNone(empty_element_dict[\"empty\"])\n+\n+        empty_xml = self.adapter._dict_to_xml_tree(empty_element_dict, \"top\")\n+        self.assertIsNone(empty_xml.find(\"empty\").text)\n+\n+        roundtrip_dict = self.adapter._xml_tree_to_dict(empty_xml)\n+        self.assertEqual(empty_element_dict, roundtrip_dict)\n+\n+    def test_bool_value(self):\n+        truthy_element = cElementTree.fromstring(\"<ntsc>TRUE</ntsc>\")\n+        self.assertTrue(self.adapter._bool_value(truthy_element))\n+\n+        falsy_element = cElementTree.fromstring(\"<ntsc>FALSE</ntsc>\")\n+        self.assertFalse(self.adapter._bool_value(falsy_element))\n+\n+    def test_backreference_for_id(self):\n+        item1 = schema.Clip(name=\"clip1\")\n+        item1_prime = schema.Clip(name=\"clip1\")\n+        item2 = schema.Clip(name=\"clip2\")\n+\n+        br_map = {}\n+        item1_id, item1_is_new = self.adapter._backreference_for_item(\n+            item1, \"clipitem\", br_map\n+        )\n+        self.assertEqual(item1_id, \"clipitem-1\")\n+        self.assertTrue(item1_is_new)\n+\n+        item2_id, item2_is_new = self.adapter._backreference_for_item(\n+            item2, \"clipitem\", br_map\n+        )\n+        self.assertEqual(item2_id, \"clipitem-2\")\n+        self.assertTrue(item2_is_new)\n+\n+        (\n+            item1_prime_id, item1_prime_is_new\n+        ) = self.adapter._backreference_for_item(\n+            item1_prime, \"clipitem\", br_map\n+        )\n+        self.assertEqual(item1_prime_id, \"clipitem-1\")\n+        self.assertFalse(item1_prime_is_new)\n+\n+    def test_backreference_for_id_preserved(self):\n+        item1 = schema.Clip(\n+            name=\"clip23\",\n+            metadata={\"fcp_xml\": {\"@id\": \"clipitem-23\"}},\n+        )\n+        item2 = schema.Clip(name=\"clip2\")\n+        conflicting_item = schema.Clip(\n+            name=\"conflicting_clip\",\n+            metadata={\"fcp_xml\": {\"@id\": \"clipitem-1\"}},\n+        )\n+\n+        # prepopulate the backref map with some ids\n+        br_map = {\n+            \"clipitem\": {\n+                \"bogus_hash\": 1, \"bogus_hash_2\": 2, \"bogus_hash_3\": 3\n+            }\n+        }\n+\n+        # Make sure the id is preserved\n+        item1_id, item1_is_new = self.adapter._backreference_for_item(\n+            item1, \"clipitem\", br_map\n+        )\n+        self.assertEqual(item1_id, \"clipitem-23\")\n+        self.assertTrue(item1_is_new)\n+\n+        # Make sure the next item continues to fill in\n+        item2_id, item2_is_new = self.adapter._backreference_for_item(\n+            item2, \"clipitem\", br_map\n+        )\n+        self.assertEqual(item2_id, \"clipitem-4\")\n+        self.assertTrue(item2_is_new)\n+\n+        # Make sure confilcting clips don't stomp existing ones\n+        item3_id, item3_is_new = self.adapter._backreference_for_item(\n+            conflicting_item, \"clipitem\", br_map\n+        )\n+        self.assertEqual(item3_id, \"clipitem-5\")\n+        self.assertTrue(item3_is_new)\n+\n+    def test_name_from_element(self):\n+        sequence_element = cElementTree.fromstring(\n+            \"\"\"\n+            <sequence>\n+                <name>My Sequence</name>\n+            </sequence>\n+            \"\"\"\n+        )\n+        name = self.adapter._name_from_element(sequence_element)\n+        self.assertEqual(name, \"My Sequence\")\n+\n+        empty_element = cElementTree.fromstring(\"<sequence></sequence>\")\n+        empty_name = self.adapter._name_from_element(empty_element)\n+        self.assertIsNone(empty_name)\n+\n+    def test_rate_for_element_ntsc_conversion_23976(self):\n+        rate_element = cElementTree.fromstring(\n+            \"\"\"\n+            <rate>\n+                <timebase>24</timebase>\n+                <ntsc>TRUE</ntsc>\n+            </rate>\n+            \"\"\"\n+        )\n+        rate = self.adapter._rate_for_element(rate_element)\n+\n+        self.assertEqual(rate, (24000 / 1001.0))\n+\n+    def test_rate_for_element_ntsc_conversion_24(self):\n+        rate_element = cElementTree.fromstring(\n+            \"\"\"\n+            <rate>\n+                <timebase>24</timebase>\n+                <ntsc>FALSE</ntsc>\n+            </rate>\n+            \"\"\"\n+        )\n+        rate = self.adapter._rate_for_element(rate_element)\n+\n+        self.assertEqual(rate, 24)\n+\n+    def test_rate_for_element_ntsc_conversion_2997(self):\n+        rate_element = cElementTree.fromstring(\n+            \"\"\"\n+            <rate>\n+                <timebase>30</timebase>\n+                <ntsc>TRUE</ntsc>\n+            </rate>\n+            \"\"\"\n+        )\n+        rate = self.adapter._rate_for_element(rate_element)\n+\n+        self.assertEqual(rate, (30000 / 1001.0))\n+\n+    def test_rate_for_element_ntsc_conversion_30(self):\n+        rate_element = cElementTree.fromstring(\n+            \"\"\"\n+            <rate>\n+                <timebase>30</timebase>\n+                <ntsc>FALSE</ntsc>\n+            </rate>\n+            \"\"\"\n+        )\n+        rate = self.adapter._rate_for_element(rate_element)\n+\n+        self.assertEqual(rate, 30)\n+\n+    def test_rate_from_context(self):\n+        sequence_elem = cElementTree.fromstring(\n+            \"\"\"\n+            <sequence>\n+                <rate>\n+                    <timebase>30</timebase>\n+                    <ntsc>TRUE</ntsc>\n+                </rate>\n+            </sequence>\n+            \"\"\"\n+        )\n+\n+        # Fetch rate from one level of context\n+        sequence_context = self.adapter._Context(sequence_elem)\n+        sequence_rate = self.adapter._rate_from_context(sequence_context)\n+        self.assertEqual(sequence_rate, (30000 / 1001.0))\n+\n+        track_elem = cElementTree.fromstring(\n+            \"\"\"\n+            <track>\n+              <rate>\n+                <timebase>24</timebase>\n+                <ntsc>TRUE</ntsc>\n+              </rate>\n+            </track>\n+            \"\"\"\n+        )\n+\n+        # make sure pushing a context with a new rate overrides the old rate\n+        track_context = sequence_context.context_pushing_element(track_elem)\n+        track_noinherit_rate = self.adapter._rate_from_context(track_context)\n+        self.assertEqual(track_noinherit_rate, (24000 / 1001.0))\n+\n+        clip_norate_elem = cElementTree.fromstring(\n+            \"\"\"\n+            <clipitem>\n+            <name>Just soeme clip</name>\n+            </clipitem>\n+            \"\"\"\n+        )\n+\n+        # Make sure pushing a context element with no rate inherits the next\n+        # level up\n+        clip_context = track_context.context_pushing_element(clip_norate_elem)\n+        clip_inherit_rate = self.adapter._rate_from_context(clip_context)\n+        self.assertEqual(clip_inherit_rate, (24000 / 1001.0))\n+\n+    def test_time_from_timecode_element(self):\n+        tc_element = cElementTree.fromstring(\n+            \"\"\"\n+            <timecode>\n+                <rate>\n+                    <timebase>30</timebase>\n+                    <ntsc>FALSE</ntsc>\n+                </rate>\n+                <string>01:00:00:00</string>\n+                <frame>108000</frame>\n+                <displayformat>NDF</displayformat>\n+            </timecode>\n+            \"\"\"\n+        )\n+        time = self.adapter._time_from_timecode_element(tc_element)\n+\n+        self.assertEqual(time, opentime.RationalTime(108000, 30))\n+\n+    def test_time_from_timecode_element_drop_frame(self):\n+        tc_element = cElementTree.fromstring(\n+            \"\"\"\n+            <timecode>\n+                <rate>\n+                    <timebase>30</timebase>\n+                    <ntsc>TRUE</ntsc>\n+                </rate>\n+                <string>10:03:00;05</string>\n+                <frame>1084319</frame>\n+                <displayformat>DF</displayformat>\n+            </timecode>\n+            \"\"\"\n+        )\n+        time = self.adapter._time_from_timecode_element(tc_element)\n+\n+        self.assertEqual(\n+            time, opentime.RationalTime(1084319, (30000 / 1001.0))\n+        )\n+\n+    def test_time_from_timecode_element_ntsc_non_drop_frame(self):\n+        tc_element = cElementTree.fromstring(\n+            \"\"\"\n+            <timecode>\n+                <rate>\n+                    <timebase>30</timebase>\n+                    <ntsc>TRUE</ntsc>\n+                </rate>\n+                <string>00:59:56:12</string>\n+                <displayformat>NDF</displayformat>\n+            </timecode>\n+            \"\"\"\n+        )\n+\n+        time = self.adapter._time_from_timecode_element(tc_element)\n+        self.assertEqual(\n+            time, opentime.RationalTime(107892, (30000 / 1001.0))\n+        )\n+\n+    def test_track_kind_from_element(self):\n+        video_element = cElementTree.fromstring(\"<video/>\")\n+        video_kind = self.adapter._track_kind_from_element(video_element)\n+        self.assertEqual(video_kind, schema.TrackKind.Video)\n+\n+        audio_element = cElementTree.fromstring(\"<audio/>\")\n+        audio_kind = self.adapter._track_kind_from_element(audio_element)\n+        self.assertEqual(audio_kind, schema.TrackKind.Audio)\n+\n+        invalid_element = cElementTree.fromstring(\"<smell/>\")\n+        with self.assertRaises(ValueError):\n+            self.adapter._track_kind_from_element(invalid_element)\n+\n+    def test_transition_cut_point(self):\n+        transition_element = cElementTree.fromstring(\n+            \"\"\"\n+            <transitionitem>\n+                <start>538</start>\n+                <end>557</end>\n+                <alignment>end-black</alignment>\n+                <cutPointTicks>160876800000</cutPointTicks>\n+                <rate>\n+                    <timebase>30</timebase>\n+                    <ntsc>FALSE</ntsc>\n+                </rate>\n+                <effect>\n+                    <name>Cross Dissolve</name>\n+                    <effectid>Cross Dissolve</effectid>\n+                    <effectcategory>Dissolve</effectcategory>\n+                    <effecttype>transition</effecttype>\n+                    <mediatype>video</mediatype>\n+                    <wipecode>0</wipecode>\n+                    <wipeaccuracy>100</wipeaccuracy>\n+                    <startratio>0</startratio>\n+                    <endratio>1</endratio>\n+                    <reverse>FALSE</reverse>\n+                </effect>\n+            </transitionitem>\n+            \"\"\"\n+        )\n+        alignment_element = transition_element.find(\"./alignment\")\n+\n+        track_element = cElementTree.fromstring(\n+            \"\"\"\n+            <track>\n+                <rate>\n+                    <timebase>30</timebase>\n+                    <ntsc>FALSE</ntsc>\n+                </rate>\n+            </track>\n+            \"\"\"\n+        )\n+        context = self.adapter._Context(track_element)\n+\n+        cut_point = self.adapter._transition_cut_point(\n+            transition_element, context\n+        )\n+        self.assertEqual(cut_point, opentime.RationalTime(557, 30))\n+\n+        alignment_element.text = \"end-black\"\n+        cut_point = self.adapter._transition_cut_point(\n+            transition_element, context\n+        )\n+        self.assertEqual(cut_point, opentime.RationalTime(557, 30))\n+\n+        for alignment in (\"start\", \"start-black\"):\n+            alignment_element.text = alignment\n+            cut_point = self.adapter._transition_cut_point(\n+                transition_element, context\n+            )\n+            self.assertEqual(cut_point, opentime.RationalTime(538, 30))\n+\n+        # TODO: Mathematically, this cut point falls at 547.5, is the rounding\n+        #       down behavior \"correct\"?\n+        alignment_element.text = \"center\"\n+        cut_point = self.adapter._transition_cut_point(\n+            transition_element, context\n+        )\n+        self.assertEqual(cut_point, opentime.RationalTime(547, 30))\n+\n+\n+class TestFcp7XmlElements(unittest.TestCase, test_utils.OTIOAssertions):\n+    \"\"\" Tests for isolated element parsers. \"\"\"\n+    adapter = adapters.from_name('fcp_xml').module()\n+\n+    def test_timeline_for_sequence(self):\n+        tree = cElementTree.parse(FCP7_XML_EXAMPLE_PATH)\n+\n+        # Get the test sequence and pare out the track definitions to keep this\n+        # test simple.\n+        seq_elem = tree.find(\"sequence\")\n+        seq_elem.find(\"./media\").clear()\n+        seq_elem.find(\"./timecode/string\").text = \"01:00:00:00\"\n+        seq_elem.find(\"./timecode/frame\").text = \"108000\"\n+\n+        parser = self.adapter.FCP7XMLParser(tree)\n+        context = self.adapter._Context()\n+        timeline = parser.timeline_for_sequence(seq_elem, context)\n+\n+        # Spot-check the sequence\n+        self.assertEqual(timeline.name, \"sc01_sh010_layerA\")\n+        self.assertEqual(\n+            timeline.global_start_time, opentime.RationalTime(108000, 30)\n+        )\n+\n+        # Spot check that metadata translated with a tag and a property\n+        adapter_metadata = timeline.metadata[\"fcp_xml\"]\n+        self.assertEqual(\n+            adapter_metadata[\"labels\"][\"label2\"], \"Forest\"\n+        )\n+        self.assertEqual(\n+            adapter_metadata[\"@MZ.Sequence.VideoTimeDisplayFormat\"], \"104\"\n+        )\n+\n+        # make sure the media and name tags were not included in the metadata\n+        for k in {\"name\", \"media\"}:\n+            with self.assertRaises(KeyError):\n+                adapter_metadata[k]\n+\n+    def test_marker_for_element(self):\n+        marker_element = cElementTree.fromstring(\n+            \"\"\"\n+            <marker>\n+              <comment>so, this happened</comment>\n+              <name>My MArker 1</name>\n+              <in>113</in>\n+              <out>-1</out>\n+            </marker>\n+            \"\"\"\n+        )\n+\n+        marker = self.adapter.marker_for_element(marker_element, 30)\n+\n+        self.assertEqual(marker.name, \"My MArker 1\")\n+        self.assertEqual(\n+            marker.marked_range,\n+            opentime.TimeRange(\n+                start_time=opentime.RationalTime(113, 30),\n+                duration=opentime.RationalTime(0, 30),\n+            )\n+        )\n+        self.assertEqual(\n+            marker.metadata[\"fcp_xml\"][\"comment\"], \"so, this happened\"\n+        )\n+        with self.assertRaises(KeyError):\n+            marker.metadata[\"fcp_xml\"][\"name\"]\n+\n+    def test_markers_from_element(self):\n+        sequence_element = cElementTree.fromstring(\n+            \"\"\"\n+            <sequence>\n+              <rate>\n+                <timebase>30</timebase>\n+                <ntsc>FALSE</ntsc>\n+              </rate>\n+              <marker>\n+                <comment>so, this happened</comment>\n+                <name>My MArker 1</name>\n+                <in>113</in>\n+                <out>-1</out>\n+              </marker>\n+              <marker>\n+                <comment>fsfsfs</comment>\n+                <name>dsf</name>\n+                <in>492</in>\n+                <out>-1</out>\n+              </marker>\n+              <marker>\n+                <comment/>\n+                <name/>\n+                <in>298</in>\n+                <out>-1</out>\n+              </marker>\n+              <labels>\n+                <label2>Forest</label2>\n+              </labels>\n+            </sequence>\n+            \"\"\"\n+        )\n+        markers = self.adapter.markers_from_element(sequence_element)\n+\n+        expected_names = [\"My MArker 1\", \"dsf\", None]\n+        self.assertEqual([m.name for m in markers], expected_names)\n+\n+    def test_stack_from_element(self):\n+        tree = cElementTree.parse(FCP7_XML_EXAMPLE_PATH)\n+\n+        # Get the test sequence and pare out the track definitions to keep this\n+        # test simple.\n+        media_elem = tree.find(\"./sequence/media\")\n+\n+        parser = self.adapter.FCP7XMLParser(tree)\n+        context = self.adapter._Context(tree.find(\"./sequence\"))\n+        tracks = parser.stack_for_element(media_elem, context)\n+\n+        self.assertEqual(len(tracks), 8)\n+\n+        audio_tracks = [\n+            t for t in tracks if t.kind == schema.TrackKind.Audio\n+        ]\n+        self.assertEqual(len(audio_tracks), 4)\n+\n+        video_tracks = [\n+            t for t in tracks if t.kind == schema.TrackKind.Video\n+        ]\n+        self.assertEqual(len(video_tracks), 4)\n+\n+    def test_track_for_element(self):\n+        tree = cElementTree.parse(FCP7_XML_EXAMPLE_PATH)\n+\n+        sequence_elem = tree.find(\"./sequence[1]\")\n+        context = self.adapter._Context(sequence_elem)\n+        # The track with \"clipitem-2\" is a decent and relatively complex\n+        # test case\n+        track_elem = sequence_elem.find(\".//clipitem[@id='clipitem-2']/..\")\n+\n+        # Make a parser and prime the id cache by parsing the file\n+        parser = self.adapter.FCP7XMLParser(tree)\n+        parser.timeline_for_sequence(sequence_elem, self.adapter._Context())\n+\n+        track = parser.track_for_element(\n+            track_elem, schema.TrackKind.Video, context\n+        )\n+\n+        expected_instance_types = [\n+            schema.Gap,\n+            schema.Clip,\n+            schema.Gap,\n+            schema.Clip,\n+            schema.Clip,\n+            schema.Transition,\n+            schema.Gap,\n+            schema.Stack,\n+        ]\n+        track_item_types = [i.__class__ for i in track]\n+        self.assertEqual(track_item_types, expected_instance_types)\n+        self.assertEqual(len(track), 8)\n+\n+    def test_media_reference_from_element(self):\n+        file_element = cElementTree.fromstring(\n+            \"\"\"\n+            <file id=\"file-3\">\n+              <name>sc01_sh030_anim.mov</name>\n+              <pathurl>file:///Scratch/media/sc01_sh030_anim.2.mov</pathurl>\n+              <rate>\n+                <timebase>30</timebase>\n+                <ntsc>FALSE</ntsc>\n+              </rate>\n+              <duration>400</duration>\n+              <timecode>\n+                <rate>\n+                  <timebase>30</timebase>\n+                  <ntsc>FALSE</ntsc>\n+                </rate>\n+                <string>01:00:00:00</string>\n+                <frame>108000</frame>\n+                <displayformat>NDF</displayformat>\n+                <reel>\n+                  <name/>\n+                </reel>\n+              </timecode>\n+              <media>\n+                <video>\n+                  <samplecharacteristics>\n+                    <rate>\n+                      <timebase>30</timebase>\n+                      <ntsc>FALSE</ntsc>\n+                    </rate>\n+                    <width>1280</width>\n+                    <height>720</height>\n+                    <anamorphic>FALSE</anamorphic>\n+                    <pixelaspectratio>square</pixelaspectratio>\n+                    <fielddominance>none</fielddominance>\n+                  </samplecharacteristics>\n+                </video>\n+                <audio>\n+                  <samplecharacteristics>\n+                    <depth>16</depth>\n+                    <samplerate>48000</samplerate>\n+                  </samplecharacteristics>\n+                  <channelcount>2</channelcount>\n+                </audio>\n+              </media>\n+            </file>\n+            \"\"\"\n+        )\n+\n+        parser = self.adapter.FCP7XMLParser(file_element)\n+        context = self.adapter._Context()\n+        ref = parser.media_reference_for_file_element(\n+            file_element,\n+            context=context,\n+        )\n+\n+        self.assertEqual(\n+            ref.target_url, \"file:///Scratch/media/sc01_sh030_anim.2.mov\"\n+        )\n+        self.assertEqual(ref.name, \"sc01_sh030_anim.mov\")\n+        self.assertEqual(\n+            ref.available_range,\n+            opentime.TimeRange(\n+                start_time=opentime.RationalTime(108000, 30),\n+                duration=opentime.RationalTime(400, 30),\n+            )\n+        )\n+\n+        # Spot-check a metadata field\n+        video_metadata = ref.metadata[\"fcp_xml\"][\"media\"][\"video\"]\n+        self.assertEqual(\n+            video_metadata[\"samplecharacteristics\"][\"height\"], \"720\"\n+        )\n+\n+    def test_missing_media_reference_from_element(self):\n+        file_element = cElementTree.fromstring(\n+            \"\"\"\n+            <file id=\"101_021_0030_FG01\">\n+              <name>101_021_0030_FG01</name>\n+              <duration>155</duration>\n+              <rate>\n+                <ntsc>FALSE</ntsc>\n+                <timebase>24</timebase>\n+              </rate>\n+              <timecode>\n+                <rate>\n+                  <ntsc>FALSE</ntsc>\n+                  <timebase>24</timebase>\n+                </rate>\n+                <frame>1308828</frame>\n+                <displayformat>NDF</displayformat>\n+                <string>15:08:54:12</string>\n+                <reel>\n+                  <name>A173C021_181204_R207</name>\n+                </reel>\n+              </timecode>\n+            </file>\n+            \"\"\"\n+        )\n+\n+        parser = self.adapter.FCP7XMLParser(file_element)\n+        context = self.adapter._Context()\n+        ref = parser.media_reference_for_file_element(\n+            file_element,\n+            context=context,\n+        )\n+\n+        self.assertTrue(isinstance(ref, schema.MissingReference))\n+        self.assertEqual(ref.name, \"101_021_0030_FG01\")\n+        self.assertEqual(\n+            ref.available_range,\n+            opentime.TimeRange(\n+                start_time=opentime.RationalTime(1308828, 24),\n+                duration=opentime.RationalTime(155, 24),\n+            )\n+        )\n+\n+        # Spot-check a metadata field\n+        reelname = ref.metadata[\"fcp_xml\"][\"timecode\"][\"reel\"][\"name\"]\n+        self.assertEqual(reelname, \"A173C021_181204_R207\")\n+\n+    def test_clip_for_element(self):\n+        tree = cElementTree.parse(FCP7_XML_EXAMPLE_PATH)\n+\n+        # Use clipitem-3 because it's self-contained and doesn't reference\n+        # other elements\n+        sequence_elem = tree.find(\".//clipitem[@id='clipitem-3']/../../../..\")\n+        clip_elem = tree.find(\".//clipitem[@id='clipitem-3']\")\n+        context = self.adapter._Context(sequence_elem)\n+\n+        # Make a parser\n+        parser = self.adapter.FCP7XMLParser(tree)\n+\n+        clip, time_range = parser.item_and_timing_for_element(\n+            clip_elem,\n+            head_transition=None,\n+            tail_transition=None,\n+            context=context,\n+        )\n+\n+        self.assertEqual(clip.name, \"sc01_sh020_anim.mov\")\n+\n+        expected_range = opentime.TimeRange(\n+            start_time=opentime.RationalTime(165, 30),\n+            duration=opentime.RationalTime(157, 30),\n+        )\n+        self.assertEqual(time_range, expected_range)\n+\n+        expected_range = opentime.TimeRange(\n+            start_time=opentime.RationalTime(0, 30),\n+            duration=opentime.RationalTime(157, 30),\n+        )\n+        self.assertEqual(clip.source_range, expected_range)\n+\n+    def test_generator_for_element(self):\n+        generator_element = cElementTree.fromstring(\n+            \"\"\"\n+            <generatoritem id=\"clipitem-29\">\n+              <name>White</name>\n+              <enabled>TRUE</enabled>\n+              <duration>1035764</duration>\n+              <start>383</start>\n+              <end>432</end>\n+              <in>86313</in>\n+              <out>86362</out>\n+              <rate>\n+                <timebase>24</timebase>\n+                <ntsc>TRUE</ntsc>\n+              </rate>\n+              <effect>\n+                <name>Color</name>\n+                <effectid>Color</effectid>\n+                <effectcategory>Matte</effectcategory>\n+                <effecttype>generator</effecttype>\n+                <mediatype>video</mediatype>\n+                <parameter authoringApp=\"PremierePro\">\n+                  <parameterid>fillcolor</parameterid>\n+                  <name>Color</name>\n+                  <value>\n+                    <alpha>0</alpha>\n+                    <red>255</red>\n+                    <green>255</green>\n+                    <blue>255</blue>\n+                  </value>\n+                </parameter>\n+              </effect>\n+            </generatoritem>\n+            \"\"\"\n+        )\n+        parent_context_element = cElementTree.fromstring(\n+            \"\"\"\n+            <track>\n+              <rate>\n+                <timebase>24</timebase>\n+                <ntsc>TRUE</ntsc>\n+              </rate>\n+            </track>\n+            \"\"\"\n+        )\n+\n+        context = self.adapter._Context(parent_context_element)\n+\n+        # Make a parser\n+        parser = self.adapter.FCP7XMLParser(generator_element)\n+\n+        clip, time_range = parser.item_and_timing_for_element(\n+            generator_element,\n+            head_transition=None,\n+            tail_transition=None,\n+            context=context,\n+        )\n+\n+        self.assertEqual(clip.name, \"White\")\n+\n+        expected_range = opentime.TimeRange(\n+            start_time=opentime.RationalTime(383, (24000 / 1001.0)),\n+            duration=opentime.RationalTime(49, (24000 / 1001.0)),\n+        )\n+        self.assertEqual(time_range, expected_range)\n+\n+        expected_source_range = opentime.TimeRange(\n+            start_time=opentime.RationalTime(86313, (24000 / 1001.0)),\n+            duration=opentime.RationalTime(49, (24000 / 1001.0)),\n+        )\n+        self.assertEqual(clip.source_range, expected_source_range)\n+\n+        ref = clip.media_reference\n+        self.assertTrue(\n+            isinstance(ref, schema.GeneratorReference)\n+        )\n+        self.assertEqual(ref.name, \"Color\")\n+        self.assertEqual(\n+            ref.metadata[\"fcp_xml\"][\"parameter\"][\"value\"][\"red\"], \"255\"\n+        )\n+\n+    def test_effect_from_filter_element(self):\n+        tree = cElementTree.parse(FILTER_XML_EXAMPLE_PATH)\n+\n+        # Make a parser\n+        parser = self.adapter.FCP7XMLParser(tree)\n+        effect = parser.effect_from_filter_element(tree)\n+\n+        self.assertEqual(effect.name, \"Time Remap\")\n+\n+        # spot-check metadata\n+        effect_meta = effect.metadata[\"fcp_xml\"]\n+        self.assertEqual(effect_meta[\"effectid\"], \"timeremap\")\n+        self.assertEqual(len(effect_meta[\"parameter\"]), 5)\n+\n+    def test_transition_for_element(self):\n+        transition_element = cElementTree.fromstring(\n+            \"\"\"\n+            <transitionitem>\n+              <start>538</start>\n+              <end>557</end>\n+              <alignment>end-black</alignment>\n+              <cutPointTicks>160876800000</cutPointTicks>\n+              <rate>\n+                <timebase>30</timebase>\n+                <ntsc>FALSE</ntsc>\n+              </rate>\n+              <effect>\n+                <name>Cross Dissolve</name>\n+                <effectid>Cross Dissolve</effectid>\n+                <effectcategory>Dissolve</effectcategory>\n+                <effecttype>transition</effecttype>\n+                <mediatype>video</mediatype>\n+                <wipecode>0</wipecode>\n+                <wipeaccuracy>100</wipeaccuracy>\n+                <startratio>0</startratio>\n+                <endratio>1</endratio>\n+                <reverse>FALSE</reverse>\n+              </effect>\n+            </transitionitem>\n+            \"\"\"\n+        )\n+\n+        track_element = cElementTree.fromstring(\n+            \"\"\"\n+            <track>\n+                <rate>\n+                    <timebase>30</timebase>\n+                    <ntsc>FALSE</ntsc>\n+                </rate>\n+            </track>\n+            \"\"\"\n+        )\n+        context = self.adapter._Context(track_element)\n+\n+        parser = self.adapter.FCP7XMLParser(transition_element)\n+        transition = parser.transition_for_element(transition_element, context)\n+\n+        self.assertEqual(transition.name, \"Cross Dissolve\")\n+        self.assertEqual(\n+            transition.transition_type,\n+            schema.TransitionTypes.SMPTE_Dissolve,\n+        )\n+\n+\n+class AdaptersFcp7XmlTest(unittest.TestCase, test_utils.OTIOAssertions):\n+    adapter = adapters.from_name('fcp_xml').module()\n \n     def __init__(self, *args, **kwargs):\n         super(AdaptersFcp7XmlTest, self).__init__(*args, **kwargs)\n         self.maxDiff = None\n \n+    def test_build_empty_file(self):\n+        media_ref = schema.MissingReference(\n+            name=\"test_clip_name\",\n+            available_range=opentime.TimeRange(\n+                opentime.RationalTime(820489, 24),\n+                opentime.RationalTime(2087, 24),\n+            ),\n+            metadata={\n+                \"fcp_xml\": {\n+                    \"timecode\": {\n+                        \"rate\": {\"ntsc\": \"FALSE\", \"timebase\": \"24\"},\n+                        \"displayformat\": \"NDF\",\n+                        \"reel\": {\n+                            \"name\": \"test_reel_name\",\n+                        },\n+                    }\n+                }\n+            },\n+        )\n+        br_map = {}\n+\n+        file_element = self.adapter._build_empty_file(\n+            media_ref, media_ref.available_range.start_time, br_map\n+        )\n+\n+        self.assertEqual(file_element.find(\"./name\").text, \"test_clip_name\")\n+        self.assertEqual(file_element.find(\"./duration\").text, \"2087\")\n+\n+        rate_element = file_element.find(\"./rate\")\n+        self.assertEqual(rate_element.find(\"./ntsc\").text, \"FALSE\")\n+        self.assertEqual(rate_element.find(\"./timebase\").text, \"24\")\n+\n+        tc_element = file_element.find(\"./timecode\")\n+        self.assertEqual(tc_element.find(\"./rate/ntsc\").text, \"FALSE\")\n+        self.assertEqual(tc_element.find(\"./rate/timebase\").text, \"24\")\n+        self.assertEqual(tc_element.find(\"./string\").text, \"09:29:47:01\")\n+        self.assertEqual(tc_element.find(\"./reel/name\").text, \"test_reel_name\")\n+\n     def test_read(self):\n-        timeline = otio.adapters.read_from_file(FCP7_XML_EXAMPLE_PATH)\n+        timeline = adapters.read_from_file(FCP7_XML_EXAMPLE_PATH)\n \n         self.assertTrue(timeline is not None)\n         self.assertEqual(len(timeline.tracks), 8)\n \n         video_tracks = [\n             t for t in timeline.tracks\n-            if t.kind == otio.schema.TrackKind.Video\n+            if t.kind == schema.TrackKind.Video\n         ]\n         audio_tracks = [\n             t for t in timeline.tracks\n-            if t.kind == otio.schema.TrackKind.Audio\n+            if t.kind == schema.TrackKind.Audio\n         ]\n \n         self.assertEqual(len(video_tracks), 4)",
        "comments": [],
        "commit_message": "Merge branch 'master' of https://github.com/PixarAnimationStudios/OpenTimelineIO into fcpx_enhancements",
        "commit_id": "858f308f6f6d2d5a89158e1739642a50fc7241f3"
    },
    {
        "pr_title": "Add support for more elements and XMLs without libraries",
        "pr_number": 515,
        "file_name": "tests/test_fcp7_xml_adapter.py",
        "code_diff": "@@ -123,23 +987,23 @@\ndef test_read(self):\n \n         for t, track in enumerate(video_tracks):\n             for c, clip in enumerate(track):\n-                if isinstance(clip, otio.schema.Transition):\n+                if isinstance(clip, schema.Transition):\n                     self.assertEqual(\n                         clip.in_offset,\n-                        otio.opentime.RationalTime(\n+                        opentime.RationalTime(\n                             *video_clip_durations[t][c][0]\n                         )\n                     )\n                     self.assertEqual(\n                         clip.out_offset,\n-                        otio.opentime.RationalTime(\n+                        opentime.RationalTime(\n                             *video_clip_durations[t][c][1]\n                         )\n                     )\n                 else:\n                     self.assertEqual(\n                         clip.source_range.duration,\n-                        otio.opentime.RationalTime(*video_clip_durations[t][c])\n+                        opentime.RationalTime(*video_clip_durations[t][c])\n                     )\n \n         audio_clip_durations = (",
        "comments": [],
        "commit_message": "Merge branch 'master' of https://github.com/PixarAnimationStudios/OpenTimelineIO into fcpx_enhancements",
        "commit_id": "858f308f6f6d2d5a89158e1739642a50fc7241f3"
    },
    {
        "pr_title": "Add support for more elements and XMLs without libraries",
        "pr_number": 515,
        "file_name": "tests/test_fcp7_xml_adapter.py",
        "code_diff": "@@ -160,7 +1024,7 @@\ndef test_read(self):\n             for c, clip in enumerate(track):\n                 self.assertEqual(\n                     clip.source_range.duration,\n-                    otio.opentime.RationalTime(*audio_clip_durations[t][c])\n+                    opentime.RationalTime(*audio_clip_durations[t][c])\n                 )\n \n         timeline_marker_names = ('My MArker 1', 'dsf', None)",
        "comments": [],
        "commit_message": "Merge branch 'master' of https://github.com/PixarAnimationStudios/OpenTimelineIO into fcpx_enhancements",
        "commit_id": "858f308f6f6d2d5a89158e1739642a50fc7241f3"
    },
    {
        "pr_title": "Add support for more elements and XMLs without libraries",
        "pr_number": 515,
        "file_name": "tests/test_fcp7_xml_adapter.py",
        "code_diff": "@@ -173,7 +1037,7 @@\ndef test_read(self):\n         for n, marker in enumerate(timeline.tracks.markers):\n             self.assertEqual(\n                 marker.marked_range.start_time,\n-                otio.opentime.RationalTime(*timeline_marker_start_times[n])\n+                opentime.RationalTime(*timeline_marker_start_times[n])\n             )\n \n         timeline_marker_comments = ('so, this happened', 'fsfsfs', None)",
        "comments": [],
        "commit_message": "Merge branch 'master' of https://github.com/PixarAnimationStudios/OpenTimelineIO into fcpx_enhancements",
        "commit_id": "858f308f6f6d2d5a89158e1739642a50fc7241f3"
    },
    {
        "pr_title": "Add support for more elements and XMLs without libraries",
        "pr_number": 515,
        "file_name": "tests/test_fcp7_xml_adapter.py",
        "code_diff": "@@ -189,196 +1053,229 @@\ndef test_read(self):\n         self.assertEqual(clip_marker.name, None)\n         self.assertEqual(\n             clip_marker.marked_range.start_time,\n-            otio.opentime.RationalTime(73, 30.0)\n+            opentime.RationalTime(73, 30.0)\n         )\n         self.assertEqual(\n             clip_marker.metadata.get('fcp_xml', {}).get('comment'),\n             None\n         )\n \n-    def test_backreference_generator_read(self):\n-        with open(SIMPLE_XML_PATH, 'r') as fo:\n-            text = fo.read()\n-\n-        adapt_mod = otio.adapters.from_name('fcp_xml').module()\n-\n-        tree = cElementTree.fromstring(text)\n-        track = adapt_mod._get_top_level_tracks(tree)[0]\n-\n-        # make sure that element_map gets populated by the function calls in\n-        # the way we want\n-        element_map = collections.defaultdict(dict)\n-\n-        self.assertEqual(adapt_mod._parse_rate(track, element_map), 30.0)\n-        self.assertEqual(track, element_map[\"all_elements\"][\"sequence-1\"])\n-        self.assertEqual(adapt_mod._parse_rate(track, element_map), 30.0)\n-        self.assertEqual(track, element_map[\"all_elements\"][\"sequence-1\"])\n-        self.assertEqual(len(element_map[\"all_elements\"].keys()), 1)\n-\n-    def test_backreference_generator_write(self):\n-\n-        adapt_mod = otio.adapters.from_name('fcp_xml').module()\n-\n-        class dummy_obj(object):\n-            def __init__(self):\n-                self.attrib = {}\n-\n-        @adapt_mod._backreference_build(\"test\")\n-        def dummy_func(item, br_map):\n-            return dummy_obj()\n-\n-        br_map = collections.defaultdict(dict)\n-        result_first = dummy_func(\"foo\", br_map)\n-        self.assertNotEqual(br_map['test'], result_first)\n-        result_second = dummy_func(\"foo\", br_map)\n-        self.assertNotEqual(result_first, result_second)\n-\n     def test_roundtrip_mem2disk2mem(self):\n-        timeline = otio.schema.Timeline('test_timeline')\n-        timeline.tracks.name = 'test_timeline'\n+        timeline = schema.Timeline('test_timeline')\n \n         RATE = 48.0\n \n-        video_reference = otio.schema.ExternalReference(\n+        video_reference = schema.ExternalReference(\n             target_url=\"/var/tmp/test1.mov\",\n-            available_range=otio.opentime.TimeRange(\n-                otio.opentime.RationalTime(value=100, rate=RATE),\n-                otio.opentime.RationalTime(value=1000, rate=RATE)\n+            available_range=opentime.TimeRange(\n+                opentime.RationalTime(value=100, rate=RATE),\n+                opentime.RationalTime(value=1000, rate=RATE)\n             )\n         )\n-        audio_reference = otio.schema.ExternalReference(\n+        video_reference.name = \"test_vid_one\"\n+        audio_reference = schema.ExternalReference(\n             target_url=\"/var/tmp/test1.wav\",\n-            available_range=otio.opentime.TimeRange(\n-                otio.opentime.RationalTime(value=0, rate=RATE),\n-                otio.opentime.RationalTime(value=1000, rate=RATE)\n-            )\n+            available_range=opentime.TimeRange(\n+                opentime.RationalTime(value=0, rate=RATE),\n+                opentime.RationalTime(value=1000, rate=RATE)\n+            ),\n+        )\n+        audio_reference.name = \"test_wav_one\"\n+        generator_reference = schema.GeneratorReference(\n+            name=\"Color\",\n+            metadata={\n+                \"fcp_xml\": {\n+                    \"effectid\": \"Color\",\n+                    \"effectcategory\": \"Matte\",\n+                    \"effecttype\": \"generator\",\n+                    \"mediatype\": \"video\",\n+                    \"parameter\": {\n+                        \"@authoringApp\": \"PremierePro\",\n+                        \"parameterid\": \"fillcolor\",\n+                        \"name\": \"Color\",\n+                        \"value\": {\n+                            \"alpha\": \"0\",\n+                            \"red\": \"255\",\n+                            \"green\": \"255\",\n+                            \"blue\": \"255\",\n+                        },\n+                    },\n+                },\n+            },\n         )\n \n-        v0 = otio.schema.Track(kind=otio.schema.track.TrackKind.Video)\n-        v1 = otio.schema.Track(kind=otio.schema.track.TrackKind.Video)\n+        v0 = schema.Track(kind=schema.track.TrackKind.Video)\n+        v1 = schema.Track(kind=schema.track.TrackKind.Video)\n \n         timeline.tracks.extend([v0, v1])\n \n-        a0 = otio.schema.Track(kind=otio.schema.track.TrackKind.Audio)\n+        a0 = schema.Track(kind=schema.track.TrackKind.Audio)\n \n         timeline.tracks.append(a0)\n \n         v0.extend(\n             [\n-                otio.schema.Clip(\n+                schema.Clip(\n                     name='test_clip1',\n                     media_reference=video_reference,\n-                    source_range=otio.opentime.TimeRange(\n-                        otio.opentime.RationalTime(value=112, rate=RATE),\n-                        otio.opentime.RationalTime(value=40, rate=RATE)\n+                    source_range=opentime.TimeRange(\n+                        opentime.RationalTime(value=112, rate=RATE),\n+                        opentime.RationalTime(value=40, rate=RATE)\n                     )\n                 ),\n-                otio.schema.Gap(\n-                    source_range=otio.opentime.TimeRange(\n-                        duration=otio.opentime.RationalTime(\n+                schema.Gap(\n+                    source_range=opentime.TimeRange(\n+                        duration=opentime.RationalTime(\n                             value=60,\n                             rate=RATE\n                         )\n                     )\n                 ),\n-                otio.schema.Clip(\n+                schema.Clip(\n                     name='test_clip2',\n                     media_reference=video_reference,\n-                    source_range=otio.opentime.TimeRange(\n-                        otio.opentime.RationalTime(value=123, rate=RATE),\n-                        otio.opentime.RationalTime(value=260, rate=RATE)\n+                    source_range=opentime.TimeRange(\n+                        opentime.RationalTime(value=123, rate=RATE),\n+                        opentime.RationalTime(value=260, rate=RATE)\n                     )\n-                )\n+                ),\n+                schema.Clip(\n+                    name='test_generator_clip',\n+                    media_reference=generator_reference,\n+                    source_range=opentime.TimeRange(\n+                        opentime.RationalTime(value=292, rate=24.0),\n+                        opentime.RationalTime(value=183, rate=24.0)\n+                    )\n+                ),\n             ]\n         )\n \n         v1.extend([\n-            otio.schema.Gap(\n-                source_range=otio.opentime.TimeRange(\n-                    duration=otio.opentime.RationalTime(value=500, rate=RATE)\n+            schema.Gap(\n+                source_range=opentime.TimeRange(\n+                    duration=opentime.RationalTime(value=500, rate=RATE)\n                 )\n             ),\n-            otio.schema.Clip(\n+            schema.Clip(\n                 name='test_clip3',\n                 media_reference=video_reference,\n-                source_range=otio.opentime.TimeRange(\n-                    otio.opentime.RationalTime(value=112, rate=RATE),\n-                    otio.opentime.RationalTime(value=55, rate=RATE)\n+                source_range=opentime.TimeRange(\n+                    opentime.RationalTime(value=112, rate=RATE),\n+                    opentime.RationalTime(value=55, rate=RATE)\n                 )\n             )\n         ])\n \n         a0.extend(\n             [\n-                otio.schema.Gap(\n-                    source_range=otio.opentime.TimeRange(\n-                        duration=otio.opentime.RationalTime(value=10, rate=RATE)\n+                schema.Gap(\n+                    source_range=opentime.TimeRange(\n+                        duration=opentime.RationalTime(value=10, rate=RATE)\n                     )\n                 ),\n-                otio.schema.Clip(\n+                schema.Clip(\n                     name='test_clip4',\n                     media_reference=audio_reference,\n-                    source_range=otio.opentime.TimeRange(\n-                        otio.opentime.RationalTime(value=152, rate=RATE),\n-                        otio.opentime.RationalTime(value=248, rate=RATE)\n-                    )\n+                    source_range=opentime.TimeRange(\n+                        opentime.RationalTime(value=152, rate=RATE),\n+                        opentime.RationalTime(value=248, rate=RATE)\n+                    ),\n                 )\n             ]\n         )\n \n         timeline.tracks.markers.append(\n-            otio.schema.Marker(\n+            schema.Marker(\n                 name='test_timeline_marker',\n-                marked_range=otio.opentime.TimeRange(\n-                    otio.opentime.RationalTime(123, RATE)\n+                marked_range=opentime.TimeRange(\n+                    opentime.RationalTime(123, RATE)\n                 ),\n                 metadata={'fcp_xml': {'comment': 'my_comment'}}\n             )\n         )\n \n         v1[1].markers.append(\n-            otio.schema.Marker(\n+            schema.Marker(\n                 name='test_clip_marker',\n-                marked_range=otio.opentime.TimeRange(\n-                    otio.opentime.RationalTime(125, RATE)\n+                marked_range=opentime.TimeRange(\n+                    opentime.RationalTime(125, RATE)\n                 ),\n                 metadata={'fcp_xml': {'comment': 'my_comment'}}\n             )\n         )\n \n         # make sure that global_start_time.rate survives the round trip\n-        timeline.global_start_time = otio.opentime.RationalTime(100, RATE)\n+        timeline.global_start_time = opentime.RationalTime(100, RATE)\n \n-        result = otio.adapters.write_to_string(\n+        result = adapters.write_to_string(\n             timeline,\n             adapter_name='fcp_xml'\n         )\n-\n-        new_timeline = otio.adapters.read_from_string(\n+        new_timeline = adapters.read_from_string(\n             result,\n             adapter_name='fcp_xml'\n         )\n \n-        # The start frame falls off during the round trip\n-        timeline.global_start_time = otio.opentime.RationalTime(0, RATE)\n-\n-        self.assertEqual(\n-            timeline.global_start_time,\n-            new_timeline.global_start_time\n-        )\n+        # Since FCP XML's \"sequence\" is a marriage of the timeline and the\n+        # main tracks stack, the tracks stack loses its name\n+        new_timeline.tracks.name = timeline.tracks.name\n+\n+        self.assertEqual(new_timeline.name, 'test_timeline')\n+\n+        # Before comparing, scrub ignorable metadata introduced in\n+        # serialization (things like unique ids minted by the adapter)\n+        # Since we seeded metadata for the generator, keep that metadata\n+        del(new_timeline.metadata[\"fcp_xml\"])\n+        for child in new_timeline.tracks.each_child():\n+            try:\n+                del(child.metadata[\"fcp_xml\"])\n+            except KeyError:\n+                pass\n+\n+            try:\n+                is_generator = isinstance(\n+                    child.media_reference, schema.GeneratorReference\n+                )\n+                if not is_generator:\n+                    del(child.media_reference.metadata[\"fcp_xml\"])\n+            except (AttributeError, KeyError):\n+                pass\n \n         self.assertJsonEqual(new_timeline, timeline)\n \n     def test_roundtrip_disk2mem2disk(self):\n-        timeline = otio.adapters.read_from_file(FCP7_XML_EXAMPLE_PATH)\n+        timeline = adapters.read_from_file(FCP7_XML_EXAMPLE_PATH)\n         tmp_path = tempfile.mkstemp(suffix=\".xml\", text=True)[1]\n \n-        otio.adapters.write_to_file(timeline, tmp_path)\n-        result = otio.adapters.read_from_file(tmp_path)\n+        adapters.write_to_file(timeline, tmp_path)\n+        result = adapters.read_from_file(tmp_path)\n+\n+        # TODO: OTIO doesn't support linking items for the moment, so the\n+        # adapter reads links to the metadata, but doesn't write them.\n+        # See _dict_to_xml_tree for more information.\n+        def scrub_md_dicts(timeline):\n+            def scrub_displayformat(md_dict):\n+                for ignore_key in {\"link\"}:\n+                    try:\n+                        del(md_dict[ignore_key])\n+                    except KeyError:\n+                        pass\n \n-        original_json = otio.adapters.write_to_string(timeline, 'otio_json')\n-        output_json = otio.adapters.write_to_string(result, 'otio_json')\n-        self.assertMultiLineEqual(original_json, output_json)\n+                for _, value in list(md_dict.items()):\n+                    if isinstance(value, dict):\n+                        scrub_displayformat(value)\n+\n+            for child in timeline.tracks.each_child():\n+                scrub_displayformat(child.metadata)\n+                try:\n+                    scrub_displayformat(child.media_reference.metadata)\n+                except AttributeError:\n+                    pass\n+\n+        scrub_md_dicts(result)\n+        scrub_md_dicts(timeline)\n+\n+        self.assertJsonEqual(result, timeline)\n \n         self.assertIsOTIOEquivalentTo(timeline, result)",
        "comments": [],
        "commit_message": "Merge branch 'master' of https://github.com/PixarAnimationStudios/OpenTimelineIO into fcpx_enhancements",
        "commit_id": "858f308f6f6d2d5a89158e1739642a50fc7241f3"
    },
    {
        "pr_title": "Add support for more elements and XMLs without libraries",
        "pr_number": 515,
        "file_name": "tests/test_fcp7_xml_adapter.py",
        "code_diff": "@@ -389,7 +1286,7 @@\ndef test_roundtrip_disk2mem2disk(self):\n                 self.assertNotEqual(original_file.read(), output_file.read())\n \n     def test_hiero_flavored_xml(self):\n-        timeline = otio.adapters.read_from_file(HIERO_XML_PATH)\n+        timeline = adapters.read_from_file(HIERO_XML_PATH)\n         self.assertTrue(len(timeline.tracks), 1)\n         self.assertTrue(timeline.tracks[0].name == 'Video 1')",
        "comments": [],
        "commit_message": "Merge branch 'master' of https://github.com/PixarAnimationStudios/OpenTimelineIO into fcpx_enhancements",
        "commit_id": "858f308f6f6d2d5a89158e1739642a50fc7241f3"
    },
    {
        "pr_title": "Add function to explicitly load SchemaDef plugins",
        "pr_number": 511,
        "file_name": "tests/test_schemadef_plugin.py",
        "code_diff": "@@ -41,13 +41,31 @@\n\"\"\"\n \n \n+def _clean_plugin_module():\n+    \"\"\"Remove the example_schemadef if its already been loaded to test\n+    autoload/explicit load behavior.\n+    \"\"\"\n+    try:\n+        del otio.schemadef.example_schemadef\n+    except AttributeError:\n+        pass\n+\n+    try:\n+        plugin = otio.schema.schemadef.from_name(\"example_schemadef\")\n+        plugin._module = None\n+    except otio.exceptions.NotSupportedError:\n+        pass\n+\n+\n class TestPluginSchemadefs(unittest.TestCase):\n     def setUp(self):\n         self.save_manifest = otio.plugins.manifest._MANIFEST\n         self.save_manifest_path = os.environ.get('OTIO_PLUGIN_MANIFEST_PATH')\n         # find the path to the baselines/schemadef_example.json\n         self.manifest_path = baseline_reader.path_to_baseline(SCHEMADEF_NAME)\n         os.environ['OTIO_PLUGIN_MANIFEST_PATH'] = self.manifest_path\n+        otio.plugins.manifest.ActiveManifest(force_reload=True)\n+        _clean_plugin_module()\n \n     def tearDown(self):\n         # restore original state",
        "comments": [],
        "commit_message": "Wipe the plugin to test auto/manual loading.",
        "commit_id": "c3538c6ff9a67afbe3d160b1efd0f6dcb74d4447"
    },
    {
        "pr_title": "Add function to explicitly load SchemaDef plugins",
        "pr_number": 511,
        "file_name": "tests/test_schemadef_plugin.py",
        "code_diff": "@@ -57,12 +75,18 @@\ndef tearDown(self):\n             del os.environ['OTIO_PLUGIN_MANIFEST_PATH']\n         otio.plugins.manifest._MANIFEST = self.save_manifest\n \n+        _clean_plugin_module()\n+\n     def test_autoloaded_plugin(self):\n+        with self.assertRaises(AttributeError):\n+            otio.schemadef.example_schemadef\n         # should force an autoload\n         thing = otio.adapters.read_from_string(TEST_STRING, \"otio_json\")\n         self.assertEqual(thing.exampleArg, \"foobar\")\n \n     def test_plugin_schemadef(self):\n+        with self.assertRaises(AttributeError):\n+            otio.schemadef.example_schemadef\n         # force loading the module\n         otio.schema.schemadef.module_from_name(\"example_schemadef\")",
        "comments": [],
        "commit_message": "Wipe the plugin to test auto/manual loading.",
        "commit_id": "c3538c6ff9a67afbe3d160b1efd0f6dcb74d4447"
    },
    {
        "pr_title": "Add function to explicitly load SchemaDef plugins",
        "pr_number": 511,
        "file_name": "tests/test_schemadef_plugin.py",
        "code_diff": "@@ -78,6 +102,9 @@\ndef test_plugin_schemadef(self):\n         self.assertEqual(example.exampleArg, peculiar_value)\n \n     def test_plugin_schemadef_namespace(self):\n+        with self.assertRaises(AttributeError):\n+            otio.schemadef.example_schemadef\n+\n         # force loading the module\n         plugin_module = otio.schema.schemadef.module_from_name(\n             \"example_schemadef\"",
        "comments": [],
        "commit_message": "Wipe the plugin to test auto/manual loading.",
        "commit_id": "c3538c6ff9a67afbe3d160b1efd0f6dcb74d4447"
    },
    {
        "pr_title": "Timecode rate enhancements",
        "pr_number": 497,
        "file_name": "opentimelineio/opentime.py",
        "code_diff": "@@ -42,15 +42,20 @@\n24,\n     25,\n     30,\n+    29.97,\n+    (30000 / 1001.0),\n     48,\n     50,\n+    59.94,\n+    (60000 / 1001.0),\n     60,\n )\n \n VALID_DROPFRAME_TIMECODE_RATES = (\n     29.97,\n     (30000 / 1001.0),\n     59.94,\n+    (60000 / 1001.0),\n )\n \n VALID_TIMECODE_RATES = (",
        "comments": [],
        "commit_message": "Added more accurate 59.94 fraction to both drop and non-drop timecode allow lists.",
        "commit_id": "4984228667a5eb3a18feff0b338dceaf7aa7c324"
    },
    {
        "pr_title": "Timecode rate enhancements",
        "pr_number": 497,
        "file_name": "opentimelineio/opentime.py",
        "code_diff": "@@ -645,12 +650,15 @@\ndef from_timecode(timecode_str, rate):\n     return RationalTime(value, rate)\n \n \n-def to_timecode(time_obj, rate=None):\n+def to_timecode(time_obj, rate=None, drop_frame=None):\n     \"\"\"Convert a RationalTime into a timecode string.\n \n     :param time_obj: (:class:`RationalTime`) instance to express as timecode.\n     :param rate: (:class:`float`) The frame-rate to calculate timecode in\n         terms of. (Default time_obj.rate)\n+    :param drop_frame: (:class:`bool`) ``True`` to make drop-frame timecode,\n+        ``False`` for non-drop. If left ``None``, a format will be guessed\n+        based on rate.\n \n     :return: (:class:`str`) The timecode.\n     \"\"\"",
        "comments": [],
        "commit_message": "Added more accurate 59.94 fraction to both drop and non-drop timecode allow lists.",
        "commit_id": "4984228667a5eb3a18feff0b338dceaf7aa7c324"
    },
    {
        "pr_title": "Timecode rate enhancements",
        "pr_number": 497,
        "file_name": "opentimelineio/opentime.py",
        "code_diff": "@@ -664,20 +672,27 @@\ndef to_timecode(time_obj, rate=None):\n \n     # Check if rate is drop frame\n     rate_is_dropframe = rate in VALID_DROPFRAME_TIMECODE_RATES\n+    if drop_frame and not rate_is_dropframe:\n+        raise ValueError(\n+            \"Invalid rate for drop-frame timecode {}\".format(time_obj.rate)\n+        )\n \n-    if not rate_is_dropframe:\n-        # Check for variantions of ~24 fps and convert to 24 for calculations\n-        if round(rate) == 24:\n-            rate = round(rate)\n+    # if in auto-detect for DFTC, use the rate to decide\n+    if drop_frame is None:\n+        drop_frame = rate_is_dropframe\n \n     dropframes = 0\n-    if rate_is_dropframe:\n-        if rate == 29.97:\n+    if drop_frame:\n+        if rate in (29.97, (30000 / 1001.0)):\n             dropframes = 2\n \n         elif rate == 59.94:\n             dropframes = 4\n \n+    # For non-dftc, use the integral frame rate\n+    if not drop_frame:\n+        rate = round(rate)\n+\n     # Number of frames in an hour\n     frames_per_hour = int(round(rate * 60 * 60))\n     # Number of frames in a day - timecode rolls over after 24 hours",
        "comments": [],
        "commit_message": "Added more accurate 59.94 fraction to both drop and non-drop timecode allow lists.",
        "commit_id": "4984228667a5eb3a18feff0b338dceaf7aa7c324"
    },
    {
        "pr_title": "Timecode rate enhancements",
        "pr_number": 497,
        "file_name": "opentimelineio/opentime.py",
        "code_diff": "@@ -698,7 +713,7 @@\ndef to_timecode(time_obj, rate=None):\n     # clock\n     value %= frames_per_24_hours\n \n-    if rate_is_dropframe:\n+    if drop_frame:\n         d = value // frames_per_10_minutes\n         m = value % frames_per_10_minutes\n         if m > dropframes:",
        "comments": [],
        "commit_message": "Added more accurate 59.94 fraction to both drop and non-drop timecode allow lists.",
        "commit_id": "4984228667a5eb3a18feff0b338dceaf7aa7c324"
    },
    {
        "pr_title": "Timecode rate enhancements",
        "pr_number": 497,
        "file_name": "tests/test_opentime.py",
        "code_diff": "@@ -190,7 +190,7 @@\ndef test_converting_negative_values_to_timecode(self):\n         with self.assertRaises(ValueError):\n             otio.opentime.to_timecode(t, 25)\n \n-    def test_timecode_2997fps(self):\n+    def test_dropframe_timecode_2997fps(self):\n         \"\"\"Test drop frame in action. Focused on minute roll overs\n \n         We nominal_fps 30 for frame calculation",
        "comments": [],
        "commit_message": "Added more accurate 59.94 fraction to both drop and non-drop timecode allow lists.",
        "commit_id": "4984228667a5eb3a18feff0b338dceaf7aa7c324"
    },
    {
        "pr_title": "Refactor FCP XML adapter - add generator and effect support",
        "pr_number": 494,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -62,12 +62,6 @@\nID_RE = re.compile(r\"^(?P<tag>[a-zA-Z]*)-(?P<id>\\d*)$\")\n \n \n-\"\"\"\n-Adapter TODOs:\n-    - Support start timecode on sequences\n-    - Support top-level objects other than sequences: project, bin, clip\n-\"\"\"\n-\n # ---------\n # utilities\n # ---------",
        "comments": [
            {
                "comment": "Does this address the issue this comment references? https://github.com/PixarAnimationStudios/OpenTimelineIO/pull/488",
                "position": null
            },
            {
                "comment": "Lets also promote these into issues (unless these are short term notes)",
                "position": null
            },
            {
                "comment": "I don't understand this comment",
                "position": null
            },
            {
                "comment": "Yes, Between #497 and #488 there were a couple of DF/NDF issues that should have been resolved. I'm working to address locally, however this particular oddity in our test file has be a touch puzzled: https://github.com/PixarAnimationStudios/OpenTimelineIO/blob/929149e97333edb61a18e53de2cb4ac40bc6a10d/tests/sample_data/premiere_example.xml#L713-L724\r\n\r\nThat is 30fps (not 29.97) but using Drop-frame timecode. opentime does not currently support this, so I'm trying to sort out the desired behavior.",
                "position": null
            },
            {
                "comment": "Done. see #514 ",
                "position": null
            },
            {
                "comment": "In this pass I did some refactoring for clarity on the output side, but I was trying to minimally impact some of the structural decisions made. This comment was an attempt to call out that this piece of logic may not be sustainable. Will update with a better comment.",
                "position": null
            },
            {
                "comment": "It may have crept in and been ignored for a while. @bashesenaxis do you have any insight on this?",
                "position": null
            },
            {
                "comment": "After digging a little deeper, I was able to resolve that in this case the start time is the `frame` over the `rate`, and the timecode is frame over the ntsc version of the rate when `displayformat` is `DF`",
                "position": null
            }
        ],
        "commit_message": "Cleanup stale comments and unused imports in FCP XML adapter",
        "commit_id": "7122c45170e916e05aab97008d707d865756134e",
        "fn_context": ""
    },
    {
        "pr_title": "AAF Writer: Fixes the crash seen when running otio -> aaf,  in some AAF files due to nesting",
        "pr_number": 493,
        "file_name": "opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -59,6 +59,10 @@\nSAMPLE_DATA_DIR,\n     \"nesting_test.aaf\"\n )\n+NESTED_STACK_EXAMPLE_PATH = os.path.join(\n+    SAMPLE_DATA_DIR,\n+    \"nested_stack.aaf\"\n+)\n NESTING_PREFLATTENED_EXAMPLE_PATH = os.path.join(\n     SAMPLE_DATA_DIR,\n     \"nesting_test_preflattened.aaf\"",
        "comments": [],
        "commit_message": "Adding nested_stack test. Reversing type_mapping dictionary logic. Fixing nested aaf test checking now that we are stackifying",
        "commit_id": "c33ea419da9f73611c0a4aaae46a33be9d997469"
    },
    {
        "pr_title": "AAF Writer: Fixes the crash seen when running otio -> aaf,  in some AAF files due to nesting",
        "pr_number": 493,
        "file_name": "opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -875,6 +879,9 @@\ndef _verify_first_clip(self, original_timeline, aaf_path):\n     def test_aaf_writer_nesting(self):\n         self._verify_aaf(NESTING_EXAMPLE_PATH)\n \n+    def test_aaf_writer_nested_stack(self):\n+        self._verify_aaf(NESTED_STACK_EXAMPLE_PATH)\n+\n     def _verify_aaf(self, aaf_path):\n         otio_timeline = otio.adapters.read_from_file(aaf_path, simplify=True)\n         fd, tmp_aaf_path = tempfile.mkstemp(suffix='.aaf')",
        "comments": [],
        "commit_message": "Adding nested_stack test. Reversing type_mapping dictionary logic. Fixing nested aaf test checking now that we are stackifying",
        "commit_id": "c33ea419da9f73611c0a4aaae46a33be9d997469"
    },
    {
        "pr_title": "Add global_start_time as a serializable_field.",
        "pr_number": 477,
        "file_name": "tests/test_fcp7_xml_adapter.py",
        "code_diff": "@@ -236,18 +236,20 @@\ndef test_roundtrip_mem2disk2mem(self):\n         timeline = otio.schema.Timeline('test_timeline')\n         timeline.tracks.name = 'test_timeline'\n \n+        RATE = 48.0\n+\n         video_reference = otio.schema.ExternalReference(\n             target_url=\"/var/tmp/test1.mov\",\n             available_range=otio.opentime.TimeRange(\n-                otio.opentime.RationalTime(value=100, rate=24.0),\n-                otio.opentime.RationalTime(value=1000, rate=24.0)\n+                otio.opentime.RationalTime(value=100, rate=RATE),\n+                otio.opentime.RationalTime(value=1000, rate=RATE)\n             )\n         )\n         audio_reference = otio.schema.ExternalReference(\n             target_url=\"/var/tmp/test1.wav\",\n             available_range=otio.opentime.TimeRange(\n-                otio.opentime.RationalTime(value=0, rate=24.0),\n-                otio.opentime.RationalTime(value=1000, rate=24.0)\n+                otio.opentime.RationalTime(value=0, rate=RATE),\n+                otio.opentime.RationalTime(value=1000, rate=RATE)\n             )\n         )",
        "comments": [],
        "commit_message": "The default value for global_start_time is None\n\n- Remove the code that was setting it to a default of 0, 24\n- Update baseline for the Timeline\n- In the FCP7 round trip test, FCP round trips the rate but not the\noffset.  Add some code to exersize this.",
        "commit_id": "86a13996492eab952986e665b68045e905b39ab4"
    },
    {
        "pr_title": "Add global_start_time as a serializable_field.",
        "pr_number": 477,
        "file_name": "tests/test_fcp7_xml_adapter.py",
        "code_diff": "@@ -266,24 +268,24 @@\ndef test_roundtrip_mem2disk2mem(self):\n                     name='test_clip1',\n                     media_reference=video_reference,\n                     source_range=otio.opentime.TimeRange(\n-                        otio.opentime.RationalTime(value=112, rate=24.0),\n-                        otio.opentime.RationalTime(value=40, rate=24.0)\n+                        otio.opentime.RationalTime(value=112, rate=RATE),\n+                        otio.opentime.RationalTime(value=40, rate=RATE)\n                     )\n                 ),\n                 otio.schema.Gap(\n                     source_range=otio.opentime.TimeRange(\n                         duration=otio.opentime.RationalTime(\n                             value=60,\n-                            rate=24.0\n+                            rate=RATE\n                         )\n                     )\n                 ),\n                 otio.schema.Clip(\n                     name='test_clip2',\n                     media_reference=video_reference,\n                     source_range=otio.opentime.TimeRange(\n-                        otio.opentime.RationalTime(value=123, rate=24.0),\n-                        otio.opentime.RationalTime(value=260, rate=24.0)\n+                        otio.opentime.RationalTime(value=123, rate=RATE),\n+                        otio.opentime.RationalTime(value=260, rate=RATE)\n                     )\n                 )\n             ]",
        "comments": [],
        "commit_message": "The default value for global_start_time is None\n\n- Remove the code that was setting it to a default of 0, 24\n- Update baseline for the Timeline\n- In the FCP7 round trip test, FCP round trips the rate but not the\noffset.  Add some code to exersize this.",
        "commit_id": "86a13996492eab952986e665b68045e905b39ab4"
    },
    {
        "pr_title": "Add global_start_time as a serializable_field.",
        "pr_number": 477,
        "file_name": "tests/test_fcp7_xml_adapter.py",
        "code_diff": "@@ -292,15 +294,15 @@\ndef test_roundtrip_mem2disk2mem(self):\n         v1.extend([\n             otio.schema.Gap(\n                 source_range=otio.opentime.TimeRange(\n-                    duration=otio.opentime.RationalTime(value=500, rate=24.0)\n+                    duration=otio.opentime.RationalTime(value=500, rate=RATE)\n                 )\n             ),\n             otio.schema.Clip(\n                 name='test_clip3',\n                 media_reference=video_reference,\n                 source_range=otio.opentime.TimeRange(\n-                    otio.opentime.RationalTime(value=112, rate=24.0),\n-                    otio.opentime.RationalTime(value=55, rate=24.0)\n+                    otio.opentime.RationalTime(value=112, rate=RATE),\n+                    otio.opentime.RationalTime(value=55, rate=RATE)\n                 )\n             )\n         ])",
        "comments": [],
        "commit_message": "The default value for global_start_time is None\n\n- Remove the code that was setting it to a default of 0, 24\n- Update baseline for the Timeline\n- In the FCP7 round trip test, FCP round trips the rate but not the\noffset.  Add some code to exersize this.",
        "commit_id": "86a13996492eab952986e665b68045e905b39ab4"
    },
    {
        "pr_title": "Add global_start_time as a serializable_field.",
        "pr_number": 477,
        "file_name": "tests/test_fcp7_xml_adapter.py",
        "code_diff": "@@ -309,15 +311,15 @@\ndef test_roundtrip_mem2disk2mem(self):\n             [\n                 otio.schema.Gap(\n                     source_range=otio.opentime.TimeRange(\n-                        duration=otio.opentime.RationalTime(value=10, rate=24.0)\n+                        duration=otio.opentime.RationalTime(value=10, rate=RATE)\n                     )\n                 ),\n                 otio.schema.Clip(\n                     name='test_clip4',\n                     media_reference=audio_reference,\n                     source_range=otio.opentime.TimeRange(\n-                        otio.opentime.RationalTime(value=152, rate=24.0),\n-                        otio.opentime.RationalTime(value=248, rate=24.0)\n+                        otio.opentime.RationalTime(value=152, rate=RATE),\n+                        otio.opentime.RationalTime(value=248, rate=RATE)\n                     )\n                 )\n             ]",
        "comments": [],
        "commit_message": "The default value for global_start_time is None\n\n- Remove the code that was setting it to a default of 0, 24\n- Update baseline for the Timeline\n- In the FCP7 round trip test, FCP round trips the rate but not the\noffset.  Add some code to exersize this.",
        "commit_id": "86a13996492eab952986e665b68045e905b39ab4"
    },
    {
        "pr_title": "Add global_start_time as a serializable_field.",
        "pr_number": 477,
        "file_name": "tests/test_fcp7_xml_adapter.py",
        "code_diff": "@@ -327,7 +329,7 @@\ndef test_roundtrip_mem2disk2mem(self):\n             otio.schema.Marker(\n                 name='test_timeline_marker',\n                 marked_range=otio.opentime.TimeRange(\n-                    otio.opentime.RationalTime(123, 24.0)\n+                    otio.opentime.RationalTime(123, RATE)\n                 ),\n                 metadata={'fcp_xml': {'comment': 'my_comment'}}\n             )",
        "comments": [],
        "commit_message": "The default value for global_start_time is None\n\n- Remove the code that was setting it to a default of 0, 24\n- Update baseline for the Timeline\n- In the FCP7 round trip test, FCP round trips the rate but not the\noffset.  Add some code to exersize this.",
        "commit_id": "86a13996492eab952986e665b68045e905b39ab4"
    },
    {
        "pr_title": "Allow non-ASCII names in AAFs",
        "pr_number": 475,
        "file_name": "opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -139,7 +139,7 @@\nnot could_import_aaf,\n     \"AAF module not found. You might need to set OTIO_AAF_PYTHON_LIB\"\n )\n-class AAFAdapterTest(unittest.TestCase):\n+class AAFReaderTests(unittest.TestCase):\n \n     def test_aaf_read(self):\n         aaf_path = SIMPLE_EXAMPLE_PATH",
        "comments": [],
        "commit_message": "Rearranged the AAF tests into three classes: AAFReaderTests, AAFWriterTests and SimplifyTests.",
        "commit_id": "544deb2b6260f9bdd3efe7f497f88af82dccb356"
    },
    {
        "pr_title": "Allow non-ASCII names in AAFs",
        "pr_number": 475,
        "file_name": "opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -199,48 +199,6 @@\ndef test_aaf_read(self):\n             ]\n         )\n \n-    def test_aaf_simplify(self):\n-        aaf_path = SIMPLE_EXAMPLE_PATH\n-        timeline = otio.adapters.read_from_file(aaf_path, simplify=True)\n-        self.assertIsNotNone(timeline)\n-        self.assertEqual(type(timeline), otio.schema.Timeline)\n-        self.assertEqual(timeline.name, \"OTIO TEST 1.Exported.01\")\n-        fps = timeline.duration().rate\n-        self.assertEqual(fps, 24.0)\n-        self.assertEqual(\n-            timeline.duration(),\n-            otio.opentime.from_timecode(\"00:02:16:18\", fps)\n-        )\n-        self.assertEqual(len(timeline.tracks), 3)\n-        self.assertEqual(otio.schema.TrackKind.Video, timeline.tracks[0].kind)\n-        self.assertEqual(otio.schema.TrackKind.Audio, timeline.tracks[1].kind)\n-        self.assertEqual(otio.schema.TrackKind.Audio, timeline.tracks[2].kind)\n-        for track in timeline.tracks:\n-            self.assertNotEqual(type(track[0]), otio.schema.Track)\n-            self.assertEqual(len(track), 5)\n-\n-    def test_aaf_no_simplify(self):\n-        aaf_path = SIMPLE_EXAMPLE_PATH\n-        collection = otio.adapters.read_from_file(aaf_path, simplify=False)\n-        self.assertIsNotNone(collection)\n-        self.assertEqual(type(collection), otio.schema.SerializableCollection)\n-        self.assertEqual(len(collection), 1)\n-\n-        timeline = collection[0]\n-        self.assertEqual(timeline.name, \"OTIO TEST 1.Exported.01\")\n-        fps = timeline.duration().rate\n-        self.assertEqual(fps, 24.0)\n-        self.assertEqual(\n-            timeline.duration(),\n-            otio.opentime.from_timecode(\"00:02:16:18\", fps)\n-        )\n-\n-        self.assertEqual(len(timeline.tracks), 12)\n-\n-        video_track = timeline.tracks[8][0]\n-        self.assertEqual(otio.schema.TrackKind.Video, video_track.kind)\n-        self.assertEqual(len(video_track), 5)\n-\n     def test_aaf_read_trims(self):\n         aaf_path = TRIMS_EXAMPLE_PATH\n         timeline = otio.adapters.read_from_file(aaf_path)",
        "comments": [],
        "commit_message": "Rearranged the AAF tests into three classes: AAFReaderTests, AAFWriterTests and SimplifyTests.",
        "commit_id": "544deb2b6260f9bdd3efe7f497f88af82dccb356"
    },
    {
        "pr_title": "Allow non-ASCII names in AAFs",
        "pr_number": 475,
        "file_name": "opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -799,6 +757,38 @@\ndef test_essence_group(self):\n             timeline.duration()\n         )\n \n+    def test_30fps(self):\n+        tl = otio.adapters.read_from_file(FPS30_CLIP_PATH)\n+        self.assertEqual(tl.duration().rate, 30)\n+\n+    def test_2997fps(self):\n+        tl = otio.adapters.read_from_file(FPS2997_CLIP_PATH)\n+        self.assertEqual(tl.duration().rate, 30000 / 1001.0)\n+\n+    def test_utf8_names(self):\n+        timeline = otio.adapters.read_from_file(UTF8_CLIP_PATH)\n+        self.assertEqual(\n+            u\"Sequence_ABCXYZ\u00f1\u00e7\u00ea\u0153\u2022\u2211\u00b4\u00ae\u2020\u00a5\u00a8\u02c6\u00f8\u03c0\u201c\u2018\u00e5\u00df\u2202\u0192\u00a9\u02d9\u2206\u02da\u00ac\u2026\u00e6\u03a9\u2248\u00e7\u221a\u222b\u02dc\u00b5\u2264\u2265\u00f7.Exported.01\",\n+            timeline.name\n+        )\n+        video_track = timeline.video_tracks()[0]\n+        first_clip = video_track[0]\n+        self.assertEqual(\n+            first_clip.name,\n+            u\"Clip_ABCXYZ\u00f1\u00e7\u00ea\u0153\u2022\u2211\u00b4\u00ae\u2020\u00a5\u00a8\u02c6\u00f8\u03c0\u201c\u2018\u00e5\u00df\u2202\u0192\u00a9\u02d9\u2206\u02da\u00ac\u2026\u00e6\u03a9\u2248\u00e7\u221a\u222b\u02dc\u00b5\u2264\u2265\u00f7\"\n+        )\n+        self.assertEqual(\n+            first_clip.media_reference.metadata[\"AAF\"][\"UserComments\"][\"Comments\"],\n+            u\"Comments_ABCXYZ\u00f1\u00e7\u00ea\u0153\u2022\u2211\u00b4\u00ae\u2020\u00a5\u00a8\u02c6\u00f8\u03c0\u201c\u2018\u00e5\u00df\u2202\u0192\u00a9\u02d9\u2206\u02da\u00ac\u2026\u00e6\u03a9\u2248\u00e7\u221a\u222b\u02dc\u00b5\u2264\u2265\u00f7\"\n+        )\n+\n+    def test_multiple_top_level_mobs(self):\n+        result = otio.adapters.read_from_file(MULTIPLE_TOP_LEVEL_MOBS_CLIP_PATH)\n+        self.assertIsInstance(result, otio.schema.SerializableCollection)\n+        self.assertEqual(2, len(result))\n+\n+\n+class AAFWriterTests(unittest.TestCase):\n     def test_aaf_writer_simple(self):\n         self._verify_aaf(SIMPLE_EXAMPLE_PATH)",
        "comments": [],
        "commit_message": "Rearranged the AAF tests into three classes: AAFReaderTests, AAFWriterTests and SimplifyTests.",
        "commit_id": "544deb2b6260f9bdd3efe7f497f88af82dccb356"
    },
    {
        "pr_title": "Allow non-ASCII names in AAFs",
        "pr_number": 475,
        "file_name": "opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -971,6 +961,48 @@\ndef _is_otio_aaf_same(self, otio_child, aaf_component):\n \n \n class SimplifyTests(unittest.TestCase):\n+    def test_aaf_simplify(self):\n+        aaf_path = SIMPLE_EXAMPLE_PATH\n+        timeline = otio.adapters.read_from_file(aaf_path, simplify=True)\n+        self.assertIsNotNone(timeline)\n+        self.assertEqual(type(timeline), otio.schema.Timeline)\n+        self.assertEqual(timeline.name, \"OTIO TEST 1.Exported.01\")\n+        fps = timeline.duration().rate\n+        self.assertEqual(fps, 24.0)\n+        self.assertEqual(\n+            timeline.duration(),\n+            otio.opentime.from_timecode(\"00:02:16:18\", fps)\n+        )\n+        self.assertEqual(len(timeline.tracks), 3)\n+        self.assertEqual(otio.schema.TrackKind.Video, timeline.tracks[0].kind)\n+        self.assertEqual(otio.schema.TrackKind.Audio, timeline.tracks[1].kind)\n+        self.assertEqual(otio.schema.TrackKind.Audio, timeline.tracks[2].kind)\n+        for track in timeline.tracks:\n+            self.assertNotEqual(type(track[0]), otio.schema.Track)\n+            self.assertEqual(len(track), 5)\n+\n+    def test_aaf_no_simplify(self):\n+        aaf_path = SIMPLE_EXAMPLE_PATH\n+        collection = otio.adapters.read_from_file(aaf_path, simplify=False)\n+        self.assertIsNotNone(collection)\n+        self.assertEqual(type(collection), otio.schema.SerializableCollection)\n+        self.assertEqual(len(collection), 1)\n+\n+        timeline = collection[0]\n+        self.assertEqual(timeline.name, \"OTIO TEST 1.Exported.01\")\n+        fps = timeline.duration().rate\n+        self.assertEqual(fps, 24.0)\n+        self.assertEqual(\n+            timeline.duration(),\n+            otio.opentime.from_timecode(\"00:02:16:18\", fps)\n+        )\n+\n+        self.assertEqual(len(timeline.tracks), 12)\n+\n+        video_track = timeline.tracks[8][0]\n+        self.assertEqual(otio.schema.TrackKind.Video, video_track.kind)\n+        self.assertEqual(len(video_track), 5)\n+\n     def test_simplify_top_level_track(self):\n         \"\"\"Test for cases where a track has a single item but should not be\n         collapsed because it is the the last track in the stack ie:",
        "comments": [],
        "commit_message": "Rearranged the AAF tests into three classes: AAFReaderTests, AAFWriterTests and SimplifyTests.",
        "commit_id": "544deb2b6260f9bdd3efe7f497f88af82dccb356"
    },
    {
        "pr_title": "Add media linker arguments to commandline tools",
        "pr_number": 466,
        "file_name": "tests/test_console.py",
        "code_diff": "@@ -109,6 +109,7 @@\ndef test_basic(self):\n                 '-i', SCREENING_EXAMPLE_PATH,\n                 '-o', tf.name,\n                 '-O', 'otio_json',\n+                '--tracks', '0',\n                 \"-a\", \"rate=24\",\n             ]\n             otio.console.otioconvert.main()",
        "comments": [],
        "commit_message": "More argument checking for otioconvert.",
        "commit_id": "5c8562d67d0fe7707d3a0379933373a8cd5cd04f"
    },
    {
        "pr_title": "Added support for non-24 fps AAFs",
        "pr_number": 465,
        "file_name": "opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -873,10 +873,9 @@\ndef _contains_something_valuable(thing):\n \n def read_from_file(filepath, simplify=True):\n \n-    f = aaf2.open(filepath)\n+    with aaf2.open(filepath) as aaf_file:\n \n-    try:\n-        storage = f.content\n+        storage = aaf_file.content\n \n         # Note: We're skipping: f.header\n         # Is there something valuable in there?",
        "comments": [
            {
                "comment": "```suggestion\r\n    result = _transcribe(storage, parent=None, editRate=None, masterMobs=masterMobs)\r\n```",
                "position": null
            },
            {
                "comment": "```suggestion\r\n        result = _transcribe(top, parent=None, editRate=None, masterMobs=masterMobs)\r\n```",
                "position": null
            },
            {
                "comment": "Could there be a comment here on why this is needed? (pyaaf2 borks for some reason on floats). I think I've seen this borking before. Maybe we should talk to Mark about it.",
                "position": null
            },
            {
                "comment": "I'll file a separate issue to track that down.",
                "position": null
            },
            {
                "comment": "Actually, I think I found the problem in the writer and fixed it.",
                "position": null
            }
        ],
        "commit_message": "Use `with` instead of try/finally.",
        "commit_id": "bc42222e1b4afbbf5a20a28deda6155c908a6d8f",
        "fn_context": "def read_from_file(filepath, simplify=True):\n\n    with aaf2.open(filepath) as aaf_file:\n\n        storage = aaf_file.content\n\n        # Note: We're skipping: f.header\n        # Is there something valuable in there?\n\n        __names.clear()\n        masterMobs = {}\n\n        result = _transcribe(storage, parent=None, editRate=None, masterMobs=masterMobs)\n        top = storage.toplevel()\n        if top:\n            # re-transcribe just the top-level mobs\n            # but use all the master mobs we found in the 1st pass\n            __names.clear()  # reset the names back to 0\n        result = _transcribe(top, parent=None, editRate=None, masterMobs=masterMobs)\n    \n    # AAF is typically more deeply nested than OTIO.\n    # Lets try to simplify the structure by collapsing or removing\n    # unnecessary stuff.\n    if simplify:\n        result = _simplify(result)\n\n    # OTIO represents transitions a bit different than AAF, so\n    # we need to iterate over them and modify the items on either side.\n    # Note that we do this *after* simplifying, since the structure\n    # may change during simplification.\n    _fix_transitions(result)\n\n    return result"
    },
    {
        "pr_title": "Add VIDEO_FORMAT header when writing to string/file from ALE adpater",
        "pr_number": 462,
        "file_name": "opentimelineio_contrib/adapters/ale.py",
        "code_diff": "@@ -29,7 +29,7 @@\nDEFAULT_VIDEO_FORMAT = '1080'\n \n \n-def AVID_PROJECT_FORMAT_FROM_WIDTH_HEIGHT(width, height):\n+def AVID_VIDEO_FORMAT_FROM_WIDTH_HEIGHT(width, height):\n     \"\"\"Utility function to map a width and height to an Avid Project Format\"\"\"\n \n     format_map = {",
        "comments": [
            {
                "comment": "Might be a good idea to bump this map up to the top level and/or add a comment about how to use this.",
                "position": null
            },
            {
                "comment": "I'm not sure what 'top level' means in an adapter file, but I added a brief comment and made it \"private\"",
                "position": null
            },
            {
                "comment": "I meant that this map could be moved outside the function as a module level constant -- `AVID_PROJECT_HEIGHT_TO_WIDTH_MAP = {...}` or something.  We're trying to call out these kinds of constant definitions to make them easier to find in the future.",
                "position": null
            },
            {
                "comment": "ok, I _think_ I understand the request, however I don't think that dict would be very useful unless you are applying the width check for resolutions wider than 1920, which avid (currently) considers CUSTOM\r\n\r\n I can move the _video_format(...) function to the top of the file and make give that a \"public\" syntax if you think that would be helpful  ",
                "position": null
            },
            {
                "comment": "@ssteinbach - I moved and renamed it.  please let me know if that is more what you are thinking",
                "position": null
            },
            {
                "comment": "Yup, looks great.  Thanks!",
                "position": null
            }
        ],
        "commit_message": "style(ale_adapter): Rename to AVID_VIDEO_FORMAT\n\nRename function to AVID_VIDEO_FORMAT_ from AVID_PROJECT_FORMAT_ to be\nmore accurate to what is being mapped",
        "commit_id": "d0845d7d004a5a3f93aab2c3d21b54c4c8a41ed6",
        "fn_context": "def AVID_VIDEO_FORMAT_FROM_WIDTH_HEIGHT(width, height):\n    \"\"\"Utility function to map a width and height to an Avid Project Format\"\"\"\n\n    format_map = {\n        '1080': \"1080\",\n        '720': \"720\",\n        '576': \"PAL\",\n        '486': \"NTSC\",\n    }\n    mapped = format_map.get(str(height), \"CUSTOM\")\n    # check for the 2K DCI 1080 format\n    if mapped == '1080' and width > 1920:\n        mapped = \"CUSTOM\"\n    return mapped"
    },
    {
        "pr_title": "Support nesting for AAF writer",
        "pr_number": 461,
        "file_name": "opentimelineio_contrib/adapters/aaf_adapter/aaf_writer.py",
        "code_diff": "@@ -477,10 +477,9 @@\ndef _create_filemob(self, otio_clip, tapemob, tapemob_slot):\n         filemob_slot.segment = filemob_clip\n         return filemob, filemob_slot\n \n-    def _create_mastermob(self, otio_clip, tapemob, filemob, filemob_slot):\n+    def _create_mastermob(self, otio_clip, filemob, filemob_slot):\n         \"\"\"\n-        Return a mastermob for an otio Clip. Needs a filemob, tapemob, and\n-        filemob slot.\n+        Return a mastermob for an otio Clip. Needs a filemob and filemob slot.\n \n         Returns:\n             Returns a tuple of (MasterMob, MasterMobSlot)",
        "comments": [],
        "commit_message": "added nesting test and renamed two_clips.aaf to one_clip.aaf\n\nCo-authored-by: Freeson Wang <freeson@pixar.com>",
        "commit_id": "c4ed3ec381f007e57cad8ae2b824a2254b2dfa1c"
    },
    {
        "pr_title": "Support nesting for AAF writer",
        "pr_number": 461,
        "file_name": "opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -828,6 +828,9 @@\ndef _target_url_fixup(timeline):\n         otio.adapters.write_to_file(otio_timeline, tmp_aaf_path, use_empty_mob_ids=True)\n         self._verify_aaf(tmp_aaf_path)\n \n+    def test_aaf_writer_nesting(self):\n+        self._verify_aaf(NESTING_EXAMPLE_PATH)\n+\n     def _verify_aaf(self, aaf_path):\n         otio_timeline = otio.adapters.read_from_file(aaf_path, simplify=True)\n         fd, tmp_aaf_path = tempfile.mkstemp(suffix='.aaf')",
        "comments": [],
        "commit_message": "added nesting test and renamed two_clips.aaf to one_clip.aaf\n\nCo-authored-by: Freeson Wang <freeson@pixar.com>",
        "commit_id": "c4ed3ec381f007e57cad8ae2b824a2254b2dfa1c"
    },
    {
        "pr_title": "Add Effect Widget to otioview",
        "pr_number": 457,
        "file_name": "opentimelineview/timeline_widget.py",
        "code_diff": "@@ -107,7 +107,7 @@\ndef _add_effects(self):\n             return\n         if not self.item.effects:\n             return\n-        effect = Effect(self.item.effects, self.rect())\n+        effect = EffectItem(self.item.effects, self.rect())\n         effect.setParentItem(self)\n \n     def _position_labels(self):",
        "comments": [
            {
                "comment": "what if this was called `EffectsBadge` or `EffectsWidget` or something else like that to disambiguate it from `otio...Effect` ",
                "position": null
            },
            {
                "comment": "We're trying to use the newer (and python3 friendlier) `.format` style of string formatting in python:\r\n\r\n```python\r\n\"{} {}\".format(name, effect_name)\r\n```",
                "position": null
            },
            {
                "comment": "what about EffecfItem ? The other pysideWidget are like ClipItem, GapItem, TransitionItem , NestedItem.",
                "position": null
            },
            {
                "comment": "Sounds great to me!",
                "position": null
            }
        ],
        "commit_message": "addressing code review issues. Change Effect to EffectItem and use .format style",
        "commit_id": "bbcf070b3325bb3d3f67cd2de20af81ad746ab67",
        "fn_context": "    def _add_effects(self):\n        if not hasattr(self.item, \"effects\"):\n            return\n        if not self.item.effects:\n            return\n        effect = EffectItem(self.item.effects, self.rect())\n        effect.setParentItem(self)\n\n    def _position_labels(self):\n        self.source_in_label.setY(LABEL_MARGIN)\n        self.source_out_label.setY(LABEL_MARGIN)\n        self.source_name_label.setY(\n            (TRACK_HEIGHT -\n             self.source_name_label.boundingRect().height()) / 2.0\n        )"
    },
    {
        "pr_title": "Add Effect Widget to otioview",
        "pr_number": 457,
        "file_name": "opentimelineview/timeline_widget.py",
        "code_diff": "@@ -214,10 +214,10 @@\ndef __init__(self, *args, **kwargs):\n         self.source_name_label.setText('GAP')\n \n \n-class Effect(QtWidgets.QGraphicsRectItem):\n+class EffectItem(QtWidgets.QGraphicsRectItem):\n \n     def __init__(self, item, rect, *args, **kwargs):\n-        super(Effect, self).__init__(rect, *args, **kwargs)\n+        super(EffectItem, self).__init__(rect, *args, **kwargs)\n         self.item = item\n         self.setFlags(QtWidgets.QGraphicsItem.ItemIsSelectable)\n         self.init()",
        "comments": [
            {
                "comment": "what if this was called `EffectsBadge` or `EffectsWidget` or something else like that to disambiguate it from `otio...Effect` ",
                "position": null
            },
            {
                "comment": "We're trying to use the newer (and python3 friendlier) `.format` style of string formatting in python:\r\n\r\n```python\r\n\"{} {}\".format(name, effect_name)\r\n```",
                "position": null
            },
            {
                "comment": "what about EffecfItem ? The other pysideWidget are like ClipItem, GapItem, TransitionItem , NestedItem.",
                "position": null
            },
            {
                "comment": "Sounds great to me!",
                "position": null
            }
        ],
        "commit_message": "addressing code review issues. Change Effect to EffectItem and use .format style",
        "commit_id": "bbcf070b3325bb3d3f67cd2de20af81ad746ab67",
        "fn_context": "    def __init__(self, *args, **kwargs):\n        super(GapItem, self).__init__(*args, **kwargs)\n        self.setBrush(\n            QtGui.QBrush(QtGui.QColor(100, 100, 100, 255))\n        )\n        self.source_name_label.setText('GAP')\n\n    def __init__(self, item, rect, *args, **kwargs):\n        super(EffectItem, self).__init__(rect, *args, **kwargs)\n        self.item = item\n        self.setFlags(QtWidgets.QGraphicsItem.ItemIsSelectable)\n        self.init()\n        self._set_tooltip()\n\n    def init(self):\n        rect = self.rect()\n        rect.setY(TRACK_HEIGHT - EFFECT_HEIGHT)\n        rect.setHeight(EFFECT_HEIGHT)\n        self.setRect(rect)\n\n        dark = QtGui.QColor(0, 0, 0, 150)\n        colour = QtGui.QColor(255, 255, 255, 200)\n        gradient = QtGui.QLinearGradient(\n                   QtCore.QPointF(0,\n                                  self.boundingRect().top()),\n                   QtCore.QPointF(0,\n                                  self.boundingRect().bottom()))\n        gradient.setColorAt(0.2, QtCore.Qt.transparent)\n        gradient.setColorAt(0.45, colour)\n        gradient.setColorAt(0.7, QtCore.Qt.transparent)\n        gradient.setColorAt(1.0, dark)\n        self.setBrush(QtGui.QBrush(gradient))\n\n        pen = self.pen()\n        pen.setColor(QtGui.QColor(0, 0, 0, 80))\n        pen.setWidth(0)\n        self.setPen(pen)"
    },
    {
        "pr_title": "Add Effect Widget to otioview",
        "pr_number": 457,
        "file_name": "opentimelineview/timeline_widget.py",
        "code_diff": "@@ -252,13 +252,13 @@\ndef _set_tooltip(self):\n         for effect in self.item:\n             name = effect.name if effect.name else \"\"\n             effect_name = effect.effect_name if effect.effect_name else \"\"\n-            tool_tips.append(\"%s %s\" % (name, effect_name))\n+            tool_tips.append(\"{} {}\".format(name, effect_name))\n         self.setToolTip(\"\\n\".join(tool_tips))\n \n     def paint(self, *args, **kwargs):\n         new_args = [args[0],\n                     QtWidgets.QStyleOptionGraphicsItem()] + list(args[2:])\n-        super(Effect, self).paint(*new_args, **kwargs)\n+        super(EffectItem, self).paint(*new_args, **kwargs)\n \n     def itemChange(self, change, value):\n         if change == QtWidgets.QGraphicsItem.ItemSelectedHasChanged:",
        "comments": [
            {
                "comment": "what if this was called `EffectsBadge` or `EffectsWidget` or something else like that to disambiguate it from `otio...Effect` ",
                "position": null
            },
            {
                "comment": "We're trying to use the newer (and python3 friendlier) `.format` style of string formatting in python:\r\n\r\n```python\r\n\"{} {}\".format(name, effect_name)\r\n```",
                "position": null
            },
            {
                "comment": "what about EffecfItem ? The other pysideWidget are like ClipItem, GapItem, TransitionItem , NestedItem.",
                "position": null
            },
            {
                "comment": "Sounds great to me!",
                "position": null
            }
        ],
        "commit_message": "addressing code review issues. Change Effect to EffectItem and use .format style",
        "commit_id": "bbcf070b3325bb3d3f67cd2de20af81ad746ab67",
        "fn_context": "    def _set_tooltip(self):\n        tool_tips = list()\n        for effect in self.item:\n            name = effect.name if effect.name else \"\"\n            effect_name = effect.effect_name if effect.effect_name else \"\"\n            tool_tips.append(\"{} {}\".format(name, effect_name))\n        self.setToolTip(\"\\n\".join(tool_tips))\n\n    def paint(self, *args, **kwargs):\n        new_args = [args[0],\n                    QtWidgets.QStyleOptionGraphicsItem()] + list(args[2:])\n        super(EffectItem, self).paint(*new_args, **kwargs)\n\n    def itemChange(self, change, value):\n        if change == QtWidgets.QGraphicsItem.ItemSelectedHasChanged:\n            pen = self.pen()\n            pen.setColor(\n                QtGui.QColor(0, 255, 0, 255) if self.isSelected()\n                else QtGui.QColor(0, 0, 0, 80)\n            )\n            self.setPen(pen)\n            self.setZValue(\n                self.zValue() + 1 if self.isSelected() else self.zValue() - 1\n            )\n\n        return super(EffectItem, self).itemChange(change, value)"
    },
    {
        "pr_title": "AAF writer: Adding support for audio transitions",
        "pr_number": 454,
        "file_name": "opentimelineio_contrib/adapters/aaf_adapter/aaf_writer.py",
        "code_diff": "@@ -41,6 +41,7 @@\n\"93994bd6-a81d-11d3-a05b-006094eb75cb\")\n AAF_PARAMETERDEF_AFX_FG_KEY_OPACITY_U = uuid.UUID(\n     \"8d56813d-847e-11d5-935a-50f857c10000\")\n+AAF_PARAMETERDEF_LEVEL = uuid.UUID(\"e4962320-2267-11d3-8a4c-0050040ef7d2\")\n AAF_VVAL_EXTRAPOLATION_ID = uuid.UUID(\"0e24dd54-66cd-4f1a-b0a0-670ac3a7a0b3\")",
        "comments": [],
        "commit_message": "Adding code for audio transitions. Creating abstract method\n_transition_parameters\n\nCo-authored-by: Freeson Wang <freeson@pixar.com>",
        "commit_id": "2ebba59d872f60b65e434a5fa534674081faadf9"
    },
    {
        "pr_title": "AAF writer: Adding support for audio transitions",
        "pr_number": 454,
        "file_name": "opentimelineio_contrib/adapters/aaf_adapter/aaf_writer.py",
        "code_diff": "@@ -218,6 +219,10 @@\ndef _create_timeline_mobslot(self):\n     def default_descriptor(self, otio_clip):\n         pass\n \n+    @abc.abstractmethod\n+    def _transition_parameters(self):\n+        pass\n+\n     def aaf_filler(self, otio_gap):\n         \"\"\"Convert an otio Gap into an aaf Filler\"\"\"\n         # length = otio_gap.duration().value  # XXX Not working for some reason",
        "comments": [],
        "commit_message": "Adding code for audio transitions. Creating abstract method\n_transition_parameters\n\nCo-authored-by: Freeson Wang <freeson@pixar.com>",
        "commit_id": "2ebba59d872f60b65e434a5fa534674081faadf9"
    },
    {
        "pr_title": "AAF writer: Adding support for audio transitions",
        "pr_number": 454,
        "file_name": "opentimelineio_contrib/adapters/aaf_adapter/aaf_writer.py",
        "code_diff": "@@ -231,7 +236,8 @@\ndef aaf_sourceclip(self, otio_clip):\n         filemob, filemob_slot = self._create_filemob(otio_clip, tapemob, tapemob_slot)\n         mastermob, mastermob_slot = self._create_mastermob(otio_clip, filemob,\n                                                            filemob_slot)\n-        length = otio_clip.duration().value\n+        # length = otio_clip.duration().value  # XXX Not working for some reason\n+        length = otio_clip.metadata[\"AAF\"][\"Length\"]\n         compmob_clip = self.compositionmob.create_source_clip(\n             slot_id=self.timeline_mobslot.slot_id,\n             length=length,",
        "comments": [],
        "commit_message": "Adding code for audio transitions. Creating abstract method\n_transition_parameters\n\nCo-authored-by: Freeson Wang <freeson@pixar.com>",
        "commit_id": "2ebba59d872f60b65e434a5fa534674081faadf9"
    },
    {
        "pr_title": "AAF writer: Adding support for audio transitions",
        "pr_number": 454,
        "file_name": "opentimelineio_contrib/adapters/aaf_adapter/aaf_writer.py",
        "code_diff": "@@ -242,51 +248,20 @@\ndef aaf_sourceclip(self, otio_clip):\n         return compmob_clip\n \n     def aaf_transition(self, otio_transition):\n-        \"\"\"Convert an otio Transition into an aaf Transition.\"\"\"\n-        # Create ParameterDef for AvidParameterByteOrder\n+        \"\"\"Convert an otio Transition into an aaf Transition\"\"\"\n         if (otio_transition.transition_type !=\n                 otio.schema.transition.TransitionTypes.SMPTE_Dissolve):\n             print(\n                 \"Unsupported transition type: {}\".format(\n                     otio_transition.transition_type))\n             return None\n-        byteorder_typedef = self.aaf_file.dictionary.lookup_typedef(\"aafUInt16\")\n-        param_byteorder = self.aaf_file.create.ParameterDef(\n-            AAF_PARAMETERDEF_AVIDPARAMETERBYTEORDER,\n-            \"AvidParameterByteOrder\",\n-            \"\",\n-            byteorder_typedef)\n-        self.aaf_file.dictionary.register_def(param_byteorder)\n-\n-        # Create ParameterDef for AvidEffectID\n-        avid_effect_typdef = self.aaf_file.dictionary.lookup_typedef(\"AvidBagOfBits\")\n-        param_effect_id = self.aaf_file.create.ParameterDef(\n-            AAF_PARAMETERDEF_AVIDEFFECTID,\n-            \"AvidEffectID\",\n-            \"\",\n-            avid_effect_typdef)\n-        self.aaf_file.dictionary.register_def(param_effect_id)\n-\n-        # Create ParameterDef for AFX_FG_KEY_OPACITY_U\n-        opacity_param_def = self.aaf_file.dictionary.lookup_typedef(\"Rational\")\n-        opacity_param = self.aaf_file.create.ParameterDef(\n-            AAF_PARAMETERDEF_AFX_FG_KEY_OPACITY_U,\n-            \"AFX_FG_KEY_OPACITY_U\",\n-            \"\",\n-            opacity_param_def)\n-        self.aaf_file.dictionary.register_def(opacity_param)\n \n-        # Create VaryingValue\n-        opacity_u = self.aaf_file.create.VaryingValue()\n-        opacity_u.parameterdef = self.aaf_file.dictionary.lookup_parameterdef(\n-            \"AFX_FG_KEY_OPACITY_U\")\n-        opacity_u[\"VVal_Extrapolation\"].value = AAF_VVAL_EXTRAPOLATION_ID\n-        opacity_u[\"VVal_FieldCount\"].value = 1\n+        transition_params, varying_value = self._transition_parameters()\n \n         interpolation_def = self.aaf_file.create.InterpolationDef(\n             aaf2.misc.LinearInterp, \"LinearInterp\", \"Linear keyframe interpolation\")\n         self.aaf_file.dictionary.register_def(interpolation_def)\n-        opacity_u[\"Interpolation\"].value = \\\n+        varying_value[\"Interpolation\"].value = \\\n             self.aaf_file.dictionary.lookup_interperlationdef(\"LinearInterp\")\n \n         pointlist = otio_transition.metadata[\"AAF\"][\"PointList\"]",
        "comments": [],
        "commit_message": "Adding code for audio transitions. Creating abstract method\n_transition_parameters\n\nCo-authored-by: Freeson Wang <freeson@pixar.com>",
        "commit_id": "2ebba59d872f60b65e434a5fa534674081faadf9"
    },
    {
        "pr_title": "AAF writer: Adding support for audio transitions",
        "pr_number": 454,
        "file_name": "opentimelineio_contrib/adapters/aaf_adapter/aaf_writer.py",
        "code_diff": "@@ -301,7 +276,7 @@\ndef aaf_transition(self, otio_transition):\n         c2.value = pointlist[1][\"Value\"]\n         c2.time = pointlist[1][\"Time\"]\n \n-        opacity_u[\"PointList\"].extend([c1, c2])\n+        varying_value[\"PointList\"].extend([c1, c2])\n \n         op_group_metadata = otio_transition.metadata[\"AAF\"][\"OperationGroup\"]\n         effect_id = op_group_metadata[\"Operation\"].get(\"Identification\")",
        "comments": [],
        "commit_message": "Adding code for audio transitions. Creating abstract method\n_transition_parameters\n\nCo-authored-by: Freeson Wang <freeson@pixar.com>",
        "commit_id": "2ebba59d872f60b65e434a5fa534674081faadf9"
    },
    {
        "pr_title": "AAF writer: Adding support for audio transitions",
        "pr_number": 454,
        "file_name": "opentimelineio_contrib/adapters/aaf_adapter/aaf_writer.py",
        "code_diff": "@@ -316,24 +291,24 @@\ndef aaf_transition(self, otio_transition):\n             \"OperationGroup\"\n         ][\"Operation\"][\"Name\"]\n \n+        # Create OperationDefinition\n         op_def = self.aaf_file.create.OperationDef(uuid.UUID(effect_id), op_def_name)\n         self.aaf_file.dictionary.register_def(op_def)\n         op_def.media_kind = self.media_kind\n-        datadef = self.aaf_file.dictionary.lookup_datadef(\"Picture\")\n+        datadef = self.aaf_file.dictionary.lookup_datadef(self.media_kind)\n         op_def[\"IsTimeWarp\"].value = is_time_warp\n         op_def[\"Bypass\"].value = by_pass\n         op_def[\"NumberInputs\"].value = number_inputs\n         op_def[\"OperationCategory\"].value = str(operation_category)\n-        op_def[\"ParametersDefined\"].extend([param_byteorder,\n-                                            param_effect_id])\n+        op_def[\"ParametersDefined\"].extend(transition_params)\n         op_def[\"DataDefinition\"].value = data_def\n         op_def[\"Description\"].value = str(description)\n \n         # Create OperationGroup\n         length = otio_transition.metadata[\"AAF\"][\"Length\"]\n         operation_group = self.aaf_file.create.OperationGroup(op_def, length)\n         operation_group[\"DataDefinition\"].value = datadef\n-        operation_group[\"Parameters\"].append(opacity_u)\n+        operation_group[\"Parameters\"].append(varying_value)\n \n         # Create Transition\n         transition = self.aaf_file.create.Transition(self.media_kind, length)",
        "comments": [],
        "commit_message": "Adding code for audio transitions. Creating abstract method\n_transition_parameters\n\nCo-authored-by: Freeson Wang <freeson@pixar.com>",
        "commit_id": "2ebba59d872f60b65e434a5fa534674081faadf9"
    },
    {
        "pr_title": "AAF writer: Adding support for audio transitions",
        "pr_number": 454,
        "file_name": "opentimelineio_contrib/adapters/aaf_adapter/aaf_writer.py",
        "code_diff": "@@ -437,6 +412,46 @@\ndef default_descriptor(self, otio_clip):\n         descriptor[\"Length\"].value = 1\n         return descriptor\n \n+    def _transition_parameters(self):\n+        \"\"\"\n+        Return video transition parameters\n+        \"\"\"\n+        # Create ParameterDef for AvidParameterByteOrder\n+        byteorder_typedef = self.aaf_file.dictionary.lookup_typedef(\"aafUInt16\")\n+        param_byteorder = self.aaf_file.create.ParameterDef(\n+            AAF_PARAMETERDEF_AVIDPARAMETERBYTEORDER,\n+            \"AvidParameterByteOrder\",\n+            \"\",\n+            byteorder_typedef)\n+        self.aaf_file.dictionary.register_def(param_byteorder)\n+\n+        # Create ParameterDef for AvidEffectID\n+        avid_effect_typdef = self.aaf_file.dictionary.lookup_typedef(\"AvidBagOfBits\")\n+        param_effect_id = self.aaf_file.create.ParameterDef(\n+            AAF_PARAMETERDEF_AVIDEFFECTID,\n+            \"AvidEffectID\",\n+            \"\",\n+            avid_effect_typdef)\n+        self.aaf_file.dictionary.register_def(param_effect_id)\n+\n+        # Create ParameterDef for AFX_FG_KEY_OPACITY_U\n+        opacity_param_def = self.aaf_file.dictionary.lookup_typedef(\"Rational\")\n+        opacity_param = self.aaf_file.create.ParameterDef(\n+            AAF_PARAMETERDEF_AFX_FG_KEY_OPACITY_U,\n+            \"AFX_FG_KEY_OPACITY_U\",\n+            \"\",\n+            opacity_param_def)\n+        self.aaf_file.dictionary.register_def(opacity_param)\n+\n+        # Create VaryingValue\n+        opacity_u = self.aaf_file.create.VaryingValue()\n+        opacity_u.parameterdef = self.aaf_file.dictionary.lookup_parameterdef(\n+            \"AFX_FG_KEY_OPACITY_U\")\n+        opacity_u[\"VVal_Extrapolation\"].value = AAF_VVAL_EXTRAPOLATION_ID\n+        opacity_u[\"VVal_FieldCount\"].value = 1\n+\n+        return [param_byteorder, param_effect_id], opacity_u\n+\n \n class AudioTrackTranscriber(_TrackTranscriber):\n     \"\"\"Audio track kind specialization of TrackTranscriber.\"\"\"",
        "comments": [],
        "commit_message": "Adding code for audio transitions. Creating abstract method\n_transition_parameters\n\nCo-authored-by: Freeson Wang <freeson@pixar.com>",
        "commit_id": "2ebba59d872f60b65e434a5fa534674081faadf9"
    },
    {
        "pr_title": "Add Ruler to otioview",
        "pr_number": 451,
        "file_name": "opentimelineview/timeline_widget.py",
        "code_diff": "@@ -35,7 +35,6 @@\nLABEL_MARGIN = 5\n MARKER_SIZE = 10\n RULER_SIZE = 10\n-EFFECT_HEIGHT = 0.25 * TRACK_HEIGHT\n \n \n class _BaseItem(QtWidgets.QGraphicsRectItem):",
        "comments": [
            {
                "comment": "if you invert this if statement:\r\n`if not (isinstance...)`\r\nthen you can dedent this block",
                "position": null
            },
            {
                "comment": "same here with the indentation",
                "position": null
            },
            {
                "comment": "could invert this if block",
                "position": null
            },
            {
                "comment": "dead code",
                "position": null
            }
        ],
        "commit_message": "Addressing pull request issue about the Ruler.",
        "commit_id": "f5352f7a6f6515a4d5bc6d1c0f85d5faa53cbe07",
        "fn_context": ""
    },
    {
        "pr_title": "Add Ruler to otioview",
        "pr_number": 451,
        "file_name": "opentimelineview/timeline_widget.py",
        "code_diff": "@@ -60,8 +59,7 @@\ndef __init__(self, item, timeline_range, *args, **kwargs):\n \n         self._add_markers()\n         self._set_labels()\n-        self._set_tooltip()\n-        self._add_effects()       \n+        self._set_tooltip()    \n \n     def paint(self, *args, **kwargs):\n         new_args = [args[0],",
        "comments": [
            {
                "comment": "if you invert this if statement:\r\n`if not (isinstance...)`\r\nthen you can dedent this block",
                "position": null
            },
            {
                "comment": "same here with the indentation",
                "position": null
            },
            {
                "comment": "could invert this if block",
                "position": null
            },
            {
                "comment": "dead code",
                "position": null
            }
        ],
        "commit_message": "Addressing pull request issue about the Ruler.",
        "commit_id": "f5352f7a6f6515a4d5bc6d1c0f85d5faa53cbe07",
        "fn_context": "    def __init__(self, item, timeline_range, *args, **kwargs):\n        super(_BaseItem, self).__init__(*args, **kwargs)\n        self.item = item\n        self.timeline_range = timeline_range\n\n        self.setFlags(QtWidgets.QGraphicsItem.ItemIsSelectable)\n        self.setBrush(\n            QtGui.QBrush(QtGui.QColor(180, 180, 180, 255))\n        )\n\n        pen = QtGui.QPen()\n        pen.setWidth(0)\n        self.setPen(pen)\n\n        self.source_in_label = QtWidgets.QGraphicsSimpleTextItem(self)\n        self.source_out_label = QtWidgets.QGraphicsSimpleTextItem(self)\n        self.source_name_label = QtWidgets.QGraphicsSimpleTextItem(self)\n\n        self._add_markers()\n        self._set_labels()\n        self._set_tooltip()    \n\n    def paint(self, *args, **kwargs):\n        new_args = [args[0],\n                    QtWidgets.QStyleOptionGraphicsItem()] + list(args[2:])\n        super(_BaseItem, self).paint(*new_args, **kwargs)"
    },
    {
        "pr_title": "Add Ruler to otioview",
        "pr_number": 451,
        "file_name": "opentimelineview/timeline_widget.py",
        "code_diff": "@@ -101,14 +99,6 @@\ndef _add_markers(self):\n             )\n             marker.setParentItem(self)\n \n-    def _add_effects(self):\n-        if not hasattr(self.item,\"effects\") :\n-            return\n-        if not self.item.effects :\n-            return\n-        effect = Effect(self.item.effects, self.timeline_range, self.rect())\n-        effect.setParentItem(self)\n-\n     def _position_labels(self):\n         self.source_in_label.setY(LABEL_MARGIN)\n         self.source_out_label.setY(LABEL_MARGIN)",
        "comments": [
            {
                "comment": "if you invert this if statement:\r\n`if not (isinstance...)`\r\nthen you can dedent this block",
                "position": null
            },
            {
                "comment": "same here with the indentation",
                "position": null
            },
            {
                "comment": "could invert this if block",
                "position": null
            },
            {
                "comment": "dead code",
                "position": null
            }
        ],
        "commit_message": "Addressing pull request issue about the Ruler.",
        "commit_id": "f5352f7a6f6515a4d5bc6d1c0f85d5faa53cbe07",
        "fn_context": "    def _add_markers(self):\n        trimmed_range = self.item.trimmed_range()\n\n        for m in self.item.markers:\n            marked_time = m.marked_range.start_time\n            if not trimmed_range.overlaps(marked_time):\n                continue\n\n            # @TODO: set the marker color if its set from the OTIO object\n            marker = Marker(m, None)\n            marker.setY(0.5 * MARKER_SIZE)\n            marker.setX(\n                (\n                    otio.opentime.to_seconds(m.marked_range.start_time) -\n                    otio.opentime.to_seconds(trimmed_range.start_time)\n                ) * TIME_MULTIPLIER\n            )\n            marker.setParentItem(self)\n\n    def _position_labels(self):\n        self.source_in_label.setY(LABEL_MARGIN)\n        self.source_out_label.setY(LABEL_MARGIN)\n        self.source_name_label.setY(\n            TRACK_HEIGHT -\n            LABEL_MARGIN -\n            self.source_name_label.boundingRect().height()\n        )\n\n    def _set_labels_rational_time(self):\n        trimmed_range = self.item.trimmed_range()\n        self.source_in_label.setText(\n            '{value}\\n@{rate}'.format(\n                value=trimmed_range.start_time.value,\n                rate=trimmed_range.start_time.rate\n            )\n        )\n        self.source_out_label.setText(\n            '{value}\\n@{rate}'.format(\n                value=trimmed_range.end_time_exclusive().value,\n                rate=trimmed_range.end_time_exclusive().rate\n            )\n        )"
    },
    {
        "pr_title": "New feature implementation of writing AAF files from otio files in the AAF adapter",
        "pr_number": 432,
        "file_name": "opentimelineio_contrib/adapters/aaf_adapter/aaf_writer.py",
        "code_diff": "@@ -28,11 +28,20 @@\n\"\"\"\n \n import aaf2\n-from aaf2.rational import AAFRational\n-from aaf2.auid import AUID\n import uuid\n import opentimelineio as otio\n-from abc import ABCMeta, abstractmethod\n+import abc\n+\n+\n+AAF_PARAMETERDEF_PAN = aaf2.auid.AUID(\"e4962322-2267-11d3-8a4c-0050040ef7d2\")\n+AAF_OPERATIONDEF_MONOAUDIOPAN = aaf2.auid.AUID(\"9d2ea893-0968-11d3-8a38-0050040ef7d2\")\n+AAF_PARAMETERDEF_AVIDPARAMETERBYTEORDER = uuid.UUID(\n+    \"c0038672-a8cf-11d3-a05b-006094eb75cb\")\n+AAF_PARAMETERDEF_AVIDEFFECTID = uuid.UUID(\n+    \"93994bd6-a81d-11d3-a05b-006094eb75cb\")\n+AAF_PARAMETERDEF_AFX_FG_KEY_OPACITY_U = uuid.UUID(\n+    \"8d56813d-847e-11d5-935a-50f857c10000\")\n+AAF_VVAL_EXTRAPOLATION_ID = uuid.UUID(\"0e24dd54-66cd-4f1a-b0a0-670ac3a7a0b3\")\n \n \n def _timecode_length(clip):",
        "comments": [
            {
                "comment": "I think it would probably help clarify that this is because Audio and Video tracks are handled differently.  (thats why this is an ABC and not a concrete class).",
                "position": null
            },
            {
                "comment": "I think this could be preceded by a `_` to signal that it isn't an entry point for external usage.",
                "position": null
            },
            {
                "comment": "Would be nice to promote these UUIDs into a more meaningful enum at the top of the file.  So AAF_UUID_TRANSITION or something.  This could happen in a future PR if you think thats a big change.",
                "position": null
            },
            {
                "comment": "Same here - breaking this out into an enum would be nice.",
                "position": null
            },
            {
                "comment": "another enum",
                "position": null
            },
            {
                "comment": "another UUID enum",
                "position": null
            },
            {
                "comment": "UUID enum",
                "position": null
            },
            {
                "comment": "Could use a small comment, like \"Video track kind specialization of TrackTranscriber\" or something.",
                "position": null
            },
            {
                "comment": "AUID enum too",
                "position": null
            },
            {
                "comment": " I like leaving the module namespace in the name to the object so its clear what is local and what comes from a depedency.  So `import abc` rather than the from import.  Same for `AUID` and `AAFRational`, unless there is some other reason for doing the `from` import.  Helps signal to the reader its external code.",
                "position": null
            },
            {
                "comment": "Inverting this would allow you to dedent the block and make the `else` condition more clear:\r\n\r\n```python\r\nif tape_mob:\r\n  return tape_mob\r\n...\r\n```",
                "position": null
            },
            {
                "comment": "Inverting this could also make it more readable (see next comment).",
                "position": null
            }
        ],
        "commit_message": "Addressing code review. Put UUID constants into enums. Renamed to\n_TrackTranscriber. Additional comments.\n\nCo-authored-by: Shahbaz Khan <shahbazkhan8194@gmail.com>",
        "commit_id": "32f34899444a77afd162d721aaad9e62f9e55a6b",
        "fn_context": "def _timecode_length(clip):\n    \"\"\"Return the timecode length (length of essence data) from an otio clip.\"\"\"\n    try:\n        timecode_length = clip.media_reference.available_range.duration.value\n    except AttributeError:\n        timecode_length = clip.metadata[\"AAF\"][\"Length\"]\n        print(\"WARNING: FIX ME! Relying on backup metadata value ('AAF.Length')\"\n              \"instead of actual media_reference.available_range.duration\")\n    return timecode_length"
    },
    {
        "pr_title": "New feature implementation of writing AAF files from otio files in the AAF adapter",
        "pr_number": 432,
        "file_name": "opentimelineio_contrib/adapters/aaf_adapter/aaf_writer.py",
        "code_diff": "@@ -74,26 +83,26 @@\ndef _unique_mastermob(self, otio_clip):\n         \"\"\"Get a unique mastermob, identified by clip metadata mob id.\"\"\"\n         # Currently, we only support clips that are already in Avid with a mob ID\n         mob_id = otio_clip.metadata[\"AAF\"][\"SourceID\"]\n-        master_mob = self._unique_mastermobs.get(mob_id)\n-        if not master_mob:\n-            master_mob = self.aaf_file.create.MasterMob()\n-            master_mob.name = otio_clip.name\n-            master_mob.mob_id = aaf2.mobid.MobID(mob_id)\n-            self.aaf_file.content.mobs.append(master_mob)\n-            self._unique_mastermobs[mob_id] = master_mob\n-        return master_mob\n+        mastermob = self._unique_mastermobs.get(mob_id)\n+        if not mastermob:\n+            mastermob = self.aaf_file.create.MasterMob()\n+            mastermob.name = otio_clip.name\n+            mastermob.mob_id = aaf2.mobid.MobID(mob_id)\n+            self.aaf_file.content.mobs.append(mastermob)\n+            self._unique_mastermobs[mob_id] = mastermob\n+        return mastermob\n \n     def _unique_tapemob(self, otio_clip):\n         \"\"\"Get a unique tapemob, identified by clip metadata mob id.\"\"\"\n         mob_id = otio_clip.metadata[\"AAF\"][\"SourceID\"]\n-        tape_mob = self._unique_tapemobs.get(mob_id)\n-        if not tape_mob:\n-            tape_mob = self.aaf_file.create.SourceMob()\n-            tape_mob.name = otio_clip.name\n-            tape_mob.descriptor = self.aaf_file.create.ImportDescriptor()\n+        tapemob = self._unique_tapemobs.get(mob_id)\n+        if not tapemob:\n+            tapemob = self.aaf_file.create.SourceMob()\n+            tapemob.name = otio_clip.name\n+            tapemob.descriptor = self.aaf_file.create.ImportDescriptor()\n             edit_rate = otio_clip.duration().rate\n-            tape_timecode_slot = tape_mob.create_timecode_slot(edit_rate,\n-                                                               edit_rate)\n+            tape_timecode_slot = tapemob.create_timecode_slot(edit_rate,\n+                                                              edit_rate)\n             try:\n                 timecode_start = \\\n                     otio_clip.media_reference.available_range.start_time.value",
        "comments": [
            {
                "comment": "I think it would probably help clarify that this is because Audio and Video tracks are handled differently.  (thats why this is an ABC and not a concrete class).",
                "position": null
            },
            {
                "comment": "I think this could be preceded by a `_` to signal that it isn't an entry point for external usage.",
                "position": null
            },
            {
                "comment": "Would be nice to promote these UUIDs into a more meaningful enum at the top of the file.  So AAF_UUID_TRANSITION or something.  This could happen in a future PR if you think thats a big change.",
                "position": null
            },
            {
                "comment": "Same here - breaking this out into an enum would be nice.",
                "position": null
            },
            {
                "comment": "another enum",
                "position": null
            },
            {
                "comment": "another UUID enum",
                "position": null
            },
            {
                "comment": "UUID enum",
                "position": null
            },
            {
                "comment": "Could use a small comment, like \"Video track kind specialization of TrackTranscriber\" or something.",
                "position": null
            },
            {
                "comment": "AUID enum too",
                "position": null
            },
            {
                "comment": " I like leaving the module namespace in the name to the object so its clear what is local and what comes from a depedency.  So `import abc` rather than the from import.  Same for `AUID` and `AAFRational`, unless there is some other reason for doing the `from` import.  Helps signal to the reader its external code.",
                "position": null
            },
            {
                "comment": "Inverting this would allow you to dedent the block and make the `else` condition more clear:\r\n\r\n```python\r\nif tape_mob:\r\n  return tape_mob\r\n...\r\n```",
                "position": null
            },
            {
                "comment": "Inverting this could also make it more readable (see next comment).",
                "position": null
            }
        ],
        "commit_message": "Addressing code review. Put UUID constants into enums. Renamed to\n_TrackTranscriber. Additional comments.\n\nCo-authored-by: Shahbaz Khan <shahbazkhan8194@gmail.com>",
        "commit_id": "32f34899444a77afd162d721aaad9e62f9e55a6b",
        "fn_context": "    def _unique_mastermob(self, otio_clip):\n        \"\"\"Get a unique mastermob, identified by clip metadata mob id.\"\"\"\n        # Currently, we only support clips that are already in Avid with a mob ID\n        mob_id = otio_clip.metadata[\"AAF\"][\"SourceID\"]\n        mastermob = self._unique_mastermobs.get(mob_id)\n        if not mastermob:\n            mastermob = self.aaf_file.create.MasterMob()\n            mastermob.name = otio_clip.name\n            mastermob.mob_id = aaf2.mobid.MobID(mob_id)\n            self.aaf_file.content.mobs.append(mastermob)\n            self._unique_mastermobs[mob_id] = mastermob\n        return mastermob\n\n    def _unique_tapemob(self, otio_clip):\n        \"\"\"Get a unique tapemob, identified by clip metadata mob id.\"\"\"\n        mob_id = otio_clip.metadata[\"AAF\"][\"SourceID\"]\n        tapemob = self._unique_tapemobs.get(mob_id)\n        if not tapemob:\n            tapemob = self.aaf_file.create.SourceMob()\n            tapemob.name = otio_clip.name\n            tapemob.descriptor = self.aaf_file.create.ImportDescriptor()\n            edit_rate = otio_clip.duration().rate\n            tape_timecode_slot = tapemob.create_timecode_slot(edit_rate,\n                                                              edit_rate)\n            try:\n                timecode_start = \\\n                    otio_clip.media_reference.available_range.start_time.value\n                # HACK: This backup here shouldn't be needed.\n            except AttributeError:\n                print(\"WARNING: FIX ME! Relying on metadata value ('AAF.StartTime')\"\n                      \"instead of actual media_reference.available_range.start_time\")\n                try:\n                    timecode_start = otio_clip.metadata[\"AAF\"][\"StartTime\"]\n                except KeyError:\n                    raise(\"AAF.StartTime not found even in backup metadata\")\n\n            timecode_length = _timecode_length(otio_clip)\n            tape_timecode_slot.segment.start = timecode_start\n            tape_timecode_slot.segment.length = timecode_length\n            self.aaf_file.content.mobs.append(tapemob)\n            self._unique_tapemobs[mob_id] = tapemob\n        return tapemob"
    },
    {
        "pr_title": "New feature implementation of writing AAF files from otio files in the AAF adapter",
        "pr_number": 432,
        "file_name": "opentimelineio_contrib/adapters/aaf_adapter/aaf_writer.py",
        "code_diff": "@@ -109,12 +118,12 @@\ndef _unique_tapemob(self, otio_clip):\n             timecode_length = _timecode_length(otio_clip)\n             tape_timecode_slot.segment.start = timecode_start\n             tape_timecode_slot.segment.length = timecode_length\n-            self.aaf_file.content.mobs.append(tape_mob)\n-            self._unique_tapemobs[mob_id] = tape_mob\n-        return tape_mob\n+            self.aaf_file.content.mobs.append(tapemob)\n+            self._unique_tapemobs[mob_id] = tapemob\n+        return tapemob\n \n     def track_transcriber(self, otio_track):\n-        \"\"\"Return an appropriate TrackTranscriber given an otio track.\"\"\"\n+        \"\"\"Return an appropriate _TrackTranscriber given an otio track.\"\"\"\n         if otio_track.kind == otio.schema.TrackKind.Video:\n             transcriber = VideoTrackTranscriber(self, otio_track)\n         elif otio_track.kind == otio.schema.TrackKind.Audio:",
        "comments": [
            {
                "comment": "I think it would probably help clarify that this is because Audio and Video tracks are handled differently.  (thats why this is an ABC and not a concrete class).",
                "position": null
            },
            {
                "comment": "I think this could be preceded by a `_` to signal that it isn't an entry point for external usage.",
                "position": null
            },
            {
                "comment": "Would be nice to promote these UUIDs into a more meaningful enum at the top of the file.  So AAF_UUID_TRANSITION or something.  This could happen in a future PR if you think thats a big change.",
                "position": null
            },
            {
                "comment": "Same here - breaking this out into an enum would be nice.",
                "position": null
            },
            {
                "comment": "another enum",
                "position": null
            },
            {
                "comment": "another UUID enum",
                "position": null
            },
            {
                "comment": "UUID enum",
                "position": null
            },
            {
                "comment": "Could use a small comment, like \"Video track kind specialization of TrackTranscriber\" or something.",
                "position": null
            },
            {
                "comment": "AUID enum too",
                "position": null
            },
            {
                "comment": " I like leaving the module namespace in the name to the object so its clear what is local and what comes from a depedency.  So `import abc` rather than the from import.  Same for `AUID` and `AAFRational`, unless there is some other reason for doing the `from` import.  Helps signal to the reader its external code.",
                "position": null
            },
            {
                "comment": "Inverting this would allow you to dedent the block and make the `else` condition more clear:\r\n\r\n```python\r\nif tape_mob:\r\n  return tape_mob\r\n...\r\n```",
                "position": null
            },
            {
                "comment": "Inverting this could also make it more readable (see next comment).",
                "position": null
            }
        ],
        "commit_message": "Addressing code review. Put UUID constants into enums. Renamed to\n_TrackTranscriber. Additional comments.\n\nCo-authored-by: Shahbaz Khan <shahbazkhan8194@gmail.com>",
        "commit_id": "32f34899444a77afd162d721aaad9e62f9e55a6b",
        "fn_context": "    def _unique_tapemob(self, otio_clip):\n        \"\"\"Get a unique tapemob, identified by clip metadata mob id.\"\"\"\n        mob_id = otio_clip.metadata[\"AAF\"][\"SourceID\"]\n        tapemob = self._unique_tapemobs.get(mob_id)\n        if not tapemob:\n            tapemob = self.aaf_file.create.SourceMob()\n            tapemob.name = otio_clip.name\n            tapemob.descriptor = self.aaf_file.create.ImportDescriptor()\n            edit_rate = otio_clip.duration().rate\n            tape_timecode_slot = tapemob.create_timecode_slot(edit_rate,\n                                                              edit_rate)\n            try:\n                timecode_start = \\\n                    otio_clip.media_reference.available_range.start_time.value\n                # HACK: This backup here shouldn't be needed.\n            except AttributeError:\n                print(\"WARNING: FIX ME! Relying on metadata value ('AAF.StartTime')\"\n                      \"instead of actual media_reference.available_range.start_time\")\n                try:\n                    timecode_start = otio_clip.metadata[\"AAF\"][\"StartTime\"]\n                except KeyError:\n                    raise(\"AAF.StartTime not found even in backup metadata\")\n\n            timecode_length = _timecode_length(otio_clip)\n            tape_timecode_slot.segment.start = timecode_start\n            tape_timecode_slot.segment.length = timecode_length\n            self.aaf_file.content.mobs.append(tapemob)\n            self._unique_tapemobs[mob_id] = tapemob\n        return tapemob\n\n    def track_transcriber(self, otio_track):\n        \"\"\"Return an appropriate _TrackTranscriber given an otio track.\"\"\"\n        if otio_track.kind == otio.schema.TrackKind.Video:\n            transcriber = VideoTrackTranscriber(self, otio_track)\n        elif otio_track.kind == otio.schema.TrackKind.Audio:\n            transcriber = AudioTrackTranscriber(self, otio_track)\n        else:\n            raise otio.exceptions.NotSupportedError(\n                \"Unsupported track kind: {}\".format(otio_track.kind))\n        return transcriber"
    },
    {
        "pr_title": "New feature implementation of writing AAF files from otio files in the AAF adapter",
        "pr_number": 432,
        "file_name": "opentimelineio_contrib/adapters/aaf_adapter/aaf_writer.py",
        "code_diff": "@@ -161,18 +170,19 @@\ndef _check(otio_child, keys_path):\n     return errors\n \n \n-class TrackTranscriber(object):\n+class _TrackTranscriber(object):\n     \"\"\"\n-    TrackTranscriber is the base class for the conversion of a given otio track.\n+    _TrackTranscriber is the base class for the conversion of a given otio track.\n \n-    TrackTranscriber is not meant to be used by itself. It provides the common\n-    functionality to inherit from.\n+    _TrackTranscriber is not meant to be used by itself. It provides the common\n+    functionality to inherit from. We need an abstract base class because Audio and\n+    Video are handled differently.\n     \"\"\"\n-    __metaclass__ = ABCMeta\n+    __metaclass__ = abc.ABCMeta\n \n     def __init__(self, root_file_transcriber, otio_track):\n         \"\"\"\n-        TrackTranscriber\n+        _TrackTranscriber\n \n         Args:\n             root_file_transcriber: the corresponding 'parent' AAFFileTranscriber object",
        "comments": [
            {
                "comment": "I think it would probably help clarify that this is because Audio and Video tracks are handled differently.  (thats why this is an ABC and not a concrete class).",
                "position": null
            },
            {
                "comment": "I think this could be preceded by a `_` to signal that it isn't an entry point for external usage.",
                "position": null
            },
            {
                "comment": "Would be nice to promote these UUIDs into a more meaningful enum at the top of the file.  So AAF_UUID_TRANSITION or something.  This could happen in a future PR if you think thats a big change.",
                "position": null
            },
            {
                "comment": "Same here - breaking this out into an enum would be nice.",
                "position": null
            },
            {
                "comment": "another enum",
                "position": null
            },
            {
                "comment": "another UUID enum",
                "position": null
            },
            {
                "comment": "UUID enum",
                "position": null
            },
            {
                "comment": "Could use a small comment, like \"Video track kind specialization of TrackTranscriber\" or something.",
                "position": null
            },
            {
                "comment": "AUID enum too",
                "position": null
            },
            {
                "comment": " I like leaving the module namespace in the name to the object so its clear what is local and what comes from a depedency.  So `import abc` rather than the from import.  Same for `AUID` and `AAFRational`, unless there is some other reason for doing the `from` import.  Helps signal to the reader its external code.",
                "position": null
            },
            {
                "comment": "Inverting this would allow you to dedent the block and make the `else` condition more clear:\r\n\r\n```python\r\nif tape_mob:\r\n  return tape_mob\r\n...\r\n```",
                "position": null
            },
            {
                "comment": "Inverting this could also make it more readable (see next comment).",
                "position": null
            }
        ],
        "commit_message": "Addressing code review. Put UUID constants into enums. Renamed to\n_TrackTranscriber. Additional comments.\n\nCo-authored-by: Shahbaz Khan <shahbazkhan8194@gmail.com>",
        "commit_id": "32f34899444a77afd162d721aaad9e62f9e55a6b",
        "fn_context": "def validate_metadata(timeline):\n    \"\"\"Print a check of necessary metadata requirements for an otio timeline.\"\"\"\n\n    errors = []\n\n    def _check(otio_child, keys_path):\n        keys = keys_path.split(\".\")\n        value = otio_child.metadata\n        try:\n            for key in keys:\n                value = value[key]\n        except KeyError:\n            print(\"{}({}) is missing required metadata {}\".format(\n                  otio_child.name, type(otio_child), keys_path))\n            errors.append((otio_child, keys_path))\n\n    for otio_child in timeline.each_child():\n        if isinstance(otio_child, otio.schema.Gap):\n            _check(otio_child, \"AAF.Length\")  # Shouldn't need\n        elif isinstance(otio_child, otio.schema.Transition):\n            _check(otio_child, \"AAF.PointList\")\n            _check(otio_child, \"AAF.OperationGroup\")\n            _check(otio_child, \"AAF.OperationGroup.Operation\")\n            _check(otio_child,\n                   \"AAF.OperationGroup.Operation.DataDefinition.Name\")\n            _check(otio_child, \"AAF.OperationGroup.Operation.Description\")\n            _check(otio_child, \"AAF.OperationGroup.Operation.Name\")\n            _check(otio_child, \"AAF.Length\")\n            _check(otio_child, \"AAF.CutPoint\")\n        elif isinstance(otio_child, otio.schema.Clip):\n            _check(otio_child, \"AAF.SourceID\")\n            _check(otio_child, \"AAF.SourceMobSlotID\")\n\n    return errors\n\n    def __init__(self, root_file_transcriber, otio_track):\n        \"\"\"\n        _TrackTranscriber\n\n        Args:\n            root_file_transcriber: the corresponding 'parent' AAFFileTranscriber object\n            otio_track: the given otio_track to convert\n        \"\"\"\n        self.root_file_transcriber = root_file_transcriber\n        self.compositionmob = root_file_transcriber.compositionmob\n        self.aaf_file = root_file_transcriber.aaf_file\n        self.otio_track = otio_track\n        self.edit_rate = next(self.otio_track.each_clip()).duration().rate\n        self.timeline_mobslot, self.sequence = self._create_timeline_mobslot()"
    },
    {
        "pr_title": "New feature implementation of writing AAF files from otio files in the AAF adapter",
        "pr_number": 432,
        "file_name": "opentimelineio_contrib/adapters/aaf_adapter/aaf_writer.py",
        "code_diff": "@@ -186,12 +196,12 @@\ndef __init__(self, root_file_transcriber, otio_track):\n         self.timeline_mobslot, self.sequence = self._create_timeline_mobslot()\n \n     @property\n-    @abstractmethod\n+    @abc.abstractmethod\n     def media_kind(self):\n         \"\"\"Return the string for what kind of track this is.\"\"\"\n         pass\n \n-    @abstractmethod\n+    @abc.abstractmethod\n     def _create_timeline_mobslot(self):\n         \"\"\"\n         Return a timeline_mobslot and sequence for this track.",
        "comments": [
            {
                "comment": "I think it would probably help clarify that this is because Audio and Video tracks are handled differently.  (thats why this is an ABC and not a concrete class).",
                "position": null
            },
            {
                "comment": "I think this could be preceded by a `_` to signal that it isn't an entry point for external usage.",
                "position": null
            },
            {
                "comment": "Would be nice to promote these UUIDs into a more meaningful enum at the top of the file.  So AAF_UUID_TRANSITION or something.  This could happen in a future PR if you think thats a big change.",
                "position": null
            },
            {
                "comment": "Same here - breaking this out into an enum would be nice.",
                "position": null
            },
            {
                "comment": "another enum",
                "position": null
            },
            {
                "comment": "another UUID enum",
                "position": null
            },
            {
                "comment": "UUID enum",
                "position": null
            },
            {
                "comment": "Could use a small comment, like \"Video track kind specialization of TrackTranscriber\" or something.",
                "position": null
            },
            {
                "comment": "AUID enum too",
                "position": null
            },
            {
                "comment": " I like leaving the module namespace in the name to the object so its clear what is local and what comes from a depedency.  So `import abc` rather than the from import.  Same for `AUID` and `AAFRational`, unless there is some other reason for doing the `from` import.  Helps signal to the reader its external code.",
                "position": null
            },
            {
                "comment": "Inverting this would allow you to dedent the block and make the `else` condition more clear:\r\n\r\n```python\r\nif tape_mob:\r\n  return tape_mob\r\n...\r\n```",
                "position": null
            },
            {
                "comment": "Inverting this could also make it more readable (see next comment).",
                "position": null
            }
        ],
        "commit_message": "Addressing code review. Put UUID constants into enums. Renamed to\n_TrackTranscriber. Additional comments.\n\nCo-authored-by: Shahbaz Khan <shahbazkhan8194@gmail.com>",
        "commit_id": "32f34899444a77afd162d721aaad9e62f9e55a6b",
        "fn_context": "    def __init__(self, root_file_transcriber, otio_track):\n        \"\"\"\n        _TrackTranscriber\n\n        Args:\n            root_file_transcriber: the corresponding 'parent' AAFFileTranscriber object\n            otio_track: the given otio_track to convert\n        \"\"\"\n        self.root_file_transcriber = root_file_transcriber\n        self.compositionmob = root_file_transcriber.compositionmob\n        self.aaf_file = root_file_transcriber.aaf_file\n        self.otio_track = otio_track\n        self.edit_rate = next(self.otio_track.each_clip()).duration().rate\n        self.timeline_mobslot, self.sequence = self._create_timeline_mobslot()\n\n    def media_kind(self):\n        \"\"\"Return the string for what kind of track this is.\"\"\"\n        pass\n\n    def _create_timeline_mobslot(self):\n        \"\"\"\n        Return a timeline_mobslot and sequence for this track.\n\n        In AAF, a TimelineMobSlot is a container for the Sequence. A Sequence is\n        analogous to an otio track.\n\n        Returns:\n            Returns a tuple of (TimelineMobSlot, Sequence)\n        \"\"\"\n        pass"
    },
    {
        "pr_title": "New feature implementation of writing AAF files from otio files in the AAF adapter",
        "pr_number": 432,
        "file_name": "opentimelineio_contrib/adapters/aaf_adapter/aaf_writer.py",
        "code_diff": "@@ -204,7 +214,7 @@\ndef _create_timeline_mobslot(self):\n         \"\"\"\n         pass\n \n-    @abstractmethod\n+    @abc.abstractmethod\n     def default_descriptor(self, otio_clip):\n         pass",
        "comments": [
            {
                "comment": "I think it would probably help clarify that this is because Audio and Video tracks are handled differently.  (thats why this is an ABC and not a concrete class).",
                "position": null
            },
            {
                "comment": "I think this could be preceded by a `_` to signal that it isn't an entry point for external usage.",
                "position": null
            },
            {
                "comment": "Would be nice to promote these UUIDs into a more meaningful enum at the top of the file.  So AAF_UUID_TRANSITION or something.  This could happen in a future PR if you think thats a big change.",
                "position": null
            },
            {
                "comment": "Same here - breaking this out into an enum would be nice.",
                "position": null
            },
            {
                "comment": "another enum",
                "position": null
            },
            {
                "comment": "another UUID enum",
                "position": null
            },
            {
                "comment": "UUID enum",
                "position": null
            },
            {
                "comment": "Could use a small comment, like \"Video track kind specialization of TrackTranscriber\" or something.",
                "position": null
            },
            {
                "comment": "AUID enum too",
                "position": null
            },
            {
                "comment": " I like leaving the module namespace in the name to the object so its clear what is local and what comes from a depedency.  So `import abc` rather than the from import.  Same for `AUID` and `AAFRational`, unless there is some other reason for doing the `from` import.  Helps signal to the reader its external code.",
                "position": null
            },
            {
                "comment": "Inverting this would allow you to dedent the block and make the `else` condition more clear:\r\n\r\n```python\r\nif tape_mob:\r\n  return tape_mob\r\n...\r\n```",
                "position": null
            },
            {
                "comment": "Inverting this could also make it more readable (see next comment).",
                "position": null
            }
        ],
        "commit_message": "Addressing code review. Put UUID constants into enums. Renamed to\n_TrackTranscriber. Additional comments.\n\nCo-authored-by: Shahbaz Khan <shahbazkhan8194@gmail.com>",
        "commit_id": "32f34899444a77afd162d721aaad9e62f9e55a6b",
        "fn_context": "    def _create_timeline_mobslot(self):\n        \"\"\"\n        Return a timeline_mobslot and sequence for this track.\n\n        In AAF, a TimelineMobSlot is a container for the Sequence. A Sequence is\n        analogous to an otio track.\n\n        Returns:\n            Returns a tuple of (TimelineMobSlot, Sequence)\n        \"\"\"\n        pass\n\n    def default_descriptor(self, otio_clip):\n        pass\n\n    def aaf_filler(self, otio_gap):\n        \"\"\"Convert an otio Gap into an aaf Filler\"\"\"\n        # length = otio_gap.duration().value  # XXX Not working for some reason\n        length = otio_gap.metadata[\"AAF\"][\"Length\"]\n        filler = self.aaf_file.create.Filler(self.media_kind, length)\n        return filler"
    },
    {
        "pr_title": "New feature implementation of writing AAF files from otio files in the AAF adapter",
        "pr_number": 432,
        "file_name": "opentimelineio_contrib/adapters/aaf_adapter/aaf_writer.py",
        "code_diff": "@@ -240,43 +250,41 @@\ndef aaf_transition(self, otio_transition):\n                 \"Unsupported transition type: {}\".format(\n                     otio_transition.transition_type))\n             return None\n-        avid_param_byteorder_id = uuid.UUID(\"c0038672-a8cf-11d3-a05b-006094eb75cb\")\n         byteorder_typedef = self.aaf_file.dictionary.lookup_typedef(\"aafUInt16\")\n-        param_byteorder = self.aaf_file.create.ParameterDef(avid_param_byteorder_id,\n-                                                            \"AvidParameterByteOrder\",\n-                                                            \"\",\n-                                                            byteorder_typedef)\n+        param_byteorder = self.aaf_file.create.ParameterDef(\n+            AAF_PARAMETERDEF_AVIDPARAMETERBYTEORDER,\n+            \"AvidParameterByteOrder\",\n+            \"\",\n+            byteorder_typedef)\n         self.aaf_file.dictionary.register_def(param_byteorder)\n \n         # Create ParameterDef for AvidEffectID\n-        avid_effect_id = uuid.UUID(\"93994bd6-a81d-11d3-a05b-006094eb75cb\")\n         avid_effect_typdef = self.aaf_file.dictionary.lookup_typedef(\"AvidBagOfBits\")\n-        param_effect_id = self.aaf_file.create.ParameterDef(avid_effect_id,\n-                                                            \"AvidEffectID\",\n-                                                            \"\",\n-                                                            avid_effect_typdef)\n+        param_effect_id = self.aaf_file.create.ParameterDef(\n+            AAF_PARAMETERDEF_AVIDEFFECTID,\n+            \"AvidEffectID\",\n+            \"\",\n+            avid_effect_typdef)\n         self.aaf_file.dictionary.register_def(param_effect_id)\n \n         # Create ParameterDef for AFX_FG_KEY_OPACITY_U\n-        opacity_param_id = uuid.UUID(\"8d56813d-847e-11d5-935a-50f857c10000\")\n         opacity_param_def = self.aaf_file.dictionary.lookup_typedef(\"Rational\")\n-        opacity_param = self.aaf_file.create.ParameterDef(opacity_param_id,\n-                                                          \"AFX_FG_KEY_OPACITY_U\",\n-                                                          \"\",\n-                                                          opacity_param_def)\n+        opacity_param = self.aaf_file.create.ParameterDef(\n+            AAF_PARAMETERDEF_AFX_FG_KEY_OPACITY_U,\n+            \"AFX_FG_KEY_OPACITY_U\",\n+            \"\",\n+            opacity_param_def)\n         self.aaf_file.dictionary.register_def(opacity_param)\n \n         # Create VaryingValue\n         opacity_u = self.aaf_file.create.VaryingValue()\n         opacity_u.parameterdef = self.aaf_file.dictionary.lookup_parameterdef(\n             \"AFX_FG_KEY_OPACITY_U\")\n-        vval_extrapolation_id = uuid.UUID(\"0e24dd54-66cd-4f1a-b0a0-670ac3a7a0b3\")\n-        opacity_u[\"VVal_Extrapolation\"].value = vval_extrapolation_id\n+        opacity_u[\"VVal_Extrapolation\"].value = AAF_VVAL_EXTRAPOLATION_ID\n         opacity_u[\"VVal_FieldCount\"].value = 1\n \n-        interpolation_id = uuid.UUID(\"5b6c85a4-0ede-11d3-80a9-006008143e6f\")\n         interpolation_def = self.aaf_file.create.InterpolationDef(\n-            interpolation_id, \"LinearInterp\", \"Linear keyframe interpolation\")\n+            aaf2.misc.LinearInterp, \"LinearInterp\", \"Linear keyframe interpolation\")\n         self.aaf_file.dictionary.register_def(interpolation_def)\n         opacity_u[\"Interpolation\"].value = \\\n             self.aaf_file.dictionary.lookup_interperlationdef(\"LinearInterp\")",
        "comments": [
            {
                "comment": "I think it would probably help clarify that this is because Audio and Video tracks are handled differently.  (thats why this is an ABC and not a concrete class).",
                "position": null
            },
            {
                "comment": "I think this could be preceded by a `_` to signal that it isn't an entry point for external usage.",
                "position": null
            },
            {
                "comment": "Would be nice to promote these UUIDs into a more meaningful enum at the top of the file.  So AAF_UUID_TRANSITION or something.  This could happen in a future PR if you think thats a big change.",
                "position": null
            },
            {
                "comment": "Same here - breaking this out into an enum would be nice.",
                "position": null
            },
            {
                "comment": "another enum",
                "position": null
            },
            {
                "comment": "another UUID enum",
                "position": null
            },
            {
                "comment": "UUID enum",
                "position": null
            },
            {
                "comment": "Could use a small comment, like \"Video track kind specialization of TrackTranscriber\" or something.",
                "position": null
            },
            {
                "comment": "AUID enum too",
                "position": null
            },
            {
                "comment": " I like leaving the module namespace in the name to the object so its clear what is local and what comes from a depedency.  So `import abc` rather than the from import.  Same for `AUID` and `AAFRational`, unless there is some other reason for doing the `from` import.  Helps signal to the reader its external code.",
                "position": null
            },
            {
                "comment": "Inverting this would allow you to dedent the block and make the `else` condition more clear:\r\n\r\n```python\r\nif tape_mob:\r\n  return tape_mob\r\n...\r\n```",
                "position": null
            },
            {
                "comment": "Inverting this could also make it more readable (see next comment).",
                "position": null
            }
        ],
        "commit_message": "Addressing code review. Put UUID constants into enums. Renamed to\n_TrackTranscriber. Additional comments.\n\nCo-authored-by: Shahbaz Khan <shahbazkhan8194@gmail.com>",
        "commit_id": "32f34899444a77afd162d721aaad9e62f9e55a6b",
        "fn_context": "    def aaf_transition(self, otio_transition):\n        \"\"\"Convert an otio Transition into an aaf Transition.\"\"\"\n        # Create ParameterDef for AvidParameterByteOrder\n        if (otio_transition.transition_type !=\n                otio.schema.transition.TransitionTypes.SMPTE_Dissolve):\n            print(\n                \"Unsupported transition type: {}\".format(\n                    otio_transition.transition_type))\n            return None\n        byteorder_typedef = self.aaf_file.dictionary.lookup_typedef(\"aafUInt16\")\n        param_byteorder = self.aaf_file.create.ParameterDef(\n            AAF_PARAMETERDEF_AVIDPARAMETERBYTEORDER,\n            \"AvidParameterByteOrder\",\n            \"\",\n            byteorder_typedef)\n        self.aaf_file.dictionary.register_def(param_byteorder)\n\n        # Create ParameterDef for AvidEffectID\n        avid_effect_typdef = self.aaf_file.dictionary.lookup_typedef(\"AvidBagOfBits\")\n        param_effect_id = self.aaf_file.create.ParameterDef(\n            AAF_PARAMETERDEF_AVIDEFFECTID,\n            \"AvidEffectID\",\n            \"\",\n            avid_effect_typdef)\n        self.aaf_file.dictionary.register_def(param_effect_id)\n\n        # Create ParameterDef for AFX_FG_KEY_OPACITY_U\n        opacity_param_def = self.aaf_file.dictionary.lookup_typedef(\"Rational\")\n        opacity_param = self.aaf_file.create.ParameterDef(\n            AAF_PARAMETERDEF_AFX_FG_KEY_OPACITY_U,\n            \"AFX_FG_KEY_OPACITY_U\",\n            \"\",\n            opacity_param_def)\n        self.aaf_file.dictionary.register_def(opacity_param)\n\n        # Create VaryingValue\n        opacity_u = self.aaf_file.create.VaryingValue()\n        opacity_u.parameterdef = self.aaf_file.dictionary.lookup_parameterdef(\n            \"AFX_FG_KEY_OPACITY_U\")\n        opacity_u[\"VVal_Extrapolation\"].value = AAF_VVAL_EXTRAPOLATION_ID\n        opacity_u[\"VVal_FieldCount\"].value = 1\n\n        interpolation_def = self.aaf_file.create.InterpolationDef(\n            aaf2.misc.LinearInterp, \"LinearInterp\", \"Linear keyframe interpolation\")\n        self.aaf_file.dictionary.register_def(interpolation_def)\n        opacity_u[\"Interpolation\"].value = \\\n            self.aaf_file.dictionary.lookup_interperlationdef(\"LinearInterp\")\n\n        pointlist = otio_transition.metadata[\"AAF\"][\"PointList\"]\n\n        c1 = self.aaf_file.create.ControlPoint()\n        c1[\"EditHint\"].value = \"Proportional\"\n        c1.value = pointlist[0][\"Value\"]\n        c1.time = pointlist[0][\"Time\"]\n\n        c2 = self.aaf_file.create.ControlPoint()\n        c2[\"EditHint\"].value = \"Proportional\"\n        c2.value = pointlist[1][\"Value\"]\n        c2.time = pointlist[1][\"Time\"]\n\n        opacity_u[\"PointList\"].extend([c1, c2])\n\n        op_group_metadata = otio_transition.metadata[\"AAF\"][\"OperationGroup\"]\n        effect_id = op_group_metadata[\"Operation\"].get(\"Identification\")\n        is_time_warp = op_group_metadata[\"Operation\"].get(\"IsTimeWarp\")\n        by_pass = op_group_metadata[\"Operation\"].get(\"Bypass\")\n        number_inputs = op_group_metadata[\"Operation\"].get(\"NumberInputs\")\n        operation_category = op_group_metadata[\"Operation\"].get(\"OperationCategory\")\n        data_def_name = op_group_metadata[\"Operation\"][\"DataDefinition\"][\"Name\"]\n        data_def = self.aaf_file.dictionary.lookup_datadef(str(data_def_name))\n        description = op_group_metadata[\"Operation\"][\"Description\"]\n        op_def_name = otio_transition.metadata[\"AAF\"][\n            \"OperationGroup\"\n        ][\"Operation\"][\"Name\"]\n\n        op_def = self.aaf_file.create.OperationDef(uuid.UUID(effect_id), op_def_name)\n        self.aaf_file.dictionary.register_def(op_def)\n        op_def.media_kind = self.media_kind\n        datadef = self.aaf_file.dictionary.lookup_datadef(\"Picture\")\n        op_def[\"IsTimeWarp\"].value = is_time_warp\n        op_def[\"Bypass\"].value = by_pass\n        op_def[\"NumberInputs\"].value = number_inputs\n        op_def[\"OperationCategory\"].value = str(operation_category)\n        op_def[\"ParametersDefined\"].extend([param_byteorder,\n                                            param_effect_id])\n        op_def[\"DataDefinition\"].value = data_def\n        op_def[\"Description\"].value = str(description)\n\n        # Create OperationGroup\n        length = otio_transition.metadata[\"AAF\"][\"Length\"]\n        operation_group = self.aaf_file.create.OperationGroup(op_def, length)\n        operation_group[\"DataDefinition\"].value = datadef\n        operation_group[\"Parameters\"].append(opacity_u)\n\n        # Create Transition\n        transition = self.aaf_file.create.Transition(self.media_kind, length)\n        transition[\"OperationGroup\"].value = operation_group\n        transition[\"CutPoint\"].value = otio_transition.metadata[\"AAF\"][\"CutPoint\"]\n        transition[\"DataDefinition\"].value = datadef\n        return transition"
    },
    {
        "pr_title": "New feature implementation of writing AAF files from otio files in the AAF adapter",
        "pr_number": 432,
        "file_name": "opentimelineio_contrib/adapters/aaf_adapter/aaf_writer.py",
        "code_diff": "@@ -395,7 +403,8 @@\ndef _create_mastermob(self, otio_clip, filemob, filemob_slot):\n         return mastermob, mastermob_slot\n \n \n-class VideoTrackTranscriber(TrackTranscriber):\n+class VideoTrackTranscriber(_TrackTranscriber):\n+    \"\"\"Video track kind specialization of TrackTranscriber.\"\"\"\n \n     @property\n     def media_kind(self):",
        "comments": [
            {
                "comment": "I think it would probably help clarify that this is because Audio and Video tracks are handled differently.  (thats why this is an ABC and not a concrete class).",
                "position": null
            },
            {
                "comment": "I think this could be preceded by a `_` to signal that it isn't an entry point for external usage.",
                "position": null
            },
            {
                "comment": "Would be nice to promote these UUIDs into a more meaningful enum at the top of the file.  So AAF_UUID_TRANSITION or something.  This could happen in a future PR if you think thats a big change.",
                "position": null
            },
            {
                "comment": "Same here - breaking this out into an enum would be nice.",
                "position": null
            },
            {
                "comment": "another enum",
                "position": null
            },
            {
                "comment": "another UUID enum",
                "position": null
            },
            {
                "comment": "UUID enum",
                "position": null
            },
            {
                "comment": "Could use a small comment, like \"Video track kind specialization of TrackTranscriber\" or something.",
                "position": null
            },
            {
                "comment": "AUID enum too",
                "position": null
            },
            {
                "comment": " I like leaving the module namespace in the name to the object so its clear what is local and what comes from a depedency.  So `import abc` rather than the from import.  Same for `AUID` and `AAFRational`, unless there is some other reason for doing the `from` import.  Helps signal to the reader its external code.",
                "position": null
            },
            {
                "comment": "Inverting this would allow you to dedent the block and make the `else` condition more clear:\r\n\r\n```python\r\nif tape_mob:\r\n  return tape_mob\r\n...\r\n```",
                "position": null
            },
            {
                "comment": "Inverting this could also make it more readable (see next comment).",
                "position": null
            }
        ],
        "commit_message": "Addressing code review. Put UUID constants into enums. Renamed to\n_TrackTranscriber. Additional comments.\n\nCo-authored-by: Shahbaz Khan <shahbazkhan8194@gmail.com>",
        "commit_id": "32f34899444a77afd162d721aaad9e62f9e55a6b",
        "fn_context": "    def _create_mastermob(self, otio_clip, filemob, filemob_slot):\n        \"\"\"\n        Return a mastermob for an otio Clip. Needs a filemob and filemob slot.\n\n        Returns:\n            Returns a tuple of (MasterMob, MasterMobSlot)\n        \"\"\"\n        mastermob = self.root_file_transcriber._unique_mastermob(otio_clip)\n        timecode_length = _timecode_length(otio_clip)\n        # Prevent duplicate slots by relying on the SlotID\n        slot_id = otio_clip.metadata[\"AAF\"][\"SourceMobSlotID\"]\n        try:\n            mastermob_slot = mastermob.slot_at(slot_id)\n        except IndexError:\n            mastermob_slot = mastermob.create_timeline_slot(\n                edit_rate=self.edit_rate, slot_id=slot_id)\n        mastermob_clip = mastermob.create_source_clip(\n            slot_id=mastermob_slot.slot_id,\n            length=timecode_length,\n            media_kind=self.media_kind)\n        mastermob_clip.mob = filemob\n        mastermob_clip.slot = filemob_slot\n        mastermob_clip.slot_id = filemob_slot.slot_id\n        mastermob_slot.segment = mastermob_clip\n        return mastermob, mastermob_slot\n\n    def media_kind(self):\n        return \"picture\""
    },
    {
        "pr_title": "New feature implementation of writing AAF files from otio files in the AAF adapter",
        "pr_number": 432,
        "file_name": "opentimelineio_contrib/adapters/aaf_adapter/aaf_writer.py",
        "code_diff": "@@ -429,17 +438,17 @@\ndef default_descriptor(self, otio_clip):\n         return descriptor\n \n \n-class AudioTrackTranscriber(TrackTranscriber):\n+class AudioTrackTranscriber(_TrackTranscriber):\n+    \"\"\"Audio track kind specialization of TrackTranscriber.\"\"\"\n \n     @property\n     def media_kind(self):\n         return \"sound\"\n \n     def aaf_sourceclip(self, otio_clip):\n         # Parameter Definition\n-        param_id = AUID(\"e4962322-2267-11d3-8a4c-0050040ef7d2\")\n         typedef = self.aaf_file.dictionary.lookup_typedef(\"Rational\")\n-        param_def = self.aaf_file.create.ParameterDef(param_id,\n+        param_def = self.aaf_file.create.ParameterDef(AAF_PARAMETERDEF_PAN,\n                                                       \"Pan\",\n                                                       \"Pan\",\n                                                       typedef)",
        "comments": [
            {
                "comment": "I think it would probably help clarify that this is because Audio and Video tracks are handled differently.  (thats why this is an ABC and not a concrete class).",
                "position": null
            },
            {
                "comment": "I think this could be preceded by a `_` to signal that it isn't an entry point for external usage.",
                "position": null
            },
            {
                "comment": "Would be nice to promote these UUIDs into a more meaningful enum at the top of the file.  So AAF_UUID_TRANSITION or something.  This could happen in a future PR if you think thats a big change.",
                "position": null
            },
            {
                "comment": "Same here - breaking this out into an enum would be nice.",
                "position": null
            },
            {
                "comment": "another enum",
                "position": null
            },
            {
                "comment": "another UUID enum",
                "position": null
            },
            {
                "comment": "UUID enum",
                "position": null
            },
            {
                "comment": "Could use a small comment, like \"Video track kind specialization of TrackTranscriber\" or something.",
                "position": null
            },
            {
                "comment": "AUID enum too",
                "position": null
            },
            {
                "comment": " I like leaving the module namespace in the name to the object so its clear what is local and what comes from a depedency.  So `import abc` rather than the from import.  Same for `AUID` and `AAFRational`, unless there is some other reason for doing the `from` import.  Helps signal to the reader its external code.",
                "position": null
            },
            {
                "comment": "Inverting this would allow you to dedent the block and make the `else` condition more clear:\r\n\r\n```python\r\nif tape_mob:\r\n  return tape_mob\r\n...\r\n```",
                "position": null
            },
            {
                "comment": "Inverting this could also make it more readable (see next comment).",
                "position": null
            }
        ],
        "commit_message": "Addressing code review. Put UUID constants into enums. Renamed to\n_TrackTranscriber. Additional comments.\n\nCo-authored-by: Shahbaz Khan <shahbazkhan8194@gmail.com>",
        "commit_id": "32f34899444a77afd162d721aaad9e62f9e55a6b",
        "fn_context": "    def default_descriptor(self, otio_clip):\n        # TODO: Determine if these values are the correct, and if so,\n        # maybe they should be in the AAF metadata\n        descriptor = self.aaf_file.create.CDCIDescriptor()\n        descriptor[\"ComponentWidth\"].value = 8\n        descriptor[\"HorizontalSubsampling\"].value = 2\n        descriptor[\"ImageAspectRatio\"].value = \"16/9\"\n        descriptor[\"StoredWidth\"].value = 1920\n        descriptor[\"StoredHeight\"].value = 1080\n        descriptor[\"FrameLayout\"].value = \"FullFrame\"\n        descriptor[\"VideoLineMap\"].value = [42, 0]\n        descriptor[\"SampleRate\"].value = 24\n        descriptor[\"Length\"].value = 1\n        return descriptor\n\n    def media_kind(self):\n        return \"sound\"\n\n    def aaf_sourceclip(self, otio_clip):\n        # Parameter Definition\n        typedef = self.aaf_file.dictionary.lookup_typedef(\"Rational\")\n        param_def = self.aaf_file.create.ParameterDef(AAF_PARAMETERDEF_PAN,\n                                                      \"Pan\",\n                                                      \"Pan\",\n                                                      typedef)\n        self.aaf_file.dictionary.register_def(param_def)\n        interp_def = self.aaf_file.create.InterpolationDef(aaf2.misc.LinearInterp,\n                                                           \"LinearInterp\",\n                                                           \"LinearInterp\")\n        self.aaf_file.dictionary.register_def(interp_def)\n        # PointList\n        # revisit duration()\n        length = otio_clip.duration().value\n        c1 = self.aaf_file.create.ControlPoint()\n        c1[\"ControlPointSource\"].value = 2\n        c1[\"Time\"].value = aaf2.rational.AAFRational(\"0/{}\".format(length))\n        c1[\"Value\"].value = 0\n        c2 = self.aaf_file.create.ControlPoint()\n        c2[\"ControlPointSource\"].value = 2\n        c2[\"Time\"].value = aaf2.rational.AAFRational(\"{}/{}\".format(length - 1, length))\n        c2[\"Value\"].value = 0\n        varying_value = self.aaf_file.create.VaryingValue()\n        varying_value.parameterdef = param_def\n        varying_value[\"Interpolation\"].value = interp_def\n        varying_value[\"PointList\"].extend([c1, c2])\n        opgroup = self.timeline_mobslot.segment\n        opgroup.parameters.append(varying_value)\n\n        return super(AudioTrackTranscriber, self).aaf_sourceclip(otio_clip)"
    },
    {
        "pr_title": "New feature implementation of writing AAF files from otio files in the AAF adapter",
        "pr_number": 432,
        "file_name": "opentimelineio_contrib/adapters/aaf_adapter/aaf_writer.py",
        "code_diff": "@@ -453,11 +462,11 @@\ndef aaf_sourceclip(self, otio_clip):\n         length = otio_clip.duration().value\n         c1 = self.aaf_file.create.ControlPoint()\n         c1[\"ControlPointSource\"].value = 2\n-        c1[\"Time\"].value = AAFRational(\"0/{}\".format(length))\n+        c1[\"Time\"].value = aaf2.rational.AAFRational(\"0/{}\".format(length))\n         c1[\"Value\"].value = 0\n         c2 = self.aaf_file.create.ControlPoint()\n         c2[\"ControlPointSource\"].value = 2\n-        c2[\"Time\"].value = AAFRational(\"{}/{}\".format(length - 1, length))\n+        c2[\"Time\"].value = aaf2.rational.AAFRational(\"{}/{}\".format(length - 1, length))\n         c2[\"Value\"].value = 0\n         varying_value = self.aaf_file.create.VaryingValue()\n         varying_value.parameterdef = param_def",
        "comments": [
            {
                "comment": "I think it would probably help clarify that this is because Audio and Video tracks are handled differently.  (thats why this is an ABC and not a concrete class).",
                "position": null
            },
            {
                "comment": "I think this could be preceded by a `_` to signal that it isn't an entry point for external usage.",
                "position": null
            },
            {
                "comment": "Would be nice to promote these UUIDs into a more meaningful enum at the top of the file.  So AAF_UUID_TRANSITION or something.  This could happen in a future PR if you think thats a big change.",
                "position": null
            },
            {
                "comment": "Same here - breaking this out into an enum would be nice.",
                "position": null
            },
            {
                "comment": "another enum",
                "position": null
            },
            {
                "comment": "another UUID enum",
                "position": null
            },
            {
                "comment": "UUID enum",
                "position": null
            },
            {
                "comment": "Could use a small comment, like \"Video track kind specialization of TrackTranscriber\" or something.",
                "position": null
            },
            {
                "comment": "AUID enum too",
                "position": null
            },
            {
                "comment": " I like leaving the module namespace in the name to the object so its clear what is local and what comes from a depedency.  So `import abc` rather than the from import.  Same for `AUID` and `AAFRational`, unless there is some other reason for doing the `from` import.  Helps signal to the reader its external code.",
                "position": null
            },
            {
                "comment": "Inverting this would allow you to dedent the block and make the `else` condition more clear:\r\n\r\n```python\r\nif tape_mob:\r\n  return tape_mob\r\n...\r\n```",
                "position": null
            },
            {
                "comment": "Inverting this could also make it more readable (see next comment).",
                "position": null
            }
        ],
        "commit_message": "Addressing code review. Put UUID constants into enums. Renamed to\n_TrackTranscriber. Additional comments.\n\nCo-authored-by: Shahbaz Khan <shahbazkhan8194@gmail.com>",
        "commit_id": "32f34899444a77afd162d721aaad9e62f9e55a6b",
        "fn_context": "    def aaf_sourceclip(self, otio_clip):\n        # Parameter Definition\n        typedef = self.aaf_file.dictionary.lookup_typedef(\"Rational\")\n        param_def = self.aaf_file.create.ParameterDef(AAF_PARAMETERDEF_PAN,\n                                                      \"Pan\",\n                                                      \"Pan\",\n                                                      typedef)\n        self.aaf_file.dictionary.register_def(param_def)\n        interp_def = self.aaf_file.create.InterpolationDef(aaf2.misc.LinearInterp,\n                                                           \"LinearInterp\",\n                                                           \"LinearInterp\")\n        self.aaf_file.dictionary.register_def(interp_def)\n        # PointList\n        # revisit duration()\n        length = otio_clip.duration().value\n        c1 = self.aaf_file.create.ControlPoint()\n        c1[\"ControlPointSource\"].value = 2\n        c1[\"Time\"].value = aaf2.rational.AAFRational(\"0/{}\".format(length))\n        c1[\"Value\"].value = 0\n        c2 = self.aaf_file.create.ControlPoint()\n        c2[\"ControlPointSource\"].value = 2\n        c2[\"Time\"].value = aaf2.rational.AAFRational(\"{}/{}\".format(length - 1, length))\n        c2[\"Value\"].value = 0\n        varying_value = self.aaf_file.create.VaryingValue()\n        varying_value.parameterdef = param_def\n        varying_value[\"Interpolation\"].value = interp_def\n        varying_value[\"PointList\"].extend([c1, c2])\n        opgroup = self.timeline_mobslot.segment\n        opgroup.parameters.append(varying_value)\n\n        return super(AudioTrackTranscriber, self).aaf_sourceclip(otio_clip)"
    },
    {
        "pr_title": "New feature implementation of writing AAF files from otio files in the AAF adapter",
        "pr_number": 432,
        "file_name": "opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -38,12 +38,13 @@\nif lib_path and lib_path not in sys.path:\n     sys.path.insert(0, lib_path)\n \n-import aaf2  # noqa: E731\n-import aaf2.content  # noqa: E731\n-import aaf2.mobs  # noqa: E731\n-import aaf2.components  # noqa: E731\n-import aaf2.core  # noqa: E731\n-from opentimelineio_contrib.adapters.aaf_adapter.aaf_writer import AAFFileTranscriber, validate_metadata  # noqa: E731\n+import aaf2  # noqa: E402\n+import aaf2.content  # noqa: E402\n+import aaf2.mobs  # noqa: E402\n+import aaf2.components  # noqa: E402\n+import aaf2.core  # noqa: E402\n+from opentimelineio_contrib.adapters.aaf_adapter import aaf_writer  # noqa: E402\n+\n \n debug = False\n __names = set()",
        "comments": [
            {
                "comment": "```suggestion\r\ndef write_to_file(input_otio, filepath):\r\n```",
                "position": null
            },
            {
                "comment": "Catch the case where this is not a Timeline (e.g. SerializableCollection)",
                "position": null
            },
            {
                "comment": "Add a case for Stack.\r\n\r\nThrow an exception here for unsupported.\r\n\r\nAnd make py3 print()",
                "position": null
            },
            {
                "comment": "Look into using Enum. (otio.schema). ",
                "position": null
            },
            {
                "comment": "totallllllll nit pick here, but if you break the line before the string:\r\n```\r\nraise otio.exceptions.NotSupportedError(\r\n   \"Currently only supporting top level Timeline.\"\r\n)\r\n```\r\nThen you wouldn't have to split in the middle of the string, which helps with readability.",
                "position": null
            },
            {
                "comment": "To make future diffing nicer, we tend to break these lines like this:\r\n```suggestion\r\nfrom opentimelineio_contrib.adapters.aaf_adapter.aaf_writer import (\r\n  AAFFileTranscriber, \r\n  validate_metadata,\r\n  )  # noqa: E731\r\n```\r\n\r\nAlso... E731, thats assigning a lambda instead of a def?  I'm surprised you're seeing that error on this line.\r\nhttps://lintlyci.github.io/Flake8Rules/rules/E731.html\r\n\r\nIt might be even cleaner to `from opentimelineio_contrib.adaptesr.aaf_adapter import aaf_writer` instead of just fetching those two functions - we like preserving namespaces so that its clear where functions come from and that they aren't local in the body of the file.  Then when using AAFFileTranscriber it would be: `aaf_writer.AAFFileTranscriber` etc.",
                "position": null
            }
        ],
        "commit_message": "Addressing code review. Put UUID constants into enums. Renamed to\n_TrackTranscriber. Additional comments.\n\nCo-authored-by: Shahbaz Khan <shahbazkhan8194@gmail.com>",
        "commit_id": "32f34899444a77afd162d721aaad9e62f9e55a6b",
        "fn_context": "def _get_parameter(item, parameter_name):\n    values = dict((value.name, value) for value in item.parameters.value)\n    return values.get(parameter_name)"
    },
    {
        "pr_title": "Remove the copy() method and make __copy__ an error.",
        "pr_number": 400,
        "file_name": "opentimelineio_contrib/adapters/tests/test_hls_playlist_adapter.py",
        "code_diff": "@@ -145,7 +145,7 @@\ndef test_media_pl_from_mem(self):\n \n         # Make a copy of the media ref specifying the byte range for the\n         # segment\n-        media_ref1 = segmented_media_ref.copy()\n+        media_ref1 = segmented_media_ref.deepcopy()\n         media_ref1.available_range = otio.opentime.TimeRange(\n             otio.opentime.RationalTime(0, 1),\n             otio.opentime.RationalTime(2.002, 1)",
        "comments": [],
        "commit_message": "Remove the copy() method and make __copy__ an error.",
        "commit_id": "05c554089854426cf908b7498fd935e2950563a9"
    },
    {
        "pr_title": "Remove the copy() method and make __copy__ an error.",
        "pr_number": 400,
        "file_name": "tests/test_composition.py",
        "code_diff": "@@ -655,7 +655,7 @@\ndef test_range(self):\n         self.assertEqual(sq.range_of_child_at_index(0), tr)\n \n         sq = otio.schema.Track(\n-            children=[it, it.copy(), it.copy(), it.copy()],\n+            children=[it, it.deepcopy(), it.deepcopy(), it.deepcopy()],\n         )\n         self.assertEqual(\n             sq.range_of_child_at_index(index=1),",
        "comments": [],
        "commit_message": "Remove the copy() method and make __copy__ an error.",
        "commit_id": "05c554089854426cf908b7498fd935e2950563a9"
    },
    {
        "pr_title": "Remove the copy() method and make __copy__ an error.",
        "pr_number": 400,
        "file_name": "tests/test_item.py",
        "code_diff": "@@ -313,16 +313,6 @@\ndef test_copy(self):\n             )\n         )\n \n-        it_copy = it.copy()\n-        self.assertIsOTIOEquivalentTo(it, it_copy)\n-        it.metadata[\"foo\"] = \"bar2\"\n-        # shallow copy, should change both dictionaries\n-        self.assertEqual(it_copy.metadata[\"foo\"], \"bar2\")\n-\n-        # name should be different\n-        it.name = \"foo\"\n-        self.assertNotEqual(it_copy.name, it.name)\n-\n         # deep copy should have different dictionaries\n         it_dcopy = it.deepcopy()\n         it_dcopy.metadata[\"foo\"] = \"not bar\"",
        "comments": [],
        "commit_message": "Remove the copy() method and make __copy__ an error.",
        "commit_id": "05c554089854426cf908b7498fd935e2950563a9"
    },
    {
        "pr_title": "Remove the copy() method and make __copy__ an error.",
        "pr_number": 400,
        "file_name": "tests/test_serializable_object.py",
        "code_diff": "@@ -75,16 +75,9 @@\ndef test_copy_lib(self):\n \n         import copy\n \n-        # shallow copy\n-        so_cp = copy.copy(so)\n-        so_cp._data[\"meta_data\"][\"foo\"] = \"not bar\"\n-        self.assertEqual(so._data, so_cp._data)\n-\n-        so.foo = \"bar\"\n-        so_cp = copy.copy(so)\n-        # copy only copies members of the _data dictionary, *not* other attrs.\n-        with self.assertRaises(AttributeError):\n-            so_cp.foo\n+        # shallow copy is an error\n+        with self.assertRaises(NotImplementedError):\n+            so_cp = copy.copy(so)\n \n         # deep copy\n         so_cp = copy.deepcopy(so)",
        "comments": [],
        "commit_message": "Remove the copy() method and make __copy__ an error.",
        "commit_id": "05c554089854426cf908b7498fd935e2950563a9"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -140,7 +140,10 @@\ndef add_clip(self, line, comments, rate=24):\n                 freeze = comment_handler.handled.get('freeze_frame')\n                 if motion is not None or freeze is not None:\n                     # Adjust the clip to match the record duration\n-                    clip.source_range.duration = rec_duration\n+                    clip.source_range = otio.opentime.TimeRange(\n+                        start_time=clip.source_range.start_time,\n+                        duration=rec_duration\n+                    )\n \n                     if freeze is not None:\n                         clip.effects.append(otio.schema.FreezeFrame())",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -206,10 +209,16 @@\ndef add_clip(self, line, comments, rate=24):\n                     duration=record_in - track_end\n                 )\n                 track.append(gap)\n-                track.source_range.duration += gap.duration()\n+                track.source_range = otio.opentime.TimeRange(\n+                    start_time=track.source_range.start_time,\n+                    duration=track.source_range.duration + gap.duration()\n+                )\n \n             track.append(clip)\n-            track.source_range.duration += clip.duration()\n+            track.source_range = otio.opentime.TimeRange(\n+                start_time=track.source_range.start_time,\n+                duration=track.source_range.duration + clip.duration()\n+            )\n \n     def guess_kind_for_track_name(self, name):\n         if name.startswith(\"V\"):",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -638,7 +647,11 @@\ndef _expand_transitions(timeline):\n                 if prev:\n                     remove_list.append((track, prev))\n \n-            expansion_clip.source_range.duration += mid_tran_cut_pre_duration\n+            sr = expansion_clip.source_range\n+            expansion_clip.source_range = otio.opentime.TimeRange(\n+                start_time=sr.start_time,\n+                duration=sr.duration + mid_tran_cut_pre_duration\n+            )\n \n             # rebuild the clip as a transition\n             new_trx = otio.schema.Transition(",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -655,8 +668,10 @@\ndef _expand_transitions(timeline):\n \n             # expand the next_clip\n             if next_clip:\n-                next_clip.source_range.start_time -= mid_tran_cut_post_duration\n-                next_clip.source_range.duration += mid_tran_cut_post_duration\n+                next_clip.source_range = otio.opentime.TimeRange(\n+                    next_clip.source_range.start_time - mid_tran_cut_post_duration,\n+                    next_clip.source_range.duration + mid_tran_cut_post_duration\n+                )\n             else:\n                 fill = otio.schema.Gap(\n                     source_range=otio.opentime.TimeRange(",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio/algorithms/stack_algo.py",
        "code_diff": "@@ -28,6 +28,7 @@\nfrom .. import (\n     schema,\n+    opentime,\n )\n from . import (\n     track_algo",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio/algorithms/track_algo.py",
        "code_diff": "@@ -29,6 +29,7 @@\nfrom .. import (\n     schema,\n     exceptions,\n+    opentime,\n )",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio/algorithms/track_algo.py",
        "code_diff": "@@ -64,15 +65,22 @@\ndef track_trimmed_to_range(in_track, trim_range):\n             # should we trim the start?\n             if trim_range.start_time > child_range.start_time:\n                 trim_amount = trim_range.start_time - child_range.start_time\n-                child_source_range.start_time += trim_amount\n-                child_source_range.duration -= trim_amount\n+                child_source_range = opentime.TimeRange(\n+                    start_time=child_source_range.start_time + trim_amount,\n+                    duration=child_source_range.duration - trim_amount\n+\n+                )\n \n             # should we trim the end?\n             trim_end = trim_range.end_time_exclusive()\n             child_end = child_range.end_time_exclusive()\n             if trim_end < child_end:\n                 trim_amount = child_end - trim_end\n-                child_source_range.duration -= trim_amount\n+                child_source_range = opentime.TimeRange(\n+                    start_time=child_source_range.start_time,\n+                    duration=child_source_range.duration - trim_amount\n+\n+                )\n \n             # set the new child's trims\n             child.source_range = child_source_range",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio/algorithms/track_algo.py",
        "code_diff": "@@ -155,11 +163,6 @@\ndef _expand_transition(target_transition, from_track):\n                 target_transition\n             )\n         )\n-    pre.name = (pre.name or \"\") + \"_transition_pre\"\n-\n-    # ensure that pre.source_range is set, because it will get manipulated\n-    pre.source_range = copy.copy(pre.trimmed_range())\n-\n     if target_transition.in_offset is None:\n         raise RuntimeError(\n             \"in_offset is None on: {}\".format(target_transition)",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio/algorithms/track_algo.py",
        "code_diff": "@@ -170,11 +173,18 @@\ndef _expand_transition(target_transition, from_track):\n             \"out_offset is None on: {}\".format(target_transition)\n         )\n \n-    pre.source_range.start_time = (\n-        pre.source_range.end_time_exclusive() - target_transition.in_offset\n-    )\n-    pre.source_range.duration = trx_duration.rescaled_to(\n-        pre.source_range.start_time\n+    pre.name = (pre.name or \"\") + \"_transition_pre\"\n+\n+    # ensure that pre.source_range is set, because it will get manipulated\n+    tr = pre.trimmed_range()\n+\n+    pre.source_range = opentime.TimeRange(\n+        start_time=(\n+            tr.end_time_exclusive() - target_transition.in_offset\n+        ),\n+        duration=trx_duration.rescaled_to(\n+            tr.start_time\n+        )\n     )\n \n     post = copy.deepcopy(result.next)",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio/algorithms/track_algo.py",
        "code_diff": "@@ -190,13 +200,13 @@\ndef _expand_transition(target_transition, from_track):\n     post.name = (post.name or \"\") + \"_transition_post\"\n \n     # ensure that post.source_range is set, because it will get manipulated\n-    post.source_range = copy.copy(post.trimmed_range())\n+    tr = post.trimmed_range()\n \n-    post.source_range.start_time = (\n-        post.source_range.start_time - target_transition.in_offset\n-    ).rescaled_to(post.source_range.start_time)\n-    post.source_range.duration = trx_duration.rescaled_to(\n-        post.source_range.start_time\n+    post.source_range = opentime.TimeRange(\n+        start_time=(\n+            tr.start_time - target_transition.in_offset\n+        ).rescaled_to(tr.start_time),\n+        duration=trx_duration.rescaled_to(tr.start_time)\n     )\n \n     return pre, target_transition, post",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio/opentime.py",
        "code_diff": "@@ -52,18 +52,26 @@\nVALID_TIMECODE_RATES = (\n     VALID_NON_DROPFRAME_TIMECODE_RATES + VALID_DROPFRAME_TIMECODE_RATES)\n \n+_fn_cache = object.__setattr__\n+\n \n class RationalTime(object):\n     \"\"\" Represents an instantaneous point in time, value * (1/rate) seconds\n     from time 0seconds.\n     \"\"\"\n \n-    def __init__(self, value=0, rate=1):\n-        self.value = value\n-        self.rate = rate\n+    # Locks RationalTime instances to only these attributes\n+    __slots__ = ['value', 'rate']\n+\n+    def __init__(self, value=0.0, rate=1.0):\n+        _fn_cache(self, \"value\", value)\n+        _fn_cache(self, \"rate\", rate)\n+\n+    def __setattr__(self, key, val):\n+        \"\"\"Enforces immutability \"\"\"\n+        raise AttributeError(\"RationalTime is Immutable.\")\n \n     def __copy__(self, memodict=None):\n-        # We just construct this directly, which is way faster for some reason\n         return RationalTime(self.value, self.rate)\n \n     # Always deepcopy, since we want this class to behave like a value type",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio/opentime.py",
        "code_diff": "@@ -72,8 +80,13 @@\ndef __copy__(self, memodict=None):\n     def rescaled_to(self, new_rate):\n         \"\"\"Returns the time for this time converted to new_rate\"\"\"\n \n-        if isinstance(new_rate, RationalTime):\n+        try:\n             new_rate = new_rate.rate\n+        except AttributeError:\n+            pass\n+\n+        if self.rate == new_rate:\n+            return copy.copy(self)\n \n         return RationalTime(\n             self.value_rescaled_to(new_rate),",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio/opentime.py",
        "code_diff": "@@ -83,17 +96,18 @@\ndef rescaled_to(self, new_rate):\n     def value_rescaled_to(self, new_rate):\n         \"\"\"Returns the time value for self converted to new_rate\"\"\"\n \n+        try:\n+            new_rate = new_rate.rate\n+        except AttributeError:\n+            pass\n+\n         if new_rate == self.rate:\n             return self.value\n \n-        if isinstance(new_rate, RationalTime):\n-            new_rate = new_rate.rate\n-\n         # TODO: This math probably needs some overrun protection\n-        # TODO: Don't we want to enforce integers here?\n         try:\n-            return (float(self.value) * float(new_rate)) / float(self.rate)\n-        except (TypeError, ValueError):\n+            return float(self.value) * float(new_rate) / float(self.rate)\n+        except (AttributeError, TypeError, ValueError):\n             raise TypeError(\n                 \"Sorry, RationalTime cannot be rescaled to a value of type \"\n                 \"'{}', only RationalTime and numbers are supported.\".format(",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio/opentime.py",
        "code_diff": "@@ -109,65 +123,61 @@\ndef almost_equal(self, other, delta=0.0):\n         except AttributeError:\n             return False\n \n-    def __iadd__(self, other):\n-        \"\"\" += operator for self with another RationalTime.\n+    def __add__(self, other):\n+        \"\"\"Returns a RationalTime object that is the sum of self and other.\n \n         If self and other have differing time rates, the result will have the\n         have the rate of the faster time.\n         \"\"\"\n \n-        if not isinstance(other, RationalTime):\n-            raise TypeError(\n-                \"RationalTime may only be added to other objects of type \"\n-                \"RationalTime, not {}.\".format(type(other))\n-            )\n-\n-        if self.rate == other.rate:\n-            self.value += other.value\n-            return self\n+        try:\n+            if self.rate == other.rate:\n+                return RationalTime(self.value + other.value, self.rate)\n+        except AttributeError:\n+            if not isinstance(other, RationalTime):\n+                raise TypeError(\n+                    \"RationalTime may only be added to other objects of type \"\n+                    \"RationalTime, not {}.\".format(type(other))\n+                )\n+            raise\n \n         if self.rate > other.rate:\n             scale = self.rate\n-            value = (self.value + other.value_rescaled_to(scale))\n+            value = self.value + other.value_rescaled_to(scale)\n         else:\n             scale = other.rate\n-            value = (self.value_rescaled_to(scale) + other.value)\n+            value = self.value_rescaled_to(scale) + other.value\n \n-        self.value = value\n-        self.rate = scale\n+        return RationalTime(value, scale)\n \n-        # @TODO: make this construct and return a new object\n-        return self\n+    # because RationalTime is immutable, += is sugar around +\n+    __iadd__ = __add__\n \n-    def __add__(self, other):\n-        \"\"\"Returns a RationalTime object that is the sum of self and other.\n+    def __sub__(self, other):\n+        \"\"\"Returns a RationalTime object that is self - other.\n \n         If self and other have differing time rates, the result will have the\n         have the rate of the faster time.\n         \"\"\"\n \n-        if not isinstance(other, RationalTime):\n-            raise TypeError(\n-                \"RationalTime may only be added to other objects of type \"\n-                \"RationalTime, not {}.\".format(type(other))\n-            )\n-        if self.rate == other.rate:\n-            return RationalTime(self.value + other.value, self.rate)\n-        elif self.rate > other.rate:\n-            scale = self.rate\n-            value = (self.value + other.value_rescaled_to(scale))\n-        else:\n-            scale = other.rate\n-            value = (self.value_rescaled_to(scale) + other.value)\n-        return RationalTime(value=value, rate=scale)\n+        try:\n+            if self.rate == other.rate:\n+                return RationalTime(self.value - other.value, self.rate)\n+        except AttributeError:\n+            if not isinstance(other, RationalTime):\n+                raise TypeError(\n+                    \"RationalTime may only be added to other objects of type \"\n+                    \"RationalTime, not {}.\".format(type(other))\n+                )\n+            raise\n \n-    def __sub__(self, other):\n         if self.rate > other.rate:\n             scale = self.rate\n-            value = (self.value - other.value_rescaled_to(scale))\n+            value = self.value - other.value_rescaled_to(scale)\n         else:\n             scale = other.rate\n-            value = (self.value_rescaled_to(scale) - other.value)\n+            value = self.value_rescaled_to(scale) - other.value\n+\n         return RationalTime(value=value, rate=scale)\n \n     def _comparable_floats(self, other):",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio/opentime.py",
        "code_diff": "@@ -176,15 +186,18 @@\ndef _comparable_floats(self, other):\n \n         If other is not of a type that can be compared, TypeError is raised\n         \"\"\"\n-        if not isinstance(other, RationalTime):\n-            raise TypeError(\n-                \"RationalTime can only be compared to other objects of type \"\n-                \"RationalTime, not {}\".format(type(other))\n+        try:\n+            return (\n+                float(self.value) / self.rate,\n+                float(other.value) / other.rate\n             )\n-        return (\n-            float(self.value) / self.rate,\n-            float(other.value) / other.rate\n-        )\n+        except AttributeError:\n+            if not isinstance(other, RationalTime):\n+                raise TypeError(\n+                    \"RationalTime can only be compared to other objects of type \"\n+                    \"RationalTime, not {}\".format(type(other))\n+                )\n+            raise\n \n     def __gt__(self, other):\n         f_self, f_other = self._comparable_floats(other)",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio/opentime.py",
        "code_diff": "@@ -235,8 +248,8 @@\nclass TimeTransform(object):\n \n     def __init__(self, offset=RationalTime(), scale=1.0, rate=None):\n         self.offset = copy.copy(offset)\n-        self.scale = scale\n-        self.rate = rate\n+        self.scale = float(scale)\n+        self.rate = float(rate) if rate else None\n \n     def applied_to(self, other):\n         if isinstance(other, TimeRange):",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio/opentime.py",
        "code_diff": "@@ -253,9 +266,8 @@\ndef applied_to(self, other):\n                 rate=target_rate\n             )\n         elif isinstance(other, RationalTime):\n-            result = RationalTime(0, other.rate)\n-            result.value = other.value * self.scale\n-            result = result + self.offset\n+            value = other.value * self.scale\n+            result = RationalTime(value, other.rate) + self.offset\n             if target_rate is not None:\n                 result = result.rescaled_to(target_rate)",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio/opentime.py",
        "code_diff": "@@ -315,9 +327,42 @@\nclass TimeRange(object):\n     start_time of the TimeRange.\n     \"\"\"\n \n-    def __init__(self, start_time=RationalTime(), duration=RationalTime()):\n-        self.start_time = copy.copy(start_time)\n-        self.duration = copy.copy(duration)\n+    __slots__ = ['start_time', 'duration']\n+\n+    def __init__(self, start_time=None, duration=None):\n+        if not isinstance(start_time, RationalTime) and start_time is not None:\n+            raise TypeError(\n+                \"start_time must be a RationalTime, not \"\n+                \"'{}'\".format(start_time)\n+            )\n+        if (\n+                duration is not None and (\n+                    not isinstance(duration, RationalTime)\n+                    or duration.value < 0.0\n+                )\n+        ):\n+            raise TypeError(\n+                \"duration must be a RationalTime with value >= 0, not \"\n+                \"'{}'\".format(duration)\n+            )\n+\n+        # if the start time has not been passed in\n+        if not start_time:\n+            if duration:\n+                # ...get the rate from the duration\n+                start_time = RationalTime(rate=duration.rate)\n+            else:\n+                # otherwise use the default\n+                start_time = RationalTime()\n+        _fn_cache(self, \"start_time\", copy.copy(start_time))\n+\n+        if not duration:\n+            # ...get the rate from the start_time\n+            duration = RationalTime(rate=start_time.rate)\n+        _fn_cache(self, \"duration\", copy.copy(duration))\n+\n+    def __setattr__(self, key, val):\n+        raise AttributeError(\"TimeRange is Immutable.\")\n \n     def __copy__(self, memodict=None):\n         # Construct a new one directly to avoid the overhead of deepcopy",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio/opentime.py",
        "code_diff": "@@ -329,19 +374,6 @@\ndef __copy__(self, memodict=None):\n     # Always deepcopy, since we want this class to behave like a value type\n     __deepcopy__ = __copy__\n \n-    @property\n-    def duration(self):\n-        return self._duration\n-\n-    @duration.setter\n-    def duration(self, val):\n-        if not isinstance(val, RationalTime) or val.value < 0.0:\n-            raise TypeError(\n-                \"duration must be a RationalTime with value >= 0, not \"\n-                \"'{}'\".format(val)\n-            )\n-        self._duration = val\n-\n     def end_time_inclusive(self):\n         \"\"\"The time of the last sample that contains data in the TimeRange.",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio/opentime.py",
        "code_diff": "@@ -359,13 +391,15 @@\ndef end_time_inclusive(self):\n         ).value > 1:\n \n             result = (\n-                self.end_time_exclusive() - RationalTime(1, self.duration.rate)\n+                self.end_time_exclusive() - RationalTime(1, self.start_time.rate)\n             )\n \n             # if the duration's value has a fractional component\n             if self.duration.value != math.floor(self.duration.value):\n-                result = self.end_time_exclusive()\n-                result.value = math.floor(result.value)\n+                result = RationalTime(\n+                    math.floor(self.end_time_exclusive().value),\n+                    result.rate\n+                )\n \n             return result\n         else:",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -30,36 +30,68 @@\nimport os\n import sys\n+from collections import Iterable\n import opentimelineio as otio\n \n lib_path = os.environ.get(\"OTIO_AAF_PYTHON_LIB\")\n if lib_path and lib_path not in sys.path:\n-    sys.path += [lib_path]\n+    sys.path.insert(0, lib_path)\n \n-import aaf  # noqa (E402 module level import not at top of file)\n-import aaf.storage  # noqa\n-import aaf.mob  # noqa\n-import aaf.define  # noqa\n-import aaf.component  # noqa\n-import aaf.base  # noqa\n+import aaf2  # noqa: E731\n+import aaf2.content  # noqa: E731\n+import aaf2.mobs  # noqa: E731\n+import aaf2.components  # noqa: E731\n+import aaf2.core  # noqa: E731\n \n debug = False\n __names = set()\n \n \n+def _get_parameter(item, parameter_name):\n+    values = dict((value.name, value) for value in item.parameters.value)\n+    return values.get(parameter_name)\n+\n+\n+def _walk_item(thing):\n+    slot = thing.slot\n+    if not slot:\n+        return\n+    segment = slot.segment\n+    if isinstance(segment, aaf2.components.SourceClip):\n+        yield segment\n+        for item in _walk_item(segment):\n+            yield item\n+    # elif isinstance(segment, aaf2.components.Sequence):\n+    #     clip = segment.component_at_time(thing.start_time)\n+    #     if isinstance(clip, SourceClip):\n+    #         yield clip\n+    #         for item in clip._walk_item():\n+    #             yield item\n+    #     else:\n+    #         raise NotImplementedError(\n+    #            \"Sequence returned {} not implemented\".format(type(segment))\n+    elif isinstance(segment, aaf2.components.EssenceGroup):\n+        yield segment\n+    elif isinstance(segment, aaf2.components.Filler):\n+        yield segment\n+    else:\n+        raise NotImplementedError(\n+            \"walking {} not implemented\".format(type(segment))\n+        )\n+\n+\n def _get_name(item):\n-    if hasattr(item, 'name'):\n-        name = item.name\n-        if name:\n-            return name\n-    if isinstance(item, aaf.component.SourceClip):\n+    if isinstance(item, aaf2.components.SourceClip):\n         try:\n-            ref = item.resolve_ref()\n+            return item.mob.name or \"Untitled SourceClip\"\n         except RuntimeError:\n             # Some AAFs produce this error:\n             # RuntimeError: failed with [-2146303738]: mob not found\n             return \"SourceClip Missing Mob?\"\n-        return ref.name or \"Untitled SourceClip\"\n+    if hasattr(item, 'name'):\n+        name = item.name\n+        if name:\n+            return name\n     return _get_class_name(item)",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -76,7 +108,7 @@\ndef _transcribe_property(prop):\n     if type(prop) in (str, type(u\"\"), int, float, bool):\n         return prop\n \n-    elif isinstance(prop, aaf.iterator.PropValueResolveIter):\n+    elif isinstance(prop, list):\n         result = {}\n         for child in prop:\n             if hasattr(child, \"name\") and hasattr(child, \"value\"):",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -94,20 +126,15 @@\ndef _transcribe_property(prop):\n         for child in prop.properties():\n             result[child.name] = _transcribe_property(child.value)\n         return result\n-    elif isinstance(prop, aaf.iterator.PropItemIter):\n-        result = {}\n-        for child in prop:\n-            result[child.name] = _transcribe_property(child.value)\n-        return result\n     else:\n         return str(prop)\n \n \n def _find_timecode_mobs(item):\n-    mobs = [item.resolve_ref()]\n+    mobs = [item.mob]\n \n-    for c in item.walk():\n-        if isinstance(c, aaf.component.EssenceGroup):\n+    for c in _walk_item(item):\n+        if isinstance(c, aaf2.components.EssenceGroup):\n             # An EssenceGroup is a Segment that has one or more\n             # alternate choices, each of which represent different variations\n             # of one actual piece of content.",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -124,7 +151,7 @@\ndef _find_timecode_mobs(item):\n             # TODO: Try CountChoices() and ChoiceAt(i)\n             # For now, lets just skip it.\n             continue\n-        mob = c.resolve_ref()\n+        mob = c.mob\n         if mob:\n             mobs.append(mob)",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -137,7 +164,7 @@\ndef _extract_start_timecode(mob):\n     \"\"\"\n \n     tc_list = []\n-    for s in mob.slots():\n+    for s in mob.slots:\n         if s.segment.media_kind != 'Timecode':\n             continue",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -176,10 +203,10 @@\ndef _transcribe(item, parent=None, editRate=24, masterMobs=None):\n     metadata[\"Name\"] = _get_name(item)\n     metadata[\"ClassName\"] = _get_class_name(item)\n \n-    if isinstance(item, aaf.component.Component):\n+    if isinstance(item, aaf2.components.Component):\n         metadata[\"Length\"] = item.length\n \n-    if isinstance(item, aaf.base.AAFObject):\n+    if isinstance(item, aaf2.core.AAFObject):\n         for prop in item.properties():\n             if hasattr(prop, 'name') and hasattr(prop, 'value'):\n                 key = str(prop.name)",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -191,31 +218,31 @@\ndef _transcribe(item, parent=None, editRate=24, masterMobs=None):\n     # is important, because the class hierarchy of AAF objects is more\n     # complex than OTIO.\n \n-    if isinstance(item, aaf.storage.ContentStorage):\n+    if isinstance(item, aaf2.content.ContentStorage):\n         result = otio.schema.SerializableCollection()\n \n         # Gather all the Master Mobs, so we can find them later by MobID\n         # when we parse the SourceClips in the composition\n         if masterMobs is None:\n             masterMobs = {}\n-        for mob in item.master_mobs():\n+        for mob in item.mastermobs():\n             child = _transcribe(mob, parent=item)\n             if child is not None:\n                 mobID = child.metadata.get(\"AAF\", {}).get(\"MobID\")\n                 masterMobs[mobID] = child\n \n-        for mob in item.composition_mobs():\n+        for mob in item.compositionmobs():\n             child = _transcribe(mob, parent=item, masterMobs=masterMobs)\n             _add_child(result, child, mob)\n \n-    elif isinstance(item, aaf.mob.Mob):\n+    elif isinstance(item, aaf2.mobs.Mob):\n         result = otio.schema.Timeline()\n \n-        for slot in item.slots():\n+        for slot in item.slots:\n             child = _transcribe(slot, parent=item, masterMobs=masterMobs)\n             _add_child(result.tracks, child, slot)\n \n-    elif isinstance(item, aaf.component.SourceClip):\n+    elif isinstance(item, aaf2.components.SourceClip):\n         result = otio.schema.Clip()\n \n         # Evidently the last mob is the one with timecode",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -245,7 +272,7 @@\ndef _transcribe(item, parent=None, editRate=24, masterMobs=None):\n                 media.metadata[\"AAF\"] = masterMob.metadata.get(\"AAF\", {})\n                 result.media_reference = media\n \n-    elif isinstance(item, aaf.component.Transition):\n+    elif isinstance(item, aaf2.components.Transition):\n         result = otio.schema.Transition()\n \n         # Does AAF support anything else?",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -256,7 +283,7 @@\ndef _transcribe(item, parent=None, editRate=24, masterMobs=None):\n         result.in_offset = otio.opentime.RationalTime(in_offset, editRate)\n         result.out_offset = otio.opentime.RationalTime(out_offset, editRate)\n \n-    elif isinstance(item, aaf.component.Filler):\n+    elif isinstance(item, aaf2.components.Filler):\n         result = otio.schema.Gap()\n \n         length = item.length",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -265,48 +292,48 @@\ndef _transcribe(item, parent=None, editRate=24, masterMobs=None):\n             otio.opentime.RationalTime(length, editRate)\n         )\n \n-    elif isinstance(item, aaf.component.NestedScope):\n+    elif isinstance(item, aaf2.components.NestedScope):\n         # TODO: Is this the right class?\n         result = otio.schema.Stack()\n \n-        for segment in item.segments():\n-            child = _transcribe(segment, parent=item, masterMobs=masterMobs)\n-            _add_child(result, child, segment)\n+        for slot in item.slots:\n+            child = _transcribe(slot, parent=item, masterMobs=masterMobs)\n+            _add_child(result, child, slot)\n \n-    elif isinstance(item, aaf.component.Sequence):\n+    elif isinstance(item, aaf2.components.Sequence):\n         result = otio.schema.Track()\n \n-        for component in item.components():\n+        for component in item.components:\n             child = _transcribe(component, parent=item, masterMobs=masterMobs)\n             _add_child(result, child, component)\n \n-    elif isinstance(item, aaf.component.OperationGroup):\n+    elif isinstance(item, aaf2.components.OperationGroup):\n         result = _transcribe_operation_group(\n             item, metadata, editRate, masterMobs\n         )\n \n-    elif isinstance(item, aaf.mob.TimelineMobSlot):\n+    elif isinstance(item, aaf2.mobslots.TimelineMobSlot):\n         result = otio.schema.Track()\n \n         child = _transcribe(item.segment, parent=item, masterMobs=masterMobs)\n         _add_child(result, child, item.segment)\n \n-    elif isinstance(item, aaf.mob.MobSlot):\n+    elif isinstance(item, aaf2.mobslots.MobSlot):\n         result = otio.schema.Track()\n \n         child = _transcribe(item.segment, parent=item, masterMobs=masterMobs)\n         _add_child(result, child, item.segment)\n \n-    elif isinstance(item, aaf.component.Timecode):\n+    elif isinstance(item, aaf2.components.Timecode):\n         pass\n \n-    elif isinstance(item, aaf.component.Pulldown):\n+    elif isinstance(item, aaf2.components.Pulldown):\n         pass\n \n-    elif isinstance(item, aaf.component.EdgeCode):\n+    elif isinstance(item, aaf2.components.EdgeCode):\n         pass\n \n-    elif isinstance(item, aaf.component.ScopeReference):\n+    elif isinstance(item, aaf2.components.ScopeReference):\n         # TODO: is this like FILLER?\n \n         result = otio.schema.Gap()",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -317,32 +344,24 @@\ndef _transcribe(item, parent=None, editRate=24, masterMobs=None):\n             otio.opentime.RationalTime(length, editRate)\n         )\n \n-    elif isinstance(item, aaf.component.DescriptiveMarker):\n+    elif isinstance(item, aaf2.components.DescriptiveMarker):\n \n         # Markers come in on their own separate Track.\n         # TODO: We should consolidate them onto the same track(s) as the clips\n         # result = otio.schema.Marker()\n         pass\n \n-    elif isinstance(item, aaf.iterator.MobIter):\n-\n-        result = otio.schema.SerializableCollection()\n-        for child in item:\n-            result.append(\n-                _transcribe(\n-                    child,\n-                    parent=item,\n-                    masterMobs=masterMobs\n-                )\n-            )\n-    elif isinstance(item, aaf.component.Selector):\n+    elif isinstance(item, aaf2.components.Selector):\n         # If you mute a clip in media composer, it becomes one of these in the\n         # AAF.\n-        result = _transcribe(item.selected, parent=item, masterMobs=masterMobs)\n+        result = _transcribe(\n+            item.getvalue(\"Selected\"),\n+            parent=item, masterMobs=masterMobs\n+        )\n \n         alternates = [\n             _transcribe(alt, parent=item, masterMobs=masterMobs)\n-            for alt in item.alternate_segments()\n+            for alt in item.getvalue(\"Alternates\")\n         ]\n \n         # muted case -- if there is only one item its muted, otherwise its",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -398,6 +417,16 @@\ndef _transcribe(item, parent=None, editRate=24, masterMobs=None):\n     #     elif isinstance(item, pyaaf.AxProperty):\n     #         self.properties['Value'] = str(item.GetValue())\n \n+    elif isinstance(item, Iterable):\n+        result = otio.schema.SerializableCollection()\n+        for child in item:\n+            result.append(\n+                _transcribe(\n+                    child,\n+                    parent=item,\n+                    masterMobs=masterMobs\n+                )\n+            )\n     else:\n         # For everything else, we just ignore it.\n         # To see what is being ignored, turn on the debug flag",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -461,13 +490,13 @@\ndef _transcribe_linear_timewarp(item, parameters):\n     # this is a linear time warp\n     effect = otio.schema.LinearTimeWarp()\n \n-    offset_map = item.parameter.get('PARAM_SPEED_OFFSET_MAP_U')\n+    offset_map = _get_parameter(item, 'PARAM_SPEED_OFFSET_MAP_U')\n \n     # If we have a LinearInterp with just 2 control points, then\n     # we can compute the time_scalar. Note that the SpeedRatio is\n     # NOT correct in many AAFs - we aren't sure why, but luckily we\n     # can compute the correct value this way.\n-    points = list(offset_map.points())\n+    points = offset_map.get(\"PointList\")\n     if len(points) > 2:\n         # This is something complicated... try the fancy version\n         return _transcribe_fancy_timewarp(item, parameters)",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -566,11 +595,11 @@\ndef _transcribe_operation_group(item, metadata, editRate, masterMobs):\n     if operation.get(\"IsTimeWarp\"):\n         if operation.get(\"Name\") == \"Motion Control\":\n \n-            offset_map = item.parameter.get('PARAM_SPEED_OFFSET_MAP_U')\n+            offset_map = _get_parameter(item, 'PARAM_SPEED_OFFSET_MAP_U')\n             # TODO: We should also check the PARAM_OFFSET_MAP_U which has\n             # an interpolation_def().name as well.\n             if offset_map is not None:\n-                interpolation = offset_map.interpolation_def().name\n+                interpolation = offset_map.interpolation.name\n             else:\n                 interpolation = None",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -599,7 +628,7 @@\ndef _transcribe_operation_group(item, metadata, editRate, masterMobs):\n             }\n         }\n \n-    for segment in item.input_segments():\n+    for segment in item.getvalue(\"InputSegments\"):\n         child = _transcribe(segment, parent=item, masterMobs=masterMobs)\n         if child:\n             _add_child(result, child, segment)",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -631,8 +660,11 @@\ndef _fix_transitions(thing):\n \n                     if child.source_range is None:\n                         child.source_range = child.trimmed_range()\n-                    child.source_range.start_time += pre_trans.in_offset\n-                    child.source_range.duration -= pre_trans.in_offset\n+                    csr = child.source_range\n+                    child.source_range = otio.opentime.TimeRange(\n+                        start_time=csr.start_time + pre_trans.in_offset,\n+                        duration=csr.duration - pre_trans.in_offset\n+                    )\n \n                 # Is the item after us a Transition?\n                 if c < len(thing) - 1 and isinstance(",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -643,7 +675,11 @@\ndef _fix_transitions(thing):\n \n                     if child.source_range is None:\n                         child.source_range = child.trimmed_range()\n-                    child.source_range.duration -= post_trans.out_offset\n+                    csr = child.source_range\n+                    child.source_range = otio.opentime.TimeRange(\n+                        start_time=csr.start_time,\n+                        duration=csr.duration - post_trans.out_offset\n+                    )\n \n         for child in thing:\n             _fix_transitions(child)",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -725,7 +761,10 @@\ndef _simplify(thing):\n                 if not result.source_range:\n                     result.source_range = result.trimmed_range()\n                 # modify the duration, but leave the start_time as is\n-                result.source_range.duration = thing.source_range.duration\n+                result.source_range = otio.opentime.TimeRange(\n+                    result.source_range.start_time,\n+                    thing.source_range.duration\n+                )\n             return result\n \n     return thing",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -774,9 +813,9 @@\ndef _contains_something_valuable(thing):\n \n def read_from_file(filepath, simplify=True):\n \n-    f = aaf.open(filepath)\n+    f = aaf2.open(filepath)\n \n-    storage = f.storage\n+    storage = f.content\n \n     # Note: We're skipping: f.header\n     # Is there something valuable in there?",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio_contrib/adapters/extern_rv.py",
        "code_diff": "@@ -86,6 +86,8 @@\ndef write_otio(otio_obj, to_session, track_kind=None):\n \n def _write_dissolve(pre_item, in_dissolve, post_item, to_session, track_kind=None):\n     rv_trx = to_session.newNode(\"CrossDissolve\", str(in_dissolve.name))\n+\n+    rate = pre_item.trimmed_range().duration.rate\n     rv_trx.setProperty(\n         \"CrossDissolve\",\n         \"\",",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio_contrib/adapters/extern_rv.py",
        "code_diff": "@@ -101,9 +103,10 @@\ndef _write_dissolve(pre_item, in_dissolve, post_item, to_session, track_kind=Non\n         \"numFrames\",\n         rvSession.gto.FLOAT,\n         int(\n-            (in_dissolve.in_offset + in_dissolve.out_offset).rescaled_to(\n-                pre_item.trimmed_range().duration.rate\n-            ).value\n+            (\n+                in_dissolve.in_offset\n+                + in_dissolve.out_offset\n+            ).rescaled_to(rate).value\n         )\n     )",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio_contrib/adapters/extern_rv.py",
        "code_diff": "@@ -113,7 +116,7 @@\ndef _write_dissolve(pre_item, in_dissolve, post_item, to_session, track_kind=Non\n         \"output\",\n         \"fps\",\n         rvSession.gto.FLOAT,\n-        pre_item.trimmed_range().duration.rate\n+        rate\n     )\n \n     pre_item_rv = write_otio(pre_item, to_session, track_kind)",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -85,8 +85,8 @@\ntry:\n     lib_path = os.environ.get(\"OTIO_AAF_PYTHON_LIB\")\n     if lib_path and lib_path not in sys.path:\n-        sys.path += [lib_path]\n-    import aaf # noqa\n+        sys.path.insert(0, lib_path)\n+    import aaf2 # noqa\n     could_import_aaf = True\n except (ImportError):\n     could_import_aaf = False",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "tests/test_opentime.py",
        "code_diff": "@@ -28,6 +28,7 @@\nimport unittest\n import copy\n+import itertools\n \n \n class TestTime(unittest.TestCase):",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "tests/test_opentime.py",
        "code_diff": "@@ -140,13 +141,13 @@\ndef test_long_running_timecode_24(self):\n         step_time = otio.opentime.RationalTime(value=1, rate=24)\n \n         # important to copy -- otherwise assigns the same thing to two names\n-        cumulative_time = copy.copy(step_time)\n+        # cumulative_time = copy.copy(step_time)\n \n-        # small optimization - remove the \".\" operator.\n-        iadd_func = cumulative_time.__iadd__\n+        cumulative_time = sum(\n+            (t for t in itertools.repeat(step_time, final_frame_number)),\n+            otio.opentime.RationalTime(0, 24)\n+        )\n \n-        for _ in range(1, final_frame_number):\n-            iadd_func(step_time)\n         self.assertEqual(cumulative_time, final_time)\n \n         # Adding by a non-multiple of 24",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "tests/test_opentime.py",
        "code_diff": "@@ -389,15 +390,12 @@\ndef test_long_running_time_string_24(self):\n \n         step_time = otio.opentime.RationalTime(value=1, rate=24)\n \n-        # important to copy -- otherwise assigns the same thing to two names\n-        cumulative_time = copy.copy(step_time)\n-\n-        # small optimization - remove the \".\" operator.\n-        iadd_func = cumulative_time.__iadd__\n+        cumulative_time = sum(\n+            (t for t in itertools.repeat(step_time, final_frame_number)),\n+            otio.opentime.RationalTime(0, 24)\n+        )\n \n-        for _ in range(1, final_frame_number):\n-            iadd_func(step_time)\n-        self.assertTrue(cumulative_time.almost_equal(final_time, delta=0.001))\n+        self.assertEqual(cumulative_time.value, final_frame_number)\n \n         # Adding by a non-multiple of 24\n         for fnum in range(1113, final_frame_number, 1113):",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "tests/test_opentime.py",
        "code_diff": "@@ -436,11 +434,11 @@\ndef test_time_string_23976_fps(self):\n             # self.assertEqual(t, t1)\n \n     def test_time_to_string(self):\n-        t = otio.opentime.RationalTime(1, 2)\n-        self.assertEqual(str(t), \"RationalTime(1, 2)\")\n+        t = otio.opentime.RationalTime(1.0, 2.0)\n+        self.assertEqual(str(t), \"RationalTime(1.0, 2.0)\")\n         self.assertEqual(\n             repr(t),\n-            \"otio.opentime.RationalTime(value=1, rate=2)\"\n+            \"otio.opentime.RationalTime(value=1.0, rate=2.0)\"\n         )\n \n     def test_frames_with_int_fps(self):",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "tests/test_opentime.py",
        "code_diff": "@@ -525,6 +523,9 @@\ndef test_math_with_different_scales(self):\n         b = otio.opentime.from_frames(75, 12)\n         self.assertEqual(b - a, gap.rescaled_to(24))\n         self.assertEqual(a + gap, b.rescaled_to(48))\n+        gap2 = copy.copy(gap)\n+        gap2 += a\n+        self.assertEqual(gap2, a + gap)\n         self.assertEqual(b - gap, a.rescaled_to(48))\n \n     def test_hash(self):",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "tests/test_opentime.py",
        "code_diff": "@@ -558,6 +559,12 @@\ndef test_incomparable_floats(self):\n         with self.assertRaises(TypeError):\n             t1 < -1\n \n+    def test_immutable(self):\n+        t1 = otio.opentime.RationalTime(12, 10)\n+\n+        with self.assertRaises(AttributeError):\n+            t1.value = 12\n+\n \n class TestTimeTransform(unittest.TestCase):",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "tests/test_opentime.py",
        "code_diff": "@@ -603,23 +610,23 @@\ndef test_rate(self):\n         self.assertEqual(txform2.rate, txform1.applied_to(txform2).rate)\n \n     def test_string(self):\n-        tstart = otio.opentime.RationalTime(12, 25)\n-        txform = otio.opentime.TimeTransform(offset=tstart, scale=2)\n+        tstart = otio.opentime.RationalTime(12.0, 25.0)\n+        txform = otio.opentime.TimeTransform(offset=tstart, scale=2.0)\n         self.assertEqual(\n             repr(txform),\n             \"otio.opentime.TimeTransform(\"\n             \"offset=otio.opentime.RationalTime(\"\n-            \"value=12, \"\n-            \"rate=25\"\n+            \"value=12.0, \"\n+            \"rate=25.0\"\n             \"), \"\n-            \"scale=2, \"\n+            \"scale=2.0, \"\n             \"rate=None\"\n             \")\"\n         )\n \n         self.assertEqual(\n             str(txform),\n-            \"TimeTransform(RationalTime(12, 25), 2, None)\"\n+            \"TimeTransform(RationalTime(12.0, 25.0), 2.0, None)\"\n         )\n \n     def test_hash(self):",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "tests/test_opentime.py",
        "code_diff": "@@ -659,13 +666,12 @@\ndef test_create(self):\n         self.assertEqual(tr.duration, blank)\n \n     def test_duration_validation(self):\n-        tr = otio.opentime.TimeRange()\n         with self.assertRaises(TypeError):\n-            setattr(tr, \"duration\", \"foo\")\n+            otio.opentime.TimeRange(duration=\"foo\")\n \n         bad_t = otio.opentime.RationalTime(-1, 1)\n         with self.assertRaises(TypeError):\n-            setattr(tr, \"duration\", bad_t)\n+            otio.opentime.TimeRange(duration=bad_t)\n \n     def test_extended_by(self):\n         # base 25 is just for testing",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "tests/test_opentime.py",
        "code_diff": "@@ -701,14 +707,14 @@\ndef test_end_time(self):\n \n     def test_repr(self):\n         tr = otio.opentime.TimeRange(\n-            otio.opentime.RationalTime(-1, 24),\n-            otio.opentime.RationalTime(6, 24)\n+            otio.opentime.RationalTime(-1.0, 24.0),\n+            otio.opentime.RationalTime(6.0, 24.0)\n         )\n         self.assertEqual(\n             repr(tr),\n             \"otio.opentime.TimeRange(\"\n-            \"start_time=otio.opentime.RationalTime(value=-1, rate=24), \"\n-            \"duration=otio.opentime.RationalTime(value=6, rate=24))\"\n+            \"start_time=otio.opentime.RationalTime(value=-1.0, rate=24.0), \"\n+            \"duration=otio.opentime.RationalTime(value=6.0, rate=24.0))\"\n         )\n \n     def test_compare(self):",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "tests/test_stack_algo.py",
        "code_diff": "@@ -1,3 +1,4 @@\n+#!/usr/bin/env python\n #\n # Copyright 2017 Pixar Animation Studios\n #",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Moving already parented item is an error",
        "pr_number": 399,
        "file_name": "tests/test_track_algo.py",
        "code_diff": "@@ -1,3 +1,4 @@\n+#!/usr/bin/env python\n #\n # Copyright 2017 Pixar Animation Studios\n #",
        "comments": [],
        "commit_message": "Merge branch 'master' into moving_already_parented_item_is_an_error",
        "commit_id": "487e3192f98314f087062dca19af24bccb6eedf8"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "opentimelineio/console/otiocat.py",
        "code_diff": "@@ -26,6 +26,8 @@\n\"\"\"Print the contents of an OTIO file to stdout.\"\"\"\n \n import argparse\n+import ast\n+import sys\n \n import opentimelineio as otio",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "opentimelineio/console/otiocat.py",
        "code_diff": "@@ -43,6 +45,16 @@\ndef _parsed_args():\n         nargs='+',\n         help='files to print the contents of'\n     )\n+    parser.add_argument(\n+        '-a',\n+        '--adapter-arg',\n+        type=str,\n+        default=[],\n+        action='append',\n+        help='Extra arguments to be passed to input adapter in the form of '\n+        'key=value. Values are strings, numbers or Python literals: True, '\n+        'False, etc. Can be used multiple times: -a burrito=\"bar\" -a taco=12.'\n+    )\n     parser.add_argument(\n         '-m',\n         '--media-linker',",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "opentimelineio/console/otiocat.py",
        "code_diff": "@@ -59,14 +71,18 @@\ndef _parsed_args():\n     return parser.parse_args()\n \n \n-def _otio_compatible_file_to_json_string(fpath, ml):\n+def _otio_compatible_file_to_json_string(fpath, ml, adapter_argument_map):\n     \"\"\"Read the file at fpath with the default otio adapter and return the json\n     as a string.\n     \"\"\"\n \n     adapter = otio.adapters.from_name(\"otio_json\")\n     return adapter.write_to_string(\n-        otio.adapters.read_from_file(fpath, media_linker_name=ml)\n+        otio.adapters.read_from_file(\n+            fpath,\n+            media_linker_name=ml,\n+            **adapter_argument_map\n+        )\n     )",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "opentimelineio/console/otioconvert.py",
        "code_diff": "@@ -25,6 +25,7 @@\nimport argparse\n import sys\n+import ast\n \n import opentimelineio as otio",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "opentimelineio/console/otioconvert.py",
        "code_diff": "@@ -89,6 +90,16 @@\ndef _parsed_args():\n             \" of the media linker to use.\"\n         )\n     )\n+    parser.add_argument(\n+        '-a',\n+        '--adapter-arg',\n+        type=str,\n+        default=[],\n+        action='append',\n+        help='Extra arguments to be passed to input adapter in the form of '\n+        'key=value. Values are strings, numbers or Python literals: True, '\n+        'False, etc. Can be used multiple times: -a burrito=\"bar\" -a taco=12.'\n+    )\n \n     return parser.parse_args()",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "opentimelineio/core/json_serializer.py",
        "code_diff": "@@ -57,14 +57,14 @@\ndef default(self, obj):\n         return json.JSONEncoder.default(self, obj)\n \n \n-def serialize_json_to_string(root, sort_keys=True, indent=4):\n+def serialize_json_to_string(root, indent=4):\n     \"\"\"Serialize a tree of SerializableObject to JSON.\n \n     Returns a JSON string.\n     \"\"\"\n \n     return _SerializableObjectEncoder(\n-        sort_keys=sort_keys,\n+        sort_keys=True,\n         indent=indent\n     ).encode(root)",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "opentimelineio/core/serializable_object.py",
        "code_diff": "@@ -60,8 +60,8 @@\nclass SerializableObject(object):\n     >>>         old_child_data_name = otio.core.deprecated_field()\n \n     >>>    @otio.core.upgrade_function_for(ExampleChild, 3)\n-    ...    def upgrade_child_to_three(data):\n-    ...        return {\"child_data\" : data[\"old_child_data_name\"]}\n+    ...    def upgrade_child_to_three(_data):\n+    ...        return {\"child_data\" : _data[\"old_child_data_name\"]}\n     \"\"\"\n \n     # Every child must define a _serializable_label attribute.",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "opentimelineio/core/serializable_object.py",
        "code_diff": "@@ -73,7 +73,7 @@\nclass SerializableObject(object):\n     _class_path = \"core.SerializableObject\"\n \n     def __init__(self):\n-        self.data = {}\n+        self._data = {}\n \n     # @{ \"Reference Type\" semantics for SerializableObject\n     # We think of the SerializableObject as a reference type - by default",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "opentimelineio/core/serializable_object.py",
        "code_diff": "@@ -89,7 +89,7 @@\ndef is_equivalent_to(self, other):\n         \"\"\"Returns true if the contents of self and other match.\"\"\"\n \n         try:\n-            if self.data == other.data:\n+            if self._data == other._data:\n                 return True\n \n             # XXX: Gross hack takes OTIO->JSON String->Python Dictionaries",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "opentimelineio/core/serializable_object.py",
        "code_diff": "@@ -116,17 +116,17 @@\ndef is_equivalent_to(self, other):\n             return False\n     # @}\n \n-    def update(self, d):\n+    def _update(self, d):\n         \"\"\"Like the dictionary .update() method.\n \n-        Update the data dictionary of this SerializableObject with the .data\n+        Update the _data dictionary of this SerializableObject with the ._data\n         of d if d is a SerializableObject or if d is a dictionary, d itself.\n         \"\"\"\n \n         if isinstance(d, SerializableObject):\n-            self.data.update(d.data)\n+            self._data.update(d._data)\n         else:\n-            self.data.update(d)\n+            self._data.update(d)\n \n     @classmethod\n     def schema_name(cls):",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "opentimelineio/core/serializable_object.py",
        "code_diff": "@@ -148,7 +148,7 @@\ndef is_unknown_schema(self):\n \n     def __copy__(self):\n         result = self.__class__()\n-        result.data = copy.copy(self.data)\n+        result._data = copy.copy(self._data)\n \n         return result",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "opentimelineio/core/serializable_object.py",
        "code_diff": "@@ -157,7 +157,7 @@\ndef copy(self):\n \n     def __deepcopy__(self, md):\n         result = type(self)()\n-        result.data = copy.deepcopy(self.data, md)\n+        result._data = copy.deepcopy(self._data, md)\n \n         return result",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "opentimelineio/core/serializable_object.py",
        "code_diff": "@@ -192,7 +192,7 @@\nclass foo(SerializableObject):\n     \"\"\"\n \n     def getter(self):\n-        return self.data[name]\n+        return self._data[name]\n \n     def setter(self, val):\n         # always allow None values regardless of value of required_type",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "opentimelineio_contrib/adapters/extern_rv.py",
        "code_diff": "@@ -66,7 +66,7 @@\nclass NoMappingForOtioTypeError(otio.exceptions.OTIOError):\n # @}\n \n \n-def write_otio(otio_obj, to_session):\n+def write_otio(otio_obj, to_session, track_kind=None):\n     WRITE_TYPE_MAP = {\n         otio.schema.Timeline: _write_timeline,\n         otio.schema.Stack: _write_stack,",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "opentimelineio_contrib/adapters/extern_rv.py",
        "code_diff": "@@ -77,14 +77,14 @@\ndef write_otio(otio_obj, to_session):\n     }\n \n     if type(otio_obj) in WRITE_TYPE_MAP:\n-        return WRITE_TYPE_MAP[type(otio_obj)](otio_obj, to_session)\n+        return WRITE_TYPE_MAP[type(otio_obj)](otio_obj, to_session, track_kind)\n \n     raise NoMappingForOtioTypeError(\n         str(type(otio_obj)) + \" on object: {}\".format(otio_obj)\n     )\n \n \n-def _write_dissolve(pre_item, in_dissolve, post_item, to_session):\n+def _write_dissolve(pre_item, in_dissolve, post_item, to_session, track_kind=None):\n     rv_trx = to_session.newNode(\"CrossDissolve\", str(in_dissolve.name))\n     rv_trx.setProperty(\n         \"CrossDissolve\",",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "opentimelineio_contrib/adapters/extern_rv.py",
        "code_diff": "@@ -116,10 +116,10 @@\ndef _write_dissolve(pre_item, in_dissolve, post_item, to_session):\n         pre_item.trimmed_range().duration.rate\n     )\n \n-    pre_item_rv = write_otio(pre_item, to_session)\n+    pre_item_rv = write_otio(pre_item, to_session, track_kind)\n     rv_trx.addInput(pre_item_rv)\n \n-    post_item_rv = write_otio(post_item, to_session)\n+    post_item_rv = write_otio(post_item, to_session, track_kind)\n \n     node_to_insert = post_item_rv",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "opentimelineio_contrib/adapters/extern_rv.py",
        "code_diff": "@@ -141,7 +141,7 @@\ndef _write_dissolve(pre_item, in_dissolve, post_item, to_session):\n             pre_item.media_reference.available_range.start_time.rate\n         )\n \n-        post_item_rv = write_otio(post_item, to_session)\n+        post_item_rv = write_otio(post_item, to_session, track_kind)\n \n         rt_node.addInput(post_item_rv)\n         node_to_insert = rt_node",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "opentimelineio_contrib/adapters/extern_rv.py",
        "code_diff": "@@ -151,7 +151,13 @@\ndef _write_dissolve(pre_item, in_dissolve, post_item, to_session):\n     return rv_trx\n \n \n-def _write_transition(pre_item, in_trx, post_item, to_session):\n+def _write_transition(\n+        pre_item,\n+        in_trx,\n+        post_item,\n+        to_session,\n+        track_kind=None\n+):\n     trx_map = {\n         otio.schema.TransitionTypes.SMPTE_Dissolve: _write_dissolve,\n     }",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "opentimelineio_contrib/adapters/extern_rv.py",
        "code_diff": "@@ -163,62 +169,79 @@\ndef _write_transition(pre_item, in_trx, post_item, to_session):\n         pre_item,\n         in_trx,\n         post_item,\n-        to_session\n+        to_session,\n+        track_kind\n     )\n \n \n-def _write_stack(in_stack, to_session):\n+def _write_stack(in_stack, to_session, track_kind=None):\n     new_stack = to_session.newNode(\"Stack\", str(in_stack.name) or \"tracks\")\n \n     for seq in in_stack:\n-        result = write_otio(seq, to_session)\n+        result = write_otio(seq, to_session, track_kind)\n         if result:\n             new_stack.addInput(result)\n \n     return new_stack\n \n \n-def _write_track(in_seq, to_session):\n+def _write_track(in_seq, to_session, _=None):\n     new_seq = to_session.newNode(\"Sequence\", str(in_seq.name) or \"track\")\n \n     items_to_serialize = otio.algorithms.track_with_expanded_transitions(\n         in_seq\n     )\n \n+    track_kind = in_seq.kind\n+\n     for thing in items_to_serialize:\n         if isinstance(thing, tuple):\n             result = _write_transition(*thing, to_session=to_session)\n         elif thing.duration().value == 0:\n             continue\n         else:\n-            result = write_otio(thing, to_session)\n+            result = write_otio(thing, to_session, track_kind)\n \n         if result:\n             new_seq.addInput(result)\n \n     return new_seq\n \n \n-def _write_timeline(tl, to_session):\n+def _write_timeline(tl, to_session, _=None):\n     result = write_otio(tl.tracks, to_session)\n     return result\n \n \n-def _create_media_reference(mr, to_session):\n-    if hasattr(mr, \"media_reference\") and mr.media_reference:\n-        if isinstance(mr.media_reference, otio.schema.ExternalReference):\n-            to_session.setMedia([str(mr.media_reference.target_url)])\n+def _create_media_reference(item, to_session, track_kind=None):\n+    if hasattr(item, \"media_reference\") and item.media_reference:\n+        if isinstance(item.media_reference, otio.schema.ExternalReference):\n+            media = [str(item.media_reference.target_url)]\n+\n+            if track_kind == otio.schema.TrackKind.Audio:\n+                # Create blank video media to accompany audio for valid source\n+                blank = \"{},start={},end={},fps={}.movieproc\".format(\n+                    \"blank\",\n+                    item.available_range().start_time.value,\n+                    item.available_range().end_time_inclusive().value,\n+                    item.available_range().duration.rate\n+                )\n+                # Appending blank to media promotes name of audio file in RV\n+                media.append(blank)\n+\n+            to_session.setMedia(media)\n             return True\n-        elif isinstance(mr.media_reference, otio.schema.GeneratorReference):\n-            if mr.media_reference.generator_kind == \"SMPTEBars\":\n+\n+        elif isinstance(item.media_reference, otio.schema.GeneratorReference):\n+            if item.media_reference.generator_kind == \"SMPTEBars\":\n                 kind = \"smptebars\"\n                 to_session.setMedia(\n                     [\n                         \"{},start={},end={},fps={}.movieproc\".format(\n                             kind,\n-                            mr.available_range().start_time.value,\n-                            mr.available_range().end_time_inclusive().value,\n-                            mr.available_range().duration.rate\n+                            item.available_range().start_time.value,\n+                            item.available_range().end_time_inclusive().value,\n+                            item.available_range().duration.rate\n                         )\n                     ]\n                 )",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "opentimelineio_contrib/adapters/extern_rv.py",
        "code_diff": "@@ -227,7 +250,7 @@\ndef _create_media_reference(mr, to_session):\n     return False\n \n \n-def _write_item(it, to_session):\n+def _write_item(it, to_session, track_kind=None):\n     src = to_session.newNode(\"Source\", str(it.name) or \"clip\")\n \n     src.setProperty(",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "opentimelineio_contrib/adapters/tests/test_rvsession.py",
        "code_diff": "@@ -98,6 +98,358 @@\n}\"\"\"\n \n \n+AUDIO_VIDEO_SAMPLE_DATA = \"\"\"{\n+    \"OTIO_SCHEMA\": \"Timeline.1\",\n+    \"metadata\": {},\n+    \"name\": null,\n+    \"tracks\": {\n+        \"OTIO_SCHEMA\": \"Stack.1\",\n+        \"children\": [\n+            {\n+                \"OTIO_SCHEMA\": \"Track.1\",\n+                \"children\": [\n+                    {\n+                        \"OTIO_SCHEMA\": \"Clip.1\",\n+                        \"effects\": [],\n+                        \"markers\": [],\n+                        \"media_reference\": {\n+                            \"OTIO_SCHEMA\": \"ExternalReference.1\",\n+                            \"available_range\": {\n+                                \"OTIO_SCHEMA\": \"TimeRange.1\",\n+                                \"duration\": {\n+                                    \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                    \"rate\": 25,\n+                                    \"value\": 67\n+                                },\n+                                \"start_time\": {\n+                                    \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                    \"rate\": 25,\n+                                    \"value\": 0\n+                                }\n+                            },\n+                            \"metadata\": {},\n+                            \"name\": null,\n+                            \"target_url\": \"/path/to/video.mov\"\n+                        },\n+                        \"metadata\": {},\n+                        \"name\": \"plyblast\",\n+                        \"source_range\": {\n+                            \"OTIO_SCHEMA\": \"TimeRange.1\",\n+                            \"duration\": {\n+                                \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                \"rate\": 25,\n+                                \"value\": 67\n+                            },\n+                            \"start_time\": {\n+                                \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                \"rate\": 25,\n+                                \"value\": 54\n+                            }\n+                        }\n+                    }\n+                ],\n+                \"effects\": [],\n+                \"kind\": \"Video\",\n+                \"markers\": [],\n+                \"metadata\": {},\n+                \"name\": \"v1\",\n+                \"source_range\": null\n+            },\n+            {\n+                \"OTIO_SCHEMA\": \"Track.1\",\n+                \"children\": [\n+                    {\n+                        \"OTIO_SCHEMA\": \"Clip.1\",\n+                        \"effects\": [],\n+                        \"markers\": [],\n+                        \"media_reference\": {\n+                            \"OTIO_SCHEMA\": \"ExternalReference.1\",\n+                            \"available_range\": {\n+                                \"OTIO_SCHEMA\": \"TimeRange.1\",\n+                                \"duration\": {\n+                                    \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                    \"rate\": 25,\n+                                    \"value\": 500\n+                                },\n+                                \"start_time\": {\n+                                    \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                    \"rate\": 25,\n+                                    \"value\": 0\n+                                }\n+                            },\n+                            \"metadata\": {},\n+                            \"name\": null,\n+                            \"target_url\": \"/path/to/audio.wav\"\n+                        },\n+                        \"metadata\": {},\n+                        \"name\": \"sound\",\n+                        \"source_range\": {\n+                            \"OTIO_SCHEMA\": \"TimeRange.1\",\n+                            \"duration\": {\n+                                \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                \"rate\": 25,\n+                                \"value\": 67\n+                            },\n+                            \"start_time\": {\n+                                \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                \"rate\": 25,\n+                                \"value\": 54\n+                            }\n+                        }\n+                    }\n+                ],\n+                \"effects\": [],\n+                \"kind\": \"Audio\",\n+                \"markers\": [],\n+                \"metadata\": {},\n+                \"name\": \"a1\",\n+                \"source_range\": null\n+            }\n+        ],\n+        \"effects\": [],\n+        \"markers\": [],\n+        \"metadata\": {},\n+        \"name\": \"tracks\",\n+        \"source_range\": null\n+    }\n+}\"\"\"\n+\n+\n+NESTED_STACK_SAMPLE_DATA = \"\"\"{\n+    \"OTIO_SCHEMA\": \"Timeline.1\",\n+    \"metadata\": {},\n+    \"name\": \"My Timeline\",\n+    \"tracks\": {\n+        \"OTIO_SCHEMA\": \"Stack.1\",\n+        \"children\": [\n+            {\n+                \"OTIO_SCHEMA\": \"Track.1\",\n+                \"children\": [\n+                    {\n+                        \"OTIO_SCHEMA\": \"Clip.1\",\n+                        \"effects\": [],\n+                        \"markers\": [],\n+                        \"media_reference\": {\n+                            \"OTIO_SCHEMA\": \"ExternalReference.1\",\n+                            \"available_range\": {\n+                                \"OTIO_SCHEMA\": \"TimeRange.1\",\n+                                \"duration\": {\n+                                    \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                    \"rate\": 24,\n+                                    \"value\": 238\n+                                },\n+                                \"start_time\": {\n+                                    \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                    \"rate\": 24,\n+                                    \"value\": 0\n+                                }\n+                            },\n+                            \"metadata\": {},\n+                            \"target_url\": \"/path/to/some/video.mov\"\n+                        },\n+                        \"metadata\": {},\n+                        \"name\": \"Normal Clip 1\",\n+                        \"source_range\": {\n+                            \"OTIO_SCHEMA\": \"TimeRange.1\",\n+                            \"duration\": {\n+                                \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                \"rate\": 24,\n+                                \"value\": 238\n+                            },\n+                            \"start_time\": {\n+                                \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                \"rate\": 24,\n+                                \"value\": 135\n+                            }\n+                        }\n+                    },\n+                    {\n+                        \"OTIO_SCHEMA\": \"Stack.1\",\n+                        \"children\": [\n+                            {\n+                                \"OTIO_SCHEMA\": \"Clip.1\",\n+                                \"effects\": [],\n+                                \"markers\": [],\n+                                \"media_reference\": {\n+                                    \"OTIO_SCHEMA\": \"ExternalReference.1\",\n+                                    \"available_range\": {\n+                                        \"OTIO_SCHEMA\": \"TimeRange.1\",\n+                                        \"duration\": {\n+                                            \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                            \"rate\": 24,\n+                                            \"value\": 238\n+                                        },\n+                                        \"start_time\": {\n+                                            \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                            \"rate\": 24,\n+                                            \"value\": 0\n+                                        }\n+                                    },\n+                                    \"metadata\": {},\n+                                    \"target_url\": \"/path/to/some/video.mov\"\n+                                },\n+                                \"metadata\": {},\n+                                \"name\": \"Clip Inside A Stack 1\",\n+                                \"source_range\": {\n+                                    \"OTIO_SCHEMA\": \"TimeRange.1\",\n+                                    \"duration\": {\n+                                        \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                        \"rate\": 24,\n+                                        \"value\": 37\n+                                    },\n+                                    \"start_time\": {\n+                                        \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                        \"rate\": 24,\n+                                        \"value\": 373\n+                                    }\n+                                }\n+                            }\n+                        ],\n+                        \"effects\": [],\n+                        \"markers\": [],\n+                        \"metadata\": {},\n+                        \"name\": \"Nested Stack 1\",\n+                        \"source_range\": {\n+                            \"OTIO_SCHEMA\": \"TimeRange.1\",\n+                            \"duration\": {\n+                                \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                \"rate\": 24,\n+                                \"value\": 31\n+                            },\n+                            \"start_time\": {\n+                                \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                \"rate\": 24,\n+                                \"value\": 0\n+                            }\n+                        }\n+                    }\n+                ],\n+                \"effects\": [],\n+                \"kind\": \"Video\",\n+                \"markers\": [],\n+                \"metadata\": {},\n+                \"name\": \"Top Level Track\",\n+                \"source_range\": null\n+            },\n+            {\n+                \"OTIO_SCHEMA\": \"Track.1\",\n+                \"children\": [\n+                    {\n+                        \"OTIO_SCHEMA\": \"Clip.1\",\n+                        \"effects\": [],\n+                        \"markers\": [],\n+                        \"media_reference\": {\n+                            \"OTIO_SCHEMA\": \"ExternalReference.1\",\n+                            \"available_range\": {\n+                                \"OTIO_SCHEMA\": \"TimeRange.1\",\n+                                \"duration\": {\n+                                    \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                    \"rate\": 24,\n+                                    \"value\": 238\n+                                },\n+                                \"start_time\": {\n+                                    \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                    \"rate\": 24,\n+                                    \"value\": 0\n+                                }\n+                            },\n+                            \"metadata\": {},\n+                            \"target_url\": \"/path/to/some/audio.wav\"\n+                        },\n+                        \"metadata\": {},\n+                        \"name\": \"Normal Clip 1\",\n+                        \"source_range\": {\n+                            \"OTIO_SCHEMA\": \"TimeRange.1\",\n+                            \"duration\": {\n+                                \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                \"rate\": 24,\n+                                \"value\": 238\n+                            },\n+                            \"start_time\": {\n+                                \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                \"rate\": 24,\n+                                \"value\": 135\n+                            }\n+                        }\n+                    },\n+                    {\n+                        \"OTIO_SCHEMA\": \"Stack.1\",\n+                        \"children\": [\n+                            {\n+                                \"OTIO_SCHEMA\": \"Clip.1\",\n+                                \"effects\": [],\n+                                \"markers\": [],\n+                                \"media_reference\": {\n+                                    \"OTIO_SCHEMA\": \"ExternalReference.1\",\n+                                    \"available_range\": {\n+                                        \"OTIO_SCHEMA\": \"TimeRange.1\",\n+                                        \"duration\": {\n+                                            \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                            \"rate\": 24,\n+                                            \"value\": 238\n+                                        },\n+                                        \"start_time\": {\n+                                            \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                            \"rate\": 24,\n+                                            \"value\": 0\n+                                        }\n+                                    },\n+                                    \"metadata\": {},\n+                                    \"target_url\": \"/path/to/some/audio.wav\"\n+                                },\n+                                \"metadata\": {},\n+                                \"name\": \"Clip Inside A Stack 1\",\n+                                \"source_range\": {\n+                                    \"OTIO_SCHEMA\": \"TimeRange.1\",\n+                                    \"duration\": {\n+                                        \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                        \"rate\": 24,\n+                                        \"value\": 37\n+                                    },\n+                                    \"start_time\": {\n+                                        \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                        \"rate\": 24,\n+                                        \"value\": 373\n+                                    }\n+                                }\n+                            }\n+                        ],\n+                        \"effects\": [],\n+                        \"markers\": [],\n+                        \"metadata\": {},\n+                        \"name\": \"Nested Stack 1\",\n+                        \"source_range\": {\n+                            \"OTIO_SCHEMA\": \"TimeRange.1\",\n+                            \"duration\": {\n+                                \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                \"rate\": 24,\n+                                \"value\": 31\n+                            },\n+                            \"start_time\": {\n+                                \"OTIO_SCHEMA\": \"RationalTime.1\",\n+                                \"rate\": 24,\n+                                \"value\": 0\n+                            }\n+                        }\n+                    }\n+                ],\n+                \"effects\": [],\n+                \"kind\": \"Audio\",\n+                \"markers\": [],\n+                \"metadata\": {},\n+                \"name\": \"Top Level Track\",\n+                \"source_range\": null\n+            }\n+        ],\n+        \"effects\": [],\n+        \"markers\": [],\n+        \"metadata\": {},\n+        \"name\": \"Top Level Stack\",\n+        \"source_range\": null\n+    }\n+}\"\"\"\n+\n+\n @unittest.skipIf(\n     \"OTIO_RV_PYTHON_LIB\" not in os.environ or\n     \"OTIO_RV_PYTHON_BIN\" not in os.environ,",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "tests/test_console.py",
        "code_diff": "@@ -65,7 +65,12 @@\ndef test_basic(self):\n \n class OTIOCatTests(ConsoleTester, unittest.TestCase):\n     def test_basic(self):\n-        sys.argv = ['otiocat', SCREENING_EXAMPLE_PATH]\n+        sys.argv = ['otiocat', SCREENING_EXAMPLE_PATH, \"-a\", \"rate=24.0\"]\n+        otio.console.otiocat.main()\n+        self.assertIn('\"name\": \"Example_Screening.01\",', sys.stdout.getvalue())\n+\n+    def test_no_media_linker(self):\n+        sys.argv = ['otiocat', SCREENING_EXAMPLE_PATH, \"-m\", \"none\"]\n         otio.console.otiocat.main()\n         self.assertIn('\"name\": \"Example_Screening.01\",', sys.stdout.getvalue())",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "tests/test_serializable_object.py",
        "code_diff": "@@ -51,46 +51,46 @@\ndef test_serialize_time(self):\n class SerializableObjTest(unittest.TestCase, otio.test_utils.OTIOAssertions):\n     def test_cons(self):\n         so = otio.core.SerializableObject()\n-        so.data['foo'] = 'bar'\n-        self.assertEqual(so.data['foo'], 'bar')\n+        so._data['foo'] = 'bar'\n+        self.assertEqual(so._data['foo'], 'bar')\n \n     def test_update(self):\n         so = otio.core.SerializableObject()\n-        so.update({\"foo\": \"bar\"})\n-        self.assertEqual(so.data[\"foo\"], \"bar\")\n+        so._update({\"foo\": \"bar\"})\n+        self.assertEqual(so._data[\"foo\"], \"bar\")\n         so_2 = otio.core.SerializableObject()\n-        so_2.data[\"foo\"] = \"not bar\"\n-        so.update(so_2)\n-        self.assertEqual(so.data[\"foo\"], \"not bar\")\n+        so_2._data[\"foo\"] = \"not bar\"\n+        so._update(so_2)\n+        self.assertEqual(so._data[\"foo\"], \"not bar\")\n \n     def test_serialize_to_error(self):\n         so = otio.core.SerializableObject()\n-        so.data['foo'] = 'bar'\n+        so._data['foo'] = 'bar'\n         with self.assertRaises(otio.exceptions.InvalidSerializableLabelError):\n             otio.adapters.otio_json.write_to_string(so)\n \n     def test_copy_lib(self):\n         so = otio.core.SerializableObject()\n-        so.data[\"metadata\"] = {\"foo\": \"bar\"}\n+        so._data[\"meta_data\"] = {\"foo\": \"bar\"}\n \n         import copy\n \n         # shallow copy\n         so_cp = copy.copy(so)\n-        so_cp.data[\"metadata\"][\"foo\"] = \"not bar\"\n-        self.assertEqual(so.data, so_cp.data)\n+        so_cp._data[\"meta_data\"][\"foo\"] = \"not bar\"\n+        self.assertEqual(so._data, so_cp._data)\n \n         so.foo = \"bar\"\n         so_cp = copy.copy(so)\n-        # copy only copies members of the data dictionary, *not* other attrs.\n+        # copy only copies members of the _data dictionary, *not* other attrs.\n         with self.assertRaises(AttributeError):\n             so_cp.foo\n \n         # deep copy\n         so_cp = copy.deepcopy(so)\n         self.assertIsOTIOEquivalentTo(so, so_cp)\n \n-        so_cp.data[\"foo\"] = \"bar\"\n+        so_cp._data[\"foo\"] = \"bar\"\n         self.assertNotEqual(so, so_cp)\n \n     def test_copy_subclass(self):",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Small general updates",
        "pr_number": 388,
        "file_name": "tests/test_serializable_object.py",
        "code_diff": "@@ -99,7 +99,7 @@\nclass Foo(otio.core.SerializableObject):\n             _serializable_label = \"Foo.1\"\n \n         foo = Foo()\n-        foo.data[\"metadata\"] = {\"foo\": \"bar\"}\n+        foo._data[\"meta_data\"] = {\"foo\": \"bar\"}\n \n         import copy",
        "comments": [],
        "commit_message": "merging a diverged remote branch back into local branch",
        "commit_id": "34ab49676455f444148228795bcc34ee0e97691b"
    },
    {
        "pr_title": "Make types in opentime immutable.",
        "pr_number": 372,
        "file_name": "opentimelineio/opentime.py",
        "code_diff": "@@ -64,8 +64,8 @@\nclass RationalTime(object):\n     __slots__ = ['value', 'rate']\n \n     def __init__(self, value=0.0, rate=1.0):\n-        _fn_cache(self, \"value\", float(value))\n-        _fn_cache(self, \"rate\", float(rate))\n+        _fn_cache(self, \"value\", value)\n+        _fn_cache(self, \"rate\", rate)\n \n     def __setattr__(self, key, val):\n         \"\"\"Enforces immutability \"\"\"",
        "comments": [],
        "commit_message": "Do not enforce float-ness in the RationalTime constructor.",
        "commit_id": "054183078f754781dd019fab79a5f0993ddcd65e"
    },
    {
        "pr_title": "Make types in opentime immutable.",
        "pr_number": 372,
        "file_name": "opentimelineio/opentime.py",
        "code_diff": "@@ -106,7 +106,7 @@\ndef value_rescaled_to(self, new_rate):\n \n         # TODO: This math probably needs some overrun protection\n         try:\n-            return (self.value * new_rate) / self.rate\n+            return float(self.value) * float(new_rate) / float(self.rate)\n         except (AttributeError, TypeError, ValueError):\n             raise TypeError(\n                 \"Sorry, RationalTime cannot be rescaled to a value of type \"",
        "comments": [],
        "commit_message": "Do not enforce float-ness in the RationalTime constructor.",
        "commit_id": "054183078f754781dd019fab79a5f0993ddcd65e"
    },
    {
        "pr_title": "Make types in opentime immutable.",
        "pr_number": 372,
        "file_name": "tests/test_opentime.py",
        "code_diff": "@@ -434,7 +434,7 @@\ndef test_time_string_23976_fps(self):\n             # self.assertEqual(t, t1)\n \n     def test_time_to_string(self):\n-        t = otio.opentime.RationalTime(1, 2)\n+        t = otio.opentime.RationalTime(1.0, 2.0)\n         self.assertEqual(str(t), \"RationalTime(1.0, 2.0)\")\n         self.assertEqual(\n             repr(t),",
        "comments": [],
        "commit_message": "Do not enforce float-ness in the RationalTime constructor.",
        "commit_id": "054183078f754781dd019fab79a5f0993ddcd65e"
    },
    {
        "pr_title": "Make types in opentime immutable.",
        "pr_number": 372,
        "file_name": "tests/test_opentime.py",
        "code_diff": "@@ -610,8 +610,8 @@\ndef test_rate(self):\n         self.assertEqual(txform2.rate, txform1.applied_to(txform2).rate)\n \n     def test_string(self):\n-        tstart = otio.opentime.RationalTime(12, 25)\n-        txform = otio.opentime.TimeTransform(offset=tstart, scale=2)\n+        tstart = otio.opentime.RationalTime(12.0, 25.0)\n+        txform = otio.opentime.TimeTransform(offset=tstart, scale=2.0)\n         self.assertEqual(\n             repr(txform),\n             \"otio.opentime.TimeTransform(\"",
        "comments": [],
        "commit_message": "Do not enforce float-ness in the RationalTime constructor.",
        "commit_id": "054183078f754781dd019fab79a5f0993ddcd65e"
    },
    {
        "pr_title": "Suggested audio handling in RV session",
        "pr_number": 363,
        "file_name": "opentimelineio_contrib/adapters/extern_rv.py",
        "code_diff": "@@ -84,7 +84,7 @@\ndef write_otio(otio_obj, to_session, track_kind=None):\n     )\n \n \n-def _write_dissolve(pre_item, in_dissolve, post_item, to_session):\n+def _write_dissolve(pre_item, in_dissolve, post_item, to_session, track_kind=None):\n     rv_trx = to_session.newNode(\"CrossDissolve\", str(in_dissolve.name))\n     rv_trx.setProperty(\n         \"CrossDissolve\",",
        "comments": [
            {
                "comment": "Not your change, but this `mr` here threw me for quite a loop as it's not really a media reference itself, but something that might contain one. Nothing to change, just something I'll ask about updating another time. ",
                "position": null
            },
            {
                "comment": "As a workaround to the movieproc naming bug I would suggest reordering the inputs like so:\r\nmedia = [str(mr.media_reference.target_url), blank]",
                "position": null
            },
            {
                "comment": "Sure! I tested switching places on a previously created session file and it work nicely. I'll update now.",
                "position": null
            },
            {
                "comment": "Great! Did you notice how switching the order preserves the name of the audio source in the Session Manager? It hides the movieproc (unless you drill into the source), but promotes the relevant naming information.\r\n\r\nThere is supposed to be a way to achieve that in movieprocs to begin with, but one of my last significant additions to RV before my departure created a bug in this movieproc feature. I have started a conversation with Shotgun/AutoDesk to see if I can get them to fix my bug, but reordering here wasn't so bad.",
                "position": null
            },
            {
                "comment": "So I happened to test this here with an otio I had which contained a clip under a stack under the track which unfortunately caused this code to fail. It's not used all that often, but nesting (https://opentimelineio.readthedocs.io/en/latest/tutorials/otio-timeline-structure.html#nested-compositions) comes up every now and then. I did put in #374 to add a convenient way to do this, but in the meantime you might need to recursively search up the parent hierarchy until you find a \"schema.Track\" before you ask for the kind.",
                "position": null
            },
            {
                "comment": "Yeah, that read much nicer. I thought for some reason that the order was important for the composition to work properly. Thanks for the tip.",
                "position": null
            },
            {
                "comment": "@avrata I'll look into this.",
                "position": null
            },
            {
                "comment": "Hi @rogergodspeed and @apetrynet, bringing up this old pull request due to a weird edge case we came across here related to this. In a few instances, folks are setting a movie file in the \"Audio\" track, however this movie brings along some old reference video content. Depending on the stack setup, this reference video can show through. \r\n\r\nI am trying a few approaches to deal with this, but one simple solution I found was to swap the order here back to `media = [blank, str(mr.media_reference.target_url)]`. This seems \"mute\" the video (ie show only black) when the media_reference is pointing to a movie. We have had a few conversations on the intent of the \"Audio\" track kind, but one could argue that it should ONLY be for audio content, and ignoring (via \"blanking\" out) any video content is appropriate. \r\n\r\nThis does bring back the question of the name in the session manager. We are getting around that by setting the clip item's name to something that makes sense for us, but I wonder if this code:\r\n\r\nhttps://github.com/PixarAnimationStudios/OpenTimelineIO/blob/master/opentimelineio_contrib/adapters/extern_rv.py#L270\r\n\r\nWould cause the ui.name to always be set (and not letting the \"blank\" movieproc though). \r\n\r\nIn anycase, I put in #426 as a possible idea for switching the order again. Feel free to try and talk me out of this, or if you have alternate ideas, I would love to hear them.\r\n\r\nThanks,\r\nRobyn",
                "position": null
            },
            {
                "comment": "Hi @avrata !\r\n\r\nI unfortunately don't have enough knowledge of the inside workings of RV, so I can't really come up with other reasons why this would be a problem other than readability. \r\n\r\nThat said, I think this edge case points at some other issues with OTIO that perhaps should be discussed. If I'm not mistaken, when looking at the media_reference for an audio item based on a movie file it points to a movie file and not an audio file or an audio track within that movie file. This might indicate that we should see this edge case quite often. One might solve this in a media linker, but when exporting from an NLE you often get the movie source as a media reference and most likely will run into this issue. \r\nThere has been some discussion (#343) regarding a way to group media. Perhaps this is a good case for looking into this again?\r\n",
                "position": null
            },
            {
                "comment": "You are correct. Currently only the track kind specifies the type of media to expect. If a clip's media reference points to something of the wrong type, or a movie with both in it, the _intent_ is that only the media that matches the parent track's kind should be relevant. It would be reasonable for an adapter to mute any audio found in a video track, and to omit any video found in an audio track. However, since RV is happy to play both that doesn't really work well in practice. OTIO is missing a way for a media reference to select just one channel/stream within a media file - if we had that (and maybe an track kind of \"Video+Audio\"), then we could make this distinction clear.",
                "position": null
            },
            {
                "comment": "Every RVFileSource has a `group.noMovieAudio` property (http://www.tweaksoftware.com/static/documentation/rv/current/html/rv_reference.html#RVFileSource), but nothing analogous for video. The main concern I have with \"muting\" video this way is that it may not be respected when the compositing mode is changed. Anyway I am increasingly less and less familiar with how compositing is handled now that I have been gone over a year. I am taking steps to loop in someone that is still involved.",
                "position": null
            },
            {
                "comment": "Thanks. I did reach out to Kessler about this and he said they could turn the \"onlyMovieAudio\" (or whatever the inverse of the noMovieAudio flag is) into a feature request. This \"blank'ing\" of the video seemed like what would work now, but he did offer a warning to maybe do it twice to handle stereo modes. ",
                "position": null
            },
            {
                "comment": "Hey all,\r\n\r\n@avrata did reach out to me and @rogergodspeed just put me in touch with @jminor who pointed me here; sorry I didn't see this sooner!  With some discussion about the original problem with Jon, it sounds like the crux of the issue comes down to how the filename is being populated.\r\n\r\nYou should be able to use one (or two for stereo) blank movieprocs with the audio movie as the last item.  When you encode the filename, however, you will want to base64-encode the filename; once you do that it should be able to carry with it any name you wish.  If the filename looks like a path, the ui in the session manager will just show the extension-less basename (just as it would if it were a normal file path).\r\n\r\nWith base64 encoding, you can include all manner of characters that will pass fluidly through the filename attribute on the movieproc:\r\n\r\n>>> import base64;base64.b64encode(\"Tr0ub4dor&3\")\r\n'VHIwdWI0ZG9yJjM='\r\n\r\n```\r\n/Applications/RV64.app/Contents/MacOS/RV64 blank,filename=VHIwdWI0ZG9yJjM=.movieproc\r\n```\r\n\r\n<img width=\"875\" alt=\"screen shot 2019-02-20 at 5 49 33 pm\" src=\"https://user-images.githubusercontent.com/1572121/53137517-e4d2bb80-3537-11e9-8afb-78eccbb12b4c.png\">\r\n\r\n\r\nOr with paths...\r\n\r\n>>> import base64;base64.b64encode(\"/correct/horse/battery/staple.mov\")\r\n'L2NvcnJlY3QvaG9yc2UvYmF0dGVyeS9zdGFwbGUubW92'\r\n\r\n```\r\n/Applications/RV64.app/Contents/MacOS/RV64 blank,filename=L2NvcnJlY3QvaG9yc2UvYmF0dGVyeS9zdGFwbGUubW92.movieproc\r\n```\r\n\r\n<img width=\"894\" alt=\"screen shot 2019-02-20 at 5 53 11 pm\" src=\"https://user-images.githubusercontent.com/1572121/53137663-67f41180-3538-11e9-8af0-b7fa7387b8aa.png\">\r\n\r\n\r\nhttp://www.tweaksoftware.com/static/documentation/rv/current/html/rv_manual.html#File_Formats_Movie_File_Formats_RV_s_movieproc_Format___movieproc_\r\n\r\n\r\nBetween this and the using the blank movieproc first in the sources, I think you should be able to get both an audio-only placeholder along with a deterministic display name.\r\n",
                "position": null
            }
        ],
        "commit_message": "Forgot adding `track_kind` arguments to a few functions",
        "commit_id": "dd18e373d92db6f13f07b9e8e44e1786afd36558",
        "fn_context": "def write_otio(otio_obj, to_session, track_kind=None):\n    WRITE_TYPE_MAP = {\n        otio.schema.Timeline: _write_timeline,\n        otio.schema.Stack: _write_stack,\n        otio.schema.Track: _write_track,\n        otio.schema.Clip: _write_item,\n        otio.schema.Gap: _write_item,\n        otio.schema.Transition: _write_transition,\n    }\n\n    if type(otio_obj) in WRITE_TYPE_MAP:\n        return WRITE_TYPE_MAP[type(otio_obj)](otio_obj, to_session, track_kind)\n\n    raise NoMappingForOtioTypeError(\n        str(type(otio_obj)) + \" on object: {}\".format(otio_obj)\n    )\n\ndef _write_dissolve(pre_item, in_dissolve, post_item, to_session, track_kind=None):\n    rv_trx = to_session.newNode(\"CrossDissolve\", str(in_dissolve.name))\n    rv_trx.setProperty(\n        \"CrossDissolve\",\n        \"\",\n        \"parameters\",\n        \"startFrame\",\n        rvSession.gto.FLOAT,\n        1.0\n    )\n    rv_trx.setProperty(\n        \"CrossDissolve\",\n        \"\",\n        \"parameters\",\n        \"numFrames\",\n        rvSession.gto.FLOAT,\n        int(\n            (in_dissolve.in_offset + in_dissolve.out_offset).rescaled_to(\n                pre_item.trimmed_range().duration.rate\n            ).value\n        )\n    )\n\n    rv_trx.setProperty(\n        \"CrossDissolve\",\n        \"\",\n        \"output\",\n        \"fps\",\n        rvSession.gto.FLOAT,\n        pre_item.trimmed_range().duration.rate\n    )\n\n    pre_item_rv = write_otio(pre_item, to_session, track_kind)\n    rv_trx.addInput(pre_item_rv)\n\n    post_item_rv = write_otio(post_item, to_session, track_kind)\n\n    node_to_insert = post_item_rv\n\n    if (\n        hasattr(pre_item, \"media_reference\") and\n        pre_item.media_reference and\n        pre_item.media_reference.available_range and\n        hasattr(post_item, \"media_reference\") and\n        post_item.media_reference and\n        post_item.media_reference.available_range and\n        (\n            post_item.media_reference.available_range.start_time.rate !=\n            pre_item.media_reference.available_range.start_time.rate\n        )\n    ):\n        # write a retime to make sure post_item is in the timebase of pre_item\n        rt_node = to_session.newNode(\"Retime\", \"transition_retime\")\n        rt_node.setTargetFps(\n            pre_item.media_reference.available_range.start_time.rate\n        )\n\n        post_item_rv = write_otio(post_item, to_session, track_kind)\n\n        rt_node.addInput(post_item_rv)\n        node_to_insert = rt_node\n\n    rv_trx.addInput(node_to_insert)\n\n    return rv_trx"
    },
    {
        "pr_title": "Suggested audio handling in RV session",
        "pr_number": 363,
        "file_name": "opentimelineio_contrib/adapters/extern_rv.py",
        "code_diff": "@@ -116,10 +116,10 @@\ndef _write_dissolve(pre_item, in_dissolve, post_item, to_session):\n         pre_item.trimmed_range().duration.rate\n     )\n \n-    pre_item_rv = write_otio(pre_item, to_session)\n+    pre_item_rv = write_otio(pre_item, to_session, track_kind)\n     rv_trx.addInput(pre_item_rv)\n \n-    post_item_rv = write_otio(post_item, to_session)\n+    post_item_rv = write_otio(post_item, to_session, track_kind)\n \n     node_to_insert = post_item_rv",
        "comments": [
            {
                "comment": "Not your change, but this `mr` here threw me for quite a loop as it's not really a media reference itself, but something that might contain one. Nothing to change, just something I'll ask about updating another time. ",
                "position": null
            },
            {
                "comment": "As a workaround to the movieproc naming bug I would suggest reordering the inputs like so:\r\nmedia = [str(mr.media_reference.target_url), blank]",
                "position": null
            },
            {
                "comment": "Sure! I tested switching places on a previously created session file and it work nicely. I'll update now.",
                "position": null
            },
            {
                "comment": "Great! Did you notice how switching the order preserves the name of the audio source in the Session Manager? It hides the movieproc (unless you drill into the source), but promotes the relevant naming information.\r\n\r\nThere is supposed to be a way to achieve that in movieprocs to begin with, but one of my last significant additions to RV before my departure created a bug in this movieproc feature. I have started a conversation with Shotgun/AutoDesk to see if I can get them to fix my bug, but reordering here wasn't so bad.",
                "position": null
            },
            {
                "comment": "can you also add a note here for the fact that putting it *after* the audio means that it puts the correct name onto the RVFileSource?",
                "position": 127
            },
            {
                "comment": "So I happened to test this here with an otio I had which contained a clip under a stack under the track which unfortunately caused this code to fail. It's not used all that often, but nesting (https://opentimelineio.readthedocs.io/en/latest/tutorials/otio-timeline-structure.html#nested-compositions) comes up every now and then. I did put in #374 to add a convenient way to do this, but in the meantime you might need to recursively search up the parent hierarchy until you find a \"schema.Track\" before you ask for the kind.",
                "position": null
            },
            {
                "comment": "Yeah, that read much nicer. I thought for some reason that the order was important for the composition to work properly. Thanks for the tip.",
                "position": null
            },
            {
                "comment": "Sure",
                "position": 127
            },
            {
                "comment": "Added comment in latest commit",
                "position": 127
            },
            {
                "comment": "@avrata I'll look into this.",
                "position": null
            },
            {
                "comment": "Hi @rogergodspeed and @apetrynet, bringing up this old pull request due to a weird edge case we came across here related to this. In a few instances, folks are setting a movie file in the \"Audio\" track, however this movie brings along some old reference video content. Depending on the stack setup, this reference video can show through. \r\n\r\nI am trying a few approaches to deal with this, but one simple solution I found was to swap the order here back to `media = [blank, str(mr.media_reference.target_url)]`. This seems \"mute\" the video (ie show only black) when the media_reference is pointing to a movie. We have had a few conversations on the intent of the \"Audio\" track kind, but one could argue that it should ONLY be for audio content, and ignoring (via \"blanking\" out) any video content is appropriate. \r\n\r\nThis does bring back the question of the name in the session manager. We are getting around that by setting the clip item's name to something that makes sense for us, but I wonder if this code:\r\n\r\nhttps://github.com/PixarAnimationStudios/OpenTimelineIO/blob/master/opentimelineio_contrib/adapters/extern_rv.py#L270\r\n\r\nWould cause the ui.name to always be set (and not letting the \"blank\" movieproc though). \r\n\r\nIn anycase, I put in #426 as a possible idea for switching the order again. Feel free to try and talk me out of this, or if you have alternate ideas, I would love to hear them.\r\n\r\nThanks,\r\nRobyn",
                "position": null
            },
            {
                "comment": "Hi @avrata !\r\n\r\nI unfortunately don't have enough knowledge of the inside workings of RV, so I can't really come up with other reasons why this would be a problem other than readability. \r\n\r\nThat said, I think this edge case points at some other issues with OTIO that perhaps should be discussed. If I'm not mistaken, when looking at the media_reference for an audio item based on a movie file it points to a movie file and not an audio file or an audio track within that movie file. This might indicate that we should see this edge case quite often. One might solve this in a media linker, but when exporting from an NLE you often get the movie source as a media reference and most likely will run into this issue. \r\nThere has been some discussion (#343) regarding a way to group media. Perhaps this is a good case for looking into this again?\r\n",
                "position": null
            },
            {
                "comment": "You are correct. Currently only the track kind specifies the type of media to expect. If a clip's media reference points to something of the wrong type, or a movie with both in it, the _intent_ is that only the media that matches the parent track's kind should be relevant. It would be reasonable for an adapter to mute any audio found in a video track, and to omit any video found in an audio track. However, since RV is happy to play both that doesn't really work well in practice. OTIO is missing a way for a media reference to select just one channel/stream within a media file - if we had that (and maybe an track kind of \"Video+Audio\"), then we could make this distinction clear.",
                "position": null
            },
            {
                "comment": "Every RVFileSource has a `group.noMovieAudio` property (http://www.tweaksoftware.com/static/documentation/rv/current/html/rv_reference.html#RVFileSource), but nothing analogous for video. The main concern I have with \"muting\" video this way is that it may not be respected when the compositing mode is changed. Anyway I am increasingly less and less familiar with how compositing is handled now that I have been gone over a year. I am taking steps to loop in someone that is still involved.",
                "position": null
            },
            {
                "comment": "Thanks. I did reach out to Kessler about this and he said they could turn the \"onlyMovieAudio\" (or whatever the inverse of the noMovieAudio flag is) into a feature request. This \"blank'ing\" of the video seemed like what would work now, but he did offer a warning to maybe do it twice to handle stereo modes. ",
                "position": null
            },
            {
                "comment": "Hey all,\r\n\r\n@avrata did reach out to me and @rogergodspeed just put me in touch with @jminor who pointed me here; sorry I didn't see this sooner!  With some discussion about the original problem with Jon, it sounds like the crux of the issue comes down to how the filename is being populated.\r\n\r\nYou should be able to use one (or two for stereo) blank movieprocs with the audio movie as the last item.  When you encode the filename, however, you will want to base64-encode the filename; once you do that it should be able to carry with it any name you wish.  If the filename looks like a path, the ui in the session manager will just show the extension-less basename (just as it would if it were a normal file path).\r\n\r\nWith base64 encoding, you can include all manner of characters that will pass fluidly through the filename attribute on the movieproc:\r\n\r\n>>> import base64;base64.b64encode(\"Tr0ub4dor&3\")\r\n'VHIwdWI0ZG9yJjM='\r\n\r\n```\r\n/Applications/RV64.app/Contents/MacOS/RV64 blank,filename=VHIwdWI0ZG9yJjM=.movieproc\r\n```\r\n\r\n<img width=\"875\" alt=\"screen shot 2019-02-20 at 5 49 33 pm\" src=\"https://user-images.githubusercontent.com/1572121/53137517-e4d2bb80-3537-11e9-8afb-78eccbb12b4c.png\">\r\n\r\n\r\nOr with paths...\r\n\r\n>>> import base64;base64.b64encode(\"/correct/horse/battery/staple.mov\")\r\n'L2NvcnJlY3QvaG9yc2UvYmF0dGVyeS9zdGFwbGUubW92'\r\n\r\n```\r\n/Applications/RV64.app/Contents/MacOS/RV64 blank,filename=L2NvcnJlY3QvaG9yc2UvYmF0dGVyeS9zdGFwbGUubW92.movieproc\r\n```\r\n\r\n<img width=\"894\" alt=\"screen shot 2019-02-20 at 5 53 11 pm\" src=\"https://user-images.githubusercontent.com/1572121/53137663-67f41180-3538-11e9-8af0-b7fa7387b8aa.png\">\r\n\r\n\r\nhttp://www.tweaksoftware.com/static/documentation/rv/current/html/rv_manual.html#File_Formats_Movie_File_Formats_RV_s_movieproc_Format___movieproc_\r\n\r\n\r\nBetween this and the using the blank movieproc first in the sources, I think you should be able to get both an audio-only placeholder along with a deterministic display name.\r\n",
                "position": null
            }
        ],
        "commit_message": "Forgot adding `track_kind` arguments to a few functions",
        "commit_id": "dd18e373d92db6f13f07b9e8e44e1786afd36558",
        "fn_context": "def _write_dissolve(pre_item, in_dissolve, post_item, to_session, track_kind=None):\n    rv_trx = to_session.newNode(\"CrossDissolve\", str(in_dissolve.name))\n    rv_trx.setProperty(\n        \"CrossDissolve\",\n        \"\",\n        \"parameters\",\n        \"startFrame\",\n        rvSession.gto.FLOAT,\n        1.0\n    )\n    rv_trx.setProperty(\n        \"CrossDissolve\",\n        \"\",\n        \"parameters\",\n        \"numFrames\",\n        rvSession.gto.FLOAT,\n        int(\n            (in_dissolve.in_offset + in_dissolve.out_offset).rescaled_to(\n                pre_item.trimmed_range().duration.rate\n            ).value\n        )\n    )\n\n    rv_trx.setProperty(\n        \"CrossDissolve\",\n        \"\",\n        \"output\",\n        \"fps\",\n        rvSession.gto.FLOAT,\n        pre_item.trimmed_range().duration.rate\n    )\n\n    pre_item_rv = write_otio(pre_item, to_session, track_kind)\n    rv_trx.addInput(pre_item_rv)\n\n    post_item_rv = write_otio(post_item, to_session, track_kind)\n\n    node_to_insert = post_item_rv\n\n    if (\n        hasattr(pre_item, \"media_reference\") and\n        pre_item.media_reference and\n        pre_item.media_reference.available_range and\n        hasattr(post_item, \"media_reference\") and\n        post_item.media_reference and\n        post_item.media_reference.available_range and\n        (\n            post_item.media_reference.available_range.start_time.rate !=\n            pre_item.media_reference.available_range.start_time.rate\n        )\n    ):\n        # write a retime to make sure post_item is in the timebase of pre_item\n        rt_node = to_session.newNode(\"Retime\", \"transition_retime\")\n        rt_node.setTargetFps(\n            pre_item.media_reference.available_range.start_time.rate\n        )\n\n        post_item_rv = write_otio(post_item, to_session, track_kind)\n\n        rt_node.addInput(post_item_rv)\n        node_to_insert = rt_node\n\n    rv_trx.addInput(node_to_insert)\n\n    return rv_trx"
    },
    {
        "pr_title": "Suggested audio handling in RV session",
        "pr_number": 363,
        "file_name": "opentimelineio_contrib/adapters/extern_rv.py",
        "code_diff": "@@ -141,7 +141,7 @@\ndef _write_dissolve(pre_item, in_dissolve, post_item, to_session):\n             pre_item.media_reference.available_range.start_time.rate\n         )\n \n-        post_item_rv = write_otio(post_item, to_session)\n+        post_item_rv = write_otio(post_item, to_session, track_kind)\n \n         rt_node.addInput(post_item_rv)\n         node_to_insert = rt_node",
        "comments": [
            {
                "comment": "Not your change, but this `mr` here threw me for quite a loop as it's not really a media reference itself, but something that might contain one. Nothing to change, just something I'll ask about updating another time. ",
                "position": null
            },
            {
                "comment": "As a workaround to the movieproc naming bug I would suggest reordering the inputs like so:\r\nmedia = [str(mr.media_reference.target_url), blank]",
                "position": null
            },
            {
                "comment": "Sure! I tested switching places on a previously created session file and it work nicely. I'll update now.",
                "position": null
            },
            {
                "comment": "Great! Did you notice how switching the order preserves the name of the audio source in the Session Manager? It hides the movieproc (unless you drill into the source), but promotes the relevant naming information.\r\n\r\nThere is supposed to be a way to achieve that in movieprocs to begin with, but one of my last significant additions to RV before my departure created a bug in this movieproc feature. I have started a conversation with Shotgun/AutoDesk to see if I can get them to fix my bug, but reordering here wasn't so bad.",
                "position": null
            },
            {
                "comment": "So I happened to test this here with an otio I had which contained a clip under a stack under the track which unfortunately caused this code to fail. It's not used all that often, but nesting (https://opentimelineio.readthedocs.io/en/latest/tutorials/otio-timeline-structure.html#nested-compositions) comes up every now and then. I did put in #374 to add a convenient way to do this, but in the meantime you might need to recursively search up the parent hierarchy until you find a \"schema.Track\" before you ask for the kind.",
                "position": null
            },
            {
                "comment": "Yeah, that read much nicer. I thought for some reason that the order was important for the composition to work properly. Thanks for the tip.",
                "position": null
            },
            {
                "comment": "@avrata I'll look into this.",
                "position": null
            },
            {
                "comment": "Hi @rogergodspeed and @apetrynet, bringing up this old pull request due to a weird edge case we came across here related to this. In a few instances, folks are setting a movie file in the \"Audio\" track, however this movie brings along some old reference video content. Depending on the stack setup, this reference video can show through. \r\n\r\nI am trying a few approaches to deal with this, but one simple solution I found was to swap the order here back to `media = [blank, str(mr.media_reference.target_url)]`. This seems \"mute\" the video (ie show only black) when the media_reference is pointing to a movie. We have had a few conversations on the intent of the \"Audio\" track kind, but one could argue that it should ONLY be for audio content, and ignoring (via \"blanking\" out) any video content is appropriate. \r\n\r\nThis does bring back the question of the name in the session manager. We are getting around that by setting the clip item's name to something that makes sense for us, but I wonder if this code:\r\n\r\nhttps://github.com/PixarAnimationStudios/OpenTimelineIO/blob/master/opentimelineio_contrib/adapters/extern_rv.py#L270\r\n\r\nWould cause the ui.name to always be set (and not letting the \"blank\" movieproc though). \r\n\r\nIn anycase, I put in #426 as a possible idea for switching the order again. Feel free to try and talk me out of this, or if you have alternate ideas, I would love to hear them.\r\n\r\nThanks,\r\nRobyn",
                "position": null
            },
            {
                "comment": "Hi @avrata !\r\n\r\nI unfortunately don't have enough knowledge of the inside workings of RV, so I can't really come up with other reasons why this would be a problem other than readability. \r\n\r\nThat said, I think this edge case points at some other issues with OTIO that perhaps should be discussed. If I'm not mistaken, when looking at the media_reference for an audio item based on a movie file it points to a movie file and not an audio file or an audio track within that movie file. This might indicate that we should see this edge case quite often. One might solve this in a media linker, but when exporting from an NLE you often get the movie source as a media reference and most likely will run into this issue. \r\nThere has been some discussion (#343) regarding a way to group media. Perhaps this is a good case for looking into this again?\r\n",
                "position": null
            },
            {
                "comment": "You are correct. Currently only the track kind specifies the type of media to expect. If a clip's media reference points to something of the wrong type, or a movie with both in it, the _intent_ is that only the media that matches the parent track's kind should be relevant. It would be reasonable for an adapter to mute any audio found in a video track, and to omit any video found in an audio track. However, since RV is happy to play both that doesn't really work well in practice. OTIO is missing a way for a media reference to select just one channel/stream within a media file - if we had that (and maybe an track kind of \"Video+Audio\"), then we could make this distinction clear.",
                "position": null
            },
            {
                "comment": "Every RVFileSource has a `group.noMovieAudio` property (http://www.tweaksoftware.com/static/documentation/rv/current/html/rv_reference.html#RVFileSource), but nothing analogous for video. The main concern I have with \"muting\" video this way is that it may not be respected when the compositing mode is changed. Anyway I am increasingly less and less familiar with how compositing is handled now that I have been gone over a year. I am taking steps to loop in someone that is still involved.",
                "position": null
            },
            {
                "comment": "Thanks. I did reach out to Kessler about this and he said they could turn the \"onlyMovieAudio\" (or whatever the inverse of the noMovieAudio flag is) into a feature request. This \"blank'ing\" of the video seemed like what would work now, but he did offer a warning to maybe do it twice to handle stereo modes. ",
                "position": null
            },
            {
                "comment": "Hey all,\r\n\r\n@avrata did reach out to me and @rogergodspeed just put me in touch with @jminor who pointed me here; sorry I didn't see this sooner!  With some discussion about the original problem with Jon, it sounds like the crux of the issue comes down to how the filename is being populated.\r\n\r\nYou should be able to use one (or two for stereo) blank movieprocs with the audio movie as the last item.  When you encode the filename, however, you will want to base64-encode the filename; once you do that it should be able to carry with it any name you wish.  If the filename looks like a path, the ui in the session manager will just show the extension-less basename (just as it would if it were a normal file path).\r\n\r\nWith base64 encoding, you can include all manner of characters that will pass fluidly through the filename attribute on the movieproc:\r\n\r\n>>> import base64;base64.b64encode(\"Tr0ub4dor&3\")\r\n'VHIwdWI0ZG9yJjM='\r\n\r\n```\r\n/Applications/RV64.app/Contents/MacOS/RV64 blank,filename=VHIwdWI0ZG9yJjM=.movieproc\r\n```\r\n\r\n<img width=\"875\" alt=\"screen shot 2019-02-20 at 5 49 33 pm\" src=\"https://user-images.githubusercontent.com/1572121/53137517-e4d2bb80-3537-11e9-8afb-78eccbb12b4c.png\">\r\n\r\n\r\nOr with paths...\r\n\r\n>>> import base64;base64.b64encode(\"/correct/horse/battery/staple.mov\")\r\n'L2NvcnJlY3QvaG9yc2UvYmF0dGVyeS9zdGFwbGUubW92'\r\n\r\n```\r\n/Applications/RV64.app/Contents/MacOS/RV64 blank,filename=L2NvcnJlY3QvaG9yc2UvYmF0dGVyeS9zdGFwbGUubW92.movieproc\r\n```\r\n\r\n<img width=\"894\" alt=\"screen shot 2019-02-20 at 5 53 11 pm\" src=\"https://user-images.githubusercontent.com/1572121/53137663-67f41180-3538-11e9-8af0-b7fa7387b8aa.png\">\r\n\r\n\r\nhttp://www.tweaksoftware.com/static/documentation/rv/current/html/rv_manual.html#File_Formats_Movie_File_Formats_RV_s_movieproc_Format___movieproc_\r\n\r\n\r\nBetween this and the using the blank movieproc first in the sources, I think you should be able to get both an audio-only placeholder along with a deterministic display name.\r\n",
                "position": null
            }
        ],
        "commit_message": "Forgot adding `track_kind` arguments to a few functions",
        "commit_id": "dd18e373d92db6f13f07b9e8e44e1786afd36558",
        "fn_context": "def _write_dissolve(pre_item, in_dissolve, post_item, to_session, track_kind=None):\n    rv_trx = to_session.newNode(\"CrossDissolve\", str(in_dissolve.name))\n    rv_trx.setProperty(\n        \"CrossDissolve\",\n        \"\",\n        \"parameters\",\n        \"startFrame\",\n        rvSession.gto.FLOAT,\n        1.0\n    )\n    rv_trx.setProperty(\n        \"CrossDissolve\",\n        \"\",\n        \"parameters\",\n        \"numFrames\",\n        rvSession.gto.FLOAT,\n        int(\n            (in_dissolve.in_offset + in_dissolve.out_offset).rescaled_to(\n                pre_item.trimmed_range().duration.rate\n            ).value\n        )\n    )\n\n    rv_trx.setProperty(\n        \"CrossDissolve\",\n        \"\",\n        \"output\",\n        \"fps\",\n        rvSession.gto.FLOAT,\n        pre_item.trimmed_range().duration.rate\n    )\n\n    pre_item_rv = write_otio(pre_item, to_session, track_kind)\n    rv_trx.addInput(pre_item_rv)\n\n    post_item_rv = write_otio(post_item, to_session, track_kind)\n\n    node_to_insert = post_item_rv\n\n    if (\n        hasattr(pre_item, \"media_reference\") and\n        pre_item.media_reference and\n        pre_item.media_reference.available_range and\n        hasattr(post_item, \"media_reference\") and\n        post_item.media_reference and\n        post_item.media_reference.available_range and\n        (\n            post_item.media_reference.available_range.start_time.rate !=\n            pre_item.media_reference.available_range.start_time.rate\n        )\n    ):\n        # write a retime to make sure post_item is in the timebase of pre_item\n        rt_node = to_session.newNode(\"Retime\", \"transition_retime\")\n        rt_node.setTargetFps(\n            pre_item.media_reference.available_range.start_time.rate\n        )\n\n        post_item_rv = write_otio(post_item, to_session, track_kind)\n\n        rt_node.addInput(post_item_rv)\n        node_to_insert = rt_node\n\n    rv_trx.addInput(node_to_insert)\n\n    return rv_trx"
    },
    {
        "pr_title": "Suggested audio handling in RV session",
        "pr_number": 363,
        "file_name": "opentimelineio_contrib/adapters/extern_rv.py",
        "code_diff": "@@ -151,7 +151,13 @@\ndef _write_dissolve(pre_item, in_dissolve, post_item, to_session):\n     return rv_trx\n \n \n-def _write_transition(pre_item, in_trx, post_item, to_session):\n+def _write_transition(\n+        pre_item,\n+        in_trx,\n+        post_item,\n+        to_session,\n+        track_kind=None\n+):\n     trx_map = {\n         otio.schema.TransitionTypes.SMPTE_Dissolve: _write_dissolve,\n     }",
        "comments": [
            {
                "comment": "Not your change, but this `mr` here threw me for quite a loop as it's not really a media reference itself, but something that might contain one. Nothing to change, just something I'll ask about updating another time. ",
                "position": null
            },
            {
                "comment": "As a workaround to the movieproc naming bug I would suggest reordering the inputs like so:\r\nmedia = [str(mr.media_reference.target_url), blank]",
                "position": null
            },
            {
                "comment": "Sure! I tested switching places on a previously created session file and it work nicely. I'll update now.",
                "position": null
            },
            {
                "comment": "Great! Did you notice how switching the order preserves the name of the audio source in the Session Manager? It hides the movieproc (unless you drill into the source), but promotes the relevant naming information.\r\n\r\nThere is supposed to be a way to achieve that in movieprocs to begin with, but one of my last significant additions to RV before my departure created a bug in this movieproc feature. I have started a conversation with Shotgun/AutoDesk to see if I can get them to fix my bug, but reordering here wasn't so bad.",
                "position": null
            },
            {
                "comment": "So I happened to test this here with an otio I had which contained a clip under a stack under the track which unfortunately caused this code to fail. It's not used all that often, but nesting (https://opentimelineio.readthedocs.io/en/latest/tutorials/otio-timeline-structure.html#nested-compositions) comes up every now and then. I did put in #374 to add a convenient way to do this, but in the meantime you might need to recursively search up the parent hierarchy until you find a \"schema.Track\" before you ask for the kind.",
                "position": null
            },
            {
                "comment": "Yeah, that read much nicer. I thought for some reason that the order was important for the composition to work properly. Thanks for the tip.",
                "position": null
            },
            {
                "comment": "@avrata I'll look into this.",
                "position": null
            },
            {
                "comment": "Hi @rogergodspeed and @apetrynet, bringing up this old pull request due to a weird edge case we came across here related to this. In a few instances, folks are setting a movie file in the \"Audio\" track, however this movie brings along some old reference video content. Depending on the stack setup, this reference video can show through. \r\n\r\nI am trying a few approaches to deal with this, but one simple solution I found was to swap the order here back to `media = [blank, str(mr.media_reference.target_url)]`. This seems \"mute\" the video (ie show only black) when the media_reference is pointing to a movie. We have had a few conversations on the intent of the \"Audio\" track kind, but one could argue that it should ONLY be for audio content, and ignoring (via \"blanking\" out) any video content is appropriate. \r\n\r\nThis does bring back the question of the name in the session manager. We are getting around that by setting the clip item's name to something that makes sense for us, but I wonder if this code:\r\n\r\nhttps://github.com/PixarAnimationStudios/OpenTimelineIO/blob/master/opentimelineio_contrib/adapters/extern_rv.py#L270\r\n\r\nWould cause the ui.name to always be set (and not letting the \"blank\" movieproc though). \r\n\r\nIn anycase, I put in #426 as a possible idea for switching the order again. Feel free to try and talk me out of this, or if you have alternate ideas, I would love to hear them.\r\n\r\nThanks,\r\nRobyn",
                "position": null
            },
            {
                "comment": "Hi @avrata !\r\n\r\nI unfortunately don't have enough knowledge of the inside workings of RV, so I can't really come up with other reasons why this would be a problem other than readability. \r\n\r\nThat said, I think this edge case points at some other issues with OTIO that perhaps should be discussed. If I'm not mistaken, when looking at the media_reference for an audio item based on a movie file it points to a movie file and not an audio file or an audio track within that movie file. This might indicate that we should see this edge case quite often. One might solve this in a media linker, but when exporting from an NLE you often get the movie source as a media reference and most likely will run into this issue. \r\nThere has been some discussion (#343) regarding a way to group media. Perhaps this is a good case for looking into this again?\r\n",
                "position": null
            },
            {
                "comment": "You are correct. Currently only the track kind specifies the type of media to expect. If a clip's media reference points to something of the wrong type, or a movie with both in it, the _intent_ is that only the media that matches the parent track's kind should be relevant. It would be reasonable for an adapter to mute any audio found in a video track, and to omit any video found in an audio track. However, since RV is happy to play both that doesn't really work well in practice. OTIO is missing a way for a media reference to select just one channel/stream within a media file - if we had that (and maybe an track kind of \"Video+Audio\"), then we could make this distinction clear.",
                "position": null
            },
            {
                "comment": "Every RVFileSource has a `group.noMovieAudio` property (http://www.tweaksoftware.com/static/documentation/rv/current/html/rv_reference.html#RVFileSource), but nothing analogous for video. The main concern I have with \"muting\" video this way is that it may not be respected when the compositing mode is changed. Anyway I am increasingly less and less familiar with how compositing is handled now that I have been gone over a year. I am taking steps to loop in someone that is still involved.",
                "position": null
            },
            {
                "comment": "Thanks. I did reach out to Kessler about this and he said they could turn the \"onlyMovieAudio\" (or whatever the inverse of the noMovieAudio flag is) into a feature request. This \"blank'ing\" of the video seemed like what would work now, but he did offer a warning to maybe do it twice to handle stereo modes. ",
                "position": null
            },
            {
                "comment": "Hey all,\r\n\r\n@avrata did reach out to me and @rogergodspeed just put me in touch with @jminor who pointed me here; sorry I didn't see this sooner!  With some discussion about the original problem with Jon, it sounds like the crux of the issue comes down to how the filename is being populated.\r\n\r\nYou should be able to use one (or two for stereo) blank movieprocs with the audio movie as the last item.  When you encode the filename, however, you will want to base64-encode the filename; once you do that it should be able to carry with it any name you wish.  If the filename looks like a path, the ui in the session manager will just show the extension-less basename (just as it would if it were a normal file path).\r\n\r\nWith base64 encoding, you can include all manner of characters that will pass fluidly through the filename attribute on the movieproc:\r\n\r\n>>> import base64;base64.b64encode(\"Tr0ub4dor&3\")\r\n'VHIwdWI0ZG9yJjM='\r\n\r\n```\r\n/Applications/RV64.app/Contents/MacOS/RV64 blank,filename=VHIwdWI0ZG9yJjM=.movieproc\r\n```\r\n\r\n<img width=\"875\" alt=\"screen shot 2019-02-20 at 5 49 33 pm\" src=\"https://user-images.githubusercontent.com/1572121/53137517-e4d2bb80-3537-11e9-8afb-78eccbb12b4c.png\">\r\n\r\n\r\nOr with paths...\r\n\r\n>>> import base64;base64.b64encode(\"/correct/horse/battery/staple.mov\")\r\n'L2NvcnJlY3QvaG9yc2UvYmF0dGVyeS9zdGFwbGUubW92'\r\n\r\n```\r\n/Applications/RV64.app/Contents/MacOS/RV64 blank,filename=L2NvcnJlY3QvaG9yc2UvYmF0dGVyeS9zdGFwbGUubW92.movieproc\r\n```\r\n\r\n<img width=\"894\" alt=\"screen shot 2019-02-20 at 5 53 11 pm\" src=\"https://user-images.githubusercontent.com/1572121/53137663-67f41180-3538-11e9-8af0-b7fa7387b8aa.png\">\r\n\r\n\r\nhttp://www.tweaksoftware.com/static/documentation/rv/current/html/rv_manual.html#File_Formats_Movie_File_Formats_RV_s_movieproc_Format___movieproc_\r\n\r\n\r\nBetween this and the using the blank movieproc first in the sources, I think you should be able to get both an audio-only placeholder along with a deterministic display name.\r\n",
                "position": null
            }
        ],
        "commit_message": "Forgot adding `track_kind` arguments to a few functions",
        "commit_id": "dd18e373d92db6f13f07b9e8e44e1786afd36558",
        "fn_context": "def _write_dissolve(pre_item, in_dissolve, post_item, to_session, track_kind=None):\n    rv_trx = to_session.newNode(\"CrossDissolve\", str(in_dissolve.name))\n    rv_trx.setProperty(\n        \"CrossDissolve\",\n        \"\",\n        \"parameters\",\n        \"startFrame\",\n        rvSession.gto.FLOAT,\n        1.0\n    )\n    rv_trx.setProperty(\n        \"CrossDissolve\",\n        \"\",\n        \"parameters\",\n        \"numFrames\",\n        rvSession.gto.FLOAT,\n        int(\n            (in_dissolve.in_offset + in_dissolve.out_offset).rescaled_to(\n                pre_item.trimmed_range().duration.rate\n            ).value\n        )\n    )\n\n    rv_trx.setProperty(\n        \"CrossDissolve\",\n        \"\",\n        \"output\",\n        \"fps\",\n        rvSession.gto.FLOAT,\n        pre_item.trimmed_range().duration.rate\n    )\n\n    pre_item_rv = write_otio(pre_item, to_session, track_kind)\n    rv_trx.addInput(pre_item_rv)\n\n    post_item_rv = write_otio(post_item, to_session, track_kind)\n\n    node_to_insert = post_item_rv\n\n    if (\n        hasattr(pre_item, \"media_reference\") and\n        pre_item.media_reference and\n        pre_item.media_reference.available_range and\n        hasattr(post_item, \"media_reference\") and\n        post_item.media_reference and\n        post_item.media_reference.available_range and\n        (\n            post_item.media_reference.available_range.start_time.rate !=\n            pre_item.media_reference.available_range.start_time.rate\n        )\n    ):\n        # write a retime to make sure post_item is in the timebase of pre_item\n        rt_node = to_session.newNode(\"Retime\", \"transition_retime\")\n        rt_node.setTargetFps(\n            pre_item.media_reference.available_range.start_time.rate\n        )\n\n        post_item_rv = write_otio(post_item, to_session, track_kind)\n\n        rt_node.addInput(post_item_rv)\n        node_to_insert = rt_node\n\n    rv_trx.addInput(node_to_insert)\n\n    return rv_trx\n\ndef _write_transition(\n        pre_item,\n        in_trx,\n        post_item,\n        to_session,\n        track_kind=None\n):\n    trx_map = {\n        otio.schema.TransitionTypes.SMPTE_Dissolve: _write_dissolve,\n    }\n\n    if in_trx.transition_type not in trx_map:\n        return\n\n    return trx_map[in_trx.transition_type](\n        pre_item,\n        in_trx,\n        post_item,\n        to_session,\n        track_kind\n    )"
    },
    {
        "pr_title": "AAF EssenceGroup fix",
        "pr_number": 361,
        "file_name": "opentimelineio_contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -103,6 +103,34 @@\ndef _transcribe_property(prop):\n         return str(prop)\n \n \n+def _find_timecode_mobs(item):\n+    mobs = [item.resolve_ref()]\n+\n+    for c in item.walk():\n+        if isinstance(c, aaf.component.EssenceGroup):\n+            # An EssenceGroup is a Segment that has one or more\n+            # alternate choices, each of which represent different variations\n+            # of one actual piece of content.\n+            # According to the AAF Object Specification and Edit Protocol\n+            # documents:\n+            # \"Typically the different representations vary in essence format,\n+            # compression, or frame size. The application is responsible for\n+            # choosing the appropriate implementation of the essence.\"\n+            # It also says they should all have the same length, but\n+            # there might be nested Sequences inside which we're not attempting\n+            # to handle here (yet). We'll need a concrete example to ensure\n+            # we're doing the right thing.\n+            # TODO: Is the Timecode for an EssenceGroup correct?\n+            # TODO: Try CountChoices() and ChoiceAt(i)\n+            # For now, lets just skip it.\n+            continue\n+        mob = c.resolve_ref()\n+        if mob:\n+            mobs.append(mob)\n+\n+    return mobs\n+\n+\n def _extract_start_timecode(mob):\n     \"\"\"Given a mob with a single timecode slot, return the timecode in that\n     slot or None if no timecode slots could be found.",
        "comments": [
            {
                "comment": "I'm not sure I follow - if it encounters an `EssenceGroup` it just keeps going and skips over it? \r\n\r\nMaybe we could add a comment here explaining why they're being skipped.\r\n\r\nAlso, should the `['WARNING']` include some trace back to the AAF adapter?  Or is this metadata already inside the AAF sub dictionary?",
                "position": null
            }
        ],
        "commit_message": "Pulled timecode walk into a separate function.\nAdded comments explaining the state of our support for EssenceGroups.",
        "commit_id": "5357f7007a3a867eb80b172ff10dc5e2775faf11",
        "fn_context": "def _transcribe_property(prop):\n    # XXX: The unicode type doesn't exist in Python 3 (all strings are unicode)\n    # so we have to use type(u\"\") which works in both Python 2 and 3.\n    if type(prop) in (str, type(u\"\"), int, float, bool):\n        return prop\n\n    elif isinstance(prop, aaf.iterator.PropValueResolveIter):\n        result = {}\n        for child in prop:\n            if hasattr(child, \"name\") and hasattr(child, \"value\"):\n                result[child.name] = _transcribe_property(child.value)\n            else:\n                # @TODO: There may be more properties that we might want also.\n                # If you want to see what is being skipped, turn on debug.\n                if debug:\n                    debug_message = \\\n                        \"Skipping unrecognized property: {} of parent {}\"\n                    print(debug_message.format(child, prop))\n        return result\n    elif hasattr(prop, \"properties\"):\n        result = {}\n        for child in prop.properties():\n            result[child.name] = _transcribe_property(child.value)\n        return result\n    elif isinstance(prop, aaf.iterator.PropItemIter):\n        result = {}\n        for child in prop:\n            result[child.name] = _transcribe_property(child.value)\n        return result\n    else:\n        return str(prop)\n\ndef _find_timecode_mobs(item):\n    mobs = [item.resolve_ref()]\n\n    for c in item.walk():\n        if isinstance(c, aaf.component.EssenceGroup):\n            # An EssenceGroup is a Segment that has one or more\n            # alternate choices, each of which represent different variations\n            # of one actual piece of content.\n            # According to the AAF Object Specification and Edit Protocol\n            # documents:\n            # \"Typically the different representations vary in essence format,\n            # compression, or frame size. The application is responsible for\n            # choosing the appropriate implementation of the essence.\"\n            # It also says they should all have the same length, but\n            # there might be nested Sequences inside which we're not attempting\n            # to handle here (yet). We'll need a concrete example to ensure\n            # we're doing the right thing.\n            # TODO: Is the Timecode for an EssenceGroup correct?\n            # TODO: Try CountChoices() and ChoiceAt(i)\n            # For now, lets just skip it.\n            continue\n        mob = c.resolve_ref()\n        if mob:\n            mobs.append(mob)\n\n    return mobs\n\ndef _extract_start_timecode(mob):\n    \"\"\"Given a mob with a single timecode slot, return the timecode in that\n    slot or None if no timecode slots could be found.\n    \"\"\"\n\n    tc_list = [\n        s.segment['Start'].value for s in mob.slots()\n        if s.segment.media_kind == 'Timecode'\n    ]\n\n    if len(tc_list) == 1:\n        return tc_list[0]\n    elif len(tc_list) > 1:\n        raise otio.exceptions.NotSupportedError(\n            \"Error: mob has more than one timecode slots, this is not\"\n            \" currently supported by the AAF adapter. found: {} slots, \"\n            \" mob name is: '{}'\".format(len(tc_list), mob.name)\n        )\n    else:\n        # tc_list is empty\n        return None"
    },
    {
        "pr_title": "Keyboard shortcuts for OTIOview",
        "pr_number": 302,
        "file_name": "opentimelineview/timeline_widget.py",
        "code_diff": "@@ -643,6 +643,11 @@\ndef _get_down_item(self, curSelectedItem):\n \n         return self.scene().itemAt(newPosition, QtGui.QTransform())\n \n+    def _deselect_all_items(self):\n+        if self.scene().selectedItems:\n+            for selectedItem in self.scene().selectedItems():\n+                selectedItem.setSelected(False)\n+\n     def _select_new_item(self, newSelectedItem):\n         # Check for text item\n         # Text item shouldn't be selected,",
        "comments": [
            {
                "comment": "Could you either put this into a `_deselect_all()` method or just comment that you're deselecting everything?",
                "position": null
            },
            {
                "comment": "The `newSelectedItem = ...` part seems the same in all three code paths, so if you pull it out of the if/else block then you'll only need it once.",
                "position": null
            },
            {
                "comment": "np! _deselect_all() sounds good to me.",
                "position": null
            },
            {
                "comment": "Yup, can take care of that as well.",
                "position": null
            },
            {
                "comment": "Hi, this is great and helps a lot, however I noticed this centerOn is being called on any keypress (even something like \"control\"). I wonder if `_get_new_item()` could return `None` for when a new item is not found and only call into `_select_new_item()` if a valid new item was returned. This would limit calls into this `_select_new_item()` method so we could skip the `centerOn()` and `_deselect_all()` code. .",
                "position": null
            },
            {
                "comment": "Good catch; didn't realize it was it was calling centerOn so frequently. Thanks!\r\n",
                "position": null
            },
            {
                "comment": "You could consider `ensureVisible()` instead of `centerOn()` but in trying it just now, the centering actually works pretty well.",
                "position": null
            }
        ],
        "commit_message": "fixed issues from code review",
        "commit_id": "09c1b3a82e3732987703c624b6e63e7cc29b0b00",
        "fn_context": "    def _get_down_item(self, curSelectedItem):\n        curItemXpos = curSelectedItem.pos().x()\n\n        if curSelectedItem.parentItem():\n            curTrackYpos = curSelectedItem.parentItem().pos().y()\n            newXpos = curItemXpos\n            newYpos = curTrackYpos + TRACK_HEIGHT\n\n            newSelectedItem = self.scene().itemAt(\n                                                  QtCore.QPointF(\n                                                                 newXpos,\n                                                                 newYpos\n                                                                 ),\n                                                  QtGui.QTransform()\n                                                  )\n\n            if not newSelectedItem or isinstance(newSelectedItem, Track):\n                newYpos = newYpos + TRANSITION_HEIGHT\n\n            if newYpos < TRACK_HEIGHT:\n                newYpos = TRACK_HEIGHT\n        else:\n            newXpos = curItemXpos\n            newYpos = MARKER_SIZE + TIME_SLIDER_HEIGHT + 1\n            newYpos = TIME_SLIDER_HEIGHT\n        newPosition = QtCore.QPointF(newXpos, newYpos)\n\n        return self.scene().itemAt(newPosition, QtGui.QTransform())\n\n    def _deselect_all_items(self):\n        if self.scene().selectedItems:\n            for selectedItem in self.scene().selectedItems():\n                selectedItem.setSelected(False)\n\n    def _select_new_item(self, newSelectedItem):\n        # Check for text item\n        # Text item shouldn't be selected,\n        # maybe a bug in the population of timeline.\n        if isinstance(newSelectedItem, QtWidgets.QGraphicsSimpleTextItem):\n            newSelectedItem = newSelectedItem.parentItem()\n\n        # Validate new item for edge cases\n        # If valid, set selected\n        if (\n            not isinstance(newSelectedItem, Track)\n            and newSelectedItem\n        ):\n            self._deselect_all_items()\n            newSelectedItem.setSelected(True)\n            self.centerOn(newSelectedItem)"
    },
    {
        "pr_title": "Keyboard shortcuts for OTIOview",
        "pr_number": 302,
        "file_name": "opentimelineview/timeline_widget.py",
        "code_diff": "@@ -656,9 +661,7 @@\ndef _select_new_item(self, newSelectedItem):\n             not isinstance(newSelectedItem, Track)\n             and newSelectedItem\n         ):\n-            if self.scene().selectedItems():\n-                for selectedItem in self.scene().selectedItems():\n-                    selectedItem.setSelected(False)\n+            self._deselect_all_items()\n             newSelectedItem.setSelected(True)\n             self.centerOn(newSelectedItem)",
        "comments": [
            {
                "comment": "Could you either put this into a `_deselect_all()` method or just comment that you're deselecting everything?",
                "position": null
            },
            {
                "comment": "The `newSelectedItem = ...` part seems the same in all three code paths, so if you pull it out of the if/else block then you'll only need it once.",
                "position": null
            },
            {
                "comment": "np! _deselect_all() sounds good to me.",
                "position": null
            },
            {
                "comment": "Yup, can take care of that as well.",
                "position": null
            },
            {
                "comment": "Hi, this is great and helps a lot, however I noticed this centerOn is being called on any keypress (even something like \"control\"). I wonder if `_get_new_item()` could return `None` for when a new item is not found and only call into `_select_new_item()` if a valid new item was returned. This would limit calls into this `_select_new_item()` method so we could skip the `centerOn()` and `_deselect_all()` code. .",
                "position": null
            },
            {
                "comment": "Good catch; didn't realize it was it was calling centerOn so frequently. Thanks!\r\n",
                "position": null
            },
            {
                "comment": "You could consider `ensureVisible()` instead of `centerOn()` but in trying it just now, the centering actually works pretty well.",
                "position": null
            }
        ],
        "commit_message": "fixed issues from code review",
        "commit_id": "09c1b3a82e3732987703c624b6e63e7cc29b0b00",
        "fn_context": "    def _select_new_item(self, newSelectedItem):\n        # Check for text item\n        # Text item shouldn't be selected,\n        # maybe a bug in the population of timeline.\n        if isinstance(newSelectedItem, QtWidgets.QGraphicsSimpleTextItem):\n            newSelectedItem = newSelectedItem.parentItem()\n\n        # Validate new item for edge cases\n        # If valid, set selected\n        if (\n            not isinstance(newSelectedItem, Track)\n            and newSelectedItem\n        ):\n            self._deselect_all_items()\n            newSelectedItem.setSelected(True)\n            self.centerOn(newSelectedItem)\n\n    def _get_new_item(self, key_event, curSelectedItem):\n        key = key_event.key()\n\n        if key in (\n               QtCore.Qt.Key_Left,\n               QtCore.Qt.Key_Right,\n               QtCore.Qt.Key_Up,\n               QtCore.Qt.Key_Down,\n               QtCore.Qt.Key_Return,\n               QtCore.Qt.Key_Enter\n               ):\n            if key == QtCore.Qt.Key_Left:\n                newSelectedItem = self._get_left_item(curSelectedItem)\n            elif key == QtCore.Qt.Key_Right:\n                newSelectedItem = self._get_right_item(curSelectedItem)\n            elif key == QtCore.Qt.Key_Up:\n                newSelectedItem = self._get_up_item(curSelectedItem)\n            elif key == QtCore.Qt.Key_Down:\n                newSelectedItem = self._get_down_item(curSelectedItem)\n            elif key in [QtCore.Qt.Key_Return, QtCore.Qt.Key_Return]:\n                if isinstance(curSelectedItem, NestedItem):\n                    curSelectedItem.keyPressEvent(key_event)\n                    newSelectedItem = None\n        else:\n            newSelectedItem = None\n\n        return newSelectedItem"
    },
    {
        "pr_title": "Keyboard shortcuts for OTIOview",
        "pr_number": 302,
        "file_name": "opentimelineview/timeline_widget.py",
        "code_diff": "@@ -684,9 +687,9 @@\ndef _get_new_item(self, key_event, curSelectedItem):\n             elif key in [QtCore.Qt.Key_Return, QtCore.Qt.Key_Return]:\n                 if isinstance(curSelectedItem, NestedItem):\n                     curSelectedItem.keyPressEvent(key_event)\n-                    newSelectedItem = curSelectedItem\n+                    newSelectedItem = None\n         else:\n-            newSelectedItem = curSelectedItem\n+            newSelectedItem = None\n \n         return newSelectedItem",
        "comments": [
            {
                "comment": "Could you either put this into a `_deselect_all()` method or just comment that you're deselecting everything?",
                "position": null
            },
            {
                "comment": "The `newSelectedItem = ...` part seems the same in all three code paths, so if you pull it out of the if/else block then you'll only need it once.",
                "position": null
            },
            {
                "comment": "np! _deselect_all() sounds good to me.",
                "position": null
            },
            {
                "comment": "Yup, can take care of that as well.",
                "position": null
            },
            {
                "comment": "Hi, this is great and helps a lot, however I noticed this centerOn is being called on any keypress (even something like \"control\"). I wonder if `_get_new_item()` could return `None` for when a new item is not found and only call into `_select_new_item()` if a valid new item was returned. This would limit calls into this `_select_new_item()` method so we could skip the `centerOn()` and `_deselect_all()` code. .",
                "position": null
            },
            {
                "comment": "Good catch; didn't realize it was it was calling centerOn so frequently. Thanks!\r\n",
                "position": null
            },
            {
                "comment": "You could consider `ensureVisible()` instead of `centerOn()` but in trying it just now, the centering actually works pretty well.",
                "position": null
            }
        ],
        "commit_message": "fixed issues from code review",
        "commit_id": "09c1b3a82e3732987703c624b6e63e7cc29b0b00",
        "fn_context": "    def _get_new_item(self, key_event, curSelectedItem):\n        key = key_event.key()\n\n        if key in (\n               QtCore.Qt.Key_Left,\n               QtCore.Qt.Key_Right,\n               QtCore.Qt.Key_Up,\n               QtCore.Qt.Key_Down,\n               QtCore.Qt.Key_Return,\n               QtCore.Qt.Key_Enter\n               ):\n            if key == QtCore.Qt.Key_Left:\n                newSelectedItem = self._get_left_item(curSelectedItem)\n            elif key == QtCore.Qt.Key_Right:\n                newSelectedItem = self._get_right_item(curSelectedItem)\n            elif key == QtCore.Qt.Key_Up:\n                newSelectedItem = self._get_up_item(curSelectedItem)\n            elif key == QtCore.Qt.Key_Down:\n                newSelectedItem = self._get_down_item(curSelectedItem)\n            elif key in [QtCore.Qt.Key_Return, QtCore.Qt.Key_Return]:\n                if isinstance(curSelectedItem, NestedItem):\n                    curSelectedItem.keyPressEvent(key_event)\n                    newSelectedItem = None\n        else:\n            newSelectedItem = None\n\n        return newSelectedItem\n\n    def keyPressEvent(self, key_event):\n        super(CompositionView, self).keyPressEvent(key_event)\n        self.setInteractive(True)\n\n        # No item selected, so select the first item\n        if len(self.scene().selectedItems()) <= 0:\n            newSelectedItem = self._get_first_item()\n        # Based on direction key, select new selected item\n        else:\n            curSelectedItem = self.scene().selectedItems()[0]\n\n            # Check to see if the current selected item is a rect item\n            # If current selected item is not a rect, then extra tests\n            # are needed.\n            if not isinstance(curSelectedItem, QtWidgets.QGraphicsRectItem):\n                if curSelectedItem.parentItem():\n                    curSelectedItem = curSelectedItem.parentItem()\n\n            newSelectedItem = self._get_new_item(key_event, curSelectedItem)\n\n        if newSelectedItem:\n            self._select_new_item(newSelectedItem)"
    },
    {
        "pr_title": "Add convienence constructor for Gap",
        "pr_number": 286,
        "file_name": "opentimelineio/schema/gap.py",
        "code_diff": "@@ -38,7 +38,11 @@\nclass Gap(core.Item):\n     def __init__(\n         self,\n         name=None,\n-        # note - only one of the following two is accepted\n+        # note - only one of the following two arguments is accepted\n+        # if neither is provided, source_range will be set to an empty\n+        # TimeRange\n+        # Duration is provided as a convienence for creating a gap of a certain\n+        # length.  IE: Gap(duration=otio.opentime.RationalTime(300, 24))\n         duration=None,\n         source_range=None,\n         effects=None,",
        "comments": [
            {
                "comment": "Super super minor, but perhaps add \"two\" (\"only one of the following two..\"). At first read I thought this applied to effects/markers/etc as well. ",
                "position": null
            },
            {
                "comment": "I agree.",
                "position": null
            }
        ],
        "commit_message": "Clarify documentation in Gap.",
        "commit_id": "d56ebad77115b428ac3779cc25d241848b91f15b",
        "fn_context": "    def __init__(\n        self,\n        name=None,\n        # note - only one of the following two arguments is accepted\n        # if neither is provided, source_range will be set to an empty\n        # TimeRange\n        # Duration is provided as a convienence for creating a gap of a certain\n        # length.  IE: Gap(duration=otio.opentime.RationalTime(300, 24))\n        duration=None,\n        source_range=None,\n        effects=None,\n        markers=None,\n        metadata=None,\n    ):\n        if duration and source_range:\n            raise RuntimeError(\n                \"Cannot instantiate with both a source range and a duration.\"\n            )\n\n        if duration:\n            source_range = opentime.TimeRange(\n                opentime.RationalTime(0, duration.rate),\n                duration\n            )\n        elif source_range is None:\n            # if neither is provided, seed TimeRange as an empty Source Range.\n            source_range = opentime.TimeRange()\n\n        core.Item.__init__(\n            self,\n            name=name,\n            source_range=source_range,\n            effects=effects,\n            markers=markers,\n            metadata=metadata\n        )"
    },
    {
        "pr_title": "AAF effects and composition fixes",
        "pr_number": 260,
        "file_name": "opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -32,15 +32,42 @@\nimport opentimelineio as otio\n \n SAMPLE_DATA_DIR = os.path.join(os.path.dirname(__file__), \"sample_data\")\n-EXAMPLE_PATH = os.path.join(SAMPLE_DATA_DIR, \"simple.aaf\")\n-EXAMPLE_PATH2 = os.path.join(SAMPLE_DATA_DIR, \"transitions.aaf\")\n-EXAMPLE_PATH3 = os.path.join(SAMPLE_DATA_DIR, \"trims.aaf\")\n-EXAMPLE_PATH4 = os.path.join(SAMPLE_DATA_DIR, \"multitrack.aaf\")\n-EXAMPLE_PATH5 = os.path.join(SAMPLE_DATA_DIR, \"preflattened.aaf\")\n-EXAMPLE_PATH6 = os.path.join(SAMPLE_DATA_DIR, \"nesting_test.aaf\")\n-EXAMPLE_PATH7 = os.path.join(SAMPLE_DATA_DIR, \"nesting_test_preflattened.aaf\")\n-EXAMPLE_PATH8 = os.path.join(SAMPLE_DATA_DIR, \"misc_speed_effects.aaf\")\n-EXAMPLE_PATH9 = os.path.join(SAMPLE_DATA_DIR, \"linear_speed_effects.aaf\")\n+SIMPLE_EXAMPLE_PATH = os.path.join(\n+    SAMPLE_DATA_DIR,\n+    \"simple.aaf\"\n+)\n+TRANSITIONS_EXAMPLE_PATH = os.path.join(\n+    SAMPLE_DATA_DIR,\n+    \"transitions.aaf\"\n+)\n+TRIMS_EXAMPLE_PATH = os.path.join(\n+    SAMPLE_DATA_DIR,\n+    \"trims.aaf\"\n+)\n+MULTITRACK_EXAMPLE_PATH = os.path.join(\n+    SAMPLE_DATA_DIR,\n+    \"multitrack.aaf\"\n+)\n+PREFLATTENED_EXAMPLE_PATH = os.path.join(\n+    SAMPLE_DATA_DIR,\n+    \"preflattened.aaf\"\n+)\n+NESTING_EXAMPLE_PATH = os.path.join(\n+    SAMPLE_DATA_DIR,\n+    \"nesting_test.aaf\"\n+)\n+NESTING_PREFLATTENED_EXAMPLE_PATH = os.path.join(\n+    SAMPLE_DATA_DIR,\n+    \"nesting_test_preflattened.aaf\"\n+)\n+MISC_SPEED_EFFECTS_EXAMPLE_PATH = os.path.join(\n+    SAMPLE_DATA_DIR,\n+    \"misc_speed_effects.aaf\"\n+)\n+LINEAR_SPEED_EFFECTS_EXAMPLE_PATH = os.path.join(\n+    SAMPLE_DATA_DIR,\n+    \"linear_speed_effects.aaf\"\n+)\n \n \n try:",
        "comments": [
            {
                "comment": "Could just collapse these example paths into a list comprehension, rather than a list of numerated examples.",
                "position": null
            },
            {
                "comment": "Hmmm... how about if I name them better than 1,2,3,4...",
                "position": null
            },
            {
                "comment": "If you put them into a list you can use a comprehension to do the path prefix.  Either way!",
                "position": null
            }
        ],
        "commit_message": "Addressed code review notes.",
        "commit_id": "ce8e3f58f747febf3e112396c3c00cfd3c020de1",
        "fn_context": ""
    },
    {
        "pr_title": "AAF effects and composition fixes",
        "pr_number": 260,
        "file_name": "opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -59,7 +86,7 @@\nclass AAFAdapterTest(unittest.TestCase):\n \n     def test_aaf_read(self):\n-        aaf_path = EXAMPLE_PATH\n+        aaf_path = SIMPLE_EXAMPLE_PATH\n         timeline = otio.adapters.read_from_file(aaf_path)\n         self.assertEqual(timeline.name, \"OTIO TEST 1.Exported.01\")\n         fps = timeline.duration().rate",
        "comments": [
            {
                "comment": "Could just collapse these example paths into a list comprehension, rather than a list of numerated examples.",
                "position": null
            },
            {
                "comment": "Hmmm... how about if I name them better than 1,2,3,4...",
                "position": null
            },
            {
                "comment": "If you put them into a list you can use a comprehension to do the path prefix.  Either way!",
                "position": null
            }
        ],
        "commit_message": "Addressed code review notes.",
        "commit_id": "ce8e3f58f747febf3e112396c3c00cfd3c020de1",
        "fn_context": "    def test_aaf_read(self):\n        aaf_path = SIMPLE_EXAMPLE_PATH\n        timeline = otio.adapters.read_from_file(aaf_path)\n        self.assertEqual(timeline.name, \"OTIO TEST 1.Exported.01\")\n        fps = timeline.duration().rate\n        self.assertEqual(fps, 24.0)\n        self.assertEqual(\n            timeline.duration(),\n            otio.opentime.from_timecode(\"00:02:16:18\", fps)\n        )\n\n        self.assertEqual(len(timeline.tracks), 3)\n\n        self.assertEqual(len(timeline.video_tracks()), 1)\n        video_track = timeline.video_tracks()[0]\n        self.assertEqual(len(video_track), 5)\n\n        self.assertEqual(len(timeline.audio_tracks()), 2)\n\n        clips = list(video_track.each_clip())\n\n        self.assertEqual(\n            [\n                \"tech.fux (loop)-HD.mp4\",\n                \"t-hawk (loop)-HD.mp4\",\n                \"out-b (loop)-HD.mp4\",\n                \"KOLL-HD.mp4\",\n                \"brokchrd (loop)-HD.mp4\"\n            ],\n            [clip.name for clip in clips]\n        )\n        self.maxDiff = None\n        self.assertEqual(\n            [clip.source_range for clip in clips],\n            [\n                otio.opentime.TimeRange(\n                    otio.opentime.from_timecode(\"00:00:00:00\", fps),\n                    otio.opentime.from_timecode(\"00:00:30:00\", fps)\n                ),\n                otio.opentime.TimeRange(\n                    otio.opentime.from_timecode(\"00:00:00:00\", fps),\n                    otio.opentime.from_timecode(\"00:00:20:00\", fps)\n                ),\n                otio.opentime.TimeRange(\n                    otio.opentime.from_timecode(\"00:00:00:00\", fps),\n                    otio.opentime.from_timecode(\"00:00:30:02\", fps)\n                ),\n                otio.opentime.TimeRange(\n                    otio.opentime.from_timecode(\"00:00:00:00\", fps),\n                    otio.opentime.from_timecode(\"00:00:26:16\", fps)\n                ),\n                otio.opentime.TimeRange(\n                    otio.opentime.from_timecode(\"00:00:00:00\", fps),\n                    otio.opentime.from_timecode(\"00:00:30:00\", fps)\n                )\n            ]\n        )"
    },
    {
        "pr_title": "AAF effects and composition fixes",
        "pr_number": 260,
        "file_name": "opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -117,7 +144,7 @@\ndef test_aaf_read(self):\n         )\n \n     def test_aaf_simplify(self):\n-        aaf_path = EXAMPLE_PATH\n+        aaf_path = SIMPLE_EXAMPLE_PATH\n         timeline = otio.adapters.read_from_file(aaf_path, simplify=True)\n         self.assertIsNotNone(timeline)\n         self.assertEqual(type(timeline), otio.schema.Timeline)",
        "comments": [
            {
                "comment": "Could just collapse these example paths into a list comprehension, rather than a list of numerated examples.",
                "position": null
            },
            {
                "comment": "Hmmm... how about if I name them better than 1,2,3,4...",
                "position": null
            },
            {
                "comment": "If you put them into a list you can use a comprehension to do the path prefix.  Either way!",
                "position": null
            }
        ],
        "commit_message": "Addressed code review notes.",
        "commit_id": "ce8e3f58f747febf3e112396c3c00cfd3c020de1",
        "fn_context": "    def test_aaf_read(self):\n        aaf_path = SIMPLE_EXAMPLE_PATH\n        timeline = otio.adapters.read_from_file(aaf_path)\n        self.assertEqual(timeline.name, \"OTIO TEST 1.Exported.01\")\n        fps = timeline.duration().rate\n        self.assertEqual(fps, 24.0)\n        self.assertEqual(\n            timeline.duration(),\n            otio.opentime.from_timecode(\"00:02:16:18\", fps)\n        )\n\n        self.assertEqual(len(timeline.tracks), 3)\n\n        self.assertEqual(len(timeline.video_tracks()), 1)\n        video_track = timeline.video_tracks()[0]\n        self.assertEqual(len(video_track), 5)\n\n        self.assertEqual(len(timeline.audio_tracks()), 2)\n\n        clips = list(video_track.each_clip())\n\n        self.assertEqual(\n            [\n                \"tech.fux (loop)-HD.mp4\",\n                \"t-hawk (loop)-HD.mp4\",\n                \"out-b (loop)-HD.mp4\",\n                \"KOLL-HD.mp4\",\n                \"brokchrd (loop)-HD.mp4\"\n            ],\n            [clip.name for clip in clips]\n        )\n        self.maxDiff = None\n        self.assertEqual(\n            [clip.source_range for clip in clips],\n            [\n                otio.opentime.TimeRange(\n                    otio.opentime.from_timecode(\"00:00:00:00\", fps),\n                    otio.opentime.from_timecode(\"00:00:30:00\", fps)\n                ),\n                otio.opentime.TimeRange(\n                    otio.opentime.from_timecode(\"00:00:00:00\", fps),\n                    otio.opentime.from_timecode(\"00:00:20:00\", fps)\n                ),\n                otio.opentime.TimeRange(\n                    otio.opentime.from_timecode(\"00:00:00:00\", fps),\n                    otio.opentime.from_timecode(\"00:00:30:02\", fps)\n                ),\n                otio.opentime.TimeRange(\n                    otio.opentime.from_timecode(\"00:00:00:00\", fps),\n                    otio.opentime.from_timecode(\"00:00:26:16\", fps)\n                ),\n                otio.opentime.TimeRange(\n                    otio.opentime.from_timecode(\"00:00:00:00\", fps),\n                    otio.opentime.from_timecode(\"00:00:30:00\", fps)\n                )\n            ]\n        )\n\n    def test_aaf_simplify(self):\n        aaf_path = SIMPLE_EXAMPLE_PATH\n        timeline = otio.adapters.read_from_file(aaf_path, simplify=True)\n        self.assertIsNotNone(timeline)\n        self.assertEqual(type(timeline), otio.schema.Timeline)\n        self.assertEqual(timeline.name, \"OTIO TEST 1.Exported.01\")\n        fps = timeline.duration().rate\n        self.assertEqual(fps, 24.0)\n        self.assertEqual(\n            timeline.duration(),\n            otio.opentime.from_timecode(\"00:02:16:18\", fps)\n        )\n        self.assertEqual(len(timeline.tracks), 3)\n        self.assertEqual(otio.schema.TrackKind.Video, timeline.tracks[0].kind)\n        self.assertEqual(otio.schema.TrackKind.Audio, timeline.tracks[1].kind)\n        self.assertEqual(otio.schema.TrackKind.Audio, timeline.tracks[2].kind)\n        for track in timeline.tracks:\n            self.assertEqual(len(track), 5)"
    },
    {
        "pr_title": "AAF effects and composition fixes",
        "pr_number": 260,
        "file_name": "opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -136,7 +163,7 @@\ndef test_aaf_simplify(self):\n             self.assertEqual(len(track), 5)\n \n     def test_aaf_no_simplify(self):\n-        aaf_path = EXAMPLE_PATH\n+        aaf_path = SIMPLE_EXAMPLE_PATH\n         collection = otio.adapters.read_from_file(aaf_path, simplify=False)\n         self.assertIsNotNone(collection)\n         self.assertEqual(type(collection), otio.schema.SerializableCollection)",
        "comments": [
            {
                "comment": "Could just collapse these example paths into a list comprehension, rather than a list of numerated examples.",
                "position": null
            },
            {
                "comment": "Hmmm... how about if I name them better than 1,2,3,4...",
                "position": null
            },
            {
                "comment": "If you put them into a list you can use a comprehension to do the path prefix.  Either way!",
                "position": null
            }
        ],
        "commit_message": "Addressed code review notes.",
        "commit_id": "ce8e3f58f747febf3e112396c3c00cfd3c020de1",
        "fn_context": "    def test_aaf_simplify(self):\n        aaf_path = SIMPLE_EXAMPLE_PATH\n        timeline = otio.adapters.read_from_file(aaf_path, simplify=True)\n        self.assertIsNotNone(timeline)\n        self.assertEqual(type(timeline), otio.schema.Timeline)\n        self.assertEqual(timeline.name, \"OTIO TEST 1.Exported.01\")\n        fps = timeline.duration().rate\n        self.assertEqual(fps, 24.0)\n        self.assertEqual(\n            timeline.duration(),\n            otio.opentime.from_timecode(\"00:02:16:18\", fps)\n        )\n        self.assertEqual(len(timeline.tracks), 3)\n        self.assertEqual(otio.schema.TrackKind.Video, timeline.tracks[0].kind)\n        self.assertEqual(otio.schema.TrackKind.Audio, timeline.tracks[1].kind)\n        self.assertEqual(otio.schema.TrackKind.Audio, timeline.tracks[2].kind)\n        for track in timeline.tracks:\n            self.assertEqual(len(track), 5)\n\n    def test_aaf_no_simplify(self):\n        aaf_path = SIMPLE_EXAMPLE_PATH\n        collection = otio.adapters.read_from_file(aaf_path, simplify=False)\n        self.assertIsNotNone(collection)\n        self.assertEqual(type(collection), otio.schema.SerializableCollection)\n        self.assertEqual(len(collection), 1)\n\n        timeline = collection[0]\n        self.assertEqual(timeline.name, \"OTIO TEST 1.Exported.01\")\n        fps = timeline.duration().rate\n        self.assertEqual(fps, 24.0)\n        self.assertEqual(\n            timeline.duration(),\n            otio.opentime.from_timecode(\"00:02:16:18\", fps)\n        )\n\n        self.assertEqual(len(timeline.tracks), 12)\n\n        video_track = timeline.tracks[8][0]\n        self.assertEqual(otio.schema.TrackKind.Video, video_track.kind)\n        self.assertEqual(len(video_track), 5)"
    },
    {
        "pr_title": "AAF effects and composition fixes",
        "pr_number": 260,
        "file_name": "opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -158,7 +185,7 @@\ndef test_aaf_no_simplify(self):\n         self.assertEqual(len(video_track), 5)\n \n     def test_aaf_read_trims(self):\n-        aaf_path = EXAMPLE_PATH3\n+        aaf_path = TRIMS_EXAMPLE_PATH\n         timeline = otio.adapters.read_from_file(aaf_path)\n         self.assertEqual(\n             timeline.name,",
        "comments": [
            {
                "comment": "Could just collapse these example paths into a list comprehension, rather than a list of numerated examples.",
                "position": null
            },
            {
                "comment": "Hmmm... how about if I name them better than 1,2,3,4...",
                "position": null
            },
            {
                "comment": "If you put them into a list you can use a comprehension to do the path prefix.  Either way!",
                "position": null
            }
        ],
        "commit_message": "Addressed code review notes.",
        "commit_id": "ce8e3f58f747febf3e112396c3c00cfd3c020de1",
        "fn_context": "    def test_aaf_no_simplify(self):\n        aaf_path = SIMPLE_EXAMPLE_PATH\n        collection = otio.adapters.read_from_file(aaf_path, simplify=False)\n        self.assertIsNotNone(collection)\n        self.assertEqual(type(collection), otio.schema.SerializableCollection)\n        self.assertEqual(len(collection), 1)\n\n        timeline = collection[0]\n        self.assertEqual(timeline.name, \"OTIO TEST 1.Exported.01\")\n        fps = timeline.duration().rate\n        self.assertEqual(fps, 24.0)\n        self.assertEqual(\n            timeline.duration(),\n            otio.opentime.from_timecode(\"00:02:16:18\", fps)\n        )\n\n        self.assertEqual(len(timeline.tracks), 12)\n\n        video_track = timeline.tracks[8][0]\n        self.assertEqual(otio.schema.TrackKind.Video, video_track.kind)\n        self.assertEqual(len(video_track), 5)\n\n    def test_aaf_read_trims(self):\n        aaf_path = TRIMS_EXAMPLE_PATH\n        timeline = otio.adapters.read_from_file(aaf_path)\n        self.assertEqual(\n            timeline.name,\n            \"OTIO TEST 1.Exported.01 - trims.Exported.02\"\n        )\n        fps = timeline.duration().rate\n        self.assertEqual(fps, 24.0)\n\n        video_tracks = timeline.video_tracks()\n        self.assertEqual(len(video_tracks), 1)\n        video_track = video_tracks[0]\n        self.assertEqual(len(video_track), 6)\n\n        self.assertEqual(\n            [type(item) for item in video_track],\n            [\n                otio.schema.Clip,\n                otio.schema.Clip,\n                otio.schema.Clip,\n                otio.schema.Clip,\n                otio.schema.Gap,\n                otio.schema.Clip,\n            ]\n        )\n\n        clips = list(video_track.each_clip())\n\n        self.assertEqual(\n            [item.name for item in video_track],\n            [\n                \"tech.fux (loop)-HD.mp4\",\n                \"t-hawk (loop)-HD.mp4\",\n                \"out-b (loop)-HD.mp4\",\n                \"KOLL-HD.mp4\",\n                \"Filler\",   # Gap\n                \"brokchrd (loop)-HD.mp4\"\n            ]\n        )\n\n        self.maxDiff = None\n        desired_ranges = [\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(0, fps),\n                otio.opentime.from_frames(720-0, fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(121, fps),\n                otio.opentime.from_frames(480-121, fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(123, fps),\n                otio.opentime.from_frames(523-123, fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(0, fps),\n                otio.opentime.from_frames(559-0, fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(69, fps),\n                otio.opentime.from_frames(720-69, fps)\n            )\n        ]\n        for clip, desired in zip(clips, desired_ranges):\n            actual = clip.source_range\n            self.assertEqual(\n                actual,\n                desired,\n                \"clip '{}' source_range should be {} not {}\".format(\n                    clip.name,\n                    desired,\n                    actual\n                )\n            )\n\n        desired_ranges = [\n            otio.opentime.TimeRange(\n                otio.opentime.from_timecode(\"00:00:00:00\", fps),\n                otio.opentime.from_timecode(\"00:00:30:00\", fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_timecode(\"00:00:30:00\", fps),\n                otio.opentime.from_timecode(\"00:00:14:23\", fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_timecode(\"00:00:44:23\", fps),\n                otio.opentime.from_timecode(\"00:00:16:16\", fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_timecode(\"00:01:01:15\", fps),\n                otio.opentime.from_timecode(\"00:00:23:07\", fps)\n            ),\n            otio.opentime.TimeRange(    # Gap\n                otio.opentime.from_timecode(\"00:01:24:22\", fps),\n                otio.opentime.from_timecode(\"00:00:04:12\", fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_timecode(\"00:01:29:10\", fps),\n                otio.opentime.from_timecode(\"00:00:27:03\", fps)\n            )\n        ]\n        for item, desired in zip(video_track, desired_ranges):\n            actual = item.trimmed_range_in_parent()\n            self.assertEqual(\n                actual,\n                desired,\n                \"item '{}' trimmed_range_in_parent should be {} not {}\".format(\n                    clip.name,\n                    desired,\n                    actual\n                )\n            )\n\n        self.assertEqual(\n            timeline.duration(),\n            otio.opentime.from_timecode(\"00:01:56:13\", fps)\n        )"
    },
    {
        "pr_title": "AAF effects and composition fixes",
        "pr_number": 260,
        "file_name": "opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -277,7 +304,7 @@\ndef test_aaf_read_trims(self):\n         )\n \n     def test_aaf_read_transitions(self):\n-        aaf_path = EXAMPLE_PATH2\n+        aaf_path = TRANSITIONS_EXAMPLE_PATH\n         timeline = otio.adapters.read_from_file(aaf_path)\n         self.assertEqual(timeline.name, \"OTIO TEST - transitions.Exported.01\")\n         fps = timeline.duration().rate",
        "comments": [
            {
                "comment": "Could just collapse these example paths into a list comprehension, rather than a list of numerated examples.",
                "position": null
            },
            {
                "comment": "Hmmm... how about if I name them better than 1,2,3,4...",
                "position": null
            },
            {
                "comment": "If you put them into a list you can use a comprehension to do the path prefix.  Either way!",
                "position": null
            }
        ],
        "commit_message": "Addressed code review notes.",
        "commit_id": "ce8e3f58f747febf3e112396c3c00cfd3c020de1",
        "fn_context": "    def test_aaf_read_trims(self):\n        aaf_path = TRIMS_EXAMPLE_PATH\n        timeline = otio.adapters.read_from_file(aaf_path)\n        self.assertEqual(\n            timeline.name,\n            \"OTIO TEST 1.Exported.01 - trims.Exported.02\"\n        )\n        fps = timeline.duration().rate\n        self.assertEqual(fps, 24.0)\n\n        video_tracks = timeline.video_tracks()\n        self.assertEqual(len(video_tracks), 1)\n        video_track = video_tracks[0]\n        self.assertEqual(len(video_track), 6)\n\n        self.assertEqual(\n            [type(item) for item in video_track],\n            [\n                otio.schema.Clip,\n                otio.schema.Clip,\n                otio.schema.Clip,\n                otio.schema.Clip,\n                otio.schema.Gap,\n                otio.schema.Clip,\n            ]\n        )\n\n        clips = list(video_track.each_clip())\n\n        self.assertEqual(\n            [item.name for item in video_track],\n            [\n                \"tech.fux (loop)-HD.mp4\",\n                \"t-hawk (loop)-HD.mp4\",\n                \"out-b (loop)-HD.mp4\",\n                \"KOLL-HD.mp4\",\n                \"Filler\",   # Gap\n                \"brokchrd (loop)-HD.mp4\"\n            ]\n        )\n\n        self.maxDiff = None\n        desired_ranges = [\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(0, fps),\n                otio.opentime.from_frames(720-0, fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(121, fps),\n                otio.opentime.from_frames(480-121, fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(123, fps),\n                otio.opentime.from_frames(523-123, fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(0, fps),\n                otio.opentime.from_frames(559-0, fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(69, fps),\n                otio.opentime.from_frames(720-69, fps)\n            )\n        ]\n        for clip, desired in zip(clips, desired_ranges):\n            actual = clip.source_range\n            self.assertEqual(\n                actual,\n                desired,\n                \"clip '{}' source_range should be {} not {}\".format(\n                    clip.name,\n                    desired,\n                    actual\n                )\n            )\n\n        desired_ranges = [\n            otio.opentime.TimeRange(\n                otio.opentime.from_timecode(\"00:00:00:00\", fps),\n                otio.opentime.from_timecode(\"00:00:30:00\", fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_timecode(\"00:00:30:00\", fps),\n                otio.opentime.from_timecode(\"00:00:14:23\", fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_timecode(\"00:00:44:23\", fps),\n                otio.opentime.from_timecode(\"00:00:16:16\", fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_timecode(\"00:01:01:15\", fps),\n                otio.opentime.from_timecode(\"00:00:23:07\", fps)\n            ),\n            otio.opentime.TimeRange(    # Gap\n                otio.opentime.from_timecode(\"00:01:24:22\", fps),\n                otio.opentime.from_timecode(\"00:00:04:12\", fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_timecode(\"00:01:29:10\", fps),\n                otio.opentime.from_timecode(\"00:00:27:03\", fps)\n            )\n        ]\n        for item, desired in zip(video_track, desired_ranges):\n            actual = item.trimmed_range_in_parent()\n            self.assertEqual(\n                actual,\n                desired,\n                \"item '{}' trimmed_range_in_parent should be {} not {}\".format(\n                    clip.name,\n                    desired,\n                    actual\n                )\n            )\n\n        self.assertEqual(\n            timeline.duration(),\n            otio.opentime.from_timecode(\"00:01:56:13\", fps)\n        )\n\n    def test_aaf_read_transitions(self):\n        aaf_path = TRANSITIONS_EXAMPLE_PATH\n        timeline = otio.adapters.read_from_file(aaf_path)\n        self.assertEqual(timeline.name, \"OTIO TEST - transitions.Exported.01\")\n        fps = timeline.duration().rate\n        self.assertEqual(fps, 24.0)\n\n        video_tracks = timeline.video_tracks()\n        self.assertEqual(len(video_tracks), 1)\n        video_track = video_tracks[0]\n        self.assertEqual(len(video_track), 12)\n\n        clips = list(video_track.each_clip())\n        self.assertEqual(len(clips), 4)\n\n        self.assertEqual(\n            [type(item) for item in video_track],\n            [\n                otio.schema.Gap,\n                otio.schema.Transition,\n                otio.schema.Clip,\n                otio.schema.Transition,\n                otio.schema.Clip,\n                otio.schema.Transition,\n                otio.schema.Gap,\n                otio.schema.Transition,\n                otio.schema.Clip,\n                otio.schema.Clip,\n                otio.schema.Transition,\n                otio.schema.Gap,\n            ]\n        )\n\n        self.assertEqual(\n            [item.name for item in video_track],\n            [\n                \"Filler\",\n                \"Transition\",\n                \"tech.fux (loop)-HD.mp4\",\n                \"Transition\",\n                \"t-hawk (loop)-HD.mp4\",\n                \"Transition\",\n                \"Filler\",\n                \"Transition\",\n                \"KOLL-HD.mp4\",\n                \"brokchrd (loop)-HD.mp4\",\n                \"Transition\",\n                \"Filler\"\n            ]\n        )\n\n        self.maxDiff = None\n        desired_ranges = [\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(0, fps),\n                otio.opentime.from_frames(117, fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(123, fps),\n                otio.opentime.from_frames(200-123, fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(55, fps),\n                otio.opentime.from_frames(199-55, fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(0, fps),\n                otio.opentime.from_frames(130, fps)\n            )\n        ]\n        for clip, desired in zip(clips, desired_ranges):\n            actual = clip.source_range\n            self.assertEqual(\n                actual,\n                desired,\n                \"clip '{}' source_range should be {} not {}\".format(\n                    clip.name,\n                    desired,\n                    actual\n                )\n            )\n\n        desired_ranges = [\n            otio.opentime.TimeRange(    # Gap\n                otio.opentime.from_timecode(\"00:00:00:00\", fps),\n                otio.opentime.from_timecode(\"00:00:00:00\", fps)\n            ),\n            otio.opentime.TimeRange(    # Transition\n                otio.opentime.from_timecode(\"00:00:00:00\", fps),\n                otio.opentime.from_timecode(\"00:00:00:12\", fps)\n            ),\n            otio.opentime.TimeRange(    # tech.fux\n                otio.opentime.from_timecode(\"00:00:00:00\", fps),\n                otio.opentime.from_timecode(\"00:00:04:21\", fps)\n            ),\n            otio.opentime.TimeRange(    # Transition\n                otio.opentime.from_timecode(\"00:00:02:21\", fps),\n                otio.opentime.from_timecode(\"00:00:02:00\", fps)\n            ),\n            otio.opentime.TimeRange(    # t-hawk\n                otio.opentime.from_timecode(\"00:00:04:21\", fps),\n                otio.opentime.from_timecode(\"00:00:03:05\", fps)\n            ),\n            otio.opentime.TimeRange(    # Transition\n                otio.opentime.from_timecode(\"00:00:07:14\", fps),\n                otio.opentime.from_timecode(\"00:00:01:00\", fps)\n            ),\n            otio.opentime.TimeRange(    # Gap\n                otio.opentime.from_timecode(\"00:00:08:02\", fps),\n                otio.opentime.from_timecode(\"00:00:02:05\", fps)\n            ),\n            otio.opentime.TimeRange(    # Transition\n                otio.opentime.from_timecode(\"00:00:09:07\", fps),\n                otio.opentime.from_timecode(\"00:00:02:00\", fps)\n            ),\n            otio.opentime.TimeRange(    # KOLL-HD\n                otio.opentime.from_timecode(\"00:00:10:07\", fps),\n                otio.opentime.from_timecode(\"00:00:06:00\", fps)\n            ),\n            otio.opentime.TimeRange(    # brokchrd\n                otio.opentime.from_timecode(\"00:00:16:07\", fps),\n                otio.opentime.from_timecode(\"00:00:05:10\", fps)\n            ),\n            otio.opentime.TimeRange(    # Transition\n                otio.opentime.from_timecode(\"00:00:19:17\", fps),\n                otio.opentime.from_timecode(\"00:00:02:00\", fps)\n            ),\n            otio.opentime.TimeRange(    # Gap\n                otio.opentime.from_timecode(\"00:00:21:17\", fps),\n                otio.opentime.from_timecode(\"00:00:00:00\", fps)\n            )\n        ]\n        for item, desired in zip(video_track, desired_ranges):\n            actual = item.trimmed_range_in_parent()\n            self.assertEqual(\n                desired,\n                actual,\n                \"item '{}' trimmed_range_in_parent should be {} not {}\".format(\n                    clip.name,\n                    desired,\n                    actual\n                )\n            )\n\n        self.assertEqual(\n            timeline.duration(),\n            otio.opentime.from_timecode(\"00:00:21:17\", fps)\n        )"
    },
    {
        "pr_title": "AAF effects and composition fixes",
        "pr_number": 260,
        "file_name": "opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -426,7 +453,7 @@\ndef test_aaf_read_transitions(self):\n         )\n \n     def test_aaf_user_comments(self):\n-        aaf_path = EXAMPLE_PATH3\n+        aaf_path = TRIMS_EXAMPLE_PATH\n         timeline = otio.adapters.read_from_file(aaf_path)\n         self.assertTrue(timeline is not None)\n         self.assertEqual(type(timeline), otio.schema.Timeline)",
        "comments": [
            {
                "comment": "Could just collapse these example paths into a list comprehension, rather than a list of numerated examples.",
                "position": null
            },
            {
                "comment": "Hmmm... how about if I name them better than 1,2,3,4...",
                "position": null
            },
            {
                "comment": "If you put them into a list you can use a comprehension to do the path prefix.  Either way!",
                "position": null
            }
        ],
        "commit_message": "Addressed code review notes.",
        "commit_id": "ce8e3f58f747febf3e112396c3c00cfd3c020de1",
        "fn_context": "    def test_aaf_read_transitions(self):\n        aaf_path = TRANSITIONS_EXAMPLE_PATH\n        timeline = otio.adapters.read_from_file(aaf_path)\n        self.assertEqual(timeline.name, \"OTIO TEST - transitions.Exported.01\")\n        fps = timeline.duration().rate\n        self.assertEqual(fps, 24.0)\n\n        video_tracks = timeline.video_tracks()\n        self.assertEqual(len(video_tracks), 1)\n        video_track = video_tracks[0]\n        self.assertEqual(len(video_track), 12)\n\n        clips = list(video_track.each_clip())\n        self.assertEqual(len(clips), 4)\n\n        self.assertEqual(\n            [type(item) for item in video_track],\n            [\n                otio.schema.Gap,\n                otio.schema.Transition,\n                otio.schema.Clip,\n                otio.schema.Transition,\n                otio.schema.Clip,\n                otio.schema.Transition,\n                otio.schema.Gap,\n                otio.schema.Transition,\n                otio.schema.Clip,\n                otio.schema.Clip,\n                otio.schema.Transition,\n                otio.schema.Gap,\n            ]\n        )\n\n        self.assertEqual(\n            [item.name for item in video_track],\n            [\n                \"Filler\",\n                \"Transition\",\n                \"tech.fux (loop)-HD.mp4\",\n                \"Transition\",\n                \"t-hawk (loop)-HD.mp4\",\n                \"Transition\",\n                \"Filler\",\n                \"Transition\",\n                \"KOLL-HD.mp4\",\n                \"brokchrd (loop)-HD.mp4\",\n                \"Transition\",\n                \"Filler\"\n            ]\n        )\n\n        self.maxDiff = None\n        desired_ranges = [\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(0, fps),\n                otio.opentime.from_frames(117, fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(123, fps),\n                otio.opentime.from_frames(200-123, fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(55, fps),\n                otio.opentime.from_frames(199-55, fps)\n            ),\n            otio.opentime.TimeRange(\n                otio.opentime.from_frames(0, fps),\n                otio.opentime.from_frames(130, fps)\n            )\n        ]\n        for clip, desired in zip(clips, desired_ranges):\n            actual = clip.source_range\n            self.assertEqual(\n                actual,\n                desired,\n                \"clip '{}' source_range should be {} not {}\".format(\n                    clip.name,\n                    desired,\n                    actual\n                )\n            )\n\n        desired_ranges = [\n            otio.opentime.TimeRange(    # Gap\n                otio.opentime.from_timecode(\"00:00:00:00\", fps),\n                otio.opentime.from_timecode(\"00:00:00:00\", fps)\n            ),\n            otio.opentime.TimeRange(    # Transition\n                otio.opentime.from_timecode(\"00:00:00:00\", fps),\n                otio.opentime.from_timecode(\"00:00:00:12\", fps)\n            ),\n            otio.opentime.TimeRange(    # tech.fux\n                otio.opentime.from_timecode(\"00:00:00:00\", fps),\n                otio.opentime.from_timecode(\"00:00:04:21\", fps)\n            ),\n            otio.opentime.TimeRange(    # Transition\n                otio.opentime.from_timecode(\"00:00:02:21\", fps),\n                otio.opentime.from_timecode(\"00:00:02:00\", fps)\n            ),\n            otio.opentime.TimeRange(    # t-hawk\n                otio.opentime.from_timecode(\"00:00:04:21\", fps),\n                otio.opentime.from_timecode(\"00:00:03:05\", fps)\n            ),\n            otio.opentime.TimeRange(    # Transition\n                otio.opentime.from_timecode(\"00:00:07:14\", fps),\n                otio.opentime.from_timecode(\"00:00:01:00\", fps)\n            ),\n            otio.opentime.TimeRange(    # Gap\n                otio.opentime.from_timecode(\"00:00:08:02\", fps),\n                otio.opentime.from_timecode(\"00:00:02:05\", fps)\n            ),\n            otio.opentime.TimeRange(    # Transition\n                otio.opentime.from_timecode(\"00:00:09:07\", fps),\n                otio.opentime.from_timecode(\"00:00:02:00\", fps)\n            ),\n            otio.opentime.TimeRange(    # KOLL-HD\n                otio.opentime.from_timecode(\"00:00:10:07\", fps),\n                otio.opentime.from_timecode(\"00:00:06:00\", fps)\n            ),\n            otio.opentime.TimeRange(    # brokchrd\n                otio.opentime.from_timecode(\"00:00:16:07\", fps),\n                otio.opentime.from_timecode(\"00:00:05:10\", fps)\n            ),\n            otio.opentime.TimeRange(    # Transition\n                otio.opentime.from_timecode(\"00:00:19:17\", fps),\n                otio.opentime.from_timecode(\"00:00:02:00\", fps)\n            ),\n            otio.opentime.TimeRange(    # Gap\n                otio.opentime.from_timecode(\"00:00:21:17\", fps),\n                otio.opentime.from_timecode(\"00:00:00:00\", fps)\n            )\n        ]\n        for item, desired in zip(video_track, desired_ranges):\n            actual = item.trimmed_range_in_parent()\n            self.assertEqual(\n                desired,\n                actual,\n                \"item '{}' trimmed_range_in_parent should be {} not {}\".format(\n                    clip.name,\n                    desired,\n                    actual\n                )\n            )\n\n        self.assertEqual(\n            timeline.duration(),\n            otio.opentime.from_timecode(\"00:00:21:17\", fps)\n        )\n\n    def test_aaf_user_comments(self):\n        aaf_path = TRIMS_EXAMPLE_PATH\n        timeline = otio.adapters.read_from_file(aaf_path)\n        self.assertTrue(timeline is not None)\n        self.assertEqual(type(timeline), otio.schema.Timeline)\n        self.assertIsNotNone(timeline.metadata.get(\"AAF\"))\n        correctWords = [\n            \"test1\",\n            \"testing 1 2 3\",\n            u\"Eyjafjallaj\\xf6kull\",\n            \"'s' \\\"d\\\" `b`\",\n            None,   # Gap\n            None\n        ]\n        for clip, correctWord in zip(timeline.tracks[0], correctWords):\n            if isinstance(clip, otio.schema.Gap):\n                continue\n            AAFmetadata = clip.media_reference.metadata.get(\"AAF\")\n            self.assertIsNotNone(AAFmetadata)\n            self.assertIsNotNone(AAFmetadata.get(\"UserComments\"))\n            self.assertEqual(\n                AAFmetadata.get(\"UserComments\").get(\"CustomTest\"),\n                correctWord\n            )"
    },
    {
        "pr_title": "AAF effects and composition fixes",
        "pr_number": 260,
        "file_name": "opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -451,8 +478,12 @@\ndef test_aaf_user_comments(self):\n             )\n \n     def test_aaf_flatten_tracks(self):\n-        multitrack_timeline = otio.adapters.read_from_file(EXAMPLE_PATH4)\n-        preflattened_timeline = otio.adapters.read_from_file(EXAMPLE_PATH5)\n+        multitrack_timeline = otio.adapters.read_from_file(\n+            MULTITRACK_EXAMPLE_PATH\n+        )\n+        preflattened_timeline = otio.adapters.read_from_file(\n+            PREFLATTENED_EXAMPLE_PATH\n+        )\n \n         # first make sure we got the structure we expected\n         self.assertEqual(3, len(preflattened_timeline.tracks))",
        "comments": [
            {
                "comment": "Could just collapse these example paths into a list comprehension, rather than a list of numerated examples.",
                "position": null
            },
            {
                "comment": "Hmmm... how about if I name them better than 1,2,3,4...",
                "position": null
            },
            {
                "comment": "If you put them into a list you can use a comprehension to do the path prefix.  Either way!",
                "position": null
            }
        ],
        "commit_message": "Addressed code review notes.",
        "commit_id": "ce8e3f58f747febf3e112396c3c00cfd3c020de1",
        "fn_context": "    def test_aaf_user_comments(self):\n        aaf_path = TRIMS_EXAMPLE_PATH\n        timeline = otio.adapters.read_from_file(aaf_path)\n        self.assertTrue(timeline is not None)\n        self.assertEqual(type(timeline), otio.schema.Timeline)\n        self.assertIsNotNone(timeline.metadata.get(\"AAF\"))\n        correctWords = [\n            \"test1\",\n            \"testing 1 2 3\",\n            u\"Eyjafjallaj\\xf6kull\",\n            \"'s' \\\"d\\\" `b`\",\n            None,   # Gap\n            None\n        ]\n        for clip, correctWord in zip(timeline.tracks[0], correctWords):\n            if isinstance(clip, otio.schema.Gap):\n                continue\n            AAFmetadata = clip.media_reference.metadata.get(\"AAF\")\n            self.assertIsNotNone(AAFmetadata)\n            self.assertIsNotNone(AAFmetadata.get(\"UserComments\"))\n            self.assertEqual(\n                AAFmetadata.get(\"UserComments\").get(\"CustomTest\"),\n                correctWord\n            )\n\n    def test_aaf_flatten_tracks(self):\n        multitrack_timeline = otio.adapters.read_from_file(\n            MULTITRACK_EXAMPLE_PATH\n        )\n        preflattened_timeline = otio.adapters.read_from_file(\n            PREFLATTENED_EXAMPLE_PATH\n        )\n\n        # first make sure we got the structure we expected\n        self.assertEqual(3, len(preflattened_timeline.tracks))\n        self.assertEqual(1, len(preflattened_timeline.video_tracks()))\n        self.assertEqual(2, len(preflattened_timeline.audio_tracks()))\n\n        self.assertEqual(3, len(multitrack_timeline.video_tracks()))\n        self.assertEqual(2, len(multitrack_timeline.audio_tracks()))\n        self.assertEqual(5, len(multitrack_timeline.tracks))\n\n\n        preflattened = preflattened_timeline.video_tracks()[0]\n        self.assertEqual(7, len(preflattened))\n        flattened = otio.algorithms.flatten_stack(\n            multitrack_timeline.video_tracks()\n        )\n        self.assertEqual(7, len(flattened))\n\n        # Lets remove some AAF metadata that will always be different\n        # so we can compare everything else.\n        for t in (preflattened, flattened):\n\n            t.name = None\n            t.metadata.pop(\"AAF\", None)\n\n            for c in t.each_child():\n                if hasattr(c, \"media_reference\") and c.media_reference:\n                    mr = c.media_reference\n                    mr.metadata.get(\"AAF\", {}).pop('LastModified', None)\n                meta = c.metadata.get(\"AAF\", {})\n                meta.pop('ComponentAttributeList', None)\n                meta.pop('DataDefinition', None)\n                meta.pop('Length', None)\n                meta.pop('StartTime', None)\n\n            # We don't care about Gap start times, only their duration matters\n            for g in t.each_child(descended_from_type=otio.schema.Gap):\n                g.source_range.start_time.value = 0\n\n        self.maxDiff = None\n        self.assertMultiLineEqual(\n            otio.adapters.write_to_string(preflattened, \"otio_json\"),\n            otio.adapters.write_to_string(flattened, \"otio_json\")\n        )"
    },
    {
        "pr_title": "AAF effects and composition fixes",
        "pr_number": 260,
        "file_name": "opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -500,7 +531,7 @@\ndef test_aaf_flatten_tracks(self):\n \n \n     def test_aaf_nesting(self):\n-        timeline = otio.adapters.read_from_file(EXAMPLE_PATH6)\n+        timeline = otio.adapters.read_from_file(NESTING_EXAMPLE_PATH)\n         self.assertEqual(1, len(timeline.tracks))\n         track = timeline.tracks[0]\n         self.assertEqual(3, len(track))",
        "comments": [
            {
                "comment": "Could just collapse these example paths into a list comprehension, rather than a list of numerated examples.",
                "position": null
            },
            {
                "comment": "Hmmm... how about if I name them better than 1,2,3,4...",
                "position": null
            },
            {
                "comment": "If you put them into a list you can use a comprehension to do the path prefix.  Either way!",
                "position": null
            }
        ],
        "commit_message": "Addressed code review notes.",
        "commit_id": "ce8e3f58f747febf3e112396c3c00cfd3c020de1",
        "fn_context": "    def test_aaf_nesting(self):\n        timeline = otio.adapters.read_from_file(NESTING_EXAMPLE_PATH)\n        self.assertEqual(1, len(timeline.tracks))\n        track = timeline.tracks[0]\n        self.assertEqual(3, len(track))\n\n        clipA, nested, clipB = track\n        self.assertEqual(otio.schema.Clip, type(clipA))\n        self.assertEqual(otio.schema.Track, type(nested))\n        self.assertEqual(otio.schema.Clip, type(clipB))\n\n        self.assertEqual(2, len(nested))\n        nestedClipA, nestedClipB = nested\n        self.assertEqual(otio.schema.Clip, type(nestedClipA))\n        self.assertEqual(otio.schema.Clip, type(nestedClipB))\n\n        self.assertEqual(\n            otio.opentime.TimeRange(\n                start_time=otio.opentime.RationalTime(24, 24),\n                duration=otio.opentime.RationalTime(16, 24)\n            ),\n            clipA.trimmed_range()\n        )\n        self.assertEqual(\n            otio.opentime.TimeRange(\n                start_time=otio.opentime.RationalTime(32, 24),\n                # TODO: should actually be this, but we're not getting the\n                # media timecode offset correctly from the AAF...\n                # start_time=otio.opentime.RationalTime(86432, 24),\n                duration=otio.opentime.RationalTime(16, 24)\n            ),\n            clipB.trimmed_range()\n        )\n\n        self.assertEqual(\n            otio.opentime.TimeRange(\n                start_time=otio.opentime.RationalTime(40, 24),\n                duration=otio.opentime.RationalTime(8, 24)\n            ),\n            nestedClipA.trimmed_range()\n        )\n        self.assertEqual(\n            otio.opentime.TimeRange(\n                start_time=otio.opentime.RationalTime(24, 24),\n                # TODO: should actually be this, but we're not getting the\n                # media timecode offset correctly from the AAF...\n                # start_time=otio.opentime.RationalTime(86424, 24),\n                duration=otio.opentime.RationalTime(8, 24)\n            ),\n            nestedClipB.trimmed_range()\n        )"
    },
    {
        "pr_title": "AAF effects and composition fixes",
        "pr_number": 260,
        "file_name": "opentimelineio_contrib/adapters/tests/test_aaf_adapter.py",
        "code_diff": "@@ -552,8 +583,25 @@\ndef test_aaf_nesting(self):\n         )\n \n \n+    # TODO: This belongs in the algorithms tests, not the AAF tests.\n+    def SKIP_test_nesting_flatten(self):\n+        nested_timeline = otio.adapters.read_from_file(\n+            NESTING_EXAMPLE_PATH\n+        )\n+        preflattened_timeline = otio.adapters.read_from_file(\n+            NESTING_PREFLATTENED_EXAMPLE_PATH\n+        )\n+        flattened_track = otio.algorithms.flatten_stack(nested_timeline.tracks)\n+        self.assertEqual(\n+            preflattened_timeline.tracks[0],\n+            flattened_track\n+        )\n+\n+\n     def test_read_linear_speed_effects(self):\n-        timeline = otio.adapters.read_from_file(EXAMPLE_PATH9)\n+        timeline = otio.adapters.read_from_file(\n+            LINEAR_SPEED_EFFECTS_EXAMPLE_PATH\n+        )\n         self.assertEqual(1, len(timeline.tracks))\n         track = timeline.tracks[0]\n         self.assertEqual(20, len(track))",
        "comments": [
            {
                "comment": "Could just collapse these example paths into a list comprehension, rather than a list of numerated examples.",
                "position": null
            },
            {
                "comment": "Hmmm... how about if I name them better than 1,2,3,4...",
                "position": null
            },
            {
                "comment": "If you put them into a list you can use a comprehension to do the path prefix.  Either way!",
                "position": null
            }
        ],
        "commit_message": "Addressed code review notes.",
        "commit_id": "ce8e3f58f747febf3e112396c3c00cfd3c020de1",
        "fn_context": "    def test_aaf_nesting(self):\n        timeline = otio.adapters.read_from_file(NESTING_EXAMPLE_PATH)\n        self.assertEqual(1, len(timeline.tracks))\n        track = timeline.tracks[0]\n        self.assertEqual(3, len(track))\n\n        clipA, nested, clipB = track\n        self.assertEqual(otio.schema.Clip, type(clipA))\n        self.assertEqual(otio.schema.Track, type(nested))\n        self.assertEqual(otio.schema.Clip, type(clipB))\n\n        self.assertEqual(2, len(nested))\n        nestedClipA, nestedClipB = nested\n        self.assertEqual(otio.schema.Clip, type(nestedClipA))\n        self.assertEqual(otio.schema.Clip, type(nestedClipB))\n\n        self.assertEqual(\n            otio.opentime.TimeRange(\n                start_time=otio.opentime.RationalTime(24, 24),\n                duration=otio.opentime.RationalTime(16, 24)\n            ),\n            clipA.trimmed_range()\n        )\n        self.assertEqual(\n            otio.opentime.TimeRange(\n                start_time=otio.opentime.RationalTime(32, 24),\n                # TODO: should actually be this, but we're not getting the\n                # media timecode offset correctly from the AAF...\n                # start_time=otio.opentime.RationalTime(86432, 24),\n                duration=otio.opentime.RationalTime(16, 24)\n            ),\n            clipB.trimmed_range()\n        )\n\n        self.assertEqual(\n            otio.opentime.TimeRange(\n                start_time=otio.opentime.RationalTime(40, 24),\n                duration=otio.opentime.RationalTime(8, 24)\n            ),\n            nestedClipA.trimmed_range()\n        )\n        self.assertEqual(\n            otio.opentime.TimeRange(\n                start_time=otio.opentime.RationalTime(24, 24),\n                # TODO: should actually be this, but we're not getting the\n                # media timecode offset correctly from the AAF...\n                # start_time=otio.opentime.RationalTime(86424, 24),\n                duration=otio.opentime.RationalTime(8, 24)\n            ),\n            nestedClipB.trimmed_range()\n        )\n\n    def SKIP_test_nesting_flatten(self):\n        nested_timeline = otio.adapters.read_from_file(\n            NESTING_EXAMPLE_PATH\n        )\n        preflattened_timeline = otio.adapters.read_from_file(\n            NESTING_PREFLATTENED_EXAMPLE_PATH\n        )\n        flattened_track = otio.algorithms.flatten_stack(nested_timeline.tracks)\n        self.assertEqual(\n            preflattened_timeline.tracks[0],\n            flattened_track\n        )\n\n    def test_read_linear_speed_effects(self):\n        timeline = otio.adapters.read_from_file(\n            LINEAR_SPEED_EFFECTS_EXAMPLE_PATH\n        )\n        self.assertEqual(1, len(timeline.tracks))\n        track = timeline.tracks[0]\n        self.assertEqual(20, len(track))\n\n        clip = track[0]\n        self.assertEqual(0, len(clip.effects))\n\n        for clip in track[1:]:\n            self.assertEqual(1, len(clip.effects))\n            effect = clip.effects[0]\n            self.assertEqual(otio.schema.LinearTimeWarp, type(effect))\n\n        expected = [\n            50.00,   #  2/1\n            33.33,   #  3/1\n            25.00,   #  4/1\n            200.00,  #  1/2\n            100.00,  #  2/2\n            66.67,   #  3/2\n            50.00,   #  4/2\n            300.00,  #  1/3\n            150.00,  #  2/3\n            100.00,  #  3/3\n            75.00,   #  4/3\n            400.00,  #  1/4\n            200.00,  #  2/4\n            133.33,  #  3/4\n            100.00,  #  4/4\n            500.00,  #  1/5\n            250.00,  #  2/5\n            166.67,  #  3/5\n            125.00   #  4/5\n        ]\n        actual = [\n         round(clip.effects[0].time_scalar * 100.0, 2) for clip in track[1:]\n        ]\n        self.assertEqual(expected, actual)"
    },
    {
        "pr_title": "otioview UI improvements",
        "pr_number": 254,
        "file_name": "opentimelineview/details_widget.py",
        "code_diff": "@@ -60,6 +60,7 @@\ndef set_item(self, item):\n             s = otio.adapters.write_to_string(item, 'otio_json')\n             self.setPlainText(s)\n \n+\n class OTIOSyntaxHighlighter(QtGui.QSyntaxHighlighter):\n     def __init__(self, palette, parent=None):\n         super(OTIOSyntaxHighlighter, self).__init__(parent)",
        "comments": [],
        "commit_message": "Lint.",
        "commit_id": "22d9c037ee3a0cb9837bf4c784fe07a83f1263a1"
    },
    {
        "pr_title": "'Freeze frame' and 'LinearTimeWarp' time effect to OTIO w/ EDL adapter support.",
        "pr_number": 251,
        "file_name": "opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -144,11 +144,10 @@\ndef add_clip(self, line, comments, rate=24):\n \n                     if freeze is not None:\n                         clip.effects.append(otio.schema.FreezeFrame())\n-                        # XXX\n+                        # XXX remove 'FF' suffix (writing edl will add it back)\n                         if clip.name.endswith(' FF'):\n                             clip.name = clip.name[:-3]\n                     elif motion is not None:\n-                        # @TODO: Non-freeze frame speed effects go here\n                         fps = float(\n                             SPEED_EFFECT_RE.match(motion).group(\"speed\")\n                         )",
        "comments": [
            {
                "comment": "Instead of effects[0] can you look through the whole list?",
                "position": null
            },
            {
                "comment": "Also, since you test for this same thing multiple times, folding this into a helper function, or flag will help.",
                "position": null
            },
            {
                "comment": "Sure, good note.",
                "position": null
            },
            {
                "comment": "Should this be otio.schema.TimingEffect to catch other things in the future?",
                "position": null
            },
            {
                "comment": "We chatted offline; I'll address this!",
                "position": null
            }
        ],
        "commit_message": "Let non-timing effects pass through in the edl adapter.",
        "commit_id": "856413a360f468a201c3ebe1d3e6de3a487dc7b9",
        "fn_context": "    def add_clip(self, line, comments, rate=24):\n        comment_handler = CommentHandler(comments)\n        clip_handler = ClipHandler(line, comment_handler.handled, rate=rate)\n        clip = clip_handler.clip\n        if comment_handler.unhandled:\n            clip.metadata.setdefault(\"cmx_3600\", {})\n            clip.metadata['cmx_3600'].setdefault(\"comments\", [])\n            clip.metadata['cmx_3600']['comments'] += (\n                comment_handler.unhandled\n            )\n\n        # each edit point between two clips is a transition. the default is a\n        # cut in the edl format the transition codes are for the transition\n        # into the clip\n        self.add_transition(\n            clip_handler,\n            clip_handler.transition_type,\n            clip_handler.transition_data\n        )\n\n        tracks = self.tracks_for_channel(clip_handler.channel_code)\n        for track in tracks:\n\n            edl_rate = clip_handler.edl_rate\n            record_in = otio.opentime.from_timecode(\n                clip_handler.record_tc_in,\n                edl_rate\n            )\n            record_out = otio.opentime.from_timecode(\n                clip_handler.record_tc_out,\n                edl_rate\n            )\n\n            src_duration = clip.duration()\n            rec_duration = record_out - record_in\n            if rec_duration != src_duration:\n                motion = comment_handler.handled.get('motion_effect')\n                freeze = comment_handler.handled.get('freeze_frame')\n                if motion is not None or freeze is not None:\n                    # Adjust the clip to match the record duration\n                    clip.source_range.duration = rec_duration\n\n                    if freeze is not None:\n                        clip.effects.append(otio.schema.FreezeFrame())\n                        # XXX remove 'FF' suffix (writing edl will add it back)\n                        if clip.name.endswith(' FF'):\n                            clip.name = clip.name[:-3]\n                    elif motion is not None:\n                        fps = float(\n                            SPEED_EFFECT_RE.match(motion).group(\"speed\")\n                        )\n                        time_scalar = fps/rate\n                        clip.effects.append(\n                            otio.schema.LinearTimeWarp(time_scalar=time_scalar)\n                        )\n\n                elif self.ignore_timecode_mismatch:\n                    # Pretend there was no problem by adjusting the record_out.\n                    # Note that we don't actually use record_out after this\n                    # point in the code, since all of the subsequent math uses\n                    # the clip's source_range. Adjusting the record_out is\n                    # just to document what the implications of ignoring the\n                    # mismatch here entails.\n                    record_out = record_in + src_duration\n\n                else:\n                    raise EDLParseError(\n                        \"Source and record duration don't match: {} != {}\"\n                        \" for clip {}\".format(\n                            src_duration,\n                            rec_duration,\n                            clip.name\n                        ))\n\n            if track.source_range is None:\n                zero = otio.opentime.RationalTime(0, edl_rate)\n                track.source_range = otio.opentime.TimeRange(\n                    start_time=zero - record_in,\n                    duration=zero\n                )\n\n            track_end = track.duration() - track.source_range.start_time\n            if record_in < track_end:\n                if self.ignore_timecode_mismatch:\n                    # shift it over\n                    record_in = track_end\n                    record_out = record_in + rec_duration\n                else:\n                    raise EDLParseError(\n                        \"Overlapping record in value: {} for clip {}\".format(\n                            clip_handler.record_tc_in,\n                            clip.name\n                        ))\n\n            # If the next clip is supposed to start beyond the end of the\n            # clips we've accumulated so far, then we need to add a Gap\n            # to fill that space. This can happen when an EDL has record\n            # timecodes that are sparse (e.g. from a single track of a\n            # multi-track composition).\n            if record_in > track_end and len(track) > 0:\n                gap = otio.schema.Gap()\n                gap.source_range = otio.opentime.TimeRange(\n                    start_time=otio.opentime.RationalTime(0, edl_rate),\n                    duration=record_in-track_end\n                )\n                track.append(gap)\n                track.source_range.duration += gap.duration()\n\n            track.append(clip)\n            track.source_range.duration += clip.duration()"
    },
    {
        "pr_title": "'Freeze frame' and 'LinearTimeWarp' time effect to OTIO w/ EDL adapter support.",
        "pr_number": 251,
        "file_name": "opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -859,7 +858,7 @@\ndef get_content_for_track_at_index(self, idx, title):\n         return content\n \n \n-def _timing_effects(clip):\n+def _supported_timing_effects(clip):\n     return [\n         fx for fx in clip.effects\n         if isinstance(fx, otio.schema.LinearTimeWarp)",
        "comments": [
            {
                "comment": "Instead of effects[0] can you look through the whole list?",
                "position": null
            },
            {
                "comment": "Also, since you test for this same thing multiple times, folding this into a helper function, or flag will help.",
                "position": null
            },
            {
                "comment": "Sure, good note.",
                "position": null
            },
            {
                "comment": "Should this be otio.schema.TimingEffect to catch other things in the future?",
                "position": null
            },
            {
                "comment": "We chatted offline; I'll address this!",
                "position": null
            }
        ],
        "commit_message": "Let non-timing effects pass through in the edl adapter.",
        "commit_id": "856413a360f468a201c3ebe1d3e6de3a487dc7b9",
        "fn_context": "    def get_content_for_track_at_index(self, idx, title):\n        track = self._tracks[idx]\n\n        # Add a gap if the last child is a transition.\n        if isinstance(track[-1], otio.schema.Transition):\n            gap = otio.schema.Gap(\n                source_range=otio.opentime.TimeRange(\n                    start_time=track[-1].duration(),\n                    duration=otio.opentime.RationalTime(0.0, self._rate)\n                )\n            )\n            track.append(gap)\n\n        # Note: Transitions in EDLs are unconventionally represented.\n        #\n        # Where a transition might normally be visualized like:\n        #            |---57.0 Trans 43.0----|\n        # |------Clip1 102.0------|----------Clip2 143.0----------|Clip3 24.0|\n        #\n        # In an EDL it can be thought of more like this:\n        #            |---0.0 Trans 100.0----|\n        # |Clip1 45.0|----------------Clip2 200.0-----------------|Clip3 24.0|\n\n        # Adjust cut points to match EDL event representation.\n        for idx, child in enumerate(track):\n            if isinstance(child, otio.schema.Transition):\n                if idx != 0:\n                    # Shorten the a-side\n                    track[idx-1].source_range.duration -= child.in_offset\n\n                # Lengthen the b-side\n                track[idx+1].source_range.start_time -= child.in_offset\n                track[idx+1].source_range.duration += child.in_offset\n\n                # Just clean up the transition for goodness sake\n                in_offset = child.in_offset\n                child.in_offset = otio.opentime.RationalTime(0.0, self._rate)\n                child.out_offset += in_offset\n\n        # Group events into either simple clip/a-side or transition and b-side\n        # to match EDL edit/event representation and edit numbers.\n        events = []\n        for idx, child in enumerate(track):\n            if isinstance(child, otio.schema.Transition):\n                # Transition will be captured in subsequent iteration.\n                continue\n\n            prv = track[idx-1] if idx > 0 else None\n\n            if isinstance(prv, otio.schema.Transition):\n                events.append(\n                    DissolveEvent(\n                        events[-1] if len(events) else None,\n                        prv,\n                        child,\n                        self._tracks,\n                        track.kind,\n                        self._rate,\n                        self._style\n                    )\n                )\n            elif isinstance(child, otio.schema.Clip):\n                events.append(\n                    Event(\n                        child,\n                        self._tracks,\n                        track.kind,\n                        self._rate,\n                        self._style\n                    )\n                )\n            elif isinstance(child, otio.schema.Gap):\n                # Gaps are represented as missing record timecode, no event\n                # needed.\n                pass\n\n        content = \"TITLE: {}\\n\\n\".format(title) if title else ''\n\n        # Convert each event/dissolve-event into plain text.\n        for idx, event in enumerate(events):\n            event.edit_number = idx + 1\n            content += event.to_edl_format() + '\\n'\n\n        return content\n\ndef _supported_timing_effects(clip):\n    return [\n        fx for fx in clip.effects\n        if isinstance(fx, otio.schema.LinearTimeWarp)\n    ]"
    },
    {
        "pr_title": "PyPi updates (#239)",
        "pr_number": 241,
        "file_name": "setup.py",
        "code_diff": "@@ -25,9 +25,10 @@\n\"\"\" Configuration file for the OpenTimelineIO Python Package.  \"\"\"\n \n-import sys\n-\n+import os\n from setuptools import setup\n+import sys\n+import unittest\n \n \n # check the python version first",
        "comments": [],
        "commit_message": "Expand setup.py with additional metadata (#239)\n\nPyPi requires additional metadata such as classifiers.\nConfigure setup.py to run the core tests with 'python setup.py test'\nAdd setup.cfg for pointing pypi to the README",
        "commit_id": "01b497edf80e03ab2d1fe9df9cee086fdf68c1f8"
    },
    {
        "pr_title": "PyPi updates (#239)",
        "pr_number": 241,
        "file_name": "setup.py",
        "code_diff": "@@ -43,13 +44,45 @@\n)\n     )\n \n+\n+def test_otio():\n+    \"\"\"Discovers and runs tests\"\"\"\n+    try:\n+        # Clear the environment of a preset media linker\n+        del os.environ['OTIO_DEFAULT_MEDIA_LINKER']\n+    except KeyError:\n+        pass\n+    return unittest.TestLoader().discover('tests')\n+\n+\n setup(\n     name='OpenTimelineIO',\n     version='0.8.dev',\n     description='Editorial interchange format and API',\n     author='Pixar Animation Studios',\n     author_email='opentimelineio@pixar.com',\n     url='http://opentimeline.io',\n+    license='Modified Apache 2.0 License',\n+\n+    classifiers=[\n+        'Development Status :: 4 - Beta',\n+        'Topic :: Multimedia :: Graphics',\n+        'Topic :: Multimedia :: Video',\n+        'Topic :: Multimedia :: Video :: Display',\n+        'Topic :: Multimedia :: Video :: Non-Linear Editor',\n+        'Topic :: Software Development :: Libraries :: Python Modules',\n+        'License :: Other/Proprietary License',\n+        'Programming Language :: Python :: 2',\n+        'Programming Language :: Python :: 2.7',\n+        'Programming Language :: Python :: 3',\n+        'Programming Language :: Python :: 3.5',\n+        'Operating System :: OS Independent',\n+        'Natural Language :: English',\n+    ],\n+\n+    keywords='film tv editing editorial edit non-linear edl time',\n+\n+    platforms='any',\n \n     packages=[\n         'opentimelineio',",
        "comments": [
            {
                "comment": "I'm not sure how the tagging is supposed to be set up.  Officially we support python 2.7 and 3.5, those are the  only two versions we test against.  It probably works in the versions you list, but I'm not sure how strict this is meant to be.",
                "position": 47
            },
            {
                "comment": "I'll remove the other specified versions of 3 from the list.",
                "position": 47
            }
        ],
        "commit_message": "Expand setup.py with additional metadata (#239)\n\nPyPi requires additional metadata such as classifiers.\nConfigure setup.py to run the core tests with 'python setup.py test'\nAdd setup.cfg for pointing pypi to the README",
        "commit_id": "01b497edf80e03ab2d1fe9df9cee086fdf68c1f8",
        "fn_context": "def test_otio():\n    \"\"\"Discovers and runs tests\"\"\"\n    try:\n        # Clear the environment of a preset media linker\n        del os.environ['OTIO_DEFAULT_MEDIA_LINKER']\n    except KeyError:\n        pass\n    return unittest.TestLoader().discover('tests')"
    },
    {
        "pr_title": "Setuptools plugins",
        "pr_number": 237,
        "file_name": "tests/test_plugin_detection.py",
        "code_diff": "@@ -90,10 +90,10 @@\ndef test_detect_plugin(self):\n \n         # Make sure adapters and linkers landed in the proper place\n         for adapter in man.adapters:\n-            self.assertTrue(isinstance(adapter, otio.adapters.Adapter))\n+            self.assertIsInstance(adapter, otio.adapters.Adapter)\n \n         for linker in man.media_linkers:\n-            self.assertTrue(isinstance(linker, otio.media_linker.MediaLinker))\n+            self.assertIsInstance(linker, otio.media_linker.MediaLinker)\n \n     def test_detect_pugin_json_manifest(self):\n         # Test detecting a plugin that rather than exposing the plugin_manifest",
        "comments": [
            {
                "comment": "My only note for this approach is to wrap this again in a `skipIf` call, similar to how we don't run unit tests for the adapters with extra requirements if their dependencies aren't met.  Since we know this will run on our CI test server, I think its safe to omit it for people running python <3.3 who don't have it installed.\r\n\r\nSee:\r\nhttps://github.com/PixarAnimationStudios/OpenTimelineIO/blob/33ad026c7d46f106eb079f77324ffae6060691d5/opentimelineio_contrib/adapters/tests/test_aaf_adapter.py#L82",
                "position": null
            },
            {
                "comment": "sounds good!",
                "position": null
            },
            {
                "comment": "There is a `assertIsInstance` function,: \r\nhttps://docs.python.org/2/library/unittest.html#unittest.TestCase.assertIsInstance\r\n",
                "position": null
            },
            {
                "comment": "Oh, I like that. I'll push it really quick",
                "position": null
            }
        ],
        "commit_message": "Switctched plugin tests to use assertIsInstance rather than assertTrue on an isinstance check",
        "commit_id": "5dc56953717840ec43a2e75e7db695c89f838eb6",
        "fn_context": "    def test_detect_plugin(self):\n        # Create a manifest and ensure it detected the mock adapter and linker\n        man = otio.plugins.manifest.load_manifest()\n\n        # Make sure the adapter is included in the adapter list\n        adapter_names = [adapter.name for adapter in man.adapters]\n        self.assertIn('mock_adapter', adapter_names)\n\n        # Make sure the linker is included in the linker list\n        linker_names = [linker.name for linker in man.media_linkers]\n        self.assertIn('mock_linker', linker_names)\n\n        # Make sure adapters and linkers landed in the proper place\n        for adapter in man.adapters:\n            self.assertIsInstance(adapter, otio.adapters.Adapter)\n\n        for linker in man.media_linkers:\n            self.assertIsInstance(linker, otio.media_linker.MediaLinker)\n\n    def test_detect_pugin_json_manifest(self):\n        # Test detecting a plugin that rather than exposing the plugin_manifest\n        # function, just simply has a plugin_manifest.json provided at the\n        # package top level.\n        man = otio.plugins.manifest.load_manifest()\n\n        # Make sure the adapter is included in the adapter list\n        adapter_names = [adapter.name for adapter in man.adapters]\n        self.assertIn('mock_adapter_json', adapter_names)\n\n        # Make sure the linker is included in the linker list\n        linker_names = [linker.name for linker in man.media_linkers]\n        self.assertIn('mock_linker_json', linker_names)\n\n        # Make sure adapters and linkers landed in the proper place\n        for adapter in man.adapters:\n            self.assertIsInstance(adapter, otio.adapters.Adapter)\n\n        for linker in man.media_linkers:\n            self.assertIsInstance(linker, otio.media_linker.MediaLinker)"
    },
    {
        "pr_title": "Added minimal support for EDL speed effects.",
        "pr_number": 233,
        "file_name": "opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -97,10 +97,11 @@\ndef __init__(self, edl_string, rate=24, ignore_timecode_mismatch=False):\n     def add_clip(self, line, comments, rate=24):\n         comment_handler = CommentHandler(comments)\n         clip_handler = ClipHandler(line, comment_handler.handled, rate=rate)\n+        clip = clip_handler.clip\n         if comment_handler.unhandled:\n-            clip_handler.clip.metadata.setdefault(\"cmx_3600\", {})\n-            clip_handler.clip.metadata['cmx_3600'].setdefault(\"comments\", [])\n-            clip_handler.clip.metadata['cmx_3600']['comments'] += (\n+            clip.metadata.setdefault(\"cmx_3600\", {})\n+            clip.metadata['cmx_3600'].setdefault(\"comments\", [])\n+            clip.metadata['cmx_3600']['comments'] += (\n                 comment_handler.unhandled\n             )",
        "comments": [
            {
                "comment": "Still a little weirded out that `record_out` is never used again, but I kinda understand why. ",
                "position": null
            },
            {
                "comment": "I added a comment explaining this. Does that help?",
                "position": null
            }
        ],
        "commit_message": "EDL motion effects and freeze frame infomation is stashed into cmx_3600 metadata so we don't lose it.",
        "commit_id": "50a472c2a6e50e0637b461ce7e46f982230f46f0",
        "fn_context": "    def add_clip(self, line, comments, rate=24):\n        comment_handler = CommentHandler(comments)\n        clip_handler = ClipHandler(line, comment_handler.handled, rate=rate)\n        clip = clip_handler.clip\n        if comment_handler.unhandled:\n            clip.metadata.setdefault(\"cmx_3600\", {})\n            clip.metadata['cmx_3600'].setdefault(\"comments\", [])\n            clip.metadata['cmx_3600']['comments'] += (\n                comment_handler.unhandled\n            )\n\n        # each edit point between two clips is a transition. the default is a\n        # cut in the edl format the transition codes are for the transition\n        # into the clip\n        self.add_transition(\n            clip_handler,\n            clip_handler.transition_type,\n            clip_handler.transition_data\n        )\n\n        tracks = self.tracks_for_channel(clip_handler.channel_code)\n        for track in tracks:\n\n            edl_rate = clip_handler.edl_rate\n            record_in = otio.opentime.from_timecode(\n                clip_handler.record_tc_in,\n                edl_rate\n            )\n            record_out = otio.opentime.from_timecode(\n                clip_handler.record_tc_out,\n                edl_rate\n            )\n\n            src_duration = clip.duration()\n            rec_duration = record_out - record_in\n            if rec_duration != src_duration:\n                motion = comment_handler.handled.get('motion_effect')\n                freeze = comment_handler.handled.get('freeze_frame')\n                if motion is not None or freeze is not None:\n                    # Adjust the clip to match the record duration\n                    clip.source_range.duration = rec_duration\n\n                    # TODO: Once OTIO has speed effects, use them here.\n                    clip.metadata.setdefault(\"cmx_3600\", {})\n                    if motion is not None:\n                        clip.metadata['cmx_3600']['motion_effect'] = motion\n                    if freeze is not None:\n                        clip.metadata['cmx_3600']['freeze_frame'] = True\n\n                elif self.ignore_timecode_mismatch:\n                    # Pretend there was no problem by adjusting the record_out.\n                    # Note that we don't actually use record_out after this\n                    # point in the code, since all of the subsequent math uses\n                    # the clip's source_range. Adjusting the record_out is\n                    # just to document what the implications of ignoring the\n                    # mismatch here entails.\n                    record_out = record_in + src_duration\n\n                else:\n                    raise EDLParseError(\n                        \"Source and record duration don't match: {} != {}\"\n                        \" for clip {}\".format(\n                            src_duration,\n                            rec_duration,\n                            clip.name\n                        ))\n\n            if track.source_range is None:\n                zero = otio.opentime.RationalTime(0, edl_rate)\n                track.source_range = otio.opentime.TimeRange(\n                    start_time=zero - record_in,\n                    duration=zero\n                )\n\n            track_end = track.duration() - track.source_range.start_time\n            if record_in < track_end:\n                if self.ignore_timecode_mismatch:\n                    # shift it over\n                    record_in = track_end\n                    record_out = record_in + rec_duration\n                else:\n                    raise EDLParseError(\n                        \"Overlapping record in value: {} for clip {}\".format(\n                            clip_handler.record_tc_in,\n                            clip.name\n                        ))\n\n            # If the next clip is supposed to start beyond the end of the\n            # clips we've accumulated so far, then we need to add a Gap\n            # to fill that space. This can happen when an EDL has record\n            # timecodes that are sparse (e.g. from a single track of a\n            # multi-track composition).\n            if record_in > track_end and len(track) > 0:\n                gap = otio.schema.Gap()\n                gap.source_range = otio.opentime.TimeRange(\n                    start_time=otio.opentime.RationalTime(0, edl_rate),\n                    duration=record_in-track_end\n                )\n                track.append(gap)\n                track.source_range.duration += gap.duration()\n\n            track.append(clip)\n            track.source_range.duration += clip.duration()"
    },
    {
        "pr_title": "Added minimal support for EDL speed effects.",
        "pr_number": 233,
        "file_name": "opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -126,15 +127,21 @@\ndef add_clip(self, line, comments, rate=24):\n                 edl_rate\n             )\n \n-            src_duration = clip_handler.clip.duration()\n+            src_duration = clip.duration()\n             rec_duration = record_out - record_in\n             if rec_duration != src_duration:\n                 motion = comment_handler.handled.get('motion_effect')\n                 freeze = comment_handler.handled.get('freeze_frame')\n                 if motion is not None or freeze is not None:\n                     # Adjust the clip to match the record duration\n+                    clip.source_range.duration = rec_duration\n+\n                     # TODO: Once OTIO has speed effects, use them here.\n-                    clip_handler.clip.source_range.duration = rec_duration\n+                    clip.metadata.setdefault(\"cmx_3600\", {})\n+                    if motion is not None:\n+                        clip.metadata['cmx_3600']['motion_effect'] = motion\n+                    if freeze is not None:\n+                        clip.metadata['cmx_3600']['freeze_frame'] = True\n \n                 elif self.ignore_timecode_mismatch:\n                     # Pretend there was no problem by adjusting the record_out.",
        "comments": [
            {
                "comment": "Still a little weirded out that `record_out` is never used again, but I kinda understand why. ",
                "position": null
            },
            {
                "comment": "I added a comment explaining this. Does that help?",
                "position": null
            }
        ],
        "commit_message": "EDL motion effects and freeze frame infomation is stashed into cmx_3600 metadata so we don't lose it.",
        "commit_id": "50a472c2a6e50e0637b461ce7e46f982230f46f0",
        "fn_context": "    def add_clip(self, line, comments, rate=24):\n        comment_handler = CommentHandler(comments)\n        clip_handler = ClipHandler(line, comment_handler.handled, rate=rate)\n        clip = clip_handler.clip\n        if comment_handler.unhandled:\n            clip.metadata.setdefault(\"cmx_3600\", {})\n            clip.metadata['cmx_3600'].setdefault(\"comments\", [])\n            clip.metadata['cmx_3600']['comments'] += (\n                comment_handler.unhandled\n            )\n\n        # each edit point between two clips is a transition. the default is a\n        # cut in the edl format the transition codes are for the transition\n        # into the clip\n        self.add_transition(\n            clip_handler,\n            clip_handler.transition_type,\n            clip_handler.transition_data\n        )\n\n        tracks = self.tracks_for_channel(clip_handler.channel_code)\n        for track in tracks:\n\n            edl_rate = clip_handler.edl_rate\n            record_in = otio.opentime.from_timecode(\n                clip_handler.record_tc_in,\n                edl_rate\n            )\n            record_out = otio.opentime.from_timecode(\n                clip_handler.record_tc_out,\n                edl_rate\n            )\n\n            src_duration = clip.duration()\n            rec_duration = record_out - record_in\n            if rec_duration != src_duration:\n                motion = comment_handler.handled.get('motion_effect')\n                freeze = comment_handler.handled.get('freeze_frame')\n                if motion is not None or freeze is not None:\n                    # Adjust the clip to match the record duration\n                    clip.source_range.duration = rec_duration\n\n                    # TODO: Once OTIO has speed effects, use them here.\n                    clip.metadata.setdefault(\"cmx_3600\", {})\n                    if motion is not None:\n                        clip.metadata['cmx_3600']['motion_effect'] = motion\n                    if freeze is not None:\n                        clip.metadata['cmx_3600']['freeze_frame'] = True\n\n                elif self.ignore_timecode_mismatch:\n                    # Pretend there was no problem by adjusting the record_out.\n                    # Note that we don't actually use record_out after this\n                    # point in the code, since all of the subsequent math uses\n                    # the clip's source_range. Adjusting the record_out is\n                    # just to document what the implications of ignoring the\n                    # mismatch here entails.\n                    record_out = record_in + src_duration\n\n                else:\n                    raise EDLParseError(\n                        \"Source and record duration don't match: {} != {}\"\n                        \" for clip {}\".format(\n                            src_duration,\n                            rec_duration,\n                            clip.name\n                        ))\n\n            if track.source_range is None:\n                zero = otio.opentime.RationalTime(0, edl_rate)\n                track.source_range = otio.opentime.TimeRange(\n                    start_time=zero - record_in,\n                    duration=zero\n                )\n\n            track_end = track.duration() - track.source_range.start_time\n            if record_in < track_end:\n                if self.ignore_timecode_mismatch:\n                    # shift it over\n                    record_in = track_end\n                    record_out = record_in + rec_duration\n                else:\n                    raise EDLParseError(\n                        \"Overlapping record in value: {} for clip {}\".format(\n                            clip_handler.record_tc_in,\n                            clip.name\n                        ))\n\n            # If the next clip is supposed to start beyond the end of the\n            # clips we've accumulated so far, then we need to add a Gap\n            # to fill that space. This can happen when an EDL has record\n            # timecodes that are sparse (e.g. from a single track of a\n            # multi-track composition).\n            if record_in > track_end and len(track) > 0:\n                gap = otio.schema.Gap()\n                gap.source_range = otio.opentime.TimeRange(\n                    start_time=otio.opentime.RationalTime(0, edl_rate),\n                    duration=record_in-track_end\n                )\n                track.append(gap)\n                track.source_range.duration += gap.duration()\n\n            track.append(clip)\n            track.source_range.duration += clip.duration()"
    },
    {
        "pr_title": "Added minimal support for EDL speed effects.",
        "pr_number": 233,
        "file_name": "opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -151,7 +158,7 @@\ndef add_clip(self, line, comments, rate=24):\n                         \" for clip {}\".format(\n                             src_duration,\n                             rec_duration,\n-                            clip_handler.clip.name\n+                            clip.name\n                         ))\n \n             if track.source_range is None:",
        "comments": [
            {
                "comment": "Still a little weirded out that `record_out` is never used again, but I kinda understand why. ",
                "position": null
            },
            {
                "comment": "I added a comment explaining this. Does that help?",
                "position": null
            }
        ],
        "commit_message": "EDL motion effects and freeze frame infomation is stashed into cmx_3600 metadata so we don't lose it.",
        "commit_id": "50a472c2a6e50e0637b461ce7e46f982230f46f0",
        "fn_context": "    def add_clip(self, line, comments, rate=24):\n        comment_handler = CommentHandler(comments)\n        clip_handler = ClipHandler(line, comment_handler.handled, rate=rate)\n        clip = clip_handler.clip\n        if comment_handler.unhandled:\n            clip.metadata.setdefault(\"cmx_3600\", {})\n            clip.metadata['cmx_3600'].setdefault(\"comments\", [])\n            clip.metadata['cmx_3600']['comments'] += (\n                comment_handler.unhandled\n            )\n\n        # each edit point between two clips is a transition. the default is a\n        # cut in the edl format the transition codes are for the transition\n        # into the clip\n        self.add_transition(\n            clip_handler,\n            clip_handler.transition_type,\n            clip_handler.transition_data\n        )\n\n        tracks = self.tracks_for_channel(clip_handler.channel_code)\n        for track in tracks:\n\n            edl_rate = clip_handler.edl_rate\n            record_in = otio.opentime.from_timecode(\n                clip_handler.record_tc_in,\n                edl_rate\n            )\n            record_out = otio.opentime.from_timecode(\n                clip_handler.record_tc_out,\n                edl_rate\n            )\n\n            src_duration = clip.duration()\n            rec_duration = record_out - record_in\n            if rec_duration != src_duration:\n                motion = comment_handler.handled.get('motion_effect')\n                freeze = comment_handler.handled.get('freeze_frame')\n                if motion is not None or freeze is not None:\n                    # Adjust the clip to match the record duration\n                    clip.source_range.duration = rec_duration\n\n                    # TODO: Once OTIO has speed effects, use them here.\n                    clip.metadata.setdefault(\"cmx_3600\", {})\n                    if motion is not None:\n                        clip.metadata['cmx_3600']['motion_effect'] = motion\n                    if freeze is not None:\n                        clip.metadata['cmx_3600']['freeze_frame'] = True\n\n                elif self.ignore_timecode_mismatch:\n                    # Pretend there was no problem by adjusting the record_out.\n                    # Note that we don't actually use record_out after this\n                    # point in the code, since all of the subsequent math uses\n                    # the clip's source_range. Adjusting the record_out is\n                    # just to document what the implications of ignoring the\n                    # mismatch here entails.\n                    record_out = record_in + src_duration\n\n                else:\n                    raise EDLParseError(\n                        \"Source and record duration don't match: {} != {}\"\n                        \" for clip {}\".format(\n                            src_duration,\n                            rec_duration,\n                            clip.name\n                        ))\n\n            if track.source_range is None:\n                zero = otio.opentime.RationalTime(0, edl_rate)\n                track.source_range = otio.opentime.TimeRange(\n                    start_time=zero - record_in,\n                    duration=zero\n                )\n\n            track_end = track.duration() - track.source_range.start_time\n            if record_in < track_end:\n                if self.ignore_timecode_mismatch:\n                    # shift it over\n                    record_in = track_end\n                    record_out = record_in + rec_duration\n                else:\n                    raise EDLParseError(\n                        \"Overlapping record in value: {} for clip {}\".format(\n                            clip_handler.record_tc_in,\n                            clip.name\n                        ))\n\n            # If the next clip is supposed to start beyond the end of the\n            # clips we've accumulated so far, then we need to add a Gap\n            # to fill that space. This can happen when an EDL has record\n            # timecodes that are sparse (e.g. from a single track of a\n            # multi-track composition).\n            if record_in > track_end and len(track) > 0:\n                gap = otio.schema.Gap()\n                gap.source_range = otio.opentime.TimeRange(\n                    start_time=otio.opentime.RationalTime(0, edl_rate),\n                    duration=record_in-track_end\n                )\n                track.append(gap)\n                track.source_range.duration += gap.duration()\n\n            track.append(clip)\n            track.source_range.duration += clip.duration()"
    },
    {
        "pr_title": "Added minimal support for EDL speed effects.",
        "pr_number": 233,
        "file_name": "opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -171,7 +178,7 @@\ndef add_clip(self, line, comments, rate=24):\n                     raise EDLParseError(\n                         \"Overlapping record in value: {} for clip {}\".format(\n                             clip_handler.record_tc_in,\n-                            clip_handler.clip.name\n+                            clip.name\n                         ))\n \n             # If the next clip is supposed to start beyond the end of the",
        "comments": [
            {
                "comment": "Still a little weirded out that `record_out` is never used again, but I kinda understand why. ",
                "position": null
            },
            {
                "comment": "I added a comment explaining this. Does that help?",
                "position": null
            }
        ],
        "commit_message": "EDL motion effects and freeze frame infomation is stashed into cmx_3600 metadata so we don't lose it.",
        "commit_id": "50a472c2a6e50e0637b461ce7e46f982230f46f0",
        "fn_context": "    def add_clip(self, line, comments, rate=24):\n        comment_handler = CommentHandler(comments)\n        clip_handler = ClipHandler(line, comment_handler.handled, rate=rate)\n        clip = clip_handler.clip\n        if comment_handler.unhandled:\n            clip.metadata.setdefault(\"cmx_3600\", {})\n            clip.metadata['cmx_3600'].setdefault(\"comments\", [])\n            clip.metadata['cmx_3600']['comments'] += (\n                comment_handler.unhandled\n            )\n\n        # each edit point between two clips is a transition. the default is a\n        # cut in the edl format the transition codes are for the transition\n        # into the clip\n        self.add_transition(\n            clip_handler,\n            clip_handler.transition_type,\n            clip_handler.transition_data\n        )\n\n        tracks = self.tracks_for_channel(clip_handler.channel_code)\n        for track in tracks:\n\n            edl_rate = clip_handler.edl_rate\n            record_in = otio.opentime.from_timecode(\n                clip_handler.record_tc_in,\n                edl_rate\n            )\n            record_out = otio.opentime.from_timecode(\n                clip_handler.record_tc_out,\n                edl_rate\n            )\n\n            src_duration = clip.duration()\n            rec_duration = record_out - record_in\n            if rec_duration != src_duration:\n                motion = comment_handler.handled.get('motion_effect')\n                freeze = comment_handler.handled.get('freeze_frame')\n                if motion is not None or freeze is not None:\n                    # Adjust the clip to match the record duration\n                    clip.source_range.duration = rec_duration\n\n                    # TODO: Once OTIO has speed effects, use them here.\n                    clip.metadata.setdefault(\"cmx_3600\", {})\n                    if motion is not None:\n                        clip.metadata['cmx_3600']['motion_effect'] = motion\n                    if freeze is not None:\n                        clip.metadata['cmx_3600']['freeze_frame'] = True\n\n                elif self.ignore_timecode_mismatch:\n                    # Pretend there was no problem by adjusting the record_out.\n                    # Note that we don't actually use record_out after this\n                    # point in the code, since all of the subsequent math uses\n                    # the clip's source_range. Adjusting the record_out is\n                    # just to document what the implications of ignoring the\n                    # mismatch here entails.\n                    record_out = record_in + src_duration\n\n                else:\n                    raise EDLParseError(\n                        \"Source and record duration don't match: {} != {}\"\n                        \" for clip {}\".format(\n                            src_duration,\n                            rec_duration,\n                            clip.name\n                        ))\n\n            if track.source_range is None:\n                zero = otio.opentime.RationalTime(0, edl_rate)\n                track.source_range = otio.opentime.TimeRange(\n                    start_time=zero - record_in,\n                    duration=zero\n                )\n\n            track_end = track.duration() - track.source_range.start_time\n            if record_in < track_end:\n                if self.ignore_timecode_mismatch:\n                    # shift it over\n                    record_in = track_end\n                    record_out = record_in + rec_duration\n                else:\n                    raise EDLParseError(\n                        \"Overlapping record in value: {} for clip {}\".format(\n                            clip_handler.record_tc_in,\n                            clip.name\n                        ))\n\n            # If the next clip is supposed to start beyond the end of the\n            # clips we've accumulated so far, then we need to add a Gap\n            # to fill that space. This can happen when an EDL has record\n            # timecodes that are sparse (e.g. from a single track of a\n            # multi-track composition).\n            if record_in > track_end and len(track) > 0:\n                gap = otio.schema.Gap()\n                gap.source_range = otio.opentime.TimeRange(\n                    start_time=otio.opentime.RationalTime(0, edl_rate),\n                    duration=record_in-track_end\n                )\n                track.append(gap)\n                track.source_range.duration += gap.duration()\n\n            track.append(clip)\n            track.source_range.duration += clip.duration()"
    },
    {
        "pr_title": "Added minimal support for EDL speed effects.",
        "pr_number": 233,
        "file_name": "tests/test_cmx_3600_adapter.py",
        "code_diff": "@@ -764,6 +764,14 @@\ndef test_speed_effects(self):\n             clip.name,\n             \"Z682_156 (LAY3) FF\"\n         )\n+        self.assertEqual(\n+            clip.metadata.get(\"cmx_3600\", {}).get(\"motion_effect\"),\n+            \"Z682_156       000.0                01:00:10:21\"\n+        )\n+        self.assertEqual(\n+            clip.metadata.get(\"cmx_3600\", {}).get(\"freeze_frame\"),\n+            True\n+        )\n         self.assertEqual(\n             clip.duration(),\n             otio.opentime.from_timecode(\"00:00:00:17\", 24)",
        "comments": [],
        "commit_message": "EDL motion effects and freeze frame infomation is stashed into cmx_3600 metadata so we don't lose it.",
        "commit_id": "50a472c2a6e50e0637b461ce7e46f982230f46f0"
    },
    {
        "pr_title": "AAF adapter",
        "pr_number": 230,
        "file_name": "contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -48,6 +48,9 @@\n__names = set()\n \n \n+# We use this _unique_name function to assign #s at the end of otherwise\n+# anonymous objects. This aids in debugging when you have loads of objects\n+# of the same type in a large composition.\n def _unique_name(name):\n     while name in __names:\n         m = re.search(r'(\\d+)$', name)",
        "comments": [
            {
                "comment": "Commented out code - did you want to prune this out or leave this in for the future?",
                "position": null
            },
            {
                "comment": "This looks like you don't need an enumerate, just a regular loop should do.",
                "position": null
            }
        ],
        "commit_message": "Addressed some code review notes.",
        "commit_id": "b05c61769b1a72dd7a7f9945e8211cf44824c9ff",
        "fn_context": "def _unique_name(name):\n    while name in __names:\n        m = re.search(r'(\\d+)$', name)\n        if m:\n            num = int(m.group(1))\n            name = re.sub(r'(\\d+)$', str(num+1), name)\n        else:\n            name = name+\" 2\"\n    __names.add(name)\n    return name"
    },
    {
        "pr_title": "AAF adapter",
        "pr_number": 230,
        "file_name": "contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -95,11 +98,11 @@\ndef _transcribe_property(prop):\n                 result[child.name] = str(child.mobID)\n             elif isinstance(child, aaf.mob.SourceMob):\n                 result[child.name] = str(child.mobID)\n-            # elif hasattr(child, \"name\"):\n-            #     result[child.name] = _transcribe(child)\n             else:\n+                # @TODO: There may be more properties that we might want also.\n+                # If you want to see what is being skipped, turn on debug.\n                 if debug:\n-                    print(\"??? {}\".format(child))\n+                    print(\"Skipping unrecognized property: {}\".format(child))\n         return result\n \n     else:",
        "comments": [
            {
                "comment": "Commented out code - did you want to prune this out or leave this in for the future?",
                "position": null
            },
            {
                "comment": "This looks like you don't need an enumerate, just a regular loop should do.",
                "position": null
            }
        ],
        "commit_message": "Addressed some code review notes.",
        "commit_id": "b05c61769b1a72dd7a7f9945e8211cf44824c9ff",
        "fn_context": "def _transcribe_property(prop):\n    if isinstance(prop, list):\n        return [_transcribe_property(child) for child in prop]\n\n    # XXX: The unicode type doesn't exist in Python 3 (all strings are unicode)\n    # so we have to use type(u\"\") which works in both Python 2 and 3.\n    elif type(prop) in (str, type(u\"\"), int, float, bool):\n        return prop\n\n    if isinstance(prop, aaf.iterator.PropValueResolveIter):\n        result = {}\n        for child in prop:\n            if isinstance(child, aaf.property.TaggedValue):\n                result[child.name] = _transcribe_property(child.value)\n            elif isinstance(child, aaf.mob.MasterMob):\n                result[child.name] = str(child.mobID)\n            elif isinstance(child, aaf.mob.SourceMob):\n                result[child.name] = str(child.mobID)\n            else:\n                # @TODO: There may be more properties that we might want also.\n                # If you want to see what is being skipped, turn on debug.\n                if debug:\n                    print(\"Skipping unrecognized property: {}\".format(child))\n        return result\n\n    else:\n        return str(prop)\n\ndef _add_child(parent, child, source):\n    if child is None:\n        if debug:\n            print(\"MISSING CHILD? {}\".format(source))\n    elif isinstance(child, otio.schema.Marker):\n        parent.markers.append(child)\n    else:\n        parent.append(child)"
    },
    {
        "pr_title": "AAF adapter",
        "pr_number": 230,
        "file_name": "contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -120,6 +123,8 @@\ndef _transcribe(item, parent=None, editRate=24, masterMobs=None):\n     result = None\n     metadata = {}\n \n+    # First lets grab some standard properties that are present on\n+    # many types of AAF objects...\n     metadata[\"Name\"] = _get_name(item)\n     metadata[\"ClassName\"] = _get_class_name(item)",
        "comments": [
            {
                "comment": "Commented out code - did you want to prune this out or leave this in for the future?",
                "position": null
            },
            {
                "comment": "This looks like you don't need an enumerate, just a regular loop should do.",
                "position": null
            }
        ],
        "commit_message": "Addressed some code review notes.",
        "commit_id": "b05c61769b1a72dd7a7f9945e8211cf44824c9ff",
        "fn_context": "def _transcribe(item, parent=None, editRate=24, masterMobs=None):\n    result = None\n    metadata = {}\n\n    # First lets grab some standard properties that are present on\n    # many types of AAF objects...\n    metadata[\"Name\"] = _get_name(item)\n    metadata[\"ClassName\"] = _get_class_name(item)\n\n    if isinstance(item, aaf.component.Component):\n        metadata[\"Length\"] = item.length\n\n    if isinstance(item, aaf.base.AAFObject):\n        for prop in item.properties():\n            if hasattr(prop, 'name') and hasattr(prop, 'value'):\n                key = str(prop.name)\n                value = prop.value\n                metadata[key] = _transcribe_property(value)\n\n    # Now we will use the item's class to determine which OTIO type\n    # to transcribe into. Note that the order of this if/elif/... chain\n    # is important, because the class hierarchy of AAF objects is more\n    # complex than OTIO.\n\n    if isinstance(item, aaf.storage.ContentStorage):\n        result = otio.schema.SerializableCollection()\n\n        # Gather all the Master Mobs, so we can find them later by MobID\n        # when we parse the SourceClips in the composition\n        if masterMobs is None:\n            masterMobs = {}\n        for mob in item.master_mobs():\n            child = _transcribe(mob, parent=item)\n            if child is not None:\n                mobID = child.metadata.get(\"AAF\", {}).get(\"MobID\")\n                masterMobs[mobID] = child\n\n        for mob in item.composition_mobs():\n            child = _transcribe(mob, parent=item, masterMobs=masterMobs)\n            _add_child(result, child, mob)\n\n    elif isinstance(item, aaf.mob.Mob):\n        result = otio.schema.Timeline()\n\n        for slot in item.slots():\n            child = _transcribe(slot, parent=item, masterMobs=masterMobs)\n            _add_child(result.tracks, child, slot)\n\n    # @TODO: There are a bunch of other AAF object types that we will\n    # likely need to add support for. I'm leaving this code here to help\n    # future efforts to extract the useful information out of these.\n\n    # elif isinstance(item, aaf.storage.File):\n    #     self.extendChildItems([item.header])\n\n    # elif isinstance(item, aaf.storage.Header):\n    #     self.extendChildItems([item.storage()])\n    #     self.extendChildItems([item.dictionary()])\n\n    # elif isinstance(item, DummyItem):\n    #     self.extendChildItems(item.item)\n\n    # elif isinstance(item, aaf.dictionary.Dictionary):\n    #     l = []\n    #     l.append(DummyItem(list(item.class_defs()), 'ClassDefs'))\n    #     l.append(DummyItem(list(item.codec_defs()), 'CodecDefs'))\n    #     l.append(DummyItem(list(item.container_defs()), 'ContainerDefs'))\n    #     l.append(DummyItem(list(item.data_defs()), 'DataDefs'))\n    #     l.append(DummyItem(list(item.interpolation_defs()),\n    #        'InterpolationDefs'))\n    #     l.append(DummyItem(list(item.klvdata_defs()), 'KLVDataDefs'))\n    #     l.append(DummyItem(list(item.operation_defs()), 'OperationDefs'))\n    #     l.append(DummyItem(list(item.parameter_defs()), 'ParameterDefs'))\n    #     l.append(DummyItem(list(item.plugin_defs()), 'PluginDefs'))\n    #     l.append(DummyItem(list(item.taggedvalue_defs()), 'TaggedValueDefs'))\n    #     l.append(DummyItem(list(item.type_defs()), 'TypeDefs'))\n    #     self.extendChildItems(l)\n    #\n    # elif isinstance(item, aaf.mob.Mob):\n    #\n    #     self.extendChildItems(list(item.slots()))\n    #\n    # elif isinstance(item, aaf.mob.MobSlot):\n    #      self.extendChildItems([item.segment])\n    # elif isinstance(item, aaf.component.NestedScope):\n    #     self.extendChildItems(list(item.segments()))\n    # elif isinstance(item, aaf.component.Sequence):\n    #     self.extendChildItems(list(item.components()))\n    #\n    # elif isinstance(item, aaf.component.SourceClip):\n    #     ref = item.resolve_ref()\n    #     name = ref.name\n    #     if name:\n    #         self.extendChildItems([name])\n    #\n    # elif isinstance(item,aaf.component.OperationGroup):\n    #     self.extendChildItems(list(item.input_segments()))\n    #\n    #     elif isinstance(item, pyaaf.AxSelector):\n    #         self.extendChildItems(list(item.EnumAlternateSegments()))\n    #\n    #     elif isinstance(item, pyaaf.AxScopeReference):\n    #         #print item, item.GetRelativeScope(),item.GetRelativeSlot()\n    #         pass\n    #\n    #     elif isinstance(item, pyaaf.AxEssenceGroup):\n    #         segments = []\n    #\n    #         for i in xrange(item.CountChoices()):\n    #             choice = item.GetChoiceAt(i)\n    #             segments.append(choice)\n    #         self.extendChildItems(segments)\n    #\n    #     elif isinstance(item, pyaaf.AxProperty):\n    #         self.properties['Value'] = str(item.GetValue())\n    #\n    # elif isinstance(item, (aaf.base.AAFObject,aaf.define.MetaDef)):\n    #     pass\n    #\n    # elif isinstance(item, aaf.component.Component):\n    #     pass\n    #\n    # else:\n    #     self.properties['Name'] = str(item)\n    #     self.properties['ClassName'] = str(type(item))\n    #     return\n\n    elif isinstance(item, aaf.component.SourceClip):\n        result = otio.schema.Clip()\n\n        length = item.length\n        startTime = int(metadata.get(\"StartTime\", \"0\"))\n        result.source_range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(startTime, editRate),\n            otio.opentime.RationalTime(length, editRate)\n        )\n\n        mobID = metadata.get(\"SourceID\")\n        if masterMobs and mobID:\n            masterMob = masterMobs.get(mobID)\n            if masterMob:\n                media = otio.schema.MissingReference()\n                # copy the metadata from the master into the media_reference\n                media.metadata[\"AAF\"] = masterMob.metadata.get(\"AAF\", {})\n                result.media_reference = media\n\n    elif isinstance(item, aaf.component.Transition):\n        result = otio.schema.Transition()\n\n        # Does AAF support anything else?\n        result.transition_type = otio.schema.TransitionTypes.SMPTE_Dissolve\n\n        in_offset = int(metadata.get(\"CutPoint\", \"0\"))\n        out_offset = item.length - in_offset\n        result.in_offset = otio.opentime.RationalTime(in_offset, editRate)\n        result.out_offset = otio.opentime.RationalTime(out_offset, editRate)\n\n    elif isinstance(item, aaf.component.Filler):\n        result = otio.schema.Gap()\n\n        length = item.length\n        result.source_range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(0, editRate),\n            otio.opentime.RationalTime(length, editRate)\n        )\n\n    elif isinstance(item, aaf.component.NestedScope):\n        # TODO: Is this the right class?\n        result = otio.schema.Stack()\n\n        for segment in item.segments():\n            child = _transcribe(segment, parent=item, masterMobs=masterMobs)\n            _add_child(result, child, segment)\n\n    elif isinstance(item, aaf.component.Sequence):\n        result = otio.schema.Track()\n\n        for component in item.components():\n            child = _transcribe(component, parent=item, masterMobs=masterMobs)\n            _add_child(result, child, component)\n\n    elif isinstance(item, aaf.mob.TimelineMobSlot):\n        result = otio.schema.Track()\n\n        child = _transcribe(item.segment, parent=item, masterMobs=masterMobs)\n        _add_child(result, child, item.segment)\n        if child is not None:\n            child.metadata[\"AAF\"][\"MediaKind\"] = str(item.segment.media_kind)\n\n    elif isinstance(item, aaf.mob.MobSlot):\n        result = otio.schema.Track()\n\n        child = _transcribe(item.segment, parent=item, masterMobs=masterMobs)\n        _add_child(result, child, item.segment)\n\n    elif isinstance(item, aaf.component.Timecode):\n        pass\n    elif isinstance(item, aaf.component.Pulldown):\n        pass\n    elif isinstance(item, aaf.component.EdgeCode):\n        pass\n    elif isinstance(item, aaf.component.ScopeReference):\n        # TODO: is this like FILLER?\n\n        result = otio.schema.Gap()\n\n        length = item.length\n        result.source_range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(0, editRate),\n            otio.opentime.RationalTime(length, editRate)\n        )\n\n    elif isinstance(item, aaf.component.DescriptiveMarker):\n\n        # TODO: We can get markers this way, but they come in on\n        # a separate Track. We need to consolidate them onto the\n        # same track(s) as the Clips.\n        # result = otio.schema.Marker()\n        pass\n\n    else:\n        if debug:\n            print(\"SKIPPING: {}: {} -- {}\".format(type(item), item, result))\n\n    if result is not None:\n        result.name = str(metadata[\"Name\"])\n        if not result.metadata:\n            result.metadata = {}\n        result.metadata[\"AAF\"] = metadata\n\n    return result"
    },
    {
        "pr_title": "AAF adapter",
        "pr_number": 230,
        "file_name": "contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -131,24 +136,14 @@\ndef _transcribe(item, parent=None, editRate=24, masterMobs=None):\n             if hasattr(prop, 'name') and hasattr(prop, 'value'):\n                 key = str(prop.name)\n                 value = prop.value\n-                # if isinstance(value, aaf.dictionary.Dictionary):\n-                # ???\n                 metadata[key] = _transcribe_property(value)\n \n-    if False:\n-        pass\n-\n-    # elif isinstance(item, aaf.storage.File):\n-    #     self.extendChildItems([item.header])\n-\n-    # elif isinstance(item, aaf.storage.Header):\n-    #     self.extendChildItems([item.storage()])\n-    #     self.extendChildItems([item.dictionary()])\n-\n-    # elif isinstance(item, DummyItem):\n-    #     self.extendChildItems(item.item)\n+    # Now we will use the item's class to determine which OTIO type\n+    # to transcribe into. Note that the order of this if/elif/... chain\n+    # is important, because the class hierarchy of AAF objects is more\n+    # complex than OTIO.\n \n-    elif isinstance(item, aaf.storage.ContentStorage):\n+    if isinstance(item, aaf.storage.ContentStorage):\n         result = otio.schema.SerializableCollection()\n \n         # Gather all the Master Mobs, so we can find them later by MobID",
        "comments": [
            {
                "comment": "Commented out code - did you want to prune this out or leave this in for the future?",
                "position": null
            },
            {
                "comment": "This looks like you don't need an enumerate, just a regular loop should do.",
                "position": null
            }
        ],
        "commit_message": "Addressed some code review notes.",
        "commit_id": "b05c61769b1a72dd7a7f9945e8211cf44824c9ff",
        "fn_context": "def _transcribe(item, parent=None, editRate=24, masterMobs=None):\n    result = None\n    metadata = {}\n\n    # First lets grab some standard properties that are present on\n    # many types of AAF objects...\n    metadata[\"Name\"] = _get_name(item)\n    metadata[\"ClassName\"] = _get_class_name(item)\n\n    if isinstance(item, aaf.component.Component):\n        metadata[\"Length\"] = item.length\n\n    if isinstance(item, aaf.base.AAFObject):\n        for prop in item.properties():\n            if hasattr(prop, 'name') and hasattr(prop, 'value'):\n                key = str(prop.name)\n                value = prop.value\n                metadata[key] = _transcribe_property(value)\n\n    # Now we will use the item's class to determine which OTIO type\n    # to transcribe into. Note that the order of this if/elif/... chain\n    # is important, because the class hierarchy of AAF objects is more\n    # complex than OTIO.\n\n    if isinstance(item, aaf.storage.ContentStorage):\n        result = otio.schema.SerializableCollection()\n\n        # Gather all the Master Mobs, so we can find them later by MobID\n        # when we parse the SourceClips in the composition\n        if masterMobs is None:\n            masterMobs = {}\n        for mob in item.master_mobs():\n            child = _transcribe(mob, parent=item)\n            if child is not None:\n                mobID = child.metadata.get(\"AAF\", {}).get(\"MobID\")\n                masterMobs[mobID] = child\n\n        for mob in item.composition_mobs():\n            child = _transcribe(mob, parent=item, masterMobs=masterMobs)\n            _add_child(result, child, mob)\n\n    elif isinstance(item, aaf.mob.Mob):\n        result = otio.schema.Timeline()\n\n        for slot in item.slots():\n            child = _transcribe(slot, parent=item, masterMobs=masterMobs)\n            _add_child(result.tracks, child, slot)\n\n    # @TODO: There are a bunch of other AAF object types that we will\n    # likely need to add support for. I'm leaving this code here to help\n    # future efforts to extract the useful information out of these.\n\n    # elif isinstance(item, aaf.storage.File):\n    #     self.extendChildItems([item.header])\n\n    # elif isinstance(item, aaf.storage.Header):\n    #     self.extendChildItems([item.storage()])\n    #     self.extendChildItems([item.dictionary()])\n\n    # elif isinstance(item, DummyItem):\n    #     self.extendChildItems(item.item)\n\n    # elif isinstance(item, aaf.dictionary.Dictionary):\n    #     l = []\n    #     l.append(DummyItem(list(item.class_defs()), 'ClassDefs'))\n    #     l.append(DummyItem(list(item.codec_defs()), 'CodecDefs'))\n    #     l.append(DummyItem(list(item.container_defs()), 'ContainerDefs'))\n    #     l.append(DummyItem(list(item.data_defs()), 'DataDefs'))\n    #     l.append(DummyItem(list(item.interpolation_defs()),\n    #        'InterpolationDefs'))\n    #     l.append(DummyItem(list(item.klvdata_defs()), 'KLVDataDefs'))\n    #     l.append(DummyItem(list(item.operation_defs()), 'OperationDefs'))\n    #     l.append(DummyItem(list(item.parameter_defs()), 'ParameterDefs'))\n    #     l.append(DummyItem(list(item.plugin_defs()), 'PluginDefs'))\n    #     l.append(DummyItem(list(item.taggedvalue_defs()), 'TaggedValueDefs'))\n    #     l.append(DummyItem(list(item.type_defs()), 'TypeDefs'))\n    #     self.extendChildItems(l)\n    #\n    # elif isinstance(item, aaf.mob.Mob):\n    #\n    #     self.extendChildItems(list(item.slots()))\n    #\n    # elif isinstance(item, aaf.mob.MobSlot):\n    #      self.extendChildItems([item.segment])\n    # elif isinstance(item, aaf.component.NestedScope):\n    #     self.extendChildItems(list(item.segments()))\n    # elif isinstance(item, aaf.component.Sequence):\n    #     self.extendChildItems(list(item.components()))\n    #\n    # elif isinstance(item, aaf.component.SourceClip):\n    #     ref = item.resolve_ref()\n    #     name = ref.name\n    #     if name:\n    #         self.extendChildItems([name])\n    #\n    # elif isinstance(item,aaf.component.OperationGroup):\n    #     self.extendChildItems(list(item.input_segments()))\n    #\n    #     elif isinstance(item, pyaaf.AxSelector):\n    #         self.extendChildItems(list(item.EnumAlternateSegments()))\n    #\n    #     elif isinstance(item, pyaaf.AxScopeReference):\n    #         #print item, item.GetRelativeScope(),item.GetRelativeSlot()\n    #         pass\n    #\n    #     elif isinstance(item, pyaaf.AxEssenceGroup):\n    #         segments = []\n    #\n    #         for i in xrange(item.CountChoices()):\n    #             choice = item.GetChoiceAt(i)\n    #             segments.append(choice)\n    #         self.extendChildItems(segments)\n    #\n    #     elif isinstance(item, pyaaf.AxProperty):\n    #         self.properties['Value'] = str(item.GetValue())\n    #\n    # elif isinstance(item, (aaf.base.AAFObject,aaf.define.MetaDef)):\n    #     pass\n    #\n    # elif isinstance(item, aaf.component.Component):\n    #     pass\n    #\n    # else:\n    #     self.properties['Name'] = str(item)\n    #     self.properties['ClassName'] = str(type(item))\n    #     return\n\n    elif isinstance(item, aaf.component.SourceClip):\n        result = otio.schema.Clip()\n\n        length = item.length\n        startTime = int(metadata.get(\"StartTime\", \"0\"))\n        result.source_range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(startTime, editRate),\n            otio.opentime.RationalTime(length, editRate)\n        )\n\n        mobID = metadata.get(\"SourceID\")\n        if masterMobs and mobID:\n            masterMob = masterMobs.get(mobID)\n            if masterMob:\n                media = otio.schema.MissingReference()\n                # copy the metadata from the master into the media_reference\n                media.metadata[\"AAF\"] = masterMob.metadata.get(\"AAF\", {})\n                result.media_reference = media\n\n    elif isinstance(item, aaf.component.Transition):\n        result = otio.schema.Transition()\n\n        # Does AAF support anything else?\n        result.transition_type = otio.schema.TransitionTypes.SMPTE_Dissolve\n\n        in_offset = int(metadata.get(\"CutPoint\", \"0\"))\n        out_offset = item.length - in_offset\n        result.in_offset = otio.opentime.RationalTime(in_offset, editRate)\n        result.out_offset = otio.opentime.RationalTime(out_offset, editRate)\n\n    elif isinstance(item, aaf.component.Filler):\n        result = otio.schema.Gap()\n\n        length = item.length\n        result.source_range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(0, editRate),\n            otio.opentime.RationalTime(length, editRate)\n        )\n\n    elif isinstance(item, aaf.component.NestedScope):\n        # TODO: Is this the right class?\n        result = otio.schema.Stack()\n\n        for segment in item.segments():\n            child = _transcribe(segment, parent=item, masterMobs=masterMobs)\n            _add_child(result, child, segment)\n\n    elif isinstance(item, aaf.component.Sequence):\n        result = otio.schema.Track()\n\n        for component in item.components():\n            child = _transcribe(component, parent=item, masterMobs=masterMobs)\n            _add_child(result, child, component)\n\n    elif isinstance(item, aaf.mob.TimelineMobSlot):\n        result = otio.schema.Track()\n\n        child = _transcribe(item.segment, parent=item, masterMobs=masterMobs)\n        _add_child(result, child, item.segment)\n        if child is not None:\n            child.metadata[\"AAF\"][\"MediaKind\"] = str(item.segment.media_kind)\n\n    elif isinstance(item, aaf.mob.MobSlot):\n        result = otio.schema.Track()\n\n        child = _transcribe(item.segment, parent=item, masterMobs=masterMobs)\n        _add_child(result, child, item.segment)\n\n    elif isinstance(item, aaf.component.Timecode):\n        pass\n    elif isinstance(item, aaf.component.Pulldown):\n        pass\n    elif isinstance(item, aaf.component.EdgeCode):\n        pass\n    elif isinstance(item, aaf.component.ScopeReference):\n        # TODO: is this like FILLER?\n\n        result = otio.schema.Gap()\n\n        length = item.length\n        result.source_range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(0, editRate),\n            otio.opentime.RationalTime(length, editRate)\n        )\n\n    elif isinstance(item, aaf.component.DescriptiveMarker):\n\n        # TODO: We can get markers this way, but they come in on\n        # a separate Track. We need to consolidate them onto the\n        # same track(s) as the Clips.\n        # result = otio.schema.Marker()\n        pass\n\n    else:\n        if debug:\n            print(\"SKIPPING: {}: {} -- {}\".format(type(item), item, result))\n\n    if result is not None:\n        result.name = str(metadata[\"Name\"])\n        if not result.metadata:\n            result.metadata = {}\n        result.metadata[\"AAF\"] = metadata\n\n    return result"
    },
    {
        "pr_title": "AAF adapter",
        "pr_number": 230,
        "file_name": "contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -165,17 +160,27 @@\ndef _transcribe(item, parent=None, editRate=24, masterMobs=None):\n             child = _transcribe(mob, parent=item, masterMobs=masterMobs)\n             _add_child(result, child, mob)\n \n-        # for mob in item.GetSourceMobs():\n-        #     source = _transcribe(mob, parent=item, masterMobs=masterMobs)\n-        #     result.append(source)\n-\n     elif isinstance(item, aaf.mob.Mob):\n         result = otio.schema.Timeline()\n \n         for slot in item.slots():\n             child = _transcribe(slot, parent=item, masterMobs=masterMobs)\n             _add_child(result.tracks, child, slot)\n \n+    # @TODO: There are a bunch of other AAF object types that we will\n+    # likely need to add support for. I'm leaving this code here to help\n+    # future efforts to extract the useful information out of these.\n+\n+    # elif isinstance(item, aaf.storage.File):\n+    #     self.extendChildItems([item.header])\n+\n+    # elif isinstance(item, aaf.storage.Header):\n+    #     self.extendChildItems([item.storage()])\n+    #     self.extendChildItems([item.dictionary()])\n+\n+    # elif isinstance(item, DummyItem):\n+    #     self.extendChildItems(item.item)\n+\n     # elif isinstance(item, aaf.dictionary.Dictionary):\n     #     l = []\n     #     l.append(DummyItem(list(item.class_defs()), 'ClassDefs'))",
        "comments": [
            {
                "comment": "Commented out code - did you want to prune this out or leave this in for the future?",
                "position": null
            },
            {
                "comment": "More commented out code - did you want to prune this?  Otherwise leaving a #@TODO comment might help leave a note for future developers",
                "position": 182
            },
            {
                "comment": "This looks like you don't need an enumerate, just a regular loop should do.",
                "position": null
            }
        ],
        "commit_message": "Addressed some code review notes.",
        "commit_id": "b05c61769b1a72dd7a7f9945e8211cf44824c9ff",
        "fn_context": "def _transcribe(item, parent=None, editRate=24, masterMobs=None):\n    result = None\n    metadata = {}\n\n    # First lets grab some standard properties that are present on\n    # many types of AAF objects...\n    metadata[\"Name\"] = _get_name(item)\n    metadata[\"ClassName\"] = _get_class_name(item)\n\n    if isinstance(item, aaf.component.Component):\n        metadata[\"Length\"] = item.length\n\n    if isinstance(item, aaf.base.AAFObject):\n        for prop in item.properties():\n            if hasattr(prop, 'name') and hasattr(prop, 'value'):\n                key = str(prop.name)\n                value = prop.value\n                metadata[key] = _transcribe_property(value)\n\n    # Now we will use the item's class to determine which OTIO type\n    # to transcribe into. Note that the order of this if/elif/... chain\n    # is important, because the class hierarchy of AAF objects is more\n    # complex than OTIO.\n\n    if isinstance(item, aaf.storage.ContentStorage):\n        result = otio.schema.SerializableCollection()\n\n        # Gather all the Master Mobs, so we can find them later by MobID\n        # when we parse the SourceClips in the composition\n        if masterMobs is None:\n            masterMobs = {}\n        for mob in item.master_mobs():\n            child = _transcribe(mob, parent=item)\n            if child is not None:\n                mobID = child.metadata.get(\"AAF\", {}).get(\"MobID\")\n                masterMobs[mobID] = child\n\n        for mob in item.composition_mobs():\n            child = _transcribe(mob, parent=item, masterMobs=masterMobs)\n            _add_child(result, child, mob)\n\n    elif isinstance(item, aaf.mob.Mob):\n        result = otio.schema.Timeline()\n\n        for slot in item.slots():\n            child = _transcribe(slot, parent=item, masterMobs=masterMobs)\n            _add_child(result.tracks, child, slot)\n\n    # @TODO: There are a bunch of other AAF object types that we will\n    # likely need to add support for. I'm leaving this code here to help\n    # future efforts to extract the useful information out of these.\n\n    # elif isinstance(item, aaf.storage.File):\n    #     self.extendChildItems([item.header])\n\n    # elif isinstance(item, aaf.storage.Header):\n    #     self.extendChildItems([item.storage()])\n    #     self.extendChildItems([item.dictionary()])\n\n    # elif isinstance(item, DummyItem):\n    #     self.extendChildItems(item.item)\n\n    # elif isinstance(item, aaf.dictionary.Dictionary):\n    #     l = []\n    #     l.append(DummyItem(list(item.class_defs()), 'ClassDefs'))\n    #     l.append(DummyItem(list(item.codec_defs()), 'CodecDefs'))\n    #     l.append(DummyItem(list(item.container_defs()), 'ContainerDefs'))\n    #     l.append(DummyItem(list(item.data_defs()), 'DataDefs'))\n    #     l.append(DummyItem(list(item.interpolation_defs()),\n    #        'InterpolationDefs'))\n    #     l.append(DummyItem(list(item.klvdata_defs()), 'KLVDataDefs'))\n    #     l.append(DummyItem(list(item.operation_defs()), 'OperationDefs'))\n    #     l.append(DummyItem(list(item.parameter_defs()), 'ParameterDefs'))\n    #     l.append(DummyItem(list(item.plugin_defs()), 'PluginDefs'))\n    #     l.append(DummyItem(list(item.taggedvalue_defs()), 'TaggedValueDefs'))\n    #     l.append(DummyItem(list(item.type_defs()), 'TypeDefs'))\n    #     self.extendChildItems(l)\n    #\n    # elif isinstance(item, aaf.mob.Mob):\n    #\n    #     self.extendChildItems(list(item.slots()))\n    #\n    # elif isinstance(item, aaf.mob.MobSlot):\n    #      self.extendChildItems([item.segment])\n    # elif isinstance(item, aaf.component.NestedScope):\n    #     self.extendChildItems(list(item.segments()))\n    # elif isinstance(item, aaf.component.Sequence):\n    #     self.extendChildItems(list(item.components()))\n    #\n    # elif isinstance(item, aaf.component.SourceClip):\n    #     ref = item.resolve_ref()\n    #     name = ref.name\n    #     if name:\n    #         self.extendChildItems([name])\n    #\n    # elif isinstance(item,aaf.component.OperationGroup):\n    #     self.extendChildItems(list(item.input_segments()))\n    #\n    #     elif isinstance(item, pyaaf.AxSelector):\n    #         self.extendChildItems(list(item.EnumAlternateSegments()))\n    #\n    #     elif isinstance(item, pyaaf.AxScopeReference):\n    #         #print item, item.GetRelativeScope(),item.GetRelativeSlot()\n    #         pass\n    #\n    #     elif isinstance(item, pyaaf.AxEssenceGroup):\n    #         segments = []\n    #\n    #         for i in xrange(item.CountChoices()):\n    #             choice = item.GetChoiceAt(i)\n    #             segments.append(choice)\n    #         self.extendChildItems(segments)\n    #\n    #     elif isinstance(item, pyaaf.AxProperty):\n    #         self.properties['Value'] = str(item.GetValue())\n    #\n    # elif isinstance(item, (aaf.base.AAFObject,aaf.define.MetaDef)):\n    #     pass\n    #\n    # elif isinstance(item, aaf.component.Component):\n    #     pass\n    #\n    # else:\n    #     self.properties['Name'] = str(item)\n    #     self.properties['ClassName'] = str(type(item))\n    #     return\n\n    elif isinstance(item, aaf.component.SourceClip):\n        result = otio.schema.Clip()\n\n        length = item.length\n        startTime = int(metadata.get(\"StartTime\", \"0\"))\n        result.source_range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(startTime, editRate),\n            otio.opentime.RationalTime(length, editRate)\n        )\n\n        mobID = metadata.get(\"SourceID\")\n        if masterMobs and mobID:\n            masterMob = masterMobs.get(mobID)\n            if masterMob:\n                media = otio.schema.MissingReference()\n                # copy the metadata from the master into the media_reference\n                media.metadata[\"AAF\"] = masterMob.metadata.get(\"AAF\", {})\n                result.media_reference = media\n\n    elif isinstance(item, aaf.component.Transition):\n        result = otio.schema.Transition()\n\n        # Does AAF support anything else?\n        result.transition_type = otio.schema.TransitionTypes.SMPTE_Dissolve\n\n        in_offset = int(metadata.get(\"CutPoint\", \"0\"))\n        out_offset = item.length - in_offset\n        result.in_offset = otio.opentime.RationalTime(in_offset, editRate)\n        result.out_offset = otio.opentime.RationalTime(out_offset, editRate)\n\n    elif isinstance(item, aaf.component.Filler):\n        result = otio.schema.Gap()\n\n        length = item.length\n        result.source_range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(0, editRate),\n            otio.opentime.RationalTime(length, editRate)\n        )\n\n    elif isinstance(item, aaf.component.NestedScope):\n        # TODO: Is this the right class?\n        result = otio.schema.Stack()\n\n        for segment in item.segments():\n            child = _transcribe(segment, parent=item, masterMobs=masterMobs)\n            _add_child(result, child, segment)\n\n    elif isinstance(item, aaf.component.Sequence):\n        result = otio.schema.Track()\n\n        for component in item.components():\n            child = _transcribe(component, parent=item, masterMobs=masterMobs)\n            _add_child(result, child, component)\n\n    elif isinstance(item, aaf.mob.TimelineMobSlot):\n        result = otio.schema.Track()\n\n        child = _transcribe(item.segment, parent=item, masterMobs=masterMobs)\n        _add_child(result, child, item.segment)\n        if child is not None:\n            child.metadata[\"AAF\"][\"MediaKind\"] = str(item.segment.media_kind)\n\n    elif isinstance(item, aaf.mob.MobSlot):\n        result = otio.schema.Track()\n\n        child = _transcribe(item.segment, parent=item, masterMobs=masterMobs)\n        _add_child(result, child, item.segment)\n\n    elif isinstance(item, aaf.component.Timecode):\n        pass\n    elif isinstance(item, aaf.component.Pulldown):\n        pass\n    elif isinstance(item, aaf.component.EdgeCode):\n        pass\n    elif isinstance(item, aaf.component.ScopeReference):\n        # TODO: is this like FILLER?\n\n        result = otio.schema.Gap()\n\n        length = item.length\n        result.source_range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(0, editRate),\n            otio.opentime.RationalTime(length, editRate)\n        )\n\n    elif isinstance(item, aaf.component.DescriptiveMarker):\n\n        # TODO: We can get markers this way, but they come in on\n        # a separate Track. We need to consolidate them onto the\n        # same track(s) as the Clips.\n        # result = otio.schema.Marker()\n        pass\n\n    else:\n        if debug:\n            print(\"SKIPPING: {}: {} -- {}\".format(type(item), item, result))\n\n    if result is not None:\n        result.name = str(metadata[\"Name\"])\n        if not result.metadata:\n            result.metadata = {}\n        result.metadata[\"AAF\"] = metadata\n\n    return result"
    },
    {
        "pr_title": "AAF adapter",
        "pr_number": 230,
        "file_name": "contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -211,24 +216,25 @@\ndef _transcribe(item, parent=None, editRate=24, masterMobs=None):\n     #\n     # elif isinstance(item,aaf.component.OperationGroup):\n     #     self.extendChildItems(list(item.input_segments()))\n-\n-#         elif isinstance(item, pyaaf.AxSelector):\n-#             self.extendChildItems(list(item.EnumAlternateSegments()))\n-#\n-#         elif isinstance(item, pyaaf.AxScopeReference):\n-#             #print item, item.GetRelativeScope(),item.GetRelativeSlot()\n-#             pass\n-#\n-#         elif isinstance(item, pyaaf.AxEssenceGroup):\n-#             segments = []\n-#\n-#             for i in xrange(item.CountChoices()):\n-#                 choice = item.GetChoiceAt(i)\n-#                 segments.append(choice)\n-#             self.extendChildItems(segments)\n-#\n-#         elif isinstance(item, pyaaf.AxProperty):\n-#             self.properties['Value'] = str(item.GetValue())\n+    #\n+    #     elif isinstance(item, pyaaf.AxSelector):\n+    #         self.extendChildItems(list(item.EnumAlternateSegments()))\n+    #\n+    #     elif isinstance(item, pyaaf.AxScopeReference):\n+    #         #print item, item.GetRelativeScope(),item.GetRelativeSlot()\n+    #         pass\n+    #\n+    #     elif isinstance(item, pyaaf.AxEssenceGroup):\n+    #         segments = []\n+    #\n+    #         for i in xrange(item.CountChoices()):\n+    #             choice = item.GetChoiceAt(i)\n+    #             segments.append(choice)\n+    #         self.extendChildItems(segments)\n+    #\n+    #     elif isinstance(item, pyaaf.AxProperty):\n+    #         self.properties['Value'] = str(item.GetValue())\n+    #\n     # elif isinstance(item, (aaf.base.AAFObject,aaf.define.MetaDef)):\n     #     pass\n     #",
        "comments": [
            {
                "comment": "Commented out code - did you want to prune this out or leave this in for the future?",
                "position": null
            },
            {
                "comment": "This looks like you don't need an enumerate, just a regular loop should do.",
                "position": null
            }
        ],
        "commit_message": "Addressed some code review notes.",
        "commit_id": "b05c61769b1a72dd7a7f9945e8211cf44824c9ff",
        "fn_context": "def _transcribe(item, parent=None, editRate=24, masterMobs=None):\n    result = None\n    metadata = {}\n\n    # First lets grab some standard properties that are present on\n    # many types of AAF objects...\n    metadata[\"Name\"] = _get_name(item)\n    metadata[\"ClassName\"] = _get_class_name(item)\n\n    if isinstance(item, aaf.component.Component):\n        metadata[\"Length\"] = item.length\n\n    if isinstance(item, aaf.base.AAFObject):\n        for prop in item.properties():\n            if hasattr(prop, 'name') and hasattr(prop, 'value'):\n                key = str(prop.name)\n                value = prop.value\n                metadata[key] = _transcribe_property(value)\n\n    # Now we will use the item's class to determine which OTIO type\n    # to transcribe into. Note that the order of this if/elif/... chain\n    # is important, because the class hierarchy of AAF objects is more\n    # complex than OTIO.\n\n    if isinstance(item, aaf.storage.ContentStorage):\n        result = otio.schema.SerializableCollection()\n\n        # Gather all the Master Mobs, so we can find them later by MobID\n        # when we parse the SourceClips in the composition\n        if masterMobs is None:\n            masterMobs = {}\n        for mob in item.master_mobs():\n            child = _transcribe(mob, parent=item)\n            if child is not None:\n                mobID = child.metadata.get(\"AAF\", {}).get(\"MobID\")\n                masterMobs[mobID] = child\n\n        for mob in item.composition_mobs():\n            child = _transcribe(mob, parent=item, masterMobs=masterMobs)\n            _add_child(result, child, mob)\n\n    elif isinstance(item, aaf.mob.Mob):\n        result = otio.schema.Timeline()\n\n        for slot in item.slots():\n            child = _transcribe(slot, parent=item, masterMobs=masterMobs)\n            _add_child(result.tracks, child, slot)\n\n    # @TODO: There are a bunch of other AAF object types that we will\n    # likely need to add support for. I'm leaving this code here to help\n    # future efforts to extract the useful information out of these.\n\n    # elif isinstance(item, aaf.storage.File):\n    #     self.extendChildItems([item.header])\n\n    # elif isinstance(item, aaf.storage.Header):\n    #     self.extendChildItems([item.storage()])\n    #     self.extendChildItems([item.dictionary()])\n\n    # elif isinstance(item, DummyItem):\n    #     self.extendChildItems(item.item)\n\n    # elif isinstance(item, aaf.dictionary.Dictionary):\n    #     l = []\n    #     l.append(DummyItem(list(item.class_defs()), 'ClassDefs'))\n    #     l.append(DummyItem(list(item.codec_defs()), 'CodecDefs'))\n    #     l.append(DummyItem(list(item.container_defs()), 'ContainerDefs'))\n    #     l.append(DummyItem(list(item.data_defs()), 'DataDefs'))\n    #     l.append(DummyItem(list(item.interpolation_defs()),\n    #        'InterpolationDefs'))\n    #     l.append(DummyItem(list(item.klvdata_defs()), 'KLVDataDefs'))\n    #     l.append(DummyItem(list(item.operation_defs()), 'OperationDefs'))\n    #     l.append(DummyItem(list(item.parameter_defs()), 'ParameterDefs'))\n    #     l.append(DummyItem(list(item.plugin_defs()), 'PluginDefs'))\n    #     l.append(DummyItem(list(item.taggedvalue_defs()), 'TaggedValueDefs'))\n    #     l.append(DummyItem(list(item.type_defs()), 'TypeDefs'))\n    #     self.extendChildItems(l)\n    #\n    # elif isinstance(item, aaf.mob.Mob):\n    #\n    #     self.extendChildItems(list(item.slots()))\n    #\n    # elif isinstance(item, aaf.mob.MobSlot):\n    #      self.extendChildItems([item.segment])\n    # elif isinstance(item, aaf.component.NestedScope):\n    #     self.extendChildItems(list(item.segments()))\n    # elif isinstance(item, aaf.component.Sequence):\n    #     self.extendChildItems(list(item.components()))\n    #\n    # elif isinstance(item, aaf.component.SourceClip):\n    #     ref = item.resolve_ref()\n    #     name = ref.name\n    #     if name:\n    #         self.extendChildItems([name])\n    #\n    # elif isinstance(item,aaf.component.OperationGroup):\n    #     self.extendChildItems(list(item.input_segments()))\n    #\n    #     elif isinstance(item, pyaaf.AxSelector):\n    #         self.extendChildItems(list(item.EnumAlternateSegments()))\n    #\n    #     elif isinstance(item, pyaaf.AxScopeReference):\n    #         #print item, item.GetRelativeScope(),item.GetRelativeSlot()\n    #         pass\n    #\n    #     elif isinstance(item, pyaaf.AxEssenceGroup):\n    #         segments = []\n    #\n    #         for i in xrange(item.CountChoices()):\n    #             choice = item.GetChoiceAt(i)\n    #             segments.append(choice)\n    #         self.extendChildItems(segments)\n    #\n    #     elif isinstance(item, pyaaf.AxProperty):\n    #         self.properties['Value'] = str(item.GetValue())\n    #\n    # elif isinstance(item, (aaf.base.AAFObject,aaf.define.MetaDef)):\n    #     pass\n    #\n    # elif isinstance(item, aaf.component.Component):\n    #     pass\n    #\n    # else:\n    #     self.properties['Name'] = str(item)\n    #     self.properties['ClassName'] = str(type(item))\n    #     return\n\n    elif isinstance(item, aaf.component.SourceClip):\n        result = otio.schema.Clip()\n\n        length = item.length\n        startTime = int(metadata.get(\"StartTime\", \"0\"))\n        result.source_range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(startTime, editRate),\n            otio.opentime.RationalTime(length, editRate)\n        )\n\n        mobID = metadata.get(\"SourceID\")\n        if masterMobs and mobID:\n            masterMob = masterMobs.get(mobID)\n            if masterMob:\n                media = otio.schema.MissingReference()\n                # copy the metadata from the master into the media_reference\n                media.metadata[\"AAF\"] = masterMob.metadata.get(\"AAF\", {})\n                result.media_reference = media\n\n    elif isinstance(item, aaf.component.Transition):\n        result = otio.schema.Transition()\n\n        # Does AAF support anything else?\n        result.transition_type = otio.schema.TransitionTypes.SMPTE_Dissolve\n\n        in_offset = int(metadata.get(\"CutPoint\", \"0\"))\n        out_offset = item.length - in_offset\n        result.in_offset = otio.opentime.RationalTime(in_offset, editRate)\n        result.out_offset = otio.opentime.RationalTime(out_offset, editRate)\n\n    elif isinstance(item, aaf.component.Filler):\n        result = otio.schema.Gap()\n\n        length = item.length\n        result.source_range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(0, editRate),\n            otio.opentime.RationalTime(length, editRate)\n        )\n\n    elif isinstance(item, aaf.component.NestedScope):\n        # TODO: Is this the right class?\n        result = otio.schema.Stack()\n\n        for segment in item.segments():\n            child = _transcribe(segment, parent=item, masterMobs=masterMobs)\n            _add_child(result, child, segment)\n\n    elif isinstance(item, aaf.component.Sequence):\n        result = otio.schema.Track()\n\n        for component in item.components():\n            child = _transcribe(component, parent=item, masterMobs=masterMobs)\n            _add_child(result, child, component)\n\n    elif isinstance(item, aaf.mob.TimelineMobSlot):\n        result = otio.schema.Track()\n\n        child = _transcribe(item.segment, parent=item, masterMobs=masterMobs)\n        _add_child(result, child, item.segment)\n        if child is not None:\n            child.metadata[\"AAF\"][\"MediaKind\"] = str(item.segment.media_kind)\n\n    elif isinstance(item, aaf.mob.MobSlot):\n        result = otio.schema.Track()\n\n        child = _transcribe(item.segment, parent=item, masterMobs=masterMobs)\n        _add_child(result, child, item.segment)\n\n    elif isinstance(item, aaf.component.Timecode):\n        pass\n    elif isinstance(item, aaf.component.Pulldown):\n        pass\n    elif isinstance(item, aaf.component.EdgeCode):\n        pass\n    elif isinstance(item, aaf.component.ScopeReference):\n        # TODO: is this like FILLER?\n\n        result = otio.schema.Gap()\n\n        length = item.length\n        result.source_range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(0, editRate),\n            otio.opentime.RationalTime(length, editRate)\n        )\n\n    elif isinstance(item, aaf.component.DescriptiveMarker):\n\n        # TODO: We can get markers this way, but they come in on\n        # a separate Track. We need to consolidate them onto the\n        # same track(s) as the Clips.\n        # result = otio.schema.Marker()\n        pass\n\n    else:\n        if debug:\n            print(\"SKIPPING: {}: {} -- {}\".format(type(item), item, result))\n\n    if result is not None:\n        result.name = str(metadata[\"Name\"])\n        if not result.metadata:\n            result.metadata = {}\n        result.metadata[\"AAF\"] = metadata\n\n    return result"
    },
    {
        "pr_title": "AAF adapter",
        "pr_number": 230,
        "file_name": "contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -334,7 +340,6 @@\ndef _transcribe(item, parent=None, editRate=24, masterMobs=None):\n         pass\n \n     else:\n-        # result = otio.core.Composition()\n         if debug:\n             print(\"SKIPPING: {}: {} -- {}\".format(type(item), item, result))",
        "comments": [
            {
                "comment": "Commented out code - did you want to prune this out or leave this in for the future?",
                "position": null
            },
            {
                "comment": "This looks like you don't need an enumerate, just a regular loop should do.",
                "position": null
            }
        ],
        "commit_message": "Addressed some code review notes.",
        "commit_id": "b05c61769b1a72dd7a7f9945e8211cf44824c9ff",
        "fn_context": "def _transcribe(item, parent=None, editRate=24, masterMobs=None):\n    result = None\n    metadata = {}\n\n    # First lets grab some standard properties that are present on\n    # many types of AAF objects...\n    metadata[\"Name\"] = _get_name(item)\n    metadata[\"ClassName\"] = _get_class_name(item)\n\n    if isinstance(item, aaf.component.Component):\n        metadata[\"Length\"] = item.length\n\n    if isinstance(item, aaf.base.AAFObject):\n        for prop in item.properties():\n            if hasattr(prop, 'name') and hasattr(prop, 'value'):\n                key = str(prop.name)\n                value = prop.value\n                metadata[key] = _transcribe_property(value)\n\n    # Now we will use the item's class to determine which OTIO type\n    # to transcribe into. Note that the order of this if/elif/... chain\n    # is important, because the class hierarchy of AAF objects is more\n    # complex than OTIO.\n\n    if isinstance(item, aaf.storage.ContentStorage):\n        result = otio.schema.SerializableCollection()\n\n        # Gather all the Master Mobs, so we can find them later by MobID\n        # when we parse the SourceClips in the composition\n        if masterMobs is None:\n            masterMobs = {}\n        for mob in item.master_mobs():\n            child = _transcribe(mob, parent=item)\n            if child is not None:\n                mobID = child.metadata.get(\"AAF\", {}).get(\"MobID\")\n                masterMobs[mobID] = child\n\n        for mob in item.composition_mobs():\n            child = _transcribe(mob, parent=item, masterMobs=masterMobs)\n            _add_child(result, child, mob)\n\n    elif isinstance(item, aaf.mob.Mob):\n        result = otio.schema.Timeline()\n\n        for slot in item.slots():\n            child = _transcribe(slot, parent=item, masterMobs=masterMobs)\n            _add_child(result.tracks, child, slot)\n\n    # @TODO: There are a bunch of other AAF object types that we will\n    # likely need to add support for. I'm leaving this code here to help\n    # future efforts to extract the useful information out of these.\n\n    # elif isinstance(item, aaf.storage.File):\n    #     self.extendChildItems([item.header])\n\n    # elif isinstance(item, aaf.storage.Header):\n    #     self.extendChildItems([item.storage()])\n    #     self.extendChildItems([item.dictionary()])\n\n    # elif isinstance(item, DummyItem):\n    #     self.extendChildItems(item.item)\n\n    # elif isinstance(item, aaf.dictionary.Dictionary):\n    #     l = []\n    #     l.append(DummyItem(list(item.class_defs()), 'ClassDefs'))\n    #     l.append(DummyItem(list(item.codec_defs()), 'CodecDefs'))\n    #     l.append(DummyItem(list(item.container_defs()), 'ContainerDefs'))\n    #     l.append(DummyItem(list(item.data_defs()), 'DataDefs'))\n    #     l.append(DummyItem(list(item.interpolation_defs()),\n    #        'InterpolationDefs'))\n    #     l.append(DummyItem(list(item.klvdata_defs()), 'KLVDataDefs'))\n    #     l.append(DummyItem(list(item.operation_defs()), 'OperationDefs'))\n    #     l.append(DummyItem(list(item.parameter_defs()), 'ParameterDefs'))\n    #     l.append(DummyItem(list(item.plugin_defs()), 'PluginDefs'))\n    #     l.append(DummyItem(list(item.taggedvalue_defs()), 'TaggedValueDefs'))\n    #     l.append(DummyItem(list(item.type_defs()), 'TypeDefs'))\n    #     self.extendChildItems(l)\n    #\n    # elif isinstance(item, aaf.mob.Mob):\n    #\n    #     self.extendChildItems(list(item.slots()))\n    #\n    # elif isinstance(item, aaf.mob.MobSlot):\n    #      self.extendChildItems([item.segment])\n    # elif isinstance(item, aaf.component.NestedScope):\n    #     self.extendChildItems(list(item.segments()))\n    # elif isinstance(item, aaf.component.Sequence):\n    #     self.extendChildItems(list(item.components()))\n    #\n    # elif isinstance(item, aaf.component.SourceClip):\n    #     ref = item.resolve_ref()\n    #     name = ref.name\n    #     if name:\n    #         self.extendChildItems([name])\n    #\n    # elif isinstance(item,aaf.component.OperationGroup):\n    #     self.extendChildItems(list(item.input_segments()))\n    #\n    #     elif isinstance(item, pyaaf.AxSelector):\n    #         self.extendChildItems(list(item.EnumAlternateSegments()))\n    #\n    #     elif isinstance(item, pyaaf.AxScopeReference):\n    #         #print item, item.GetRelativeScope(),item.GetRelativeSlot()\n    #         pass\n    #\n    #     elif isinstance(item, pyaaf.AxEssenceGroup):\n    #         segments = []\n    #\n    #         for i in xrange(item.CountChoices()):\n    #             choice = item.GetChoiceAt(i)\n    #             segments.append(choice)\n    #         self.extendChildItems(segments)\n    #\n    #     elif isinstance(item, pyaaf.AxProperty):\n    #         self.properties['Value'] = str(item.GetValue())\n    #\n    # elif isinstance(item, (aaf.base.AAFObject,aaf.define.MetaDef)):\n    #     pass\n    #\n    # elif isinstance(item, aaf.component.Component):\n    #     pass\n    #\n    # else:\n    #     self.properties['Name'] = str(item)\n    #     self.properties['ClassName'] = str(type(item))\n    #     return\n\n    elif isinstance(item, aaf.component.SourceClip):\n        result = otio.schema.Clip()\n\n        length = item.length\n        startTime = int(metadata.get(\"StartTime\", \"0\"))\n        result.source_range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(startTime, editRate),\n            otio.opentime.RationalTime(length, editRate)\n        )\n\n        mobID = metadata.get(\"SourceID\")\n        if masterMobs and mobID:\n            masterMob = masterMobs.get(mobID)\n            if masterMob:\n                media = otio.schema.MissingReference()\n                # copy the metadata from the master into the media_reference\n                media.metadata[\"AAF\"] = masterMob.metadata.get(\"AAF\", {})\n                result.media_reference = media\n\n    elif isinstance(item, aaf.component.Transition):\n        result = otio.schema.Transition()\n\n        # Does AAF support anything else?\n        result.transition_type = otio.schema.TransitionTypes.SMPTE_Dissolve\n\n        in_offset = int(metadata.get(\"CutPoint\", \"0\"))\n        out_offset = item.length - in_offset\n        result.in_offset = otio.opentime.RationalTime(in_offset, editRate)\n        result.out_offset = otio.opentime.RationalTime(out_offset, editRate)\n\n    elif isinstance(item, aaf.component.Filler):\n        result = otio.schema.Gap()\n\n        length = item.length\n        result.source_range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(0, editRate),\n            otio.opentime.RationalTime(length, editRate)\n        )\n\n    elif isinstance(item, aaf.component.NestedScope):\n        # TODO: Is this the right class?\n        result = otio.schema.Stack()\n\n        for segment in item.segments():\n            child = _transcribe(segment, parent=item, masterMobs=masterMobs)\n            _add_child(result, child, segment)\n\n    elif isinstance(item, aaf.component.Sequence):\n        result = otio.schema.Track()\n\n        for component in item.components():\n            child = _transcribe(component, parent=item, masterMobs=masterMobs)\n            _add_child(result, child, component)\n\n    elif isinstance(item, aaf.mob.TimelineMobSlot):\n        result = otio.schema.Track()\n\n        child = _transcribe(item.segment, parent=item, masterMobs=masterMobs)\n        _add_child(result, child, item.segment)\n        if child is not None:\n            child.metadata[\"AAF\"][\"MediaKind\"] = str(item.segment.media_kind)\n\n    elif isinstance(item, aaf.mob.MobSlot):\n        result = otio.schema.Track()\n\n        child = _transcribe(item.segment, parent=item, masterMobs=masterMobs)\n        _add_child(result, child, item.segment)\n\n    elif isinstance(item, aaf.component.Timecode):\n        pass\n    elif isinstance(item, aaf.component.Pulldown):\n        pass\n    elif isinstance(item, aaf.component.EdgeCode):\n        pass\n    elif isinstance(item, aaf.component.ScopeReference):\n        # TODO: is this like FILLER?\n\n        result = otio.schema.Gap()\n\n        length = item.length\n        result.source_range = otio.opentime.TimeRange(\n            otio.opentime.RationalTime(0, editRate),\n            otio.opentime.RationalTime(length, editRate)\n        )\n\n    elif isinstance(item, aaf.component.DescriptiveMarker):\n\n        # TODO: We can get markers this way, but they come in on\n        # a separate Track. We need to consolidate them onto the\n        # same track(s) as the Clips.\n        # result = otio.schema.Marker()\n        pass\n\n    else:\n        if debug:\n            print(\"SKIPPING: {}: {} -- {}\".format(type(item), item, result))\n\n    if result is not None:\n        result.name = str(metadata[\"Name\"])\n        if not result.metadata:\n            result.metadata = {}\n        result.metadata[\"AAF\"] = metadata\n\n    return result"
    },
    {
        "pr_title": "AAF adapter",
        "pr_number": 230,
        "file_name": "contrib/adapters/advanced_authoring_format.py",
        "code_diff": "@@ -385,7 +390,7 @@\ndef _fix_transitions(thing):\n                         child.source_range = child.trimmed_range()\n                     child.source_range.duration -= after.out_offset\n \n-        for c, child in enumerate(thing):\n+        for child in thing:\n             _fix_transitions(child)",
        "comments": [
            {
                "comment": "Commented out code - did you want to prune this out or leave this in for the future?",
                "position": null
            },
            {
                "comment": "This looks like you don't need an enumerate, just a regular loop should do.",
                "position": null
            }
        ],
        "commit_message": "Addressed some code review notes.",
        "commit_id": "b05c61769b1a72dd7a7f9945e8211cf44824c9ff",
        "fn_context": "def _fix_transitions(thing):\n    if isinstance(thing, otio.schema.Timeline):\n        _fix_transitions(thing.tracks)\n    elif (\n        isinstance(thing, otio.core.Composition) or\n        isinstance(thing, otio.schema.SerializableCollection)\n    ):\n        if isinstance(thing, otio.schema.Track):\n            for c, child in enumerate(thing):\n\n                # Don't touch the Transitions themselves,\n                # only the Clips & Gaps next to them.\n                if not isinstance(child, otio.core.Item):\n                    continue\n\n                # Was the item before us a Transition?\n                if c > 0 and isinstance(\n                    thing[c-1],\n                    otio.schema.Transition\n                ):\n                    trans = thing[c-1]\n\n                    if child.source_range is None:\n                        child.source_range = child.trimmed_range()\n                    child.source_range.start_time += trans.in_offset\n                    child.source_range.duration -= trans.in_offset\n\n                # Is the item after us a Transition?\n                if c < len(thing)-1 and isinstance(\n                    thing[c+1],\n                    otio.schema.Transition\n                ):\n                    after = thing[c+1]\n\n                    if child.source_range is None:\n                        child.source_range = child.trimmed_range()\n                    child.source_range.duration -= after.out_offset\n\n        for child in thing:\n            _fix_transitions(child)"
    },
    {
        "pr_title": "Convenience methods for tracks, visible_range, etc.",
        "pr_number": 212,
        "file_name": "tests/test_documentation.py",
        "code_diff": "@@ -42,7 +42,7 @@\ndef test_clip(self):\n         gapA, clip, transition, gapB = track[:]\n \n         self.assertEqual(\n-            otio.opentime.RationalTime(19,24),\n+            otio.opentime.RationalTime(19, 24),\n             track.duration()\n         )\n         self.assertEqual(",
        "comments": [],
        "commit_message": "Replaced the ambiguous visible_range_of_child() method with handles_of_child()",
        "commit_id": "03050f1b2ab0e4db644865b6090d01190cd88dc7"
    },
    {
        "pr_title": "Convenience methods for tracks, visible_range, etc.",
        "pr_number": 212,
        "file_name": "tests/test_documentation.py",
        "code_diff": "@@ -52,6 +52,20 @@\ndef test_clip(self):\n             ),\n             track.trimmed_range()\n         )\n+        self.assertEqual(\n+            otio.opentime.TimeRange(\n+                start_time=otio.opentime.RationalTime(0, 24),\n+                duration=otio.opentime.RationalTime(19, 24)\n+            ),\n+            track.available_range()\n+        )\n+        self.assertEqual(\n+            otio.opentime.TimeRange(\n+                start_time=otio.opentime.RationalTime(0, 24),\n+                duration=otio.opentime.RationalTime(19, 24)\n+            ),\n+            track.visible_range()\n+        )\n \n         self.assertEqual(\n             otio.opentime.TimeRange(",
        "comments": [],
        "commit_message": "Replaced the ambiguous visible_range_of_child() method with handles_of_child()",
        "commit_id": "03050f1b2ab0e4db644865b6090d01190cd88dc7"
    },
    {
        "pr_title": "Convenience methods for tracks, visible_range, etc.",
        "pr_number": 212,
        "file_name": "tests/test_documentation.py",
        "code_diff": "@@ -60,21 +74,13 @@\ndef test_clip(self):\n             ),\n             track.trimmed_range_of_child(clip)\n         )\n-\n-        before = otio.adapters.write_to_string(timeline, 'otio_json')\n-\n         self.assertEqual(\n-            otio.opentime.TimeRange(\n-                start_time=otio.opentime.RationalTime(3, 24),\n-                duration=otio.opentime.RationalTime(4, 24)\n+            (\n+                None,\n+                otio.opentime.RationalTime(1, 24)\n             ),\n-            track.visible_range_of_child(clip)\n+            track.handles_of_child(clip)\n         )\n-\n-        after = otio.adapters.write_to_string(timeline, 'otio_json')\n-        self.maxDiff = None\n-        self.assertMultiLineEqual(before, after)\n-\n         self.assertEqual(\n             otio.opentime.TimeRange(\n                 start_time=otio.opentime.RationalTime(8, 24),",
        "comments": [],
        "commit_message": "Replaced the ambiguous visible_range_of_child() method with handles_of_child()",
        "commit_id": "03050f1b2ab0e4db644865b6090d01190cd88dc7"
    },
    {
        "pr_title": "Convenience methods for tracks, visible_range, etc.",
        "pr_number": 212,
        "file_name": "tests/test_documentation.py",
        "code_diff": "@@ -91,12 +97,12 @@\ndef test_clip(self):\n         )\n \n         self.assertEqual(\n-            otio.opentime.RationalTime(8,24),\n+            otio.opentime.RationalTime(8, 24),\n             gapA.duration()\n         )\n \n         self.assertEqual(\n-            otio.opentime.RationalTime(3,24),\n+            otio.opentime.RationalTime(3, 24),\n             clip.duration()\n         )\n         self.assertEqual(",
        "comments": [],
        "commit_message": "Replaced the ambiguous visible_range_of_child() method with handles_of_child()",
        "commit_id": "03050f1b2ab0e4db644865b6090d01190cd88dc7"
    },
    {
        "pr_title": "Support EDL dissolve writes",
        "pr_number": 200,
        "file_name": "opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -512,7 +512,7 @@\ndef parse(self, comment):\n                 self.unhandled.append(stripped)\n \n \n-def expand_transitions(timeline):\n+def _expand_transitions(timeline):\n     \"\"\"Convert clips with metadata/transition == 'D' into OTIO transitions.\"\"\"\n \n     tracks = timeline.tracks",
        "comments": [
            {
                "comment": "A media_reference will never be a Gap. The `clip` argument might be a Gap.",
                "position": null
            },
            {
                "comment": "Top level functions like this should be prefixed with _ to make them hidden, so people outside this module won't accidentally find them (in help, etc.)",
                "position": null
            },
            {
                "comment": "Sounds great.  Updated.  There are existing top-level functions that don't follow convention and will need similar treatment in the future.",
                "position": null
            },
            {
                "comment": "Great catch!  This was existing logic but I've updated it here.",
                "position": null
            },
            {
                "comment": "If you're willing to _ those too, that'd be great, or I'm happy to do it after this lands.\r\n",
                "position": null
            },
            {
                "comment": "Looks like there was only one more \ud83c\udf89 .  Updated.",
                "position": null
            }
        ],
        "commit_message": "Hide one last top-level function",
        "commit_id": "1f81db2fe56b79c211d39319500c05804d1e1fa2",
        "fn_context": "    def parse(self, comment):\n        for comment_id, comment_type in self.comment_id_map.items():\n            regex = self.regex_template.format(id=comment_id)\n            match = re.match(regex, comment)\n            if match:\n                self.handled[comment_type] = match.group(\n                    'comment_body'\n                ).strip()\n                break\n        else:\n            stripped = comment.lstrip('*').strip()\n            if stripped:\n                self.unhandled.append(stripped)\n\ndef _expand_transitions(timeline):\n    \"\"\"Convert clips with metadata/transition == 'D' into OTIO transitions.\"\"\"\n\n    tracks = timeline.tracks\n    remove_list = []\n    replace_list = []\n    append_list = []\n    for track in tracks:\n        track_iter = iter(track)\n        # avid inserts an extra clip for the source\n        prev_prev = None\n        prev = None\n        clip = next(track_iter, None)\n        next_clip = next(track_iter, None)\n        while clip is not None:\n            transition_type = clip.metadata.get('cmx_3600', {}).get(\n                'transition',\n                'C'\n            )\n\n            if transition_type == 'C':\n                # nothing to do, continue to the next iteration of the loop\n                prev_prev = prev\n                prev = clip\n                clip = next_clip\n                next_clip = next(track_iter, None)\n                continue\n            if transition_type not in ['D']:\n                raise EDLParseError(\n                    \"Transition type '{}' not supported by the CMX EDL reader \"\n                    \"currently.\".format(transition_type)\n                )\n\n            transition_duration = clip.duration()\n\n            # EDL doesn't have enough data to know where the cut point was, so\n            # this arbitrarily puts it in the middle of the transition\n            pre_cut = math.floor(transition_duration.value / 2)\n            post_cut = transition_duration.value - pre_cut\n            mid_tran_cut_pre_duration = otio.opentime.RationalTime(\n                pre_cut,\n                transition_duration.rate\n            )\n            mid_tran_cut_post_duration = otio.opentime.RationalTime(\n                post_cut,\n                transition_duration.rate\n            )\n\n            # expand the previous\n            expansion_clip = None\n            if prev and not prev_prev:\n                expansion_clip = prev\n            elif prev_prev:\n                expansion_clip = prev_prev\n                if prev:\n                    remove_list.append((track, prev))\n\n            expansion_clip.source_range.duration += mid_tran_cut_pre_duration\n\n            # rebuild the clip as a transition\n            new_trx = otio.schema.Transition(\n                name=clip.name,\n                # only supported type at the moment\n                transition_type=otio.schema.TransitionTypes.SMPTE_Dissolve,\n                metadata=clip.metadata\n            )\n            new_trx.in_offset = mid_tran_cut_pre_duration\n            new_trx.out_offset = mid_tran_cut_post_duration\n\n            #                   in     from  to\n            replace_list.append((track, clip, new_trx))\n\n            # expand the next_clip\n            if next_clip:\n                next_clip.source_range.start_time -= mid_tran_cut_post_duration\n                next_clip.source_range.duration += mid_tran_cut_post_duration\n            else:\n                fill = otio.schema.Gap(\n                    source_range=otio.opentime.TimeRange(\n                        duration=mid_tran_cut_post_duration,\n                        start_time=otio.opentime.RationalTime(\n                            0,\n                            transition_duration.rate\n                        )\n                    )\n                )\n                append_list.append((track, fill))\n\n            prev = clip\n            clip = next_clip\n            next_clip = next(track_iter, None)\n\n    for (track, from_clip, to_transition) in replace_list:\n        track[track.index(from_clip)] = to_transition\n\n    for (track, clip_to_remove) in list(set(remove_list)):\n        # if clip_to_remove in track:\n        track.remove(clip_to_remove)\n\n    for (track, clip) in append_list:\n        track.append(clip)\n\n    return timeline"
    },
    {
        "pr_title": "Add support for Nucoda EDLs",
        "pr_number": 191,
        "file_name": "opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -33,7 +33,6 @@\n# TODO: currently tracks with linked audio/video will lose their linkage when\n #       read into OTIO.\n \n-import enum\n import os\n import re\n import math",
        "comments": [
            {
                "comment": "Python 2.7 doesn't have the `enum` module. (OTIO needs to support 2.7 and 3.5+) This came up last time we tried to use the enum module, but now I'm confused about why the Travis tests passed since it runs against both...",
                "position": null
            },
            {
                "comment": "Can you add some advice to future developers about adding new styles? Something along the lines of \"When adding a new style, please make sure to add a test to verify that you are getting the results you want without disrupting the existing styles.\"",
                "position": null
            },
            {
                "comment": "Ah, yes. Thank you. The enum34 package is required by flake8, which is why `enum` is available when the tests run and there's no error. \r\n\r\nAre you opposed to adding enum34 as a dependency? It seems like it could be useful elsewhere in the code as well.  I'm also fine rewriting it to not use `enum`.",
                "position": null
            },
            {
                "comment": "Lets avoid the dependency for now. Also, maybe this explains why I'm not able to run flake8 on my machine :)",
                "position": null
            },
            {
                "comment": "Done!",
                "position": null
            },
            {
                "comment": "Just put up a revision that removes the `enum` dependency",
                "position": null
            }
        ],
        "commit_message": "Remove enum dependency for Python 2x compatibility and add additional comment for adding new styles",
        "commit_id": "5e031e3736eeda8df8ec8c5ddd7f25bc6ae1eee3",
        "fn_context": ""
    },
    {
        "pr_title": "Add support for Nucoda EDLs",
        "pr_number": 191,
        "file_name": "opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -73,12 +72,12 @@\nclass EDLParseError(otio.exceptions.OTIOError):\n \n \n # Currently, the 'style' argument determines\n-# the comment string for the media reference.\n+# the comment string for the media reference:\n #   'avid': '* FROM CLIP:' (default)\n #   'nucoda': '* FROM FILE:'\n-class Style(enum.Enum):\n-    avid = 0\n-    nucoda = 1\n+# When adding a new style, please be sure to add sufficient tests\n+# to verify both the new and existing styles.\n+VALID_EDL_STYLES = ['avid', 'nucoda']\n \n \n class EDLParser(object):",
        "comments": [
            {
                "comment": "Python 2.7 doesn't have the `enum` module. (OTIO needs to support 2.7 and 3.5+) This came up last time we tried to use the enum module, but now I'm confused about why the Travis tests passed since it runs against both...",
                "position": null
            },
            {
                "comment": "Can you add some advice to future developers about adding new styles? Something along the lines of \"When adding a new style, please make sure to add a test to verify that you are getting the results you want without disrupting the existing styles.\"",
                "position": null
            },
            {
                "comment": "Ah, yes. Thank you. The enum34 package is required by flake8, which is why `enum` is available when the tests run and there's no error. \r\n\r\nAre you opposed to adding enum34 as a dependency? It seems like it could be useful elsewhere in the code as well.  I'm also fine rewriting it to not use `enum`.",
                "position": null
            },
            {
                "comment": "Lets avoid the dependency for now. Also, maybe this explains why I'm not able to run flake8 on my machine :)",
                "position": null
            },
            {
                "comment": "Done!",
                "position": null
            },
            {
                "comment": "Just put up a revision that removes the `enum` dependency",
                "position": null
            }
        ],
        "commit_message": "Remove enum dependency for Python 2x compatibility and add additional comment for adding new styles",
        "commit_id": "5e031e3736eeda8df8ec8c5ddd7f25bc6ae1eee3",
        "fn_context": "    def __init__(self, edl_string, rate=24):\n        self.timeline = otio.schema.Timeline()\n\n        # Start with no tracks. They will be added as we encounter them.\n        # This dict maps a track name (e.g \"A2\" or \"V\") to an OTIO Track.\n        self.tracks_by_name = {}\n\n        self.parse_edl(edl_string, rate=rate)\n\n        # TODO: Sort the tracks V, then A1,A2,etc."
    },
    {
        "pr_title": "Add support for Nucoda EDLs",
        "pr_number": 191,
        "file_name": "opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -567,9 +566,7 @@\ndef write_to_string(input_otio, rate=None, style='avid'):\n     # TODO: We should have convenience functions in Timeline for this?\n     # also only works for a single video track at the moment\n \n-    try:\n-        edl_style = Style[style]\n-    except KeyError:\n+    if style not in VALID_EDL_STYLES:\n         raise otio.exceptions.NotSupportedError(\n             \"The EDL style '{}' is not supported.\".format(\n                 style",
        "comments": [
            {
                "comment": "Python 2.7 doesn't have the `enum` module. (OTIO needs to support 2.7 and 3.5+) This came up last time we tried to use the enum module, but now I'm confused about why the Travis tests passed since it runs against both...",
                "position": null
            },
            {
                "comment": "Can you add some advice to future developers about adding new styles? Something along the lines of \"When adding a new style, please make sure to add a test to verify that you are getting the results you want without disrupting the existing styles.\"",
                "position": null
            },
            {
                "comment": "Ah, yes. Thank you. The enum34 package is required by flake8, which is why `enum` is available when the tests run and there's no error. \r\n\r\nAre you opposed to adding enum34 as a dependency? It seems like it could be useful elsewhere in the code as well.  I'm also fine rewriting it to not use `enum`.",
                "position": null
            },
            {
                "comment": "Lets avoid the dependency for now. Also, maybe this explains why I'm not able to run flake8 on my machine :)",
                "position": null
            },
            {
                "comment": "Done!",
                "position": null
            },
            {
                "comment": "Just put up a revision that removes the `enum` dependency",
                "position": null
            }
        ],
        "commit_message": "Remove enum dependency for Python 2x compatibility and add additional comment for adding new styles",
        "commit_id": "5e031e3736eeda8df8ec8c5ddd7f25bc6ae1eee3",
        "fn_context": "def write_to_string(input_otio, rate=None, style='avid'):\n    # TODO: We should have convenience functions in Timeline for this?\n    # also only works for a single video track at the moment\n\n    if style not in VALID_EDL_STYLES:\n        raise otio.exceptions.NotSupportedError(\n            \"The EDL style '{}' is not supported.\".format(\n                style\n            )\n        )\n\n    video_tracks = [t for t in input_otio.tracks\n                    if t.kind == otio.schema.TrackKind.Video]\n    audio_tracks = [t for t in input_otio.tracks\n                    if t.kind == otio.schema.TrackKind.Audio]\n\n    if len(video_tracks) != 1:\n        raise otio.exceptions.NotSupportedError(\n            \"Only a single video track is supported, got: {}\".format(\n                len(video_tracks)\n            )\n        )\n\n    if len(audio_tracks) > 2:\n        raise otio.exceptions.NotSupportedError(\n            \"No more than 2 audio tracks are supported.\"\n        )\n    # if audio_tracks:\n    #     raise otio.exceptions.NotSupportedError(\n    #         \"No audio tracks are currently supported.\"\n    #     )\n\n    lines = []\n\n    lines.append(\"TITLE: {}\".format(input_otio.name))\n    # TODO: We should try to detect the frame rate and output an\n    # appropriate \"FCM: NON-DROP FRAME\" etc here.\n    lines.append(\"\")\n\n    edit_number = 1\n\n    track = input_otio.tracks[0]\n    edl_rate = rate or track.duration().rate\n    for i, clip in enumerate(track):\n        source_tc_in = otio.opentime.to_timecode(\n            clip.source_range.start_time,\n            edl_rate\n        )\n        source_tc_out = otio.opentime.to_timecode(\n            clip.source_range.end_time_exclusive(),\n            edl_rate\n        )\n\n        range_in_track = track.range_of_child_at_index(i)\n        record_tc_in = otio.opentime.to_timecode(\n            range_in_track.start_time,\n            edl_rate\n        )\n        record_tc_out = otio.opentime.to_timecode(\n            range_in_track.end_time_exclusive(),\n            edl_rate\n        )\n\n        reel = \"AX\"\n        name = None\n        url = None\n\n        if clip.media_reference:\n            if isinstance(clip.media_reference, otio.schema.Gap):\n                reel = \"BL\"\n            elif hasattr(clip.media_reference, 'target_url'):\n                url = clip.media_reference.target_url\n        else:\n            url = clip.name\n\n        name = clip.name\n\n        kind = \"V\" if track.kind == otio.schema.TrackKind.Video else \"A\"\n\n        lines.append(\n            \"{:03d}  {:8} {:5} C        {} {} {} {}\".format(\n                edit_number,\n                reel,\n                kind,\n                source_tc_in,\n                source_tc_out,\n                record_tc_in,\n                record_tc_out\n            )\n        )\n\n        if name:\n            # Avid Media Composer outputs two spaces before the\n            # clip name so we match that.\n            lines.append(\"* FROM CLIP NAME:  {}\".format(name))\n        if url and style == 'avid':\n            lines.append(\"* FROM CLIP: {}\".format(url))\n        if url and style == 'nucoda':\n            lines.append(\"* FROM FILE: {}\".format(url))\n\n        cdl = clip.metadata.get('cdl')\n        if cdl:\n            asc_sop = cdl.get('asc_sop')\n            asc_sat = cdl.get('asc_sat')\n            if asc_sop:\n                lines.append(\n                    \"*ASC_SOP ({} {} {}) ({} {} {}) ({} {} {})\".format(\n                        asc_sop['slope'][0],\n                        asc_sop['slope'][1],\n                        asc_sop['slope'][2],\n                        asc_sop['offset'][0],\n                        asc_sop['offset'][1],\n                        asc_sop['offset'][2],\n                        asc_sop['power'][0],\n                        asc_sop['power'][1],\n                        asc_sop['power'][2]\n                    ))\n            if asc_sat:\n                lines.append(\"*ASC_SAT {}\".format(\n                    asc_sat\n                ))\n\n        # Output any markers on this clip\n        for marker in clip.markers:\n            timecode = otio.opentime.to_timecode(\n                marker.marked_range.start_time,\n                edl_rate\n            )\n\n            color = marker.color\n            meta = marker.metadata.get(\"cmx_3600\")\n            if not color and meta and meta.get(\"color\"):\n                color = meta.get(\"color\").upper()\n            comment = (marker.name or '').upper()\n            lines.append(\"* LOC: {} {:7} {}\".format(timecode, color, comment))\n\n        # If we are carrying any unhandled CMX 3600 comments on this clip\n        # then output them blindly.\n        extra_comments = clip.metadata.get('cmx_3600', {}).get('comments', [])\n        for comment in extra_comments:\n            lines.append(\"* {}\".format(comment))\n\n        lines.append(\"\")\n        edit_number += 1\n\n    text = \"\\n\".join(lines)\n    return text"
    },
    {
        "pr_title": "Custom fps cmx3600",
        "pr_number": 187,
        "file_name": "opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -551,7 +551,7 @@\ndef read_from_string(input_str, rate=24):\n     return result\n \n \n-def write_to_string(input_otio):\n+def write_to_string(input_otio, rate=None):\n     # TODO: We should have convenience functions in Timeline for this?\n     # also only works for a single video track at the moment\n     video_tracks = [t for t in input_otio.tracks",
        "comments": [
            {
                "comment": "I think you could use `rate=24` instead of `**edl_args` here. I don't anticipate many more arguments, so it should be fine.",
                "position": null
            },
            {
                "comment": "I'll change it. Thanks.",
                "position": null
            },
            {
                "comment": "You could remove this comment.",
                "position": null
            },
            {
                "comment": "If the rate of clips in your timeline varies, then you'll get an EDL with mixed rates in it, which is unlikely to work properly. It would be better to use a single edl_rate to make sure they are all consistent.\r\n\r\nI would suggest adding an `edl_rate=None` parameter to `write_to_string` and then add some code to detect the rate of the timeline when edl_rate is None (maybe `timeline.duration().rate`). That way the caller could override it, but it would auto-detect if you don't specify a rate.\r\n",
                "position": null
            }
        ],
        "commit_message": "frame rate in args to be able to override",
        "commit_id": "a05e47b1d73614c007c47c3db85790d8dfa44b6d",
        "fn_context": "def read_from_string(input_str, rate=24):\n    parser = EDLParser(input_str, rate=rate)\n    result = parser.timeline\n    result = expand_transitions(result)\n    return result\n\ndef write_to_string(input_otio, rate=None):\n    # TODO: We should have convenience functions in Timeline for this?\n    # also only works for a single video track at the moment\n    video_tracks = [t for t in input_otio.tracks\n                    if t.kind == otio.schema.TrackKind.Video]\n    audio_tracks = [t for t in input_otio.tracks\n                    if t.kind == otio.schema.TrackKind.Audio]\n\n    if len(video_tracks) != 1:\n        raise otio.exceptions.NotSupportedError(\n            \"Only a single video track is supported, got: {}\".format(\n                len(video_tracks)\n            )\n        )\n\n    if len(audio_tracks) > 2:\n        raise otio.exceptions.NotSupportedError(\n            \"No more than 2 audio tracks are supported.\"\n        )\n    # if audio_tracks:\n    #     raise otio.exceptions.NotSupportedError(\n    #         \"No audio tracks are currently supported.\"\n    #     )\n\n    lines = []\n\n    lines.append(\"TITLE: {}\".format(input_otio.name))\n    # TODO: We should try to detect the frame rate and output an\n    # appropriate \"FCM: NON-DROP FRAME\" etc here.\n    lines.append(\"\")\n\n    edit_number = 1\n\n    track = input_otio.tracks[0]\n    edl_rate = rate or track.duration().rate\n    for i, clip in enumerate(track):\n        source_tc_in = otio.opentime.to_timecode(\n            clip.source_range.start_time,\n            edl_rate\n        )\n        source_tc_out = otio.opentime.to_timecode(\n            clip.source_range.end_time_exclusive(),\n            edl_rate\n        )\n\n        range_in_track = track.range_of_child_at_index(i)\n        record_tc_in = otio.opentime.to_timecode(\n            range_in_track.start_time,\n            edl_rate\n        )\n        record_tc_out = otio.opentime.to_timecode(\n            range_in_track.end_time_exclusive(),\n            edl_rate\n        )\n\n        reel = \"AX\"\n        name = None\n        url = None\n\n        if clip.media_reference:\n            if isinstance(clip.media_reference, otio.schema.Gap):\n                reel = \"BL\"\n            elif hasattr(clip.media_reference, 'target_url'):\n                url = clip.media_reference.target_url\n        else:\n            url = clip.name\n\n        name = clip.name\n\n        kind = \"V\" if track.kind == otio.schema.TrackKind.Video else \"A\"\n\n        lines.append(\n            \"{:03d}  {:8} {:5} C        {} {} {} {}\".format(\n                edit_number,\n                reel,\n                kind,\n                source_tc_in,\n                source_tc_out,\n                record_tc_in,\n                record_tc_out\n            )\n        )\n\n        if name:\n            # Avid Media Composer outputs two spaces before the\n            # clip name so we match that.\n            lines.append(\"* FROM CLIP NAME:  {}\".format(name))\n        if url:\n            lines.append(\"* FROM CLIP: {}\".format(url))\n\n        cdl = clip.metadata.get('cdl')\n        if cdl:\n            asc_sop = cdl.get('asc_sop')\n            asc_sat = cdl.get('asc_sat')\n            if asc_sop:\n                lines.append(\n                    \"*ASC_SOP ({} {} {}) ({} {} {}) ({} {} {})\".format(\n                        asc_sop['slope'][0],\n                        asc_sop['slope'][1],\n                        asc_sop['slope'][2],\n                        asc_sop['offset'][0],\n                        asc_sop['offset'][1],\n                        asc_sop['offset'][2],\n                        asc_sop['power'][0],\n                        asc_sop['power'][1],\n                        asc_sop['power'][2]\n                    ))\n            if asc_sat:\n                lines.append(\"*ASC_SAT {}\".format(\n                    asc_sat\n                ))\n\n        # Output any markers on this clip\n        for marker in clip.markers:\n            timecode = otio.opentime.to_timecode(\n                marker.marked_range.start_time,\n                edl_rate\n            )\n\n            color = marker.color\n            meta = marker.metadata.get(\"cmx_3600\")\n            if not color and meta and meta.get(\"color\"):\n                color = meta.get(\"color\").upper()\n            comment = (marker.name or '').upper()\n            lines.append(\"* LOC: {} {:7} {}\".format(timecode, color, comment))\n\n        # If we are carrying any unhandled CMX 3600 comments on this clip\n        # then output them blindly.\n        extra_comments = clip.metadata.get('cmx_3600', {}).get('comments', [])\n        for comment in extra_comments:\n            lines.append(\"* {}\".format(comment))\n\n        lines.append(\"\")\n        edit_number += 1\n\n    text = \"\\n\".join(lines)\n    return text"
    },
    {
        "pr_title": "Custom fps cmx3600",
        "pr_number": 187,
        "file_name": "opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -585,24 +585,25 @@\ndef write_to_string(input_otio):\n     edit_number = 1\n \n     track = input_otio.tracks[0]\n+    edl_rate = rate or track.duration().rate\n     for i, clip in enumerate(track):\n         source_tc_in = otio.opentime.to_timecode(\n             clip.source_range.start_time,\n-            clip.source_range.start_time.rate\n+            edl_rate\n         )\n         source_tc_out = otio.opentime.to_timecode(\n             clip.source_range.end_time_exclusive(),\n-            clip.source_range.end_time_exclusive().rate\n+            edl_rate\n         )\n \n         range_in_track = track.range_of_child_at_index(i)\n         record_tc_in = otio.opentime.to_timecode(\n             range_in_track.start_time,\n-            range_in_track.start_time.rate\n+            edl_rate\n         )\n         record_tc_out = otio.opentime.to_timecode(\n             range_in_track.end_time_exclusive(),\n-            range_in_track.end_time_exclusive().rate\n+            edl_rate\n         )\n \n         reel = \"AX\"",
        "comments": [
            {
                "comment": "I think you could use `rate=24` instead of `**edl_args` here. I don't anticipate many more arguments, so it should be fine.",
                "position": null
            },
            {
                "comment": "I'll change it. Thanks.",
                "position": null
            },
            {
                "comment": "You could remove this comment.",
                "position": null
            },
            {
                "comment": "If the rate of clips in your timeline varies, then you'll get an EDL with mixed rates in it, which is unlikely to work properly. It would be better to use a single edl_rate to make sure they are all consistent.\r\n\r\nI would suggest adding an `edl_rate=None` parameter to `write_to_string` and then add some code to detect the rate of the timeline when edl_rate is None (maybe `timeline.duration().rate`). That way the caller could override it, but it would auto-detect if you don't specify a rate.\r\n",
                "position": null
            }
        ],
        "commit_message": "frame rate in args to be able to override",
        "commit_id": "a05e47b1d73614c007c47c3db85790d8dfa44b6d",
        "fn_context": "def write_to_string(input_otio, rate=None):\n    # TODO: We should have convenience functions in Timeline for this?\n    # also only works for a single video track at the moment\n    video_tracks = [t for t in input_otio.tracks\n                    if t.kind == otio.schema.TrackKind.Video]\n    audio_tracks = [t for t in input_otio.tracks\n                    if t.kind == otio.schema.TrackKind.Audio]\n\n    if len(video_tracks) != 1:\n        raise otio.exceptions.NotSupportedError(\n            \"Only a single video track is supported, got: {}\".format(\n                len(video_tracks)\n            )\n        )\n\n    if len(audio_tracks) > 2:\n        raise otio.exceptions.NotSupportedError(\n            \"No more than 2 audio tracks are supported.\"\n        )\n    # if audio_tracks:\n    #     raise otio.exceptions.NotSupportedError(\n    #         \"No audio tracks are currently supported.\"\n    #     )\n\n    lines = []\n\n    lines.append(\"TITLE: {}\".format(input_otio.name))\n    # TODO: We should try to detect the frame rate and output an\n    # appropriate \"FCM: NON-DROP FRAME\" etc here.\n    lines.append(\"\")\n\n    edit_number = 1\n\n    track = input_otio.tracks[0]\n    edl_rate = rate or track.duration().rate\n    for i, clip in enumerate(track):\n        source_tc_in = otio.opentime.to_timecode(\n            clip.source_range.start_time,\n            edl_rate\n        )\n        source_tc_out = otio.opentime.to_timecode(\n            clip.source_range.end_time_exclusive(),\n            edl_rate\n        )\n\n        range_in_track = track.range_of_child_at_index(i)\n        record_tc_in = otio.opentime.to_timecode(\n            range_in_track.start_time,\n            edl_rate\n        )\n        record_tc_out = otio.opentime.to_timecode(\n            range_in_track.end_time_exclusive(),\n            edl_rate\n        )\n\n        reel = \"AX\"\n        name = None\n        url = None\n\n        if clip.media_reference:\n            if isinstance(clip.media_reference, otio.schema.Gap):\n                reel = \"BL\"\n            elif hasattr(clip.media_reference, 'target_url'):\n                url = clip.media_reference.target_url\n        else:\n            url = clip.name\n\n        name = clip.name\n\n        kind = \"V\" if track.kind == otio.schema.TrackKind.Video else \"A\"\n\n        lines.append(\n            \"{:03d}  {:8} {:5} C        {} {} {} {}\".format(\n                edit_number,\n                reel,\n                kind,\n                source_tc_in,\n                source_tc_out,\n                record_tc_in,\n                record_tc_out\n            )\n        )\n\n        if name:\n            # Avid Media Composer outputs two spaces before the\n            # clip name so we match that.\n            lines.append(\"* FROM CLIP NAME:  {}\".format(name))\n        if url:\n            lines.append(\"* FROM CLIP: {}\".format(url))\n\n        cdl = clip.metadata.get('cdl')\n        if cdl:\n            asc_sop = cdl.get('asc_sop')\n            asc_sat = cdl.get('asc_sat')\n            if asc_sop:\n                lines.append(\n                    \"*ASC_SOP ({} {} {}) ({} {} {}) ({} {} {})\".format(\n                        asc_sop['slope'][0],\n                        asc_sop['slope'][1],\n                        asc_sop['slope'][2],\n                        asc_sop['offset'][0],\n                        asc_sop['offset'][1],\n                        asc_sop['offset'][2],\n                        asc_sop['power'][0],\n                        asc_sop['power'][1],\n                        asc_sop['power'][2]\n                    ))\n            if asc_sat:\n                lines.append(\"*ASC_SAT {}\".format(\n                    asc_sat\n                ))\n\n        # Output any markers on this clip\n        for marker in clip.markers:\n            timecode = otio.opentime.to_timecode(\n                marker.marked_range.start_time,\n                edl_rate\n            )\n\n            color = marker.color\n            meta = marker.metadata.get(\"cmx_3600\")\n            if not color and meta and meta.get(\"color\"):\n                color = meta.get(\"color\").upper()\n            comment = (marker.name or '').upper()\n            lines.append(\"* LOC: {} {:7} {}\".format(timecode, color, comment))\n\n        # If we are carrying any unhandled CMX 3600 comments on this clip\n        # then output them blindly.\n        extra_comments = clip.metadata.get('cmx_3600', {}).get('comments', [])\n        for comment in extra_comments:\n            lines.append(\"* {}\".format(comment))\n\n        lines.append(\"\")\n        edit_number += 1\n\n    text = \"\\n\".join(lines)\n    return text"
    },
    {
        "pr_title": "FCP adapter uses clip item rate instead of source rate",
        "pr_number": 177,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -274,7 +274,6 @@\ndef _parse_clip_item(clip_item, transition_offsets, element_map):\n         clip_item.find('./file'),\n         element_map\n     )\n-    src_rate = _parse_rate(clip_item.find('./file'), element_map)\n     item_rate = _parse_rate(clip_item, element_map)\n \n     # transition offsets are provided in timeline rate. If they deviate they",
        "comments": [],
        "commit_message": "adresses change requests in #177\n- maintain clip item rate on both read and write",
        "commit_id": "0b5ede17c9e67402259b1fd5e672828b054f360f"
    },
    {
        "pr_title": "FCP adapter uses clip item rate instead of source rate",
        "pr_number": 177,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -297,11 +296,10 @@\ndef _parse_clip_item(clip_item, transition_offsets, element_map):\n     # source_start in xml is taken relative to the start of the media, whereas\n     # we want the absolute start time, taking into account the timecode\n     start_time = otio.opentime.RationalTime(in_frame, item_rate) + timecode\n-    duration = otio.opentime.RationalTime(out_frame - in_frame, item_rate)\n \n     source_range = otio.opentime.TimeRange(\n-        start_time=start_time.rescaled_to(src_rate),\n-        duration=duration.rescaled_to(src_rate)\n+        start_time=start_time.rescaled_to(item_rate),\n+        duration=otio.opentime.RationalTime(out_frame - in_frame, item_rate)\n     )\n \n     # get the clip name from the media reference if not defined on the clip",
        "comments": [],
        "commit_message": "adresses change requests in #177\n- maintain clip item rate on both read and write",
        "commit_id": "0b5ede17c9e67402259b1fd5e672828b054f360f"
    },
    {
        "pr_title": "FCP adapter uses clip item rate instead of source rate",
        "pr_number": 177,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -318,16 +316,12 @@\ndef _parse_clip_item(clip_item, transition_offsets, element_map):\n         source_range=source_range\n     )\n \n-    for m in markers:\n-        marker = _parse_marker(m, item_rate)\n-        marker.marked_range.start_time = \\\n-            marker.marked_range.start_time.rescaled_to(src_rate)\n-        clip.markers.append(marker)\n+    clip.markers.extend([_parse_marker(m, item_rate) for m in markers])\n \n     return clip\n \n \n-def _parse_transition_item(transition_item, track_rate, element_map):\n+def _parse_transition_item(transition_item, element_map):\n     rate = _parse_rate(transition_item, element_map)\n     start = otio.opentime.RationalTime(\n         int(transition_item.find('./start').text),",
        "comments": [],
        "commit_message": "adresses change requests in #177\n- maintain clip item rate on both read and write",
        "commit_id": "0b5ede17c9e67402259b1fd5e672828b054f360f"
    },
    {
        "pr_title": "FCP adapter uses clip item rate instead of source rate",
        "pr_number": 177,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -347,8 +341,8 @@\ndef _parse_transition_item(transition_item, track_rate, element_map):\n     transition = otio.schema.Transition(\n         name=transition_item.find('./effect/name').text,\n         transition_type=otio.schema.TransitionTypes.SMPTE_Dissolve,\n-        in_offset=(cut_point - start).rescaled_to(track_rate),\n-        out_offset=(end - cut_point).rescaled_to(track_rate),\n+        in_offset=cut_point - start,\n+        out_offset=end - cut_point,\n         metadata=metadata\n     )\n     return transition",
        "comments": [],
        "commit_message": "adresses change requests in #177\n- maintain clip item rate on both read and write",
        "commit_id": "0b5ede17c9e67402259b1fd5e672828b054f360f"
    },
    {
        "pr_title": "FCP adapter uses clip item rate instead of source rate",
        "pr_number": 177,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -385,7 +379,7 @@\ndef _parse_item(track_item, track_rate, transition_offsets, element_map):\n     # depending on the content of the clip-item, we return either a clip, a\n     # stack or a transition.\n     if track_item.tag == 'transitionitem':\n-        return _parse_transition_item(track_item, track_rate, element_map)\n+        return _parse_transition_item(track_item, element_map)\n \n     file_e = track_item.find('./file')\n     if file_e is not None:",
        "comments": [],
        "commit_message": "adresses change requests in #177\n- maintain clip item rate on both read and write",
        "commit_id": "0b5ede17c9e67402259b1fd5e672828b054f360f"
    },
    {
        "pr_title": "FCP adapter uses clip item rate instead of source rate",
        "pr_number": 177,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -559,8 +553,11 @@\ndef _build_item_timings(\n     # source_start is absolute time taking into account the timecode of the\n     # media. But xml regards the source in point from the start of the media.\n     # So we subtract the media timecode.\n-    source_start = item.source_range.start_time - timecode\n-    source_end = item.source_range.end_time_exclusive() - timecode\n+    item_rate = item.source_range.start_time.rate\n+    source_start = (item.source_range.start_time - timecode) \\\n+        .rescaled_to(item_rate)\n+    source_end = (item.source_range.end_time_exclusive() - timecode) \\\n+        .rescaled_to(item_rate)\n     start = '{:.0f}'.format(timeline_range.start_time.value)\n     end = '{:.0f}'.format(timeline_range.end_time_exclusive().value)",
        "comments": [],
        "commit_message": "adresses change requests in #177\n- maintain clip item rate on both read and write",
        "commit_id": "0b5ede17c9e67402259b1fd5e672828b054f360f"
    },
    {
        "pr_title": "Burnin adapter (second attempt)",
        "pr_number": 167,
        "file_name": "contrib/adapters/ffmpeg_burnins.py",
        "code_diff": "@@ -49,14 +49,19 @@\ndef _system_font():\n         fonts = ('arial.ttf', 'times.ttf', 'couri.ttf')\n \n     system_font = None\n+    backup = None\n     for font in fonts:\n         font = os.path.join(font_path, font)\n         if os.path.exists(font):\n             system_font = font\n             break\n     else:\n-        raise OSError(\"Could not determine system font.\")\n-    return system_font\n+        if os.path.exists(font_path):\n+            for each in os.listdir(font_path):\n+                ext = os.path.splitext(each)[-1]\n+                if ext[1:].startswith('tt'):\n+                    system_font = os.path.join(font_path, each)\n+    return system_font or backup\n \n \n # Default valuues",
        "comments": [
            {
                "comment": "Can you move the text \"OpenTimelineIO bundles ffmpeg_burnins, which is available under the MIT License.  For details contrib/adapters.\" to the bottom of the top-level LICENSE.txt file here:\r\nhttps://github.com/PixarAnimationStudios/OpenTimelineIO/blob/master/LICENSE.txt\r\n",
                "position": null
            },
            {
                "comment": "and mention the exact filename of the ffmpeg_burnins.py file, to clarify which file(s) the MIT license applies to.",
                "position": null
            },
            {
                "comment": "The PIL issue can be resolved by editing the `.travis.yml` file. Look for `pip install` and add PIL to the list.",
                "position": null
            },
            {
                "comment": "I may have found a smarter way to do this. All examples of `ffmpeg` I looked at before were using `drawbox`, but just now found one that doesn't use it. Doing some tests now (if this works it will make the code cleaner too)",
                "position": null
            },
            {
                "comment": "You can make fun of me later for missing this (so much easier)\r\n\r\n```\r\nffmpeg -loglevel panic -i BUSH0005.MOV -vf \"drawtext=text='Bottom Right+50':x=1437.5:y=h-text_h-5:fontcolor=white@1.0:fontsize=48:fontfile=/System/Library/Fonts/Menlo.ttc:box=1:boxborderw=5:boxcolor=black\" test.mov\r\n```",
                "position": null
            },
            {
                "comment": "Nice!",
                "position": null
            },
            {
                "comment": "From a syntax perspective this is cleaner, however PIL getting the dimensions of the text is really useful and kind of needed. If you're using either of the right alignment parameters it helps to know how to offset the text from the right side (offsets appear to be calculated from the left side of the text). \r\n\r\nI think PIL will be needed, sorry for the complication.",
                "position": null
            },
            {
                "comment": "The alignment parameters are a convenience so that people don't have to bang their head against a keyboard getting the right x value (which can change on resolution). If you you the right alignments and give it an offset the tool can figure out what the actual X position of the text is.",
                "position": null
            }
        ],
        "commit_message": "Merge pull request #3 from repsac/burnin\n\nadjusting the ffmpeg_burnins lib to not parse a system font",
        "commit_id": "6aa40d07f74c693e85cd0a6f41c754bda740c758",
        "fn_context": "def _system_font():\n    \"\"\"\n    attempts to determine a default system font\n\n    :rtype: str\n    \"\"\"\n    if sys.platform.startswith('win') or sys.platform.startswith('cygwin'):\n        font_path = os.path.join(os.environ['WINDIR'], 'Fonts')\n        fonts = ('arial.ttf', 'calibri.ttf', 'times.ttf')\n    elif sys.platform.startswith('darwin'):\n        font_path = '/System/Library/Fonts'\n        fonts = ('Menlo.ttc',)\n    else:\n        # assuming linux\n        font_path = 'usr/share/fonts/msttcorefonts'\n        fonts = ('arial.ttf', 'times.ttf', 'couri.ttf')\n\n    system_font = None\n    backup = None\n    for font in fonts:\n        font = os.path.join(font_path, font)\n        if os.path.exists(font):\n            system_font = font\n            break\n    else:\n        if os.path.exists(font_path):\n            for each in os.listdir(font_path):\n                ext = os.path.splitext(each)[-1]\n                if ext[1:].startswith('tt'):\n                    system_font = os.path.join(font_path, each)\n    return system_font or backup"
    },
    {
        "pr_title": "Burnin adapter (second attempt)",
        "pr_number": 167,
        "file_name": "contrib/adapters/ffmpeg_burnins.py",
        "code_diff": "@@ -254,8 +259,6 @@\ndef _add_burnin(self, text, align, options):\n         :param enum align: alignment, must use provided enum flags\n         :param dict options:\n         \"\"\"\n-        ifont = ImageFont.truetype(options['font'],\n-                                   options['font_size'])\n         resolution = self.resolution\n         data = {\n             'text': options.get('expression') or text,",
        "comments": [
            {
                "comment": "Can you move the text \"OpenTimelineIO bundles ffmpeg_burnins, which is available under the MIT License.  For details contrib/adapters.\" to the bottom of the top-level LICENSE.txt file here:\r\nhttps://github.com/PixarAnimationStudios/OpenTimelineIO/blob/master/LICENSE.txt\r\n",
                "position": null
            },
            {
                "comment": "and mention the exact filename of the ffmpeg_burnins.py file, to clarify which file(s) the MIT license applies to.",
                "position": null
            },
            {
                "comment": "The PIL issue can be resolved by editing the `.travis.yml` file. Look for `pip install` and add PIL to the list.",
                "position": null
            },
            {
                "comment": "I may have found a smarter way to do this. All examples of `ffmpeg` I looked at before were using `drawbox`, but just now found one that doesn't use it. Doing some tests now (if this works it will make the code cleaner too)",
                "position": null
            },
            {
                "comment": "You can make fun of me later for missing this (so much easier)\r\n\r\n```\r\nffmpeg -loglevel panic -i BUSH0005.MOV -vf \"drawtext=text='Bottom Right+50':x=1437.5:y=h-text_h-5:fontcolor=white@1.0:fontsize=48:fontfile=/System/Library/Fonts/Menlo.ttc:box=1:boxborderw=5:boxcolor=black\" test.mov\r\n```",
                "position": null
            },
            {
                "comment": "Nice!",
                "position": null
            },
            {
                "comment": "From a syntax perspective this is cleaner, however PIL getting the dimensions of the text is really useful and kind of needed. If you're using either of the right alignment parameters it helps to know how to offset the text from the right side (offsets appear to be calculated from the left side of the text). \r\n\r\nI think PIL will be needed, sorry for the complication.",
                "position": null
            },
            {
                "comment": "The alignment parameters are a convenience so that people don't have to bang their head against a keyboard getting the right x value (which can change on resolution). If you you the right alignments and give it an offset the tool can figure out what the actual X position of the text is.",
                "position": null
            }
        ],
        "commit_message": "Merge pull request #3 from repsac/burnin\n\nadjusting the ffmpeg_burnins lib to not parse a system font",
        "commit_id": "6aa40d07f74c693e85cd0a6f41c754bda740c758",
        "fn_context": "    def _add_burnin(self, text, align, options):\n        \"\"\"\n        Generic method for building the filter flags.\n\n        :param str text: text to apply to the drawtext\n        :param enum align: alignment, must use provided enum flags\n        :param dict options:\n        \"\"\"\n        resolution = self.resolution\n        data = {\n            'text': options.get('expression') or text,\n            'color': options['font_color'],\n            'size': options['font_size'],\n            'font': options['font'],\n            'opacity': options['opacity']\n        }\n        data.update(_drawtext(align, resolution, text, options))\n        self.filters['drawtext'].append(DRAWTEXT % data)\n\n        if options.get('bg_color') is not None:\n            box = BOX % {\n                'border': options['bg_padding'],\n                'color': options['bg_color'],\n                'opacity': options['opacity']\n            }\n            self.filters['drawtext'][-1] += ':%s' % box"
    },
    {
        "pr_title": "Burnin adapter (second attempt)",
        "pr_number": 167,
        "file_name": "contrib/adapters/ffmpeg_burnins.py",
        "code_diff": "@@ -264,10 +267,7 @@\ndef _add_burnin(self, text, align, options):\n             'font': options['font'],\n             'opacity': options['opacity']\n         }\n-        data.update(_drawtext(align, resolution,\n-                              ifont.getsize(text),\n-                              [options['x_offset'],\n-                               options['y_offset']]))\n+        data.update(_drawtext(align, resolution, text, options))\n         self.filters['drawtext'].append(DRAWTEXT % data)\n \n         if options.get('bg_color') is not None:",
        "comments": [
            {
                "comment": "Can you move the text \"OpenTimelineIO bundles ffmpeg_burnins, which is available under the MIT License.  For details contrib/adapters.\" to the bottom of the top-level LICENSE.txt file here:\r\nhttps://github.com/PixarAnimationStudios/OpenTimelineIO/blob/master/LICENSE.txt\r\n",
                "position": null
            },
            {
                "comment": "and mention the exact filename of the ffmpeg_burnins.py file, to clarify which file(s) the MIT license applies to.",
                "position": null
            },
            {
                "comment": "The PIL issue can be resolved by editing the `.travis.yml` file. Look for `pip install` and add PIL to the list.",
                "position": null
            },
            {
                "comment": "I may have found a smarter way to do this. All examples of `ffmpeg` I looked at before were using `drawbox`, but just now found one that doesn't use it. Doing some tests now (if this works it will make the code cleaner too)",
                "position": null
            },
            {
                "comment": "You can make fun of me later for missing this (so much easier)\r\n\r\n```\r\nffmpeg -loglevel panic -i BUSH0005.MOV -vf \"drawtext=text='Bottom Right+50':x=1437.5:y=h-text_h-5:fontcolor=white@1.0:fontsize=48:fontfile=/System/Library/Fonts/Menlo.ttc:box=1:boxborderw=5:boxcolor=black\" test.mov\r\n```",
                "position": null
            },
            {
                "comment": "Nice!",
                "position": null
            },
            {
                "comment": "From a syntax perspective this is cleaner, however PIL getting the dimensions of the text is really useful and kind of needed. If you're using either of the right alignment parameters it helps to know how to offset the text from the right side (offsets appear to be calculated from the left side of the text). \r\n\r\nI think PIL will be needed, sorry for the complication.",
                "position": null
            },
            {
                "comment": "The alignment parameters are a convenience so that people don't have to bang their head against a keyboard getting the right x value (which can change on resolution). If you you the right alignments and give it an offset the tool can figure out what the actual X position of the text is.",
                "position": null
            }
        ],
        "commit_message": "Merge pull request #3 from repsac/burnin\n\nadjusting the ffmpeg_burnins lib to not parse a system font",
        "commit_id": "6aa40d07f74c693e85cd0a6f41c754bda740c758",
        "fn_context": "    def _add_burnin(self, text, align, options):\n        \"\"\"\n        Generic method for building the filter flags.\n\n        :param str text: text to apply to the drawtext\n        :param enum align: alignment, must use provided enum flags\n        :param dict options:\n        \"\"\"\n        resolution = self.resolution\n        data = {\n            'text': options.get('expression') or text,\n            'color': options['font_color'],\n            'size': options['font_size'],\n            'font': options['font'],\n            'opacity': options['opacity']\n        }\n        data.update(_drawtext(align, resolution, text, options))\n        self.filters['drawtext'].append(DRAWTEXT % data)\n\n        if options.get('bg_color') is not None:\n            box = BOX % {\n                'border': options['bg_padding'],\n                'color': options['bg_color'],\n                'opacity': options['opacity']\n            }\n            self.filters['drawtext'][-1] += ':%s' % box"
    },
    {
        "pr_title": "Add nested sequence support to OTIOView",
        "pr_number": 163,
        "file_name": "opentimelineview/timeline_widget.py",
        "code_diff": "@@ -325,10 +325,10 @@\ndef __init__(self, *args, **kwargs):\n         self.setBrush(QtGui.QBrush(QtGui.QColor(64, 78, 87, 255)))\n \n \n-class StackScene(QtGui.QGraphicsScene):\n-    def __init__(self, stack, *args, **kwargs):\n-        super(StackScene, self).__init__(*args, **kwargs)\n-        self.stack = stack\n+class CompositionWidget(QtGui.QGraphicsScene):\n+    def __init__(self, composition, *args, **kwargs):\n+        super(CompositionWidget, self).__init__(*args, **kwargs)\n+        self.composition = composition\n \n         self.setBackgroundBrush(QtGui.QBrush(QtGui.QColor(64, 78, 87, 255)))",
        "comments": [
            {
                "comment": "This would be better as an elif checking for schema.Sequence, with another else to complain about unknown composition types (which may exist in the future).\r\n\r\nAlso, self.stack could be self.composition to make it clear that it isn't always a Stack.",
                "position": null
            },
            {
                "comment": "Would be great to show tracks with unknown types also. Silently ignoring them could be super confusing to people with odd track types.\r\n\r\nChecking the track types is also a great thing for the validator to report.",
                "position": null
            }
        ],
        "commit_message": "Address review notes.\n\n- stack->composition\n- Elif Sequence in case of future composition types\n- Handle non standard sequence types more elegantly.",
        "commit_id": "3b883db280cbc27d34312fcde32912b23311d789",
        "fn_context": "    def __init__(self, *args, **kwargs):\n        super(TimeSlider, self).__init__(*args, **kwargs)\n        self.setBrush(QtGui.QBrush(QtGui.QColor(64, 78, 87, 255)))\n\n    def __init__(self, composition, *args, **kwargs):\n        super(CompositionWidget, self).__init__(*args, **kwargs)\n        self.composition = composition\n\n        self.setBackgroundBrush(QtGui.QBrush(QtGui.QColor(64, 78, 87, 255)))\n\n        self._adjust_scene_size()\n        self._add_time_slider()\n        self._add_tracks()\n        self._add_markers()"
    },
    {
        "pr_title": "Add nested sequence support to OTIOView",
        "pr_number": 163,
        "file_name": "opentimelineview/timeline_widget.py",
        "code_diff": "@@ -338,24 +338,36 @@\ndef __init__(self, stack, *args, **kwargs):\n         self._add_markers()\n \n     def _adjust_scene_size(self):\n-        scene_range = self.stack.trimmed_range()\n+        scene_range = self.composition.trimmed_range()\n \n         start_time = otio.opentime.to_seconds(scene_range.start_time)\n         duration = otio.opentime.to_seconds(scene_range.end_time_exclusive())\n \n-        if isinstance(self.stack, otio.schema.Stack):\n+        if isinstance(self.composition, otio.schema.Stack):\n+            # non audio tracks are sorted into one area\n             has_video_tracks = any(\n-                t.kind == otio.schema.SequenceKind.Video for t in self.stack\n+                t.kind != otio.schema.SequenceKind.Audio\n+                for t in self.composition\n             )\n             has_audio_tracks = any(\n-                t.kind == otio.schema.SequenceKind.Audio for t in self.stack\n+                t.kind == otio.schema.SequenceKind.Audio\n+                for t in self.composition\n             )\n-        else:\n+        elif isinstance(self.composition, otio.schema.SequenceKind):\n             has_video_tracks = (\n-                self.stack.kind == otio.schema.SequenceKind.Video\n+                self.composition.kind != otio.schema.SequenceKind.Audio\n             )\n             has_audio_tracks = (\n-                self.stack.kind == otio.schema.SequenceKind.Audio\n+                self.composition.kind == otio.schema.SequenceKind.Audio\n+            )\n+        else:\n+            raise otio.exceptions.NotSupportedError(\n+                \"Error: file includes composition '{}', of type '{}',\"\n+                \" not supported by opentimeview.  Only supports children of\"\n+                \" otio.schema.Stack and otio.schema.Sequence\".format(\n+                    self.composition,\n+                    type(self.composition)\n+                )\n             )\n \n         height = (",
        "comments": [
            {
                "comment": "This would be better as an elif checking for schema.Sequence, with another else to complain about unknown composition types (which may exist in the future).\r\n\r\nAlso, self.stack could be self.composition to make it clear that it isn't always a Stack.",
                "position": null
            },
            {
                "comment": "Would be great to show tracks with unknown types also. Silently ignoring them could be super confusing to people with odd track types.\r\n\r\nChecking the track types is also a great thing for the validator to report.",
                "position": null
            }
        ],
        "commit_message": "Address review notes.\n\n- stack->composition\n- Elif Sequence in case of future composition types\n- Handle non standard sequence types more elegantly.",
        "commit_id": "3b883db280cbc27d34312fcde32912b23311d789",
        "fn_context": "    def __init__(self, composition, *args, **kwargs):\n        super(CompositionWidget, self).__init__(*args, **kwargs)\n        self.composition = composition\n\n        self.setBackgroundBrush(QtGui.QBrush(QtGui.QColor(64, 78, 87, 255)))\n\n        self._adjust_scene_size()\n        self._add_time_slider()\n        self._add_tracks()\n        self._add_markers()\n\n    def _adjust_scene_size(self):\n        scene_range = self.composition.trimmed_range()\n\n        start_time = otio.opentime.to_seconds(scene_range.start_time)\n        duration = otio.opentime.to_seconds(scene_range.end_time_exclusive())\n\n        if isinstance(self.composition, otio.schema.Stack):\n            # non audio tracks are sorted into one area\n            has_video_tracks = any(\n                t.kind != otio.schema.SequenceKind.Audio\n                for t in self.composition\n            )\n            has_audio_tracks = any(\n                t.kind == otio.schema.SequenceKind.Audio\n                for t in self.composition\n            )\n        elif isinstance(self.composition, otio.schema.SequenceKind):\n            has_video_tracks = (\n                self.composition.kind != otio.schema.SequenceKind.Audio\n            )\n            has_audio_tracks = (\n                self.composition.kind == otio.schema.SequenceKind.Audio\n            )\n        else:\n            raise otio.exceptions.NotSupportedError(\n                \"Error: file includes composition '{}', of type '{}',\"\n                \" not supported by opentimeview.  Only supports children of\"\n                \" otio.schema.Stack and otio.schema.Sequence\".format(\n                    self.composition,\n                    type(self.composition)\n                )\n            )\n\n        height = (\n            TIME_SLIDER_HEIGHT\n            + (\n                int(has_video_tracks and has_audio_tracks)\n                * MEDIA_TYPE_SEPARATOR_HEIGHT\n            )\n            + len(self.composition) * TRACK_HEIGHT\n        )\n\n        self.setSceneRect(\n            start_time * TIME_MULTIPLIER,\n            0,\n            duration * TIME_MULTIPLIER,\n            height\n        )"
    },
    {
        "pr_title": "Add nested sequence support to OTIOView",
        "pr_number": 163,
        "file_name": "opentimelineview/timeline_widget.py",
        "code_diff": "@@ -364,7 +376,7 @@\ndef _adjust_scene_size(self):\n                 int(has_video_tracks and has_audio_tracks)\n                 * MEDIA_TYPE_SEPARATOR_HEIGHT\n             )\n-            + len(self.stack) * TRACK_HEIGHT\n+            + len(self.composition) * TRACK_HEIGHT\n         )\n \n         self.setSceneRect(",
        "comments": [
            {
                "comment": "This would be better as an elif checking for schema.Sequence, with another else to complain about unknown composition types (which may exist in the future).\r\n\r\nAlso, self.stack could be self.composition to make it clear that it isn't always a Stack.",
                "position": null
            },
            {
                "comment": "Would be great to show tracks with unknown types also. Silently ignoring them could be super confusing to people with odd track types.\r\n\r\nChecking the track types is also a great thing for the validator to report.",
                "position": null
            }
        ],
        "commit_message": "Address review notes.\n\n- stack->composition\n- Elif Sequence in case of future composition types\n- Handle non standard sequence types more elegantly.",
        "commit_id": "3b883db280cbc27d34312fcde32912b23311d789",
        "fn_context": "    def _adjust_scene_size(self):\n        scene_range = self.composition.trimmed_range()\n\n        start_time = otio.opentime.to_seconds(scene_range.start_time)\n        duration = otio.opentime.to_seconds(scene_range.end_time_exclusive())\n\n        if isinstance(self.composition, otio.schema.Stack):\n            # non audio tracks are sorted into one area\n            has_video_tracks = any(\n                t.kind != otio.schema.SequenceKind.Audio\n                for t in self.composition\n            )\n            has_audio_tracks = any(\n                t.kind == otio.schema.SequenceKind.Audio\n                for t in self.composition\n            )\n        elif isinstance(self.composition, otio.schema.SequenceKind):\n            has_video_tracks = (\n                self.composition.kind != otio.schema.SequenceKind.Audio\n            )\n            has_audio_tracks = (\n                self.composition.kind == otio.schema.SequenceKind.Audio\n            )\n        else:\n            raise otio.exceptions.NotSupportedError(\n                \"Error: file includes composition '{}', of type '{}',\"\n                \" not supported by opentimeview.  Only supports children of\"\n                \" otio.schema.Stack and otio.schema.Sequence\".format(\n                    self.composition,\n                    type(self.composition)\n                )\n            )\n\n        height = (\n            TIME_SLIDER_HEIGHT\n            + (\n                int(has_video_tracks and has_audio_tracks)\n                * MEDIA_TYPE_SEPARATOR_HEIGHT\n            )\n            + len(self.composition) * TRACK_HEIGHT\n        )\n\n        self.setSceneRect(\n            start_time * TIME_MULTIPLIER,\n            0,\n            duration * TIME_MULTIPLIER,\n            height\n        )"
    },
    {
        "pr_title": "Add nested sequence support to OTIOView",
        "pr_number": 163,
        "file_name": "opentimelineview/timeline_widget.py",
        "code_diff": "@@ -376,6 +388,7 @@\ndef _adjust_scene_size(self):\n \n     def _add_time_slider(self):\n         scene_rect = self.sceneRect()\n+\n         scene_rect.setWidth(scene_rect.width() * 10)\n         scene_rect.setHeight(TIME_SLIDER_HEIGHT)\n         self.addItem(TimeSlider(scene_rect))",
        "comments": [
            {
                "comment": "This would be better as an elif checking for schema.Sequence, with another else to complain about unknown composition types (which may exist in the future).\r\n\r\nAlso, self.stack could be self.composition to make it clear that it isn't always a Stack.",
                "position": null
            },
            {
                "comment": "Would be great to show tracks with unknown types also. Silently ignoring them could be super confusing to people with odd track types.\r\n\r\nChecking the track types is also a great thing for the validator to report.",
                "position": null
            }
        ],
        "commit_message": "Address review notes.\n\n- stack->composition\n- Elif Sequence in case of future composition types\n- Handle non standard sequence types more elegantly.",
        "commit_id": "3b883db280cbc27d34312fcde32912b23311d789",
        "fn_context": "    def _add_time_slider(self):\n        scene_rect = self.sceneRect()\n\n        scene_rect.setWidth(scene_rect.width() * 10)\n        scene_rect.setHeight(TIME_SLIDER_HEIGHT)\n        self.addItem(TimeSlider(scene_rect))"
    },
    {
        "pr_title": "Add nested sequence support to OTIOView",
        "pr_number": 163,
        "file_name": "opentimelineview/timeline_widget.py",
        "code_diff": "@@ -393,22 +406,45 @@\ndef _add_tracks(self):\n \n         video_tracks = []\n         audio_tracks = []\n+        other_tracks = []\n \n-        if isinstance(self.stack, otio.schema.Stack):\n+        if isinstance(self.composition, otio.schema.Stack):\n             video_tracks = [\n-                t for t in self.stack\n+                t for t in self.composition\n                 if t.kind == otio.schema.SequenceKind.Video and list(t)\n             ]\n             audio_tracks = [\n-                t for t in self.stack\n+                t for t in self.composition\n                 if t.kind == otio.schema.SequenceKind.Audio and list(t)\n             ]\n             video_tracks.reverse()\n+\n+            other_tracks = [\n+                t for t in self.composition\n+                if (\n+                    t.kind not in (\n+                        otio.schema.SequenceKind.Video,\n+                        otio.schema.SequenceKind.Audio\n+                    )\n+                    and list(t)\n+                )\n+            ]\n         else:\n-            if self.stack.kind == otio.schema.SequenceKind.Video:\n-                video_tracks = [self.stack]\n-            elif self.stack.kind == otio.schema.SequenceKind.Audio:\n-                audio_tracks = [self.stack]\n+            if self.composition.kind == otio.schema.SequenceKind.Video:\n+                video_tracks = [self.composition]\n+            elif self.composition.kind == otio.schema.SequenceKind.Audio:\n+                audio_tracks = [self.composition]\n+            else:\n+                other_tracks = [self.composition]\n+\n+        if other_tracks:\n+            for t in other_tracks:\n+                print(\n+                    \"Warning: track named '{}' has nonstandard track type:\"\n+                    \" '{}'\".format(t.name, t.kind)\n+                )\n+\n+            video_tracks.extend(other_tracks)\n \n         video_tracks_top = TIME_SLIDER_HEIGHT\n         audio_tracks_top = (",
        "comments": [
            {
                "comment": "This would be better as an elif checking for schema.Sequence, with another else to complain about unknown composition types (which may exist in the future).\r\n\r\nAlso, self.stack could be self.composition to make it clear that it isn't always a Stack.",
                "position": null
            },
            {
                "comment": "Would be great to show tracks with unknown types also. Silently ignoring them could be super confusing to people with odd track types.\r\n\r\nChecking the track types is also a great thing for the validator to report.",
                "position": null
            }
        ],
        "commit_message": "Address review notes.\n\n- stack->composition\n- Elif Sequence in case of future composition types\n- Handle non standard sequence types more elegantly.",
        "commit_id": "3b883db280cbc27d34312fcde32912b23311d789",
        "fn_context": "    def _add_tracks(self):\n        video_tracks_top = TIME_SLIDER_HEIGHT\n        audio_tracks_top = TIME_SLIDER_HEIGHT\n\n        video_tracks = []\n        audio_tracks = []\n        other_tracks = []\n\n        if isinstance(self.composition, otio.schema.Stack):\n            video_tracks = [\n                t for t in self.composition\n                if t.kind == otio.schema.SequenceKind.Video and list(t)\n            ]\n            audio_tracks = [\n                t for t in self.composition\n                if t.kind == otio.schema.SequenceKind.Audio and list(t)\n            ]\n            video_tracks.reverse()\n\n            other_tracks = [\n                t for t in self.composition\n                if (\n                    t.kind not in (\n                        otio.schema.SequenceKind.Video,\n                        otio.schema.SequenceKind.Audio\n                    )\n                    and list(t)\n                )\n            ]\n        else:\n            if self.composition.kind == otio.schema.SequenceKind.Video:\n                video_tracks = [self.composition]\n            elif self.composition.kind == otio.schema.SequenceKind.Audio:\n                audio_tracks = [self.composition]\n            else:\n                other_tracks = [self.composition]\n\n        if other_tracks:\n            for t in other_tracks:\n                print(\n                    \"Warning: track named '{}' has nonstandard track type:\"\n                    \" '{}'\".format(t.name, t.kind)\n                )\n\n            video_tracks.extend(other_tracks)\n\n        video_tracks_top = TIME_SLIDER_HEIGHT\n        audio_tracks_top = (\n            TIME_SLIDER_HEIGHT\n            + len(video_tracks) * TRACK_HEIGHT\n            + int(\n                bool(video_tracks) and bool(audio_tracks)\n            ) * MEDIA_TYPE_SEPARATOR_HEIGHT\n        )\n\n        for i, track in enumerate(audio_tracks):\n            self._add_track(track, audio_tracks_top + i * TRACK_HEIGHT)\n\n        for i, track in enumerate(video_tracks):\n            self._add_track(track, video_tracks_top + i * TRACK_HEIGHT)"
    },
    {
        "pr_title": "Add nested sequence support to OTIOView",
        "pr_number": 163,
        "file_name": "opentimelineview/timeline_widget.py",
        "code_diff": "@@ -426,7 +462,7 @@\ndef _add_tracks(self):\n             self._add_track(track, video_tracks_top + i * TRACK_HEIGHT)\n \n     def _add_markers(self):\n-        for m in self.stack.markers:\n+        for m in self.composition.markers:\n             marker = Marker(m, None, self)\n             marker.setX(\n                 otio.opentime.to_seconds(m.marked_range.start_time)",
        "comments": [
            {
                "comment": "This would be better as an elif checking for schema.Sequence, with another else to complain about unknown composition types (which may exist in the future).\r\n\r\nAlso, self.stack could be self.composition to make it clear that it isn't always a Stack.",
                "position": null
            },
            {
                "comment": "Would be great to show tracks with unknown types also. Silently ignoring them could be super confusing to people with odd track types.\r\n\r\nChecking the track types is also a great thing for the validator to report.",
                "position": null
            }
        ],
        "commit_message": "Address review notes.\n\n- stack->composition\n- Elif Sequence in case of future composition types\n- Handle non standard sequence types more elegantly.",
        "commit_id": "3b883db280cbc27d34312fcde32912b23311d789",
        "fn_context": "    def _add_tracks(self):\n        video_tracks_top = TIME_SLIDER_HEIGHT\n        audio_tracks_top = TIME_SLIDER_HEIGHT\n\n        video_tracks = []\n        audio_tracks = []\n        other_tracks = []\n\n        if isinstance(self.composition, otio.schema.Stack):\n            video_tracks = [\n                t for t in self.composition\n                if t.kind == otio.schema.SequenceKind.Video and list(t)\n            ]\n            audio_tracks = [\n                t for t in self.composition\n                if t.kind == otio.schema.SequenceKind.Audio and list(t)\n            ]\n            video_tracks.reverse()\n\n            other_tracks = [\n                t for t in self.composition\n                if (\n                    t.kind not in (\n                        otio.schema.SequenceKind.Video,\n                        otio.schema.SequenceKind.Audio\n                    )\n                    and list(t)\n                )\n            ]\n        else:\n            if self.composition.kind == otio.schema.SequenceKind.Video:\n                video_tracks = [self.composition]\n            elif self.composition.kind == otio.schema.SequenceKind.Audio:\n                audio_tracks = [self.composition]\n            else:\n                other_tracks = [self.composition]\n\n        if other_tracks:\n            for t in other_tracks:\n                print(\n                    \"Warning: track named '{}' has nonstandard track type:\"\n                    \" '{}'\".format(t.name, t.kind)\n                )\n\n            video_tracks.extend(other_tracks)\n\n        video_tracks_top = TIME_SLIDER_HEIGHT\n        audio_tracks_top = (\n            TIME_SLIDER_HEIGHT\n            + len(video_tracks) * TRACK_HEIGHT\n            + int(\n                bool(video_tracks) and bool(audio_tracks)\n            ) * MEDIA_TYPE_SEPARATOR_HEIGHT\n        )\n\n        for i, track in enumerate(audio_tracks):\n            self._add_track(track, audio_tracks_top + i * TRACK_HEIGHT)\n\n        for i, track in enumerate(video_tracks):\n            self._add_track(track, video_tracks_top + i * TRACK_HEIGHT)\n\n    def _add_markers(self):\n        for m in self.composition.markers:\n            marker = Marker(m, None, self)\n            marker.setX(\n                otio.opentime.to_seconds(m.marked_range.start_time)\n                * TIME_MULTIPLIER\n            )\n            marker.setY(TIME_SLIDER_HEIGHT - MARKER_SIZE)\n            self.addItem(marker)"
    },
    {
        "pr_title": "Add nested sequence support to OTIOView",
        "pr_number": 163,
        "file_name": "opentimelineview/timeline_widget.py",
        "code_diff": "@@ -436,16 +472,16 @@\ndef _add_markers(self):\n             self.addItem(marker)\n \n \n-class StackView(QtGui.QGraphicsView):\n+class CompositionView(QtGui.QGraphicsView):\n \n     open_stack = QtCore.Signal(otio.schema.Stack)\n     selection_changed = QtCore.Signal(otio.core.SerializableObject)\n \n     def __init__(self, stack, *args, **kwargs):\n-        super(StackView, self).__init__(*args, **kwargs)\n+        super(CompositionView, self).__init__(*args, **kwargs)\n         self.setResizeAnchor(QtGui.QGraphicsView.AnchorUnderMouse)\n         self.setTransformationAnchor(QtGui.QGraphicsView.AnchorUnderMouse)\n-        self.setScene(StackScene(stack, parent=self))\n+        self.setScene(CompositionWidget(stack, parent=self))\n         self.setAlignment((QtCore.Qt.AlignLeft | QtCore.Qt.AlignTop))\n \n         self.scene().selectionChanged.connect(self.parse_selection_change)",
        "comments": [
            {
                "comment": "This would be better as an elif checking for schema.Sequence, with another else to complain about unknown composition types (which may exist in the future).\r\n\r\nAlso, self.stack could be self.composition to make it clear that it isn't always a Stack.",
                "position": null
            },
            {
                "comment": "Would be great to show tracks with unknown types also. Silently ignoring them could be super confusing to people with odd track types.\r\n\r\nChecking the track types is also a great thing for the validator to report.",
                "position": null
            }
        ],
        "commit_message": "Address review notes.\n\n- stack->composition\n- Elif Sequence in case of future composition types\n- Handle non standard sequence types more elegantly.",
        "commit_id": "3b883db280cbc27d34312fcde32912b23311d789",
        "fn_context": "    def _add_markers(self):\n        for m in self.composition.markers:\n            marker = Marker(m, None, self)\n            marker.setX(\n                otio.opentime.to_seconds(m.marked_range.start_time)\n                * TIME_MULTIPLIER\n            )\n            marker.setY(TIME_SLIDER_HEIGHT - MARKER_SIZE)\n            self.addItem(marker)\n\n    def __init__(self, stack, *args, **kwargs):\n        super(CompositionView, self).__init__(*args, **kwargs)\n        self.setResizeAnchor(QtGui.QGraphicsView.AnchorUnderMouse)\n        self.setTransformationAnchor(QtGui.QGraphicsView.AnchorUnderMouse)\n        self.setScene(CompositionWidget(stack, parent=self))\n        self.setAlignment((QtCore.Qt.AlignLeft | QtCore.Qt.AlignTop))\n\n        self.scene().selectionChanged.connect(self.parse_selection_change)\n\n    def parse_selection_change(self):\n        selection = self.scene().selectedItems()\n        if selection:\n            self.selection_changed.emit(selection[-1].item)"
    },
    {
        "pr_title": "Add nested sequence support to OTIOView",
        "pr_number": 163,
        "file_name": "opentimelineview/timeline_widget.py",
        "code_diff": "@@ -463,10 +499,10 @@\ndef mousePressEvent(self, mouse_event):\n             else QtGui.QGraphicsView.NoDrag\n         )\n         self.setInteractive(not modifiers == QtCore.Qt.AltModifier)\n-        super(StackView, self).mousePressEvent(mouse_event)\n+        super(CompositionView, self).mousePressEvent(mouse_event)\n \n     def mouseReleaseEvent(self, mouse_event):\n-        super(StackView, self).mouseReleaseEvent(mouse_event)\n+        super(CompositionView, self).mouseReleaseEvent(mouse_event)\n         self.setDragMode(QtGui.QGraphicsView.NoDrag)\n \n     def wheelEvent(self, event):",
        "comments": [
            {
                "comment": "This would be better as an elif checking for schema.Sequence, with another else to complain about unknown composition types (which may exist in the future).\r\n\r\nAlso, self.stack could be self.composition to make it clear that it isn't always a Stack.",
                "position": null
            },
            {
                "comment": "Would be great to show tracks with unknown types also. Silently ignoring them could be super confusing to people with odd track types.\r\n\r\nChecking the track types is also a great thing for the validator to report.",
                "position": null
            }
        ],
        "commit_message": "Address review notes.\n\n- stack->composition\n- Elif Sequence in case of future composition types\n- Handle non standard sequence types more elegantly.",
        "commit_id": "3b883db280cbc27d34312fcde32912b23311d789",
        "fn_context": "    def mousePressEvent(self, mouse_event):\n        modifiers = QtGui.QApplication.keyboardModifiers()\n        self.setDragMode(\n            QtGui.QGraphicsView.ScrollHandDrag\n            if modifiers == QtCore.Qt.AltModifier\n            else QtGui.QGraphicsView.NoDrag\n        )\n        self.setInteractive(not modifiers == QtCore.Qt.AltModifier)\n        super(CompositionView, self).mousePressEvent(mouse_event)\n\n    def mouseReleaseEvent(self, mouse_event):\n        super(CompositionView, self).mouseReleaseEvent(mouse_event)\n        self.setDragMode(QtGui.QGraphicsView.NoDrag)\n\n    def wheelEvent(self, event):\n        scale_by = 1.0 + float(event.delta()) / 1000\n        self.scale(scale_by, 1)\n        zoom_level = 1.0 / self.matrix().m11()\n\n        # some items we do want to keep the same visual size. So we need to\n        # inverse the effect of the zoom\n        items_to_scale = [\n            i for i in self.scene().items()\n            if isinstance(i, _BaseItem) or isinstance(i, Marker)\n        ]\n\n        for item in items_to_scale:\n            item.counteract_zoom(zoom_level)"
    },
    {
        "pr_title": "ALE adapter",
        "pr_number": 158,
        "file_name": "contrib/adapters/ale.py",
        "code_diff": "@@ -35,7 +35,8 @@\ndef _parse_data_line(line, columns, fps):\n     row = line.split(\"\\t\")\n \n     if len(row) < len(columns):\n-        raise ALEParseError(\"Too few values on row: \"+line)\n+        # Fill in blanks for any missing fields in this row\n+        row.extend([\"\"] * (len(columns) - len(row)))\n \n     if len(row) > len(columns):\n         raise ALEParseError(\"Too many values on row: \"+line)",
        "comments": [
            {
                "comment": "Looks like you should rebase onto your other change once you land that?",
                "position": null
            },
            {
                "comment": "Refactoring this to iterate with `for lineno, line in enumerate(lines):` using continues might be better - then you can get line numbers in your error messages.",
                "position": null
            },
            {
                "comment": "suggest changing to:\r\n`lines = input_str.splitlines()`\r\n\r\nRan into several ALEs that used old-school Macintosh line endings that didn't parse so well.",
                "position": null
            },
            {
                "comment": "It looks like some ALEs will have a `Duration` field in the header, but will include no actual data.\r\nWe could change this block of code to be something like:\r\n```\r\nraw_duration = metadata[\"Duration\"]\r\nif raw_duration:\r\n    duration = otio.opentime.from_timecode(\r\n        raw_duration, fps\r\n    )\r\ndel metadata[\"Duration\"]\r\n```\r\n\r\nThe fallback code below should then handle the duration calculation",
                "position": null
            },
            {
                "comment": "There are some ALEs that rather than writing back-to-back delimiters will just prematurely end the line. I suggest we fill in with empty strings in this case:\r\n\r\n`columns.extend([\"\"] * (len(row) - len(columns)))`",
                "position": null
            },
            {
                "comment": "I wrapped each timecode parse in a try/catch in case of blanks or malformed values, so they are skipped. I *could* make the blank a special case and complain about malformed values, that might be a better strategy...",
                "position": null
            },
            {
                "comment": "Will do.",
                "position": null
            },
            {
                "comment": "Good idea.",
                "position": null
            },
            {
                "comment": "General note is that we haven't really built any kind of formal error/warning system into the adapters yet.  Still not sure exactly what the needs are there.",
                "position": null
            },
            {
                "comment": "I think you can do: `if metadata.get(\"Duration\"):` since in python, `\"\"` is the same truthiness as `None`",
                "position": null
            }
        ],
        "commit_message": "Addressed notes from code review.\nHandle truncated lines gracefully.\nComplain about malformed timecode, but ignore blank timecode fields.\nSupport varying line endings via splitlines()",
        "commit_id": "14cce17378688b5198c46ad75a896423a3dd5c4e",
        "fn_context": "def _parse_data_line(line, columns, fps):\n    row = line.split(\"\\t\")\n\n    if len(row) < len(columns):\n        # Fill in blanks for any missing fields in this row\n        row.extend([\"\"] * (len(columns) - len(row)))\n\n    if len(row) > len(columns):\n        raise ALEParseError(\"Too many values on row: \"+line)\n\n    try:\n\n        metadata = dict(zip(columns, row))\n\n        clip = otio.schema.Clip()\n        clip.name = metadata.get(\"Name\")\n        del metadata[\"Name\"]\n\n        if \"Start\" in metadata:\n            try:\n                start = otio.opentime.from_timecode(metadata[\"Start\"], fps)\n            except:\n                raise ALEParseError(\"Invalid Start timecode: {}\".format(\n                    metadata[\"Start\"]\n                ))\n            del metadata[\"Start\"]\n            duration = None\n            end = None\n            if metadata.get(\"Duration\", \"\") != \"\":\n                try:\n                    duration = otio.opentime.from_timecode(\n                        metadata[\"Duration\"], fps\n                    )\n                    del metadata[\"Duration\"]\n                except:\n                    raise ALEParseError(\"Invalid Duration timecode: {}\".format(\n                        metadata[\"Duration\"]\n                    ))\n            if metadata.get(\"End\", \"\") != \"\":\n                try:\n                    end = otio.opentime.from_timecode(metadata[\"End\"], fps)\n                    del metadata[\"End\"]\n                except:\n                    raise ALEParseError(\"Invalid End timecode: {}\".format(\n                        metadata[\"End\"]\n                    ))\n            if duration is None:\n                duration = end - start\n            if end is None:\n                end = start + duration\n            if end != start + duration:\n                raise ALEParseError(\n                    \"Inconsistent Start, End, Duration: \"+line\n                )\n            clip.source_range = otio.opentime.TimeRange(\n                start,\n                duration\n            )\n\n        clip.metadata[\"ALE\"] = metadata\n\n        return clip\n    except Exception as ex:\n        raise ALEParseError(\"Error parsing line: {}\\n{}\".format(\n            line, repr(ex)\n        ))"
    },
    {
        "pr_title": "ALE adapter",
        "pr_number": 158,
        "file_name": "contrib/adapters/ale.py",
        "code_diff": "@@ -58,22 +59,24 @@\ndef _parse_data_line(line, columns, fps):\n             del metadata[\"Start\"]\n             duration = None\n             end = None\n-            if \"Duration\" in metadata:\n+            if metadata.get(\"Duration\", \"\") != \"\":\n                 try:\n                     duration = otio.opentime.from_timecode(\n                         metadata[\"Duration\"], fps\n                     )\n                     del metadata[\"Duration\"]\n                 except:\n-                    # Could not parse Duration timecode, ignore this\n-                    pass\n-            if \"End\" in metadata:\n+                    raise ALEParseError(\"Invalid Duration timecode: {}\".format(\n+                        metadata[\"Duration\"]\n+                    ))\n+            if metadata.get(\"End\", \"\") != \"\":\n                 try:\n                     end = otio.opentime.from_timecode(metadata[\"End\"], fps)\n                     del metadata[\"End\"]\n                 except:\n-                    # Could not parse End timecode, ignore this\n-                    pass\n+                    raise ALEParseError(\"Invalid End timecode: {}\".format(\n+                        metadata[\"End\"]\n+                    ))\n             if duration is None:\n                 duration = end - start\n             if end is None:",
        "comments": [
            {
                "comment": "Looks like you should rebase onto your other change once you land that?",
                "position": null
            },
            {
                "comment": "Refactoring this to iterate with `for lineno, line in enumerate(lines):` using continues might be better - then you can get line numbers in your error messages.",
                "position": null
            },
            {
                "comment": "suggest changing to:\r\n`lines = input_str.splitlines()`\r\n\r\nRan into several ALEs that used old-school Macintosh line endings that didn't parse so well.",
                "position": null
            },
            {
                "comment": "It looks like some ALEs will have a `Duration` field in the header, but will include no actual data.\r\nWe could change this block of code to be something like:\r\n```\r\nraw_duration = metadata[\"Duration\"]\r\nif raw_duration:\r\n    duration = otio.opentime.from_timecode(\r\n        raw_duration, fps\r\n    )\r\ndel metadata[\"Duration\"]\r\n```\r\n\r\nThe fallback code below should then handle the duration calculation",
                "position": null
            },
            {
                "comment": "There are some ALEs that rather than writing back-to-back delimiters will just prematurely end the line. I suggest we fill in with empty strings in this case:\r\n\r\n`columns.extend([\"\"] * (len(row) - len(columns)))`",
                "position": null
            },
            {
                "comment": "I wrapped each timecode parse in a try/catch in case of blanks or malformed values, so they are skipped. I *could* make the blank a special case and complain about malformed values, that might be a better strategy...",
                "position": null
            },
            {
                "comment": "Will do.",
                "position": null
            },
            {
                "comment": "Good idea.",
                "position": null
            },
            {
                "comment": "General note is that we haven't really built any kind of formal error/warning system into the adapters yet.  Still not sure exactly what the needs are there.",
                "position": null
            },
            {
                "comment": "I think you can do: `if metadata.get(\"Duration\"):` since in python, `\"\"` is the same truthiness as `None`",
                "position": null
            }
        ],
        "commit_message": "Addressed notes from code review.\nHandle truncated lines gracefully.\nComplain about malformed timecode, but ignore blank timecode fields.\nSupport varying line endings via splitlines()",
        "commit_id": "14cce17378688b5198c46ad75a896423a3dd5c4e",
        "fn_context": "def _parse_data_line(line, columns, fps):\n    row = line.split(\"\\t\")\n\n    if len(row) < len(columns):\n        # Fill in blanks for any missing fields in this row\n        row.extend([\"\"] * (len(columns) - len(row)))\n\n    if len(row) > len(columns):\n        raise ALEParseError(\"Too many values on row: \"+line)\n\n    try:\n\n        metadata = dict(zip(columns, row))\n\n        clip = otio.schema.Clip()\n        clip.name = metadata.get(\"Name\")\n        del metadata[\"Name\"]\n\n        if \"Start\" in metadata:\n            try:\n                start = otio.opentime.from_timecode(metadata[\"Start\"], fps)\n            except:\n                raise ALEParseError(\"Invalid Start timecode: {}\".format(\n                    metadata[\"Start\"]\n                ))\n            del metadata[\"Start\"]\n            duration = None\n            end = None\n            if metadata.get(\"Duration\", \"\") != \"\":\n                try:\n                    duration = otio.opentime.from_timecode(\n                        metadata[\"Duration\"], fps\n                    )\n                    del metadata[\"Duration\"]\n                except:\n                    raise ALEParseError(\"Invalid Duration timecode: {}\".format(\n                        metadata[\"Duration\"]\n                    ))\n            if metadata.get(\"End\", \"\") != \"\":\n                try:\n                    end = otio.opentime.from_timecode(metadata[\"End\"], fps)\n                    del metadata[\"End\"]\n                except:\n                    raise ALEParseError(\"Invalid End timecode: {}\".format(\n                        metadata[\"End\"]\n                    ))\n            if duration is None:\n                duration = end - start\n            if end is None:\n                end = start + duration\n            if end != start + duration:\n                raise ALEParseError(\n                    \"Inconsistent Start, End, Duration: \"+line\n                )\n            clip.source_range = otio.opentime.TimeRange(\n                start,\n                duration\n            )\n\n        clip.metadata[\"ALE\"] = metadata\n\n        return clip\n    except Exception as ex:\n        raise ALEParseError(\"Error parsing line: {}\\n{}\".format(\n            line, repr(ex)\n        ))"
    },
    {
        "pr_title": "ALE adapter",
        "pr_number": 158,
        "file_name": "contrib/adapters/ale.py",
        "code_diff": "@@ -103,9 +106,9 @@\ndef read_from_string(input_str, fps=24):\n     columns = []\n \n     def nextline(lines):\n-        return lines.pop(0).rstrip('\\r')\n+        return lines.pop(0)\n \n-    lines = input_str.split(\"\\n\")\n+    lines = input_str.splitlines()\n     while len(lines):\n         line = nextline(lines)",
        "comments": [
            {
                "comment": "Looks like you should rebase onto your other change once you land that?",
                "position": null
            },
            {
                "comment": "Refactoring this to iterate with `for lineno, line in enumerate(lines):` using continues might be better - then you can get line numbers in your error messages.",
                "position": null
            },
            {
                "comment": "suggest changing to:\r\n`lines = input_str.splitlines()`\r\n\r\nRan into several ALEs that used old-school Macintosh line endings that didn't parse so well.",
                "position": null
            },
            {
                "comment": "It looks like some ALEs will have a `Duration` field in the header, but will include no actual data.\r\nWe could change this block of code to be something like:\r\n```\r\nraw_duration = metadata[\"Duration\"]\r\nif raw_duration:\r\n    duration = otio.opentime.from_timecode(\r\n        raw_duration, fps\r\n    )\r\ndel metadata[\"Duration\"]\r\n```\r\n\r\nThe fallback code below should then handle the duration calculation",
                "position": null
            },
            {
                "comment": "There are some ALEs that rather than writing back-to-back delimiters will just prematurely end the line. I suggest we fill in with empty strings in this case:\r\n\r\n`columns.extend([\"\"] * (len(row) - len(columns)))`",
                "position": null
            },
            {
                "comment": "I wrapped each timecode parse in a try/catch in case of blanks or malformed values, so they are skipped. I *could* make the blank a special case and complain about malformed values, that might be a better strategy...",
                "position": null
            },
            {
                "comment": "Will do.",
                "position": null
            },
            {
                "comment": "Good idea.",
                "position": null
            },
            {
                "comment": "General note is that we haven't really built any kind of formal error/warning system into the adapters yet.  Still not sure exactly what the needs are there.",
                "position": null
            },
            {
                "comment": "I think you can do: `if metadata.get(\"Duration\"):` since in python, `\"\"` is the same truthiness as `None`",
                "position": null
            }
        ],
        "commit_message": "Addressed notes from code review.\nHandle truncated lines gracefully.\nComplain about malformed timecode, but ignore blank timecode fields.\nSupport varying line endings via splitlines()",
        "commit_id": "14cce17378688b5198c46ad75a896423a3dd5c4e",
        "fn_context": "def read_from_string(input_str, fps=24):\n\n    collection = otio.schema.SerializableCollection()\n    header = {}\n    columns = []\n\n    def nextline(lines):\n        return lines.pop(0)\n\n    lines = input_str.splitlines()\n    while len(lines):\n        line = nextline(lines)\n\n        # skip blank lines\n        if line.strip() == \"\":\n            continue\n\n        if line.strip() == \"Heading\":\n            while len(lines):\n                line = nextline(lines)\n\n                if line.strip() == \"\":\n                    break\n\n                if \"\\t\" not in line:\n                    raise ALEParseError(\"Invalid Heading line: \"+line)\n\n                segments = line.split(\"\\t\")\n                while len(segments) >= 2:\n                    key, val = segments.pop(0), segments.pop(0)\n                    header[key] = val\n                if len(segments) != 0:\n                    raise ALEParseError(\"Invalid Heading line: \"+line)\n\n        if \"FPS\" in header:\n            fps = float(header[\"FPS\"])\n\n        if line.strip() == \"Column\":\n            if len(lines) == 0:\n                raise ALEParseError(\"Unexpected end of file after: \"+line)\n\n            line = nextline(lines)\n            columns = line.split(\"\\t\")\n\n        if line.strip() == \"Data\":\n            while len(lines):\n                line = nextline(lines)\n\n                if line.strip() == \"\":\n                    continue\n\n                clip = _parse_data_line(line, columns, fps)\n\n                collection.append(clip)\n\n    collection.metadata[\"ALE\"] = {\n        \"header\": header,\n        \"columns\": columns\n    }\n\n    return collection"
    },
    {
        "pr_title": "Added CDL support to the EDL adapter.",
        "pr_number": 143,
        "file_name": "opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -40,6 +40,11 @@\nimport opentimelineio as otio\n \n+\n+class EDLParseError(otio.exceptions.OTIOError):\n+    pass\n+\n+\n # these are all CMX_3600 transition codes\n # the wipe is written in regex format because it is W### where the ### is\n # a 'wipe code'",
        "comments": [],
        "commit_message": "Added CDL writing support to the EDL adapter.",
        "commit_id": "f7f53397c6441c01166659952850077fbb62a50f"
    },
    {
        "pr_title": "Added CDL support to the EDL adapter.",
        "pr_number": 143,
        "file_name": "opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -174,11 +179,11 @@\ndef parse_edl(self, edl_string):\n                 if 'VIDEO DELAY' in line:\n                     video_delay = line.split()[-1].strip()\n                 if audio_delay and video_delay:\n-                    raise RuntimeError(\n+                    raise EDLParseError(\n                         'both audio and video delay declared after SPLIT.'\n                     )\n                 if not (audio_delay or video_delay):\n-                    raise RuntimeError(\n+                    raise EDLParseError(\n                         'either audio or video delay declared after SPLIT.'\n                     )",
        "comments": [],
        "commit_message": "Added CDL writing support to the EDL adapter.",
        "commit_id": "f7f53397c6441c01166659952850077fbb62a50f"
    },
    {
        "pr_title": "Added CDL support to the EDL adapter.",
        "pr_number": 143,
        "file_name": "opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -212,7 +217,7 @@\ndef parse_edl(self, edl_string):\n                 self.add_clip(line, comments)\n \n             else:\n-                raise RuntimeError('Unknown event type')\n+                raise EDLParseError('Unknown event type')\n \n \n class ClipHandler(object):",
        "comments": [],
        "commit_message": "Added CDL writing support to the EDL adapter.",
        "commit_id": "f7f53397c6441c01166659952850077fbb62a50f"
    },
    {
        "pr_title": "Added CDL support to the EDL adapter.",
        "pr_number": 143,
        "file_name": "opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -275,14 +280,18 @@\ndef make_clip(self, comment_data):\n                 if asc_sop:\n                     triple = r'([\\d.]+) ([\\d.]+) ([\\d.]+)'\n                     m = re.match(\n-                        r'\\('+triple+'\\)\\('+triple+'\\)\\('+triple+'\\)',\n+                        r'\\('+triple+'\\)\\s*\\('+triple+'\\)\\s*\\('+triple+'\\)',\n                         asc_sop\n                     )\n                     if m:\n                         floats = [float(g) for g in m.groups()]\n                         slope = [floats[0], floats[1], floats[2]]\n                         offset = [floats[3], floats[4], floats[5]]\n                         power = [floats[6], floats[7], floats[8]]\n+                    else:\n+                        raise EDLParseError(\n+                            'Invalid ASC_SOP found: {}'.format(asc_sop)\n+                            )\n \n                 if asc_sat:\n                     sat = float(asc_sat)",
        "comments": [],
        "commit_message": "Added CDL writing support to the EDL adapter.",
        "commit_id": "f7f53397c6441c01166659952850077fbb62a50f"
    },
    {
        "pr_title": "Added CDL support to the EDL adapter.",
        "pr_number": 143,
        "file_name": "opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -388,7 +397,7 @@\ndef parse(self, line):\n             ) = fields\n \n         else:\n-            raise RuntimeError(\n+            raise EDLParseError(\n                 'incorrect number of fields [{0}] in form statement: {1}'\n                 ''.format(field_count, line))",
        "comments": [],
        "commit_message": "Added CDL writing support to the EDL adapter.",
        "commit_id": "f7f53397c6441c01166659952850077fbb62a50f"
    },
    {
        "pr_title": "Added CDL support to the EDL adapter.",
        "pr_number": 143,
        "file_name": "opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -458,7 +467,7 @@\ndef expand_transitions(timeline):\n                 next_clip = next(track_iter, None)\n                 continue\n             if transition_type not in ['D']:\n-                raise RuntimeError(\n+                raise EDLParseError(\n                     \"Transition type '{}' not supported by the CMX EDL reader \"\n                     \"currently.\".format(transition_type)\n                 )",
        "comments": [],
        "commit_message": "Added CDL writing support to the EDL adapter.",
        "commit_id": "f7f53397c6441c01166659952850077fbb62a50f"
    },
    {
        "pr_title": "Add marker color string support.",
        "pr_number": 134,
        "file_name": "opentimelineio/adapters/cmx_3600.py",
        "code_diff": "@@ -254,7 +254,7 @@\ndef make_clip(self, comment_data):\n                         duration=otio.opentime.RationalTime()\n                     )\n \n-                    # always write the source value into metadata, in case it \n+                    # always write the source value into metadata, in case it\n                     # is not a valid enum somehow.\n                     color_parsed_from_file = m.group(2)",
        "comments": [
            {
                "comment": "Should we validate this against the enum?",
                "position": null
            }
        ],
        "commit_message": "Lint pass.",
        "commit_id": "3a6d53390489b51049977a3eab7aceeb07758572",
        "fn_context": "    def make_clip(self, comment_data):\n        if self.reel == 'BL':\n            # TODO make this an explicit path\n            # this is the only special tape name code we care about\n            # AX exists but means nothing in our context. We aren't using tapes\n            clip = otio.schema.Gap()\n        else:\n            clip = otio.schema.Clip()\n            clip.name = str(self.clip_num)\n\n            if 'media_reference' in comment_data:\n                clip.media_reference = otio.media_reference.External()\n                clip.media_reference.target_url = comment_data[\n                    'media_reference'\n                ]\n            else:\n                clip.media_reference = otio.media_reference.MissingReference()\n\n            # this could currently break without a 'FROM CLIP' comment.\n            # Without that there is no 'media_reference' Do we have a default\n            # clip name?\n            if 'clip_name' in comment_data:\n                clip.name = comment_data[\"clip_name\"]\n            elif (\n                clip.media_reference and\n                hasattr(clip.media_reference, 'target_url') and\n                clip.media_reference.target_url is not None\n            ):\n                clip.name = os.path.splitext(\n                    os.path.basename(clip.media_reference.target_url)\n                )[0]\n\n            if 'locator' in comment_data:\n                # An example EDL locator line looks like this:\n                # * LOC: 01:00:01:14 RED     ANIM FIX NEEDED\n                # We get the part after \"LOC: \" as the comment_data entry\n                # Given the fixed-width nature of these, we could be more\n                # strict about the field widths, but there are many\n                # variations of EDL, so if we are lenient then maybe we\n                # can handle more of them? Only real-world testing will\n                # determine this for sure...\n                m = re.match(\n                    r'(\\d\\d:\\d\\d:\\d\\d:\\d\\d)\\s+(\\w*)\\s+(.*)',\n                    comment_data[\"locator\"])\n                if m:\n                    marker = otio.schema.Marker()\n                    marker.marked_range = otio.opentime.TimeRange(\n                        start_time=otio.opentime.from_timecode(\n                            m.group(1),\n                            self.edl_rate\n                        ),\n                        duration=otio.opentime.RationalTime()\n                    )\n\n                    # always write the source value into metadata, in case it\n                    # is not a valid enum somehow.\n                    color_parsed_from_file = m.group(2)\n\n                    marker.metadata = {\n                        \"cmx_3600\": {\n                            \"color\": color_parsed_from_file\n                        }\n                    }\n\n                    # @TODO: if it is a valid\n                    if hasattr(\n                        otio.schema.MarkerColor,\n                        color_parsed_from_file.upper()\n                    ):\n                        marker.color = color_parsed_from_file.upper()\n                    else:\n                        marker.color = otio.schema.MarkerColor.RED\n\n                    marker.name = m.group(3)\n                    clip.markers.append(marker)\n                else:\n                    # TODO: Should we report this as a warning somehow?\n                    pass\n\n        clip.source_range = otio.opentime.range_from_start_end_time(\n            otio.opentime.from_timecode(self.source_tc_in, self.edl_rate),\n            otio.opentime.from_timecode(self.source_tc_out, self.edl_rate)\n        )\n\n        return clip"
    },
    {
        "pr_title": "Implement multiple timeline support for xml and viewer",
        "pr_number": 130,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -195,8 +195,18 @@\ndef _parse_clip_item_without_media(clip_item, sequence_rate,\n                                    transition_offsets, element_map):\n     markers = clip_item.findall('./marker')\n     rate = _parse_rate(clip_item, element_map)\n-    in_frame = int(clip_item.find('./in').text) + transition_offsets[0]\n-    out_frame = int(clip_item.find('./out').text) - transition_offsets[1]\n+\n+    # transition offsets are provided in timeline rate. If they deviate they\n+    # need to be rescaled to clip item rate\n+    context_transition_offsets = [\n+        transition_offsets[0].rescaled_to(rate),\n+        transition_offsets[1].rescaled_to(rate)\n+    ]\n+\n+    in_frame = int(clip_item.find('./in').text) + \\\n+        int(round(context_transition_offsets[0].value))\n+    out_frame = int(clip_item.find('./out').text) - \\\n+        int(round(context_transition_offsets[1].value))\n \n     source_range = otio.opentime.TimeRange(\n         start_time=otio.opentime.RationalTime(in_frame, sequence_rate),",
        "comments": [],
        "commit_message": "Bugfixes\n\n- writing incorrect frame ranges around transitions where source rate\n  differs from timeline rate\n- failing unittest trying to _get_single_sequence",
        "commit_id": "f41eca34ff7f27faaaa79664303cc1c5ff381cb6"
    },
    {
        "pr_title": "Implement multiple timeline support for xml and viewer",
        "pr_number": 130,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -225,8 +235,17 @@\ndef _parse_clip_item(clip_item, transition_offsets, element_map):\n     )\n     src_rate = _parse_rate(clip_item.find('./file'), element_map)\n \n-    in_frame = int(clip_item.find('./in').text) + transition_offsets[0]\n-    out_frame = int(clip_item.find('./out').text) - transition_offsets[1]\n+    # transition offsets are provided in timeline rate. If they deviate they\n+    # need to be rescaled to clip item rate\n+    context_transition_offsets = [\n+        transition_offsets[0].rescaled_to(src_rate),\n+        transition_offsets[1].rescaled_to(src_rate)\n+    ]\n+\n+    in_frame = int(clip_item.find('./in').text) + \\\n+        int(round(context_transition_offsets[0].value))\n+    out_frame = int(clip_item.find('./out').text) - \\\n+        int(round(context_transition_offsets[1].value))\n     timecode = media_reference.available_range.start_time\n \n     # source_start in xml is taken relative to the start of the media, whereas",
        "comments": [],
        "commit_message": "Bugfixes\n\n- writing incorrect frame ranges around transitions where source rate\n  differs from timeline rate\n- failing unittest trying to _get_single_sequence",
        "commit_id": "f41eca34ff7f27faaaa79664303cc1c5ff381cb6"
    },
    {
        "pr_title": "Implement multiple timeline support for xml and viewer",
        "pr_number": 130,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -274,8 +293,17 @@\ndef _parse_sequence_item(sequence_item, transition_offsets, element_map):\n     sequence = _parse_sequence(sequence_item.find('./sequence'), element_map)\n     source_rate = _parse_rate(sequence_item.find('./sequence'), element_map)\n \n-    in_frame = int(sequence_item.find('./in').text) + transition_offsets[0]\n-    out_frame = int(sequence_item.find('./out').text) - transition_offsets[1]\n+    # transition offsets are provided in timeline rate. If they deviate they\n+    # need to be rescaled to clip item rate\n+    context_transition_offsets = [\n+        transition_offsets[0].rescaled_to(source_rate),\n+        transition_offsets[1].rescaled_to(source_rate)\n+    ]\n+\n+    in_frame = int(sequence_item.find('./in').text) + \\\n+        int(round(context_transition_offsets[0].value, 0))\n+    out_frame = int(sequence_item.find('./out').text) - \\\n+        int(round(context_transition_offsets[1].value))\n \n     sequence.source_range = otio.opentime.TimeRange(\n         start_time=otio.opentime.RationalTime(in_frame, source_rate),",
        "comments": [],
        "commit_message": "Bugfixes\n\n- writing incorrect frame ranges around transitions where source rate\n  differs from timeline rate\n- failing unittest trying to _get_single_sequence",
        "commit_id": "f41eca34ff7f27faaaa79664303cc1c5ff381cb6"
    },
    {
        "pr_title": "Add transition support for XML and otio view",
        "pr_number": 126,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -50,12 +50,13 @@\ndef wrapper(item, *args, **kwargs):\n                 item_hash = item.__hash__()\n             item_id = br_map[tag].get(item_hash, None)\n             if item_id is not None:\n-                return cElementTree.Element(tag, id='%s-%d' % (tag, item_id))\n+                return cElementTree.Element(tag,\n+                                            id='{}-{}'.format(tag, item_id))\n             item_id = br_map[tag].setdefault(item_hash,\n                                              1 if not br_map[tag] else\n                                              max(br_map[tag].values()) + 1)\n             elem = func(item, *args, **kwargs)\n-            elem.attrib['id'] = '%s-%d' % (tag, item_id)\n+            elem.attrib['id'] = '{}-{}'.format(tag, item_id)\n             return elem\n \n         return wrapper",
        "comments": [
            {
                "comment": "for python 3 compatability we're using `.format()` instead of `%` syntax, if thats alright",
                "position": null
            }
        ],
        "commit_message": "Address change notes\n\n- switch to .format syntax in xml adapter\n- add overlapping() to Composable, defaults to False\n- set overlapping() to True on Transition\n- adjust range_of_child_at_index() to use overlapping()\n- adjust unittest for range_of_child_at_index() with Transitions",
        "commit_id": "107e60d9b39e501604646848bf112dad468eff55",
        "fn_context": "def _backreference_build(tag):\n    # We can also encode these back-references if an item is accessed multiple\n    # times. To do this we store an id attribute on the element. For back-\n    # references we then only need to return an empty element of that type with\n    # the id we logged before\n\n    def singleton_decorator(func):\n        @functools.wraps(func)\n        def wrapper(item, *args, **kwargs):\n            br_map = args[-1]\n            if isinstance(item, otio.media_reference.External):\n                item_hash = hash(str(item.target_url))\n            elif isinstance(item, otio.media_reference.MissingReference):\n                item_hash = 'missing_ref'\n            else:\n                item_hash = item.__hash__()\n            item_id = br_map[tag].get(item_hash, None)\n            if item_id is not None:\n                return cElementTree.Element(tag,\n                                            id='{}-{}'.format(tag, item_id))\n            item_id = br_map[tag].setdefault(item_hash,\n                                             1 if not br_map[tag] else\n                                             max(br_map[tag].values()) + 1)\n            elem = func(item, *args, **kwargs)\n            elem.attrib['id'] = '{}-{}'.format(tag, item_id)\n            return elem\n\n        return wrapper\n\n    return singleton_decorator"
    },
    {
        "pr_title": "Plugin package",
        "pr_number": 96,
        "file_name": "opentimelineio/adapters/__init__.py",
        "code_diff": "@@ -32,6 +32,7 @@\ndef suffixes_with_defined_adapters():\n         )\n     )\n \n+\n def available_adapter_names():\n     \"\"\"Return a string list of the available adapters.\"\"\"",
        "comments": [],
        "commit_message": "Lint pass.",
        "commit_id": "e35303568afe69e3030b1a5b193ff61f5bdffa45"
    },
    {
        "pr_title": "Plugin package",
        "pr_number": 96,
        "file_name": "opentimelineio/adapters/otio_json.py",
        "code_diff": "@@ -4,6 +4,10 @@\ncore\n )\n \n+\n+# @TODO: Implement out of process plugins that hand around JSON\n+\n+\n def read_from_file(filepath):\n     return core.deserialize_json_from_file(filepath)",
        "comments": [],
        "commit_message": "Lint pass.",
        "commit_id": "e35303568afe69e3030b1a5b193ff61f5bdffa45"
    },
    {
        "pr_title": "Plugin package",
        "pr_number": 96,
        "file_name": "opentimelineio/plugins/manifest.py",
        "code_diff": "@@ -1,12 +1,13 @@\n\"\"\"Implementation of an adapter registry system for OTIO.\"\"\"\n- \n+\n import os\n \n from .. import (\n     core,\n     exceptions,\n )\n \n+\n def manifest_from_file(filepath):\n     \"\"\"Read the .json file at filepath into a Manifest object.\"\"\"",
        "comments": [],
        "commit_message": "Lint pass.",
        "commit_id": "e35303568afe69e3030b1a5b193ff61f5bdffa45"
    },
    {
        "pr_title": "Plugin package",
        "pr_number": 96,
        "file_name": "opentimelineio/plugins/manifest.py",
        "code_diff": "@@ -15,6 +16,7 @@\ndef manifest_from_file(filepath):\n     result._update_adapter_source(filepath)\n     return result\n \n+\n @core.register_type\n class Manifest(core.SerializeableObject):\n     \"\"\"Defines an OTIO plugin Manifest.",
        "comments": [],
        "commit_message": "Lint pass.",
        "commit_id": "e35303568afe69e3030b1a5b193ff61f5bdffa45"
    },
    {
        "pr_title": "Plugin package",
        "pr_number": 96,
        "file_name": "opentimelineio/plugins/manifest.py",
        "code_diff": "@@ -72,6 +74,10 @@\ndef adapter_module_from_name(self, name):\n         adp = self.from_name(name)\n         return adp.module()\n \n+\n+_MANIFEST = None\n+\n+\n def load_manifest():\n     # build the manifest of adapters, starting with builtin adapters\n     result = manifest_from_file(",
        "comments": [],
        "commit_message": "Lint pass.",
        "commit_id": "e35303568afe69e3030b1a5b193ff61f5bdffa45"
    },
    {
        "pr_title": "Plugin package",
        "pr_number": 96,
        "file_name": "opentimelineio/plugins/manifest.py",
        "code_diff": "@@ -82,7 +88,6 @@\ndef load_manifest():\n         )\n     )\n \n-\n     # read local adapter manifests, if they exist\n     _local_manifest_path = os.environ.get(\"OTIO_PLUGIN_MANIFEST_PATH\", None)\n     if _local_manifest_path is not None:",
        "comments": [],
        "commit_message": "Lint pass.",
        "commit_id": "e35303568afe69e3030b1a5b193ff61f5bdffa45"
    },
    {
        "pr_title": "Plugin package",
        "pr_number": 96,
        "file_name": "opentimelineio/plugins/python_plugin.py",
        "code_diff": "@@ -8,8 +8,9 @@\nexceptions,\n )\n \n+\n class PythonPlugin(core.SerializeableObject):\n-    \"\"\" A class of plugin that is encoded in a python module, exposed via a \n+    \"\"\" A class of plugin that is encoded in a python module, exposed via a\n     manifest.\n     \"\"\"",
        "comments": [],
        "commit_message": "Lint pass.",
        "commit_id": "e35303568afe69e3030b1a5b193ff61f5bdffa45"
    },
    {
        "pr_title": "Switch to using `imp.load_module` in the adapter module loader",
        "pr_number": 95,
        "file_name": "opentimelineio/adapters/adapter.py",
        "code_diff": "@@ -84,7 +84,7 @@\ndef module_abs_path(self):\n \n         return filepath\n \n-    def _imported_module(self):\n+    def _find_and_load_module(self):\n         \"\"\"Load the module this adapter points at.\"\"\"\n \n         pyname = os.path.splitext(os.path.basename(self.module_abs_path()))[0]",
        "comments": [
            {
                "comment": "The use of `file` here masks the global type `file`",
                "position": null
            }
        ],
        "commit_message": "Change _imported_module name to be more explicit.",
        "commit_id": "cd512a6eac17519b2185e5333aff77e21e717216",
        "fn_context": "    def module_abs_path(self):\n        \"\"\"Return an absolute path to the module implementing this adapter.\"\"\"\n\n        filepath = self.filepath\n        if not os.path.isabs(filepath):\n            filepath = os.path.join(os.path.dirname(self._json_path), filepath)\n\n        return filepath\n\n    def _find_and_load_module(self):\n        \"\"\"Load the module this adapter points at.\"\"\"\n\n        pyname = os.path.splitext(os.path.basename(self.module_abs_path()))[0]\n        pydir = os.path.dirname(self.module_abs_path())\n\n        (file_obj, pathname, description) = imp.find_module(pyname, [pydir])\n\n        with file_obj:\n            # this will reload the module if it has already been loaded.\n            mod = imp.load_module(\n                \"opentimelineio.adapters.{}\".format(self.name),\n                file_obj,\n                pathname,\n                description\n            )\n\n            return mod"
    },
    {
        "pr_title": "Initial Transition support",
        "pr_number": 82,
        "file_name": "opentimelineio/algorithms/sequence_algo.py",
        "code_diff": "@@ -1,4 +1,4 @@\n-\"\"\" Algorithms for sequence objects.  \"\"\"\n+\"\"\" Algorithms for sequence objects. \"\"\"\n \n import copy",
        "comments": [
            {
                "comment": "Totally dig this model of having an algorithms package to hold these sorts of utilities!",
                "position": null
            }
        ],
        "commit_message": "Documentation and cleanup in the algorithms library.",
        "commit_id": "28979152f4883b5b202fecf8fabde803e9d4df8d",
        "fn_context": ""
    },
    {
        "pr_title": "Initial Transition support",
        "pr_number": 82,
        "file_name": "opentimelineio/algorithms/sequence_algo.py",
        "code_diff": "@@ -9,9 +9,10 @@\ndef sequence_with_expanded_transitions(in_seq):\n-    \"\"\"\n-    Expands transitions such that neighboring clips are trimmed into regions of\n-    overlap.  For example, if your sequence is:\n+    \"\"\" Expands transitions such that neighboring clips are trimmed into \n+    regions of overlap.  \n+    \n+    For example, if your sequence is:\n         Clip1, T, Clip2\n \n     will return:",
        "comments": [
            {
                "comment": "Totally dig this model of having an algorithms package to hold these sorts of utilities!",
                "position": null
            }
        ],
        "commit_message": "Documentation and cleanup in the algorithms library.",
        "commit_id": "28979152f4883b5b202fecf8fabde803e9d4df8d",
        "fn_context": "def sequence_with_expanded_transitions(in_seq):\n    \"\"\" Expands transitions such that neighboring clips are trimmed into \n    regions of overlap.  \n    \n    For example, if your sequence is:\n        Clip1, T, Clip2\n\n    will return:\n        Clip1', Clip1_t, T, Clip2_t, Clip2'\n\n    Where Clip1' is the part of Clip1 not in the transition, Clip1_t is the\n    part inside the transition and so on.\n    \"\"\"\n\n    result_sequence = []\n\n    seq_iter = iter(in_seq)\n    prev_thing = None\n    thing = next(seq_iter, None)\n    next_thing = next(seq_iter, None)\n\n    while thing is not None:\n        if isinstance(thing, schema.Transition):\n            result_sequence.append(_expand_transition(thing, in_seq))\n        else:\n            # not a transition, but might be trimmed by one before or after\n            # in the sequence\n            pre_transition = None\n            next_transition = None\n\n            if isinstance(prev_thing, schema.Transition):\n                pre_transition = prev_thing\n\n            if isinstance(next_thing, schema.Transition):\n                next_transition = next_thing\n\n            result_sequence.append(\n                _trim_from_transitions(\n                    thing,\n                    pre=pre_transition,\n                    post=next_transition\n                )\n            )\n\n        # loop\n        prev_thing = thing\n        thing = next_thing\n        next_thing = next(seq_iter, None)\n\n    return result_sequence"
    },
    {
        "pr_title": "Initial Transition support",
        "pr_number": 82,
        "file_name": "opentimelineio/algorithms/sequence_algo.py",
        "code_diff": "@@ -21,22 +22,19 @@\ndef sequence_with_expanded_transitions(in_seq):\n     part inside the transition and so on.\n     \"\"\"\n \n-    sequence_to_modify = copy.deepcopy(in_seq)\n-\n-    # we want to copy all the top level parameters and settings\n     result_sequence = []\n \n-    iterable = iter(sequence_to_modify)\n+    seq_iter = iter(in_seq)\n     prev_thing = None\n-    thing = next(iterable, None)\n-    next_thing = next(iterable, None)\n+    thing = next(seq_iter, None)\n+    next_thing = next(seq_iter, None)\n \n     while thing is not None:\n         if isinstance(thing, schema.Transition):\n-            expanded_trx = _expand_transition(thing, sequence_to_modify)\n-            result_sequence.append(expanded_trx)\n+            result_sequence.append(_expand_transition(thing, in_seq))\n         else:\n-            # not a transition, but might be trimmed by one coming up\n+            # not a transition, but might be trimmed by one before or after\n+            # in the sequence\n             pre_transition = None\n             next_transition = None",
        "comments": [
            {
                "comment": "Totally dig this model of having an algorithms package to hold these sorts of utilities!",
                "position": null
            }
        ],
        "commit_message": "Documentation and cleanup in the algorithms library.",
        "commit_id": "28979152f4883b5b202fecf8fabde803e9d4df8d",
        "fn_context": "def sequence_with_expanded_transitions(in_seq):\n    \"\"\" Expands transitions such that neighboring clips are trimmed into \n    regions of overlap.  \n    \n    For example, if your sequence is:\n        Clip1, T, Clip2\n\n    will return:\n        Clip1', Clip1_t, T, Clip2_t, Clip2'\n\n    Where Clip1' is the part of Clip1 not in the transition, Clip1_t is the\n    part inside the transition and so on.\n    \"\"\"\n\n    result_sequence = []\n\n    seq_iter = iter(in_seq)\n    prev_thing = None\n    thing = next(seq_iter, None)\n    next_thing = next(seq_iter, None)\n\n    while thing is not None:\n        if isinstance(thing, schema.Transition):\n            result_sequence.append(_expand_transition(thing, in_seq))\n        else:\n            # not a transition, but might be trimmed by one before or after\n            # in the sequence\n            pre_transition = None\n            next_transition = None\n\n            if isinstance(prev_thing, schema.Transition):\n                pre_transition = prev_thing\n\n            if isinstance(next_thing, schema.Transition):\n                next_transition = next_thing\n\n            result_sequence.append(\n                _trim_from_transitions(\n                    thing,\n                    pre=pre_transition,\n                    post=next_transition\n                )\n            )\n\n        # loop\n        prev_thing = thing\n        thing = next_thing\n        next_thing = next(seq_iter, None)\n\n    return result_sequence"
    },
    {
        "pr_title": "Initial Transition support",
        "pr_number": 82,
        "file_name": "opentimelineio/algorithms/sequence_algo.py",
        "code_diff": "@@ -53,14 +51,20 @@\ndef sequence_with_expanded_transitions(in_seq):\n                     post=next_transition\n                 )\n             )\n+\n+        # loop\n         prev_thing = thing\n         thing = next_thing\n-        next_thing = next(iterable, None)\n+        next_thing = next(seq_iter, None)\n \n     return result_sequence\n \n \n def _expand_transition(target_transition, from_sequence):\n+    \"\"\" Expand transitions into the portions of pre-and-post clips that \n+    overlap with the transition.\n+    \"\"\"\n+\n     result = from_sequence.neighbors_of(\n         target_transition,\n         schema.NeighborFillerPolicy.around_transitions",
        "comments": [
            {
                "comment": "Totally dig this model of having an algorithms package to hold these sorts of utilities!",
                "position": null
            }
        ],
        "commit_message": "Documentation and cleanup in the algorithms library.",
        "commit_id": "28979152f4883b5b202fecf8fabde803e9d4df8d",
        "fn_context": "def sequence_with_expanded_transitions(in_seq):\n    \"\"\" Expands transitions such that neighboring clips are trimmed into \n    regions of overlap.  \n    \n    For example, if your sequence is:\n        Clip1, T, Clip2\n\n    will return:\n        Clip1', Clip1_t, T, Clip2_t, Clip2'\n\n    Where Clip1' is the part of Clip1 not in the transition, Clip1_t is the\n    part inside the transition and so on.\n    \"\"\"\n\n    result_sequence = []\n\n    seq_iter = iter(in_seq)\n    prev_thing = None\n    thing = next(seq_iter, None)\n    next_thing = next(seq_iter, None)\n\n    while thing is not None:\n        if isinstance(thing, schema.Transition):\n            result_sequence.append(_expand_transition(thing, in_seq))\n        else:\n            # not a transition, but might be trimmed by one before or after\n            # in the sequence\n            pre_transition = None\n            next_transition = None\n\n            if isinstance(prev_thing, schema.Transition):\n                pre_transition = prev_thing\n\n            if isinstance(next_thing, schema.Transition):\n                next_transition = next_thing\n\n            result_sequence.append(\n                _trim_from_transitions(\n                    thing,\n                    pre=pre_transition,\n                    post=next_transition\n                )\n            )\n\n        # loop\n        prev_thing = thing\n        thing = next_thing\n        next_thing = next(seq_iter, None)\n\n    return result_sequence\n\ndef _expand_transition(target_transition, from_sequence):\n    \"\"\" Expand transitions into the portions of pre-and-post clips that \n    overlap with the transition.\n    \"\"\"\n\n    result = from_sequence.neighbors_of(\n        target_transition,\n        schema.NeighborFillerPolicy.around_transitions\n    )\n\n    trx_duration = target_transition.in_offset + target_transition.out_offset\n\n    # make copies of the before and after, and modify their in/out points\n    pre = copy.deepcopy(result[0])\n\n    if isinstance(pre, schema.Transition):\n        raise exceptions.TransitionFollowingATransitionError(\n            \"cannot put two transitions next to each other in a  sequence: \"\n            \"{}, {}\".format(\n                pre,\n                target_transition\n            )\n        )\n    pre.name = (pre.name or \"\") + \"_transition_pre\"\n\n    # ensure that pre.source_range is set, because it will get manipulated\n    pre.source_range = copy.deepcopy(pre.trimmed_range())\n\n    if target_transition.in_offset is None:\n        raise RuntimeError(\n            \"in_offset is None on: {}\".format(target_transition)\n        )\n\n    if target_transition.out_offset is None:\n        raise RuntimeError(\n            \"out_offset is None on: {}\".format(target_transition)\n        )\n\n    pre.source_range.start_time = (\n        pre.source_range.end_time_exclusive()\n        - target_transition.in_offset\n    )\n    pre.source_range.duration = trx_duration.rescaled_to(\n        pre.source_range.start_time\n    )\n\n    post = copy.deepcopy(result[2])\n    if isinstance(post, schema.Transition):\n        raise exceptions.TransitionFollowingATransitionError(\n            \"cannot put two transitions next to each other in a  sequence: \"\n            \"{}, {}\".format(\n                target_transition,\n                post\n            )\n        )\n\n    post.name = (post.name or \"\") + \"_transition_post\"\n\n    # ensure that post.source_range is set, because it will get manipulated\n    post.source_range = copy.deepcopy(post.trimmed_range())\n\n    post.source_range.start_time = (\n        post.source_range.start_time - target_transition.in_offset\n    ).rescaled_to(post.source_range.start_time)\n    post.source_range.duration = trx_duration.rescaled_to(\n        post.source_range.start_time\n    )\n\n    return (pre, target_transition, post)"
    },
    {
        "pr_title": "handle clips without media reference",
        "pr_number": 72,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -44,6 +44,8 @@\ndef wrapper(item, *args, **kwargs):\n             br_map = args[-1]\n             if isinstance(item, otio.media_reference.External):\n                 item_hash = hash(str(item.target_url))\n+            elif isinstance(item, otio.media_reference.MissingReference):\n+                item_hash = 'missing_ref'\n             else:\n                 item_hash = item.__hash__()\n             item_id = br_map[tag].get(item_hash, None)",
        "comments": [
            {
                "comment": "I would change this so that in both cases you're noting what the final name will be, so:\r\n`import urlparse.urlparse as urllib_parse` and\r\n`import urllib.parse as urllib_parse` to indicate intent of the import.  Also a comment noting that this is for python 2/3 changes?",
                "position": null
            },
            {
                "comment": "If you see the above comment, then you can replace the try/except here with simply: `urllib_parse(url)`",
                "position": null
            },
            {
                "comment": "So if I'm reading this correctly, this creates a 5 second duration if the URL is not set?  Can `available_range` simply be `None`?",
                "position": null
            },
            {
                "comment": "I have removed the duration from this, because it was kind of random. But the rest of the code would need more refactoring if the available range were completely gone.\r\nI can revisit this if necessary. Are there currently any guidelines on how we want to handle media-less clips? Or is this something that will be more fleshed out when we get to generator clips?",
                "position": null
            }
        ],
        "commit_message": "refactored to set media reference to None if there is no linked media.",
        "commit_id": "f4263b2e20d0a9b9c3a665912c9f7df92b1b1e3b",
        "fn_context": "def _backreference_build(tag):\n    # We can also encode these back-references if an item is accessed multiple\n    # times. To do this we store an id attribute on the element. For back-\n    # references we then only need to return an empty element of that type with\n    # the id we logged before\n\n    def singleton_decorator(func):\n        @functools.wraps(func)\n        def wrapper(item, *args, **kwargs):\n            br_map = args[-1]\n            if isinstance(item, otio.media_reference.External):\n                item_hash = hash(str(item.target_url))\n            elif isinstance(item, otio.media_reference.MissingReference):\n                item_hash = 'missing_ref'\n            else:\n                item_hash = item.__hash__()\n            item_id = br_map[tag].get(item_hash, None)\n            if item_id is not None:\n                return cElementTree.Element(tag, id='%s-%d' % (tag, item_id))\n            item_id = br_map[tag].setdefault(item_hash,\n                                             1 if not br_map[tag] else\n                                             max(br_map[tag].values()) + 1)\n            elem = func(item, *args, **kwargs)\n            elem.attrib['id'] = '%s-%d' % (tag, item_id)\n            return elem\n\n        return wrapper\n\n    return singleton_decorator"
    },
    {
        "pr_title": "handle clips without media reference",
        "pr_number": 72,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -113,22 +115,10 @@\ndef _parse_rate(elem, element_map):\n def _parse_media_reference(file_e, element_map):\n     file_e = _resolved_backreference(file_e, 'file', element_map)\n \n+    url = file_e.find('./pathurl').text\n     file_rate = _parse_rate(file_e, element_map)\n     timecode_rate = _parse_rate(file_e.find('./timecode'), element_map)\n     timecode_frame = int(file_e.find('./timecode/frame').text)\n-    url_e = file_e.find('./pathurl')\n-\n-    if url_e is None:\n-        available_range = otio.opentime.TimeRange(\n-            start_time=otio.opentime.RationalTime(timecode_frame,\n-                                                  timecode_rate)\n-        )\n-        return otio.media_reference.External(\n-            target_url=None,\n-            available_range=available_range\n-        )\n-\n-    url = url_e.text\n     duration = int(file_e.find('./duration').text)\n \n     available_range = otio.opentime.TimeRange(",
        "comments": [
            {
                "comment": "I would change this so that in both cases you're noting what the final name will be, so:\r\n`import urlparse.urlparse as urllib_parse` and\r\n`import urllib.parse as urllib_parse` to indicate intent of the import.  Also a comment noting that this is for python 2/3 changes?",
                "position": null
            },
            {
                "comment": "If you see the above comment, then you can replace the try/except here with simply: `urllib_parse(url)`",
                "position": null
            },
            {
                "comment": "So if I'm reading this correctly, this creates a 5 second duration if the URL is not set?  Can `available_range` simply be `None`?",
                "position": null
            },
            {
                "comment": "I have removed the duration from this, because it was kind of random. But the rest of the code would need more refactoring if the available range were completely gone.\r\nI can revisit this if necessary. Are there currently any guidelines on how we want to handle media-less clips? Or is this something that will be more fleshed out when we get to generator clips?",
                "position": null
            }
        ],
        "commit_message": "refactored to set media reference to None if there is no linked media.",
        "commit_id": "f4263b2e20d0a9b9c3a665912c9f7df92b1b1e3b",
        "fn_context": "def _parse_media_reference(file_e, element_map):\n    file_e = _resolved_backreference(file_e, 'file', element_map)\n\n    url = file_e.find('./pathurl').text\n    file_rate = _parse_rate(file_e, element_map)\n    timecode_rate = _parse_rate(file_e.find('./timecode'), element_map)\n    timecode_frame = int(file_e.find('./timecode/frame').text)\n    duration = int(file_e.find('./duration').text)\n\n    available_range = otio.opentime.TimeRange(\n        start_time=otio.opentime.RationalTime(timecode_frame, timecode_rate),\n        duration=otio.opentime.RationalTime(duration, file_rate)\n    )\n\n    return otio.media_reference.External(\n        target_url=url.strip(),\n        available_range=available_range\n    )\n\ndef _parse_clip_item_without_media(clip_item, element_map):\n    markers = clip_item.findall('./marker')\n    rate = _parse_rate(clip_item, element_map)\n\n    name_item = clip_item.find('name')\n    if name_item is not None:\n        name = name_item.text\n    else:\n        name = None\n\n    clip = otio.schema.Clip(name=name)\n    clip.markers.extend(\n        [_parse_marker(m, rate) for m in markers])\n    return clip"
    },
    {
        "pr_title": "handle clips without media reference",
        "pr_number": 72,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -142,6 +132,22 @@\ndef _parse_media_reference(file_e, element_map):\n     )\n \n \n+def _parse_clip_item_without_media(clip_item, element_map):\n+    markers = clip_item.findall('./marker')\n+    rate = _parse_rate(clip_item, element_map)\n+\n+    name_item = clip_item.find('name')\n+    if name_item is not None:\n+        name = name_item.text\n+    else:\n+        name = None\n+\n+    clip = otio.schema.Clip(name=name)\n+    clip.markers.extend(\n+        [_parse_marker(m, rate) for m in markers])\n+    return clip\n+\n+\n def _parse_clip_item(clip_item, element_map):\n     markers = clip_item.findall('./marker')",
        "comments": [
            {
                "comment": "I would change this so that in both cases you're noting what the final name will be, so:\r\n`import urlparse.urlparse as urllib_parse` and\r\n`import urllib.parse as urllib_parse` to indicate intent of the import.  Also a comment noting that this is for python 2/3 changes?",
                "position": null
            },
            {
                "comment": "If you see the above comment, then you can replace the try/except here with simply: `urllib_parse(url)`",
                "position": null
            },
            {
                "comment": "So if I'm reading this correctly, this creates a 5 second duration if the URL is not set?  Can `available_range` simply be `None`?",
                "position": null
            },
            {
                "comment": "I have removed the duration from this, because it was kind of random. But the rest of the code would need more refactoring if the available range were completely gone.\r\nI can revisit this if necessary. Are there currently any guidelines on how we want to handle media-less clips? Or is this something that will be more fleshed out when we get to generator clips?",
                "position": null
            }
        ],
        "commit_message": "refactored to set media reference to None if there is no linked media.",
        "commit_id": "f4263b2e20d0a9b9c3a665912c9f7df92b1b1e3b",
        "fn_context": "def _parse_media_reference(file_e, element_map):\n    file_e = _resolved_backreference(file_e, 'file', element_map)\n\n    url = file_e.find('./pathurl').text\n    file_rate = _parse_rate(file_e, element_map)\n    timecode_rate = _parse_rate(file_e.find('./timecode'), element_map)\n    timecode_frame = int(file_e.find('./timecode/frame').text)\n    duration = int(file_e.find('./duration').text)\n\n    available_range = otio.opentime.TimeRange(\n        start_time=otio.opentime.RationalTime(timecode_frame, timecode_rate),\n        duration=otio.opentime.RationalTime(duration, file_rate)\n    )\n\n    return otio.media_reference.External(\n        target_url=url.strip(),\n        available_range=available_range\n    )\n\ndef _parse_clip_item_without_media(clip_item, element_map):\n    markers = clip_item.findall('./marker')\n    rate = _parse_rate(clip_item, element_map)\n\n    name_item = clip_item.find('name')\n    if name_item is not None:\n        name = name_item.text\n    else:\n        name = None\n\n    clip = otio.schema.Clip(name=name)\n    clip.markers.extend(\n        [_parse_marker(m, rate) for m in markers])\n    return clip\n\ndef _parse_clip_item(clip_item, element_map):\n    markers = clip_item.findall('./marker')\n\n    media_reference = _parse_media_reference(\n        clip_item.find('./file'),\n        element_map\n    )\n    source_rate = _parse_rate(clip_item.find('./file'), element_map)\n\n    # get the clip name from the media reference if not defined on the clip\n    name_item = clip_item.find('name')\n    if name_item is not None:\n        name = name_item.text\n    else:\n        url_path = _url_to_path(media_reference.target_url)\n        name = os.path.basename(url_path)\n\n    clip = otio.schema.Clip(name=name,\n                            media_reference=media_reference)\n    clip.markers.extend(\n        [_parse_marker(m, source_rate) for m in markers])\n    return clip"
    },
    {
        "pr_title": "handle clips without media reference",
        "pr_number": 72,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -166,13 +172,22 @@\ndef _parse_clip_item(clip_item, element_map):\n     return clip\n \n \n-def _parse_item(clip_item, element_map):\n+def _parse_item(clip_item, sequence_rate, element_map):\n     # depending on the content of the clip-item, we return either a clip or a\n     # stack. In either case we set the source range as well\n-    if clip_item.find('./file') is not None:\n-        item = _parse_clip_item(clip_item, element_map)\n-        source_rate = _parse_rate(clip_item.find('./file'), element_map)\n-        timecode = item.media_reference.available_range.start_time\n+    file_e = clip_item.find('./file')\n+    if file_e is not None:\n+        file_e = _resolved_backreference(file_e, 'file', element_map)\n+\n+    if file_e is not None:\n+        if file_e.find('./pathurl') is None:\n+            item = _parse_clip_item_without_media(clip_item, element_map)\n+            source_rate = sequence_rate\n+            timecode = otio.opentime.RationalTime(0, sequence_rate)\n+        else:\n+            item = _parse_clip_item(clip_item, element_map)\n+            source_rate = _parse_rate(clip_item.find('./file'), element_map)\n+            timecode = item.media_reference.available_range.start_time\n     elif clip_item.find('./sequence') is not None:\n         item = _parse_sequence(clip_item.find('./sequence'), element_map)\n         source_rate = _parse_rate(clip_item.find('./sequence'), element_map)",
        "comments": [
            {
                "comment": "I would change this so that in both cases you're noting what the final name will be, so:\r\n`import urlparse.urlparse as urllib_parse` and\r\n`import urllib.parse as urllib_parse` to indicate intent of the import.  Also a comment noting that this is for python 2/3 changes?",
                "position": null
            },
            {
                "comment": "If you see the above comment, then you can replace the try/except here with simply: `urllib_parse(url)`",
                "position": null
            },
            {
                "comment": "So if I'm reading this correctly, this creates a 5 second duration if the URL is not set?  Can `available_range` simply be `None`?",
                "position": null
            },
            {
                "comment": "I have removed the duration from this, because it was kind of random. But the rest of the code would need more refactoring if the available range were completely gone.\r\nI can revisit this if necessary. Are there currently any guidelines on how we want to handle media-less clips? Or is this something that will be more fleshed out when we get to generator clips?",
                "position": null
            }
        ],
        "commit_message": "refactored to set media reference to None if there is no linked media.",
        "commit_id": "f4263b2e20d0a9b9c3a665912c9f7df92b1b1e3b",
        "fn_context": "def _parse_clip_item(clip_item, element_map):\n    markers = clip_item.findall('./marker')\n\n    media_reference = _parse_media_reference(\n        clip_item.find('./file'),\n        element_map\n    )\n    source_rate = _parse_rate(clip_item.find('./file'), element_map)\n\n    # get the clip name from the media reference if not defined on the clip\n    name_item = clip_item.find('name')\n    if name_item is not None:\n        name = name_item.text\n    else:\n        url_path = _url_to_path(media_reference.target_url)\n        name = os.path.basename(url_path)\n\n    clip = otio.schema.Clip(name=name,\n                            media_reference=media_reference)\n    clip.markers.extend(\n        [_parse_marker(m, source_rate) for m in markers])\n    return clip\n\ndef _parse_item(clip_item, sequence_rate, element_map):\n    # depending on the content of the clip-item, we return either a clip or a\n    # stack. In either case we set the source range as well\n    file_e = clip_item.find('./file')\n    if file_e is not None:\n        file_e = _resolved_backreference(file_e, 'file', element_map)\n\n    if file_e is not None:\n        if file_e.find('./pathurl') is None:\n            item = _parse_clip_item_without_media(clip_item, element_map)\n            source_rate = sequence_rate\n            timecode = otio.opentime.RationalTime(0, sequence_rate)\n        else:\n            item = _parse_clip_item(clip_item, element_map)\n            source_rate = _parse_rate(clip_item.find('./file'), element_map)\n            timecode = item.media_reference.available_range.start_time\n    elif clip_item.find('./sequence') is not None:\n        item = _parse_sequence(clip_item.find('./sequence'), element_map)\n        source_rate = _parse_rate(clip_item.find('./sequence'), element_map)\n        timecode = otio.opentime.RationalTime(0, source_rate)\n    else:\n        raise TypeError('Type of clip item is not supported %s' %\n                        clip_item.attrib['id'])\n\n    in_frame = int(clip_item.find('./in').text)\n    out_frame = int(clip_item.find('./out').text)\n\n    # source_start in xml is taken relative to the start of the media, whereas\n    # we want the absolute start time, taking into account the timecode\n    source_start = otio.opentime.RationalTime(in_frame, source_rate) + timecode\n    source_range = otio.opentime.TimeRange(\n        start_time=source_start,\n        duration=otio.opentime.RationalTime(out_frame - in_frame, source_rate)\n    )\n    item.source_range = source_range\n\n    return item"
    },
    {
        "pr_title": "handle clips without media reference",
        "pr_number": 72,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -228,7 +243,7 @@\ndef _parse_track(track_e, kind, rate, element_map):\n             track.append(otio.schema.Filler(source_range=filler_range))\n \n         # finally add the clip-item itself\n-        track.append(_parse_item(clip_item, element_map))\n+        track.append(_parse_item(clip_item, rate, element_map))\n \n     return track",
        "comments": [
            {
                "comment": "I would change this so that in both cases you're noting what the final name will be, so:\r\n`import urlparse.urlparse as urllib_parse` and\r\n`import urllib.parse as urllib_parse` to indicate intent of the import.  Also a comment noting that this is for python 2/3 changes?",
                "position": null
            },
            {
                "comment": "If you see the above comment, then you can replace the try/except here with simply: `urllib_parse(url)`",
                "position": null
            },
            {
                "comment": "So if I'm reading this correctly, this creates a 5 second duration if the URL is not set?  Can `available_range` simply be `None`?",
                "position": null
            },
            {
                "comment": "I have removed the duration from this, because it was kind of random. But the rest of the code would need more refactoring if the available range were completely gone.\r\nI can revisit this if necessary. Are there currently any guidelines on how we want to handle media-less clips? Or is this something that will be more fleshed out when we get to generator clips?",
                "position": null
            }
        ],
        "commit_message": "refactored to set media reference to None if there is no linked media.",
        "commit_id": "f4263b2e20d0a9b9c3a665912c9f7df92b1b1e3b",
        "fn_context": "def _parse_track(track_e, kind, rate, element_map):\n    track = otio.schema.Sequence(kind=kind)\n    clip_items = track_e.findall('./clipitem')\n\n    if not clip_items:\n        return track\n\n    last_clip_end = 0\n    for clip_item in clip_items:\n        clip_item_index = list(track_e).index(clip_item)\n        start = int(clip_item.find('./start').text)\n        end = int(clip_item.find('./end').text)\n\n        # start time and end time on the timeline can be set to -1. This means\n        # that there is a transition at that end of the clip-item. So the time\n        # on the timeline has to be taken from that object.\n        if start == -1:\n            in_transition = list(track_e)[clip_item_index - 1]\n            start = int(in_transition.find('./start').text)\n        if end == -1:\n            out_transition = list(track_e)[clip_item_index + 1]\n            end = int(out_transition.find('./end').text)\n\n        # see if we need to add a filler before this clip-item\n        fill_time = start - last_clip_end\n        last_clip_end = end\n        if fill_time:\n            filler_range = otio.opentime.TimeRange(\n                duration=otio.opentime.RationalTime(fill_time, rate))\n            track.append(otio.schema.Filler(source_range=filler_range))\n\n        # finally add the clip-item itself\n        track.append(_parse_item(clip_item, rate, element_map))\n\n    return track"
    },
    {
        "pr_title": "handle clips without media reference",
        "pr_number": 72,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -284,14 +299,28 @@\ndef _build_rate(time):\n     return rate_e\n \n \n+@_backreference_build('file')\n+def _build_empty_file(media_ref, source_start, br_map):\n+    file_e = cElementTree.Element('file')\n+    file_e.append(_build_rate(source_start))\n+    file_media_e = _insert_new_sub_element(file_e, 'media')\n+    _insert_new_sub_element(file_media_e, 'video')\n+\n+    return file_e\n+\n+\n @_backreference_build('file')\n def _build_file(media_reference, br_map):\n     file_e = cElementTree.Element('file')\n \n     available_range = media_reference.available_range\n+    url_path = _url_to_path(media_reference.target_url)\n+\n+    _insert_new_sub_element(file_e, 'name', text=os.path.basename(url_path))\n     file_e.append(_build_rate(available_range.start_time))\n     _insert_new_sub_element(file_e, 'duration',\n                             text=str(available_range.duration.value))\n+    _insert_new_sub_element(file_e, 'pathurl', text=media_reference.target_url)\n \n     # timecode\n     timecode = available_range.start_time",
        "comments": [
            {
                "comment": "I would change this so that in both cases you're noting what the final name will be, so:\r\n`import urlparse.urlparse as urllib_parse` and\r\n`import urllib.parse as urllib_parse` to indicate intent of the import.  Also a comment noting that this is for python 2/3 changes?",
                "position": null
            },
            {
                "comment": "If you see the above comment, then you can replace the try/except here with simply: `urllib_parse(url)`",
                "position": null
            },
            {
                "comment": "So if I'm reading this correctly, this creates a 5 second duration if the URL is not set?  Can `available_range` simply be `None`?",
                "position": null
            },
            {
                "comment": "I have removed the duration from this, because it was kind of random. But the rest of the code would need more refactoring if the available range were completely gone.\r\nI can revisit this if necessary. Are there currently any guidelines on how we want to handle media-less clips? Or is this something that will be more fleshed out when we get to generator clips?",
                "position": null
            }
        ],
        "commit_message": "refactored to set media reference to None if there is no linked media.",
        "commit_id": "f4263b2e20d0a9b9c3a665912c9f7df92b1b1e3b",
        "fn_context": "def _build_rate(time):\n    rate = math.ceil(time.rate)\n\n    rate_e = cElementTree.Element('rate')\n    _insert_new_sub_element(rate_e, 'timebase', text=str(int(rate)))\n    _insert_new_sub_element(rate_e, 'ntsc',\n                            text='FALSE' if rate == time.rate else 'TRUE')\n    return rate_e\n\ndef _build_empty_file(media_ref, source_start, br_map):\n    file_e = cElementTree.Element('file')\n    file_e.append(_build_rate(source_start))\n    file_media_e = _insert_new_sub_element(file_e, 'media')\n    _insert_new_sub_element(file_media_e, 'video')\n\n    return file_e\n\ndef _build_file(media_reference, br_map):\n    file_e = cElementTree.Element('file')\n\n    available_range = media_reference.available_range\n    url_path = _url_to_path(media_reference.target_url)\n\n    _insert_new_sub_element(file_e, 'name', text=os.path.basename(url_path))\n    file_e.append(_build_rate(available_range.start_time))\n    _insert_new_sub_element(file_e, 'duration',\n                            text=str(available_range.duration.value))\n    _insert_new_sub_element(file_e, 'pathurl', text=media_reference.target_url)\n\n    # timecode\n    timecode = available_range.start_time\n    timecode_e = _insert_new_sub_element(file_e, 'timecode')\n    timecode_e.append(_build_rate(timecode))\n    _insert_new_sub_element(timecode_e, 'string',\n                            text=otio.opentime.to_timecode(timecode))\n    _insert_new_sub_element(timecode_e, 'frame', text=str(int(timecode.value)))\n    display_format = 'DF' if (math.ceil(timecode.rate) == 30\n                              and math.ceil(timecode.rate) != timecode.rate) \\\n        else 'NDF'\n    _insert_new_sub_element(timecode_e, 'displayformat', text=display_format)\n\n    # we need to flag the file reference with the content types, otherwise it\n    # will not get recognized\n    file_media_e = _insert_new_sub_element(file_e, 'media')\n    content_types = []\n    if not os.path.splitext(url_path)[1].lower() in ('.wav', '.aac', '.mp3'):\n        content_types.append('video')\n    content_types.append('audio')\n\n    for kind in content_types:\n        _insert_new_sub_element(file_media_e, kind)\n\n    return file_e"
    },
    {
        "pr_title": "handle clips without media reference",
        "pr_number": 72,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -305,16 +334,6 @@\ndef _build_file(media_reference, br_map):\n         else 'NDF'\n     _insert_new_sub_element(timecode_e, 'displayformat', text=display_format)\n \n-    if media_reference.target_url is not None:\n-        url_path = _url_to_path(media_reference.target_url)\n-\n-        _insert_new_sub_element(file_e, 'name',\n-                                text=os.path.basename(url_path))\n-        _insert_new_sub_element(file_e, 'pathurl',\n-                                text=media_reference.target_url)\n-    else:\n-        return file_e\n-\n     # we need to flag the file reference with the content types, otherwise it\n     # will not get recognized\n     file_media_e = _insert_new_sub_element(file_e, 'media')",
        "comments": [
            {
                "comment": "I would change this so that in both cases you're noting what the final name will be, so:\r\n`import urlparse.urlparse as urllib_parse` and\r\n`import urllib.parse as urllib_parse` to indicate intent of the import.  Also a comment noting that this is for python 2/3 changes?",
                "position": null
            },
            {
                "comment": "If you see the above comment, then you can replace the try/except here with simply: `urllib_parse(url)`",
                "position": null
            },
            {
                "comment": "So if I'm reading this correctly, this creates a 5 second duration if the URL is not set?  Can `available_range` simply be `None`?",
                "position": null
            },
            {
                "comment": "I have removed the duration from this, because it was kind of random. But the rest of the code would need more refactoring if the available range were completely gone.\r\nI can revisit this if necessary. Are there currently any guidelines on how we want to handle media-less clips? Or is this something that will be more fleshed out when we get to generator clips?",
                "position": null
            }
        ],
        "commit_message": "refactored to set media reference to None if there is no linked media.",
        "commit_id": "f4263b2e20d0a9b9c3a665912c9f7df92b1b1e3b",
        "fn_context": "def _build_file(media_reference, br_map):\n    file_e = cElementTree.Element('file')\n\n    available_range = media_reference.available_range\n    url_path = _url_to_path(media_reference.target_url)\n\n    _insert_new_sub_element(file_e, 'name', text=os.path.basename(url_path))\n    file_e.append(_build_rate(available_range.start_time))\n    _insert_new_sub_element(file_e, 'duration',\n                            text=str(available_range.duration.value))\n    _insert_new_sub_element(file_e, 'pathurl', text=media_reference.target_url)\n\n    # timecode\n    timecode = available_range.start_time\n    timecode_e = _insert_new_sub_element(file_e, 'timecode')\n    timecode_e.append(_build_rate(timecode))\n    _insert_new_sub_element(timecode_e, 'string',\n                            text=otio.opentime.to_timecode(timecode))\n    _insert_new_sub_element(timecode_e, 'frame', text=str(int(timecode.value)))\n    display_format = 'DF' if (math.ceil(timecode.rate) == 30\n                              and math.ceil(timecode.rate) != timecode.rate) \\\n        else 'NDF'\n    _insert_new_sub_element(timecode_e, 'displayformat', text=display_format)\n\n    # we need to flag the file reference with the content types, otherwise it\n    # will not get recognized\n    file_media_e = _insert_new_sub_element(file_e, 'media')\n    content_types = []\n    if not os.path.splitext(url_path)[1].lower() in ('.wav', '.aac', '.mp3'):\n        content_types.append('video')\n    content_types.append('audio')\n\n    for kind in content_types:\n        _insert_new_sub_element(file_media_e, kind)\n\n    return file_e"
    },
    {
        "pr_title": "handle clips without media reference",
        "pr_number": 72,
        "file_name": "opentimelineio/adapters/fcp_xml.py",
        "code_diff": "@@ -329,6 +348,20 @@\ndef _build_file(media_reference, br_map):\n     return file_e\n \n \n+def _build_clip_item_without_media(clip_item, br_map):\n+    clip_item_e = cElementTree.Element('clipitem', frameBlend='FALSE')\n+    start_time = clip_item.source_range.start_time\n+\n+    _insert_new_sub_element(clip_item_e, 'name', text=clip_item.name)\n+    clip_item_e.append(_build_rate(start_time))\n+    clip_item_e.append(\n+        _build_empty_file(clip_item.media_reference, start_time, br_map)\n+    )\n+    clip_item_e.extend([_build_marker(m) for m in clip_item.markers])\n+\n+    return clip_item_e\n+\n+\n def _build_clip_item(clip_item, br_map):\n     clip_item_e = cElementTree.Element('clipitem', frameBlend='FALSE')",
        "comments": [
            {
                "comment": "I would change this so that in both cases you're noting what the final name will be, so:\r\n`import urlparse.urlparse as urllib_parse` and\r\n`import urllib.parse as urllib_parse` to indicate intent of the import.  Also a comment noting that this is for python 2/3 changes?",
                "position": null
            },
            {
                "comment": "If you see the above comment, then you can replace the try/except here with simply: `urllib_parse(url)`",
                "position": null
            },
            {
                "comment": "So if I'm reading this correctly, this creates a 5 second duration if the URL is not set?  Can `available_range` simply be `None`?",
                "position": null
            },
            {
                "comment": "I have removed the duration from this, because it was kind of random. But the rest of the code would need more refactoring if the available range were completely gone.\r\nI can revisit this if necessary. Are there currently any guidelines on how we want to handle media-less clips? Or is this something that will be more fleshed out when we get to generator clips?",
                "position": null
            }
        ],
        "commit_message": "refactored to set media reference to None if there is no linked media.",
        "commit_id": "f4263b2e20d0a9b9c3a665912c9f7df92b1b1e3b",
        "fn_context": "def _build_file(media_reference, br_map):\n    file_e = cElementTree.Element('file')\n\n    available_range = media_reference.available_range\n    url_path = _url_to_path(media_reference.target_url)\n\n    _insert_new_sub_element(file_e, 'name', text=os.path.basename(url_path))\n    file_e.append(_build_rate(available_range.start_time))\n    _insert_new_sub_element(file_e, 'duration',\n                            text=str(available_range.duration.value))\n    _insert_new_sub_element(file_e, 'pathurl', text=media_reference.target_url)\n\n    # timecode\n    timecode = available_range.start_time\n    timecode_e = _insert_new_sub_element(file_e, 'timecode')\n    timecode_e.append(_build_rate(timecode))\n    _insert_new_sub_element(timecode_e, 'string',\n                            text=otio.opentime.to_timecode(timecode))\n    _insert_new_sub_element(timecode_e, 'frame', text=str(int(timecode.value)))\n    display_format = 'DF' if (math.ceil(timecode.rate) == 30\n                              and math.ceil(timecode.rate) != timecode.rate) \\\n        else 'NDF'\n    _insert_new_sub_element(timecode_e, 'displayformat', text=display_format)\n\n    # we need to flag the file reference with the content types, otherwise it\n    # will not get recognized\n    file_media_e = _insert_new_sub_element(file_e, 'media')\n    content_types = []\n    if not os.path.splitext(url_path)[1].lower() in ('.wav', '.aac', '.mp3'):\n        content_types.append('video')\n    content_types.append('audio')\n\n    for kind in content_types:\n        _insert_new_sub_element(file_media_e, kind)\n\n    return file_e\n\ndef _build_clip_item_without_media(clip_item, br_map):\n    clip_item_e = cElementTree.Element('clipitem', frameBlend='FALSE')\n    start_time = clip_item.source_range.start_time\n\n    _insert_new_sub_element(clip_item_e, 'name', text=clip_item.name)\n    clip_item_e.append(_build_rate(start_time))\n    clip_item_e.append(\n        _build_empty_file(clip_item.media_reference, start_time, br_map)\n    )\n    clip_item_e.extend([_build_marker(m) for m in clip_item.markers])\n\n    return clip_item_e\n\ndef _build_clip_item(clip_item, br_map):\n    clip_item_e = cElementTree.Element('clipitem', frameBlend='FALSE')\n\n    # set the clip name from the media reference if not defined on the clip\n    if clip_item.name is not None:\n        name = clip_item.name\n    else:\n        url_path = _url_to_path(clip_item.media_reference.target_url)\n        name = os.path.basename(url_path)\n\n    _insert_new_sub_element(clip_item_e, 'name',\n                            text=name)\n    clip_item_e.append(_build_file(clip_item.media_reference, br_map))\n    clip_item_e.append(_build_rate(\n        clip_item.media_reference.available_range.start_time))\n    clip_item_e.extend([_build_marker(m) for m in clip_item.markers])\n\n    return clip_item_e"
    },
    {
        "pr_title": "Add contrib directory and initial HLS support",
        "pr_number": 60,
        "file_name": "contrib/adapters/tests/test_hls_playlist_adapter.py",
        "code_diff": "@@ -633,23 +633,25 @@\ndef test_explicit_master_pl_from_mem(self):\n         t.metadata.update(\n             {\n                 'HLS': {\n-                    \"master_playlist\": True\n+                    'master_playlist': True\n                 }\n             }\n         )\n \n         # build a track\n-        track = otio.schema.Sequence(\"v1\")\n+        track = otio.schema.Sequence('v1')\n         track.metadata.update(\n             {\n-                'bandwidth': 123456,\n-                'codec': 'avc.test',\n-                'width': 1920,\n-                'height': 1080,\n-                'frame_rate': 23.976,\n+                'streaming': {\n+                    'bandwidth': 123456,\n+                    'codec': 'avc.test',\n+                    'width': 1920,\n+                    'height': 1080,\n+                    'frame_rate': 23.976,\n+                },\n                 'HLS': {\n-                    \"EXT-X-INDEPENDENT-SEGMENTS\": None,\n-                    \"EXT-X-PLAYLIST-TYPE\": \"VOD\",\n+                    'EXT-X-INDEPENDENT-SEGMENTS': None,\n+                    'EXT-X-PLAYLIST-TYPE': 'VOD',\n                     'uri': 'v1/prog_index.m3u8',\n                     'iframe_uri': 'v1/iframe_index.m3u8'\n                 }",
        "comments": [],
        "commit_message": "Fixed metadata name spacing in unittest",
        "commit_id": "b914352d16b70784e07eb0f9b3524e1144068e63"
    },
    {
        "pr_title": "Add contrib directory and initial HLS support",
        "pr_number": 60,
        "file_name": "contrib/adapters/tests/test_hls_playlist_adapter.py",
        "code_diff": "@@ -661,11 +663,13 @@\ndef test_explicit_master_pl_from_mem(self):\n         segmented_media_ref = otio.media_reference.External(\n             target_url='video1.mp4',\n             metadata={\n-                \"init_byterange\": {\n-                    \"byte_count\": 729,\n-                    \"byte_offset\": 0\n-                },\n-                \"init_uri\": \"media-video-1.mp4\"\n+                'streaming': {\n+                    'init_byterange': {\n+                        'byte_count': 729,\n+                        'byte_offset': 0\n+                    },\n+                    'init_uri': 'media-video-1.mp4'\n+                }\n             }\n         )",
        "comments": [],
        "commit_message": "Fixed metadata name spacing in unittest",
        "commit_id": "b914352d16b70784e07eb0f9b3524e1144068e63"
    },
    {
        "pr_title": "Add contrib directory and initial HLS support",
        "pr_number": 60,
        "file_name": "contrib/adapters/tests/test_hls_playlist_adapter.py",
        "code_diff": "@@ -678,8 +682,10 @@\ndef test_explicit_master_pl_from_mem(self):\n         )\n         media_ref1.metadata.update(\n             {\n-                \"byte_count\": 534220,\n-                \"byte_offset\": 1361\n+                'streaming': {\n+                    'byte_count': 534220,\n+                    'byte_offset': 1361\n+                }\n             }\n         )",
        "comments": [],
        "commit_message": "Fixed metadata name spacing in unittest",
        "commit_id": "b914352d16b70784e07eb0f9b3524e1144068e63"
    },
    {
        "pr_title": "Addressed python3 support. resolves #19",
        "pr_number": 32,
        "file_name": "opentimelineio/adapters/pretty_print_string.py",
        "code_diff": "@@ -1,7 +1,7 @@\n\"\"\"\n Adapter that prints to string.\n \"\"\"\n-from io import StringIO\n+import io\n \n import opentimelineio as otio",
        "comments": [
            {
                "comment": "I would prefer we not do a from import here, I'd rather use an `import io` with `io.StringIO`.  Do you by chance have a unicode-named file that we could test this with?  That would probably be a good idea.\n",
                "position": null
            }
        ],
        "commit_message": "Added python3.5 to README and converted from io import StringIO to import io",
        "commit_id": "0d081c608e78a988dd3f112b01fb9e9ced44abdd",
        "fn_context": ""
    },
    {
        "pr_title": "Add support for finding the range of children.",
        "pr_number": 13,
        "file_name": "opentimelineio/core/composition.py",
        "code_diff": "@@ -151,7 +151,22 @@\ndef _path_to_child(self, child):\n         return parents\n \n     def range_of_child(self, child, reference_space=None):\n-        \"\"\" Return range of the child in reference_space coordinates.  \"\"\"\n+        \"\"\" \n+        Return range of the child in reference_space coordinates, before the\n+        self.source_range.\n+\n+        For example,\n+\n+        |     [-----]     | seq\n+        [-----------------] Clip A\n+\n+        If ClipA has duration 17, and seq has source_range: 5, duration 15,\n+        seq.range_of_child(Clip A) will return (0, 17) \n+        ignoring the source range of seq.\n+\n+        To get the range of the child with the source_range applied, use the\n+        trimmed_range_of_child() method.\n+        \"\"\"\n \n         if not reference_space:\n             reference_space = self",
        "comments": [
            {
                "comment": "Since _parent is a implementation detail in Item, we should make a getter/setter parent property on Item. That can deal with unparenting the old one and reparenting the new one without needing to reach into _children.\n",
                "position": null
            },
            {
                "comment": "Great idea.  What I'll do is add a method to Item: `set_parent` that encodes this logic.\n",
                "position": null
            }
        ],
        "commit_message": "Clean up documentation on [trimmed_]range_of_child.",
        "commit_id": "9a98078b8ac2b4e12d203f661ba03b9b2afc3653",
        "fn_context": "    def _path_to_child(self, child):\n        if not isinstance(child, item.Item):\n            raise TypeError(\n                \"An object child of 'Item' is required, not type '{}'\"\n                \"\".format(type(child))\n            )\n\n        current = child\n        parents = []\n\n        while(current is not self):\n            try:\n                current = current._parent\n            except AttributeError:\n                raise exceptions.NotAChildError(\n                    \"Item '{}' is not a child of '{}'.\"\n                    \"\".format(child, self)\n                )\n\n            parents.append(current)\n\n        return parents\n\n    def range_of_child(self, child, reference_space=None):\n        \"\"\" \n        Return range of the child in reference_space coordinates, before the\n        self.source_range.\n\n        For example,\n\n        |     [-----]     | seq\n        [-----------------] Clip A\n\n        If ClipA has duration 17, and seq has source_range: 5, duration 15,\n        seq.range_of_child(Clip A) will return (0, 17) \n        ignoring the source range of seq.\n\n        To get the range of the child with the source_range applied, use the\n        trimmed_range_of_child() method.\n        \"\"\"\n\n        if not reference_space:\n            reference_space = self\n\n        parents = self._path_to_child(child)\n\n        result_range = child.source_range\n\n        current = child\n        result_range = None\n\n        for parent in parents:\n            index = parent.index(current)\n            parent_range = parent.range_of_child_at_index(index)\n\n            if not result_range:\n                result_range = parent_range\n                current = parent\n                continue\n\n            result_range.start_time = (\n                result_range.start_time +\n                parent_range.start_time\n            )\n            result_range.duration = result_range.duration\n            current = parent\n\n        if reference_space is not self:\n            result_range = self.transformed_time_range(\n                result_range,\n                reference_space\n            )\n\n        return result_range"
    }
]